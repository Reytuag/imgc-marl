{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for generating plots of trained agents\n",
    "\n",
    "This code will load the results for the specified experiments, generate plots, and save them in the `experiments_dir` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'legend.fontsize': 6,\n",
    "                  \"figure.autolayout\": True,\n",
    "                  'font.size': 8,\n",
    "                  'pdf.fonttype':42,\n",
    "                  'ps.fonttype':42}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "cm = 1 / 2.54  # for converting inches to cm\n",
    "fig_size = (10.48 * cm, 6 * cm)  # these dimensions chosen to fit in latex column\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set the directory in which all the experiments to be plotted together are stored.\n",
    "* Set the list of subdirs (experiments) you want to include in the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/media/elena/LaCie/aamas_2023/3_landmarks/classic_reward/2': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!ls /media/elena/LaCie/aamas_2023/3_landmarks/classic_reward/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['centralized', 'naming_game_2_steps', 'naming_game_30msg_t30_a10', 'independent']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "experiments_dir = \"/media/elena/LaCie/aamas_2023/6_landmarks/cooperative/2\"\n",
    "#experiments_dir = \"/media/elena/LaCie/elias_expe/2_agents/cooperative/modified_reward/3_landmarks\"\n",
    "#list_of_experiments = [\"centralized\",\"independent\",\"50align\",\"naming_game_t30_a10\"]\n",
    "list_of_experiments =  [o for o in os.listdir(experiments_dir) if os.path.isdir(experiments_dir + \"/\" + o)]\n",
    "print(list_of_experiments)\n",
    "\n",
    "#list_of_experiments = [\"centralized\",\"independent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-05_14-27-44gdu_himi\n",
      "2022-10-05_14-27-52ure2zpjr\n",
      "2022-10-05_14-27-59vmaz9yqg\n",
      "2022-10-05_14-27-440zxf_z9b\n",
      "2022-10-05_14-27-48l1dt6vqn\n",
      "centralized\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [208], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m ctr\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(experiment_name[:\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp_est\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m experiment_name[:\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0_5di\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m( (\u001b[43mexperiment_name\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5\u001b[39m\u001b[38;5;124m\"\u001b[39m ) \u001b[38;5;129;01mor\u001b[39;00m experiment\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnaming_game\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(experiment_name)\n\u001b[1;32m     22\u001b[0m         r \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for experiment in list_of_experiments:\n",
    "    subdir = os.path.join(experiments_dir, experiment)\n",
    "    eval_reward = pd.DataFrame()\n",
    "    train_reward = pd.DataFrame()\n",
    "    episode_len = pd.DataFrame()\n",
    "    alignment = pd.DataFrame()\n",
    "    train_x = []\n",
    "    eval_x = []\n",
    "    ctr=0\n",
    "    for j, experiment_name in enumerate(os.listdir(subdir)):\n",
    "        \n",
    "        #print(subdir+\"/\"+experiment_name)\n",
    "        if(os.path.isdir(subdir+\"/\"+experiment_name)):\n",
    "          \n",
    "            ctr+=1\n",
    "            if(experiment_name[:5]!=\"p_est\" and experiment_name[:5]!=\"0_5di\"):\n",
    "            \n",
    "                if( (experiment_name[9]==\"5\" ) or experiment!=\"naming_game\"):\n",
    "                    print(experiment_name)\n",
    "\n",
    "                    r = []\n",
    "                    r_t = []\n",
    "                    l = []\n",
    "                    x_ = []\n",
    "                    a = []\n",
    "                    y_ = []\n",
    "                    result_raw = open(os.path.join(subdir, experiment_name, \"result.json\"), \"r\")\n",
    "                    #print(result_raw)\n",
    "\n",
    "                    for result in result_raw:\n",
    "                        dump = json.loads(result)\n",
    "                        \n",
    "                        #if(ctr>5):\n",
    "                            #print(dump.keys())\n",
    "                        a.append(dump[\"custom_metrics\"].get(\"goal_alignment_mean\"))\n",
    "                        y_.append(dump[\"timesteps_total\"])\n",
    "                        r_t.append(dump[\"episode_reward_mean\"])\n",
    "\n",
    "                        metrics = dump.get(\"evaluation\")\n",
    "                        if metrics is not None:\n",
    "                            custom = metrics.get(\"custom_metrics\")\n",
    "                            x_.append(dump[\"timesteps_total\"])\n",
    "                            r.append(metrics[\"episode_reward_mean\"])\n",
    "                            l.append(metrics[\"episode_len_mean\"])\n",
    "\n",
    "                    eval_reward = pd.concat(\n",
    "                        [eval_reward, pd.DataFrame(r)], ignore_index=True, axis=1\n",
    "                    )\n",
    "                    episode_len = pd.concat(\n",
    "                        [episode_len, pd.DataFrame(l)], ignore_index=True, axis=1\n",
    "                    )\n",
    "                    alignment = pd.concat([alignment, pd.DataFrame(a)], ignore_index=True, axis=1)\n",
    "                    train_reward = pd.concat(\n",
    "                        [train_reward, pd.DataFrame(r_t)], ignore_index=True, axis=1\n",
    "                    )\n",
    "\n",
    "\n",
    "                    if len(x_) > len(eval_x):\n",
    "                        eval_x = x_\n",
    "                    if len(y_) > len(train_x):\n",
    "                        train_x = y_\n",
    "    print(experiment)\n",
    "    results[experiment] = {\n",
    "        \"eval_reward\": eval_reward,\n",
    "        \"train_reward\": train_reward,\n",
    "        \"episode_len\": episode_len,\n",
    "        \"alignment\": alignment,\n",
    "        \"train_x\": train_x,\n",
    "        \"eval_x\": eval_x,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=fig_size)\n",
    "\n",
    "slice=1000\n",
    "\n",
    "i = 1\n",
    "n_exp = len(results)\n",
    "for label, result in results.items():\n",
    "    if \"naming_game\" in label:\n",
    "        label_to_print = \"naming_game\"\n",
    "    else:\n",
    "        label_to_print = label\n",
    "\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.plot(result[\"train_x\"][:slice], result[\"train_reward\"][:slice].mean(axis=1), label=label_to_print)\n",
    "    plt.fill_between(\n",
    "        result[\"train_x\"][:slice],\n",
    "        result[\"train_reward\"][:slice].mean(axis=1) - result[\"train_reward\"][:slice].std(axis=1),\n",
    "        result[\"train_reward\"][:slice].mean(axis=1) + result[\"train_reward\"][:slice].std(axis=1),\n",
    "        alpha=0.4,\n",
    "    )\n",
    "    if i == n_exp:\n",
    "        plt.legend()\n",
    "        #plt.grid()\n",
    "        plt.ylabel(\"Training Reward, $R_{train}$\")\n",
    "        plt.xlabel(\"Timestep, \\n $t_{train}$\")\n",
    "\n",
    "    plt.subplot(122)\n",
    "\n",
    "\n",
    "\n",
    "    plt.plot(result[\"train_x\"][:slice], result[\"alignment\"][:slice].mean(axis=1), label=label_to_print)\n",
    "    plt.fill_between(\n",
    "        result[\"train_x\"][:slice],\n",
    "        result[\"alignment\"][:slice].mean(axis=1) - result[\"alignment\"][:slice].std(axis=1),\n",
    "        result[\"alignment\"][:slice].mean(axis=1) + result[\"alignment\"][:slice].std(axis=1),\n",
    "        alpha=0.4,\n",
    "    )\n",
    "    if i == n_exp:\n",
    "        #plt.legend()\n",
    "        #plt.grid()\n",
    "        plt.ylabel(\"Alignment, $A$\")\n",
    "        plt.xlabel(\"Timestep,\\n  $t_{train}$\")\n",
    "\n",
    "    i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "# Uncomment this line for saving the plot\n",
    "plt.savefig(os.path.join(experiments_dir, \"train_plot.png\"), dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=fig_size)\n",
    "slice=100\n",
    "i = 1\n",
    "n_exp = len(results)\n",
    "for label, result in results.items():\n",
    "    if \"naming_game\" in label:\n",
    "        label_to_print = \"naming_game\"\n",
    "    else:\n",
    "        label_to_print = label\n",
    "    ax=plt.subplot(121)\n",
    "    plt.plot(result[\"eval_x\"][:slice], result[\"eval_reward\"][:slice].mean(axis=1), label=label_to_print)\n",
    "    plt.fill_between(\n",
    "        result[\"eval_x\"][:slice],\n",
    "        result[\"eval_reward\"][:slice].mean(axis=1) - result[\"eval_reward\"][:slice].std(axis=1),\n",
    "        result[\"eval_reward\"][:slice].mean(axis=1) + result[\"eval_reward\"][:slice].std(axis=1),\n",
    "        alpha=0.4,\n",
    "    )\n",
    "    if i == n_exp:\n",
    "        plt.legend()\n",
    "        #plt.grid()\n",
    "        plt.xlabel(\"Timestep, $t_{eval}$\")\n",
    "        plt.ylabel(\"Evaluation Reward, $R_{eval}$\")\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(result[\"eval_x\"][:slice], result[\"episode_len\"][:slice].mean(axis=1), label=label_to_print)\n",
    "    plt.fill_between(\n",
    "        result[\"eval_x\"][:slice],\n",
    "        result[\"episode_len\"][:slice].mean(axis=1) - result[\"episode_len\"][:slice].std(axis=1),\n",
    "        result[\"episode_len\"][:slice].mean(axis=1) + result[\"episode_len\"][:slice].std(axis=1),\n",
    "        alpha=0.4,\n",
    "    )\n",
    "    if i == n_exp:\n",
    "        #plt.legend()\n",
    "        #plt.grid()\n",
    "        plt.xlabel(\"Timestep, \\n $t_{eval}$\")\n",
    "        plt.ylabel(\"Episode length, $L$\")\n",
    "    i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "# Uncomment this line for saving the plot\n",
    "plt.savefig(os.path.join(experiments_dir, \"evaluation_plot.png\"), dpi=300, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgc-marl",
   "language": "python",
   "name": "imgc-marl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba8aa1b69f904e3342c40bcdd1b69ae4a3cf2054115c347781ce67fba2dd2c9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
