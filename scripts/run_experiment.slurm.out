custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.08333333333333333
  reward for individual goal_min: 0.0
episode_len_mean: 249.75757575757575
episode_reward_max: 2.0
episode_reward_mean: 0.09090909090909091
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 66
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.030303030303030304
  agent_1: 0.06060606060606061
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.66758370399475
time_total_s: 56.66758370399475
timers:
  learn_throughput: 382.634
  learn_time_ms: 43122.117
  load_throughput: 38130036.364
  load_time_ms: 0.433
  training_iteration_time_ms: 56639.195
  update_time_ms: 3.292
timesteps_total: 16500
training_iteration: 1

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 0.5
  reward for individual goal_mean: 0.08823529411764706
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.09090909090909091
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 66
policy_reward_max:
  agent_0: 1.0
  agent_1: 0.0
policy_reward_mean:
  agent_0: 0.09090909090909091
  agent_1: 0.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.16166090965271
time_total_s: 53.16166090965271
timers:
  learn_throughput: 411.534
  learn_time_ms: 40093.915
  load_throughput: 35472073.808
  load_time_ms: 0.465
  training_iteration_time_ms: 53066.519
  update_time_ms: 3.011
timesteps_total: 16500
training_iteration: 1

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.1
  reward for individual goal_min: 0.0
episode_len_mean: 248.93939393939394
episode_reward_max: 2.0
episode_reward_mean: 0.12121212121212122
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 66
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.07575757575757576
  agent_1: 0.045454545454545456
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.216118812561035
time_total_s: 60.216118812561035
timers:
  learn_throughput: 358.477
  learn_time_ms: 46028.1
  load_throughput: 33304146.295
  load_time_ms: 0.495
  training_iteration_time_ms: 60100.94
  update_time_ms: 3.281
timesteps_total: 16500
training_iteration: 1

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 0.5
  reward for individual goal_mean: 0.03488372093023256
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.045454545454545456
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 66
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.030303030303030304
  agent_1: 0.015151515151515152
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.29164123535156
time_total_s: 58.29164123535156
timers:
  learn_throughput: 373.241
  learn_time_ms: 44207.413
  load_throughput: 35839469.705
  load_time_ms: 0.46
  training_iteration_time_ms: 58247.55
  update_time_ms: 3.171
timesteps_total: 16500
training_iteration: 1

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.049019607843137254
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.045454545454545456
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 66
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.030303030303030304
  agent_1: 0.015151515151515152
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.20048952102661
time_total_s: 59.20048952102661
timers:
  learn_throughput: 365.415
  learn_time_ms: 45154.143
  load_throughput: 37962707.625
  load_time_ms: 0.435
  training_iteration_time_ms: 59173.86
  update_time_ms: 3.552
timesteps_total: 16500
training_iteration: 1

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 0.5
  reward for individual goal_mean: 0.08
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.06060606060606061
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 66
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.045454545454545456
  agent_1: 0.015151515151515152
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 67.17210364341736
time_total_s: 67.17210364341736
timers:
  learn_throughput: 317.858
  learn_time_ms: 51909.97
  load_throughput: 31877483.187
  load_time_ms: 0.518
  training_iteration_time_ms: 67097.999
  update_time_ms: 3.448
timesteps_total: 16500
training_iteration: 1

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 0.5
  reward for individual goal_mean: 0.05952380952380952
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.07575757575757576
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 66
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.06060606060606061
  agent_1: 0.015151515151515152
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.69553661346436
time_total_s: 65.69553661346436
timers:
  learn_throughput: 325.228
  learn_time_ms: 50733.709
  load_throughput: 33224203.553
  load_time_ms: 0.497
  training_iteration_time_ms: 65670.09
  update_time_ms: 3.299
timesteps_total: 16500
training_iteration: 1

custom_metrics:
  reward for collective goal_max: 0.5
  reward for collective goal_mean: 0.01
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.02830188679245283
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.045454545454545456
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 66
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.030303030303030304
  agent_1: 0.015151515151515152
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.82164645195007
time_total_s: 65.82164645195007
timers:
  learn_throughput: 324.759
  learn_time_ms: 50806.944
  load_throughput: 35876628.305
  load_time_ms: 0.46
  training_iteration_time_ms: 65793.337
  update_time_ms: 3.33
timesteps_total: 16500
training_iteration: 1

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.06862745098039216
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.07575757575757576
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 66
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.06060606060606061
  agent_1: 0.015151515151515152
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 66.639564037323
time_total_s: 66.639564037323
timers:
  learn_throughput: 328.783
  learn_time_ms: 50185.035
  load_throughput: 34226516.32
  load_time_ms: 0.482
  training_iteration_time_ms: 66307.598
  update_time_ms: 3.51
timesteps_total: 16500
training_iteration: 1

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.04807692307692308
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.06060606060606061
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 66
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.015151515151515152
  agent_1: 0.045454545454545456
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 69.5828959941864
time_total_s: 69.5828959941864
timers:
  learn_throughput: 328.683
  learn_time_ms: 50200.277
  load_throughput: 35951177.143
  load_time_ms: 0.459
  training_iteration_time_ms: 69553.443
  update_time_ms: 3.265
timesteps_total: 16500
training_iteration: 1

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.07446808510638298
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.07575757575757576
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 66
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.045454545454545456
  agent_1: 0.030303030303030304
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 72.11616325378418
time_total_s: 72.11616325378418
timers:
  learn_throughput: 299.375
  learn_time_ms: 55114.906
  load_throughput: 33049673.352
  load_time_ms: 0.499
  training_iteration_time_ms: 71652.516
  update_time_ms: 3.543
timesteps_total: 16500
training_iteration: 1

custom_metrics:
  reward for collective goal_max: 0.5
  reward for collective goal_mean: 0.01
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.02040816326530612
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.030303030303030304
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 66
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.015151515151515152
  agent_1: 0.015151515151515152
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.3171558380127
time_total_s: 65.3171558380127
timers:
  learn_throughput: 349.527
  learn_time_ms: 47206.711
  load_throughput: 36462600.632
  load_time_ms: 0.453
  training_iteration_time_ms: 65272.79
  update_time_ms: 3.31
timesteps_total: 16500
training_iteration: 1

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.049019607843137254
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.06060606060606061
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 66
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.030303030303030304
  agent_1: 0.030303030303030304
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.22835278511047
time_total_s: 65.22835278511047
timers:
  learn_throughput: 348.531
  learn_time_ms: 47341.514
  load_throughput: 35058772.036
  load_time_ms: 0.471
  training_iteration_time_ms: 65200.402
  update_time_ms: 3.4
timesteps_total: 16500
training_iteration: 1

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.020833333333333332
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.015151515151515152
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 66
policy_reward_max:
  agent_0: 1.0
  agent_1: 0.0
policy_reward_mean:
  agent_0: 0.015151515151515152
  agent_1: 0.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 73.31261658668518
time_total_s: 73.31261658668518
timers:
  learn_throughput: 290.532
  learn_time_ms: 56792.411
  load_throughput: 35345258.427
  load_time_ms: 0.467
  training_iteration_time_ms: 73282.958
  update_time_ms: 3.515
timesteps_total: 16500
training_iteration: 1

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.08450704225352113
  reward for individual goal_min: 0.0
episode_len_mean: 249.84
episode_reward_max: 2.0
episode_reward_mean: 0.09
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 132
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.06
  agent_1: 0.03
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.70937180519104
time_total_s: 112.37695550918579
timers:
  learn_throughput: 383.806
  learn_time_ms: 42990.484
  load_throughput: 9161505.957
  load_time_ms: 1.801
  training_iteration_time_ms: 56156.79
  update_time_ms: 2.892
timesteps_total: 33000
training_iteration: 2

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 0.5
  reward for individual goal_mean: 0.06
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.06
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 132
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.03
  agent_1: 0.03
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.56049680709839
time_total_s: 106.7221577167511
timers:
  learn_throughput: 406.362
  learn_time_ms: 40604.152
  load_throughput: 9924143.687
  load_time_ms: 1.663
  training_iteration_time_ms: 53296.066
  update_time_ms: 2.778
timesteps_total: 33000
training_iteration: 2

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.023809523809523808
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.11206896551724138
  reward for individual goal_min: 0.0
episode_len_mean: 247.79
episode_reward_max: 2.0
episode_reward_mean: 0.15
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 132
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.05
  agent_1: 0.1
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.08356523513794
time_total_s: 119.29968404769897
timers:
  learn_throughput: 358.908
  learn_time_ms: 45972.789
  load_throughput: 10233791.645
  load_time_ms: 1.612
  training_iteration_time_ms: 59574.728
  update_time_ms: 2.887
timesteps_total: 33000
training_iteration: 2

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.06756756756756757
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.07
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 132
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.03
  agent_1: 0.04
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.83265233039856
time_total_s: 116.12429356575012
timers:
  learn_throughput: 371.651
  learn_time_ms: 44396.464
  load_throughput: 8957548.02
  load_time_ms: 1.842
  training_iteration_time_ms: 58022.176
  update_time_ms: 2.881
timesteps_total: 33000
training_iteration: 2

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.05194805194805195
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.05
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 132
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.03
  agent_1: 0.02
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.712329149246216
time_total_s: 117.91281867027283
timers:
  learn_throughput: 365.142
  learn_time_ms: 45187.853
  load_throughput: 9959133.113
  load_time_ms: 1.657
  training_iteration_time_ms: 58924.874
  update_time_ms: 3.06
timesteps_total: 33000
training_iteration: 2

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 0.5
  reward for individual goal_mean: 0.041666666666666664
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.04
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 132
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.01
  agent_1: 0.03
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 67.15035200119019
time_total_s: 134.32245564460754
timers:
  learn_throughput: 316.329
  learn_time_ms: 52160.957
  load_throughput: 10435952.047
  load_time_ms: 1.581
  training_iteration_time_ms: 67107.165
  update_time_ms: 3.11
timesteps_total: 33000
training_iteration: 2

custom_metrics:
  reward for collective goal_max: 0.5
  reward for collective goal_mean: 0.012987012987012988
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.0759493670886076
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.09
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 132
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.05
  agent_1: 0.04
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.2411437034607
time_total_s: 131.06279015541077
timers:
  learn_throughput: 325.275
  learn_time_ms: 50726.349
  load_throughput: 8798120.519
  load_time_ms: 1.875
  training_iteration_time_ms: 65498.797
  update_time_ms: 2.961
timesteps_total: 33000
training_iteration: 2

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 0.5
  reward for individual goal_mean: 0.06779661016949153
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.08
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 132
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.06
  agent_1: 0.02
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.71596646308899
time_total_s: 131.41150307655334
timers:
  learn_throughput: 323.327
  learn_time_ms: 51031.953
  load_throughput: 8893088.666
  load_time_ms: 1.855
  training_iteration_time_ms: 65675.568
  update_time_ms: 2.905
timesteps_total: 33000
training_iteration: 2

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.06493506493506493
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.08
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 132
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.06
  agent_1: 0.02
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.55601143836975
time_total_s: 132.19557547569275
timers:
  learn_throughput: 329.203
  learn_time_ms: 50121.054
  load_throughput: 9107852.339
  load_time_ms: 1.812
  training_iteration_time_ms: 65912.311
  update_time_ms: 3.224
timesteps_total: 33000
training_iteration: 2

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.046052631578947366
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.04
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 132
policy_reward_max:
  agent_0: 0.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.0
  agent_1: 0.04
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.34152698516846
time_total_s: 134.92442297935486
timers:
  learn_throughput: 326.935
  learn_time_ms: 50468.701
  load_throughput: 9303134.292
  load_time_ms: 1.774
  training_iteration_time_ms: 67429.238
  update_time_ms: 3.009
timesteps_total: 33000
training_iteration: 2

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.06164383561643835
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.06
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 132
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.02
  agent_1: 0.04
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.453853130340576
time_total_s: 126.77100896835327
timers:
  learn_throughput: 348.796
  learn_time_ms: 47305.549
  load_throughput: 8581031.122
  load_time_ms: 1.923
  training_iteration_time_ms: 63344.771
  update_time_ms: 2.991
timesteps_total: 33000
training_iteration: 2

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.08571428571428572
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.07
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 132
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.06
  agent_1: 0.01
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.617342710494995
time_total_s: 126.84569549560547
timers:
  learn_throughput: 347.476
  learn_time_ms: 47485.251
  load_throughput: 8951175.839
  load_time_ms: 1.843
  training_iteration_time_ms: 63390.949
  update_time_ms: 3.109
timesteps_total: 33000
training_iteration: 2

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.024691358024691357
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.16666666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 249.49
episode_reward_max: 2.0
episode_reward_mean: 0.17
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 198
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.1
  agent_1: 0.07
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.50273132324219
time_total_s: 167.87968683242798
timers:
  learn_throughput: 384.822
  learn_time_ms: 42876.953
  load_throughput: 7760552.013
  load_time_ms: 2.126
  training_iteration_time_ms: 55926.581
  update_time_ms: 2.783
timesteps_total: 49500
training_iteration: 3

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.021739130434782608
  reward for collective goal_min: 0.0
  reward for individual goal_max: 0.5
  reward for individual goal_mean: 0.1111111111111111
  reward for individual goal_min: 0.0
episode_len_mean: 249.83
episode_reward_max: 2.0
episode_reward_mean: 0.14
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 198
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.02
  agent_1: 0.12
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.08813714981079
time_total_s: 158.8102948665619
timers:
  learn_throughput: 409.952
  learn_time_ms: 40248.649
  load_throughput: 7633858.44
  load_time_ms: 2.161
  training_iteration_time_ms: 52882.299
  update_time_ms: 2.625
timesteps_total: 49500
training_iteration: 3

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.08571428571428572
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.08
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 132
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.05
  agent_1: 0.03
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 72.33215308189392
time_total_s: 144.4483163356781
timers:
  learn_throughput: 295.438
  learn_time_ms: 55849.265
  load_throughput: 8796443.089
  load_time_ms: 1.876
  training_iteration_time_ms: 71974.647
  update_time_ms: 3.229
timesteps_total: 33000
training_iteration: 2

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.04
  reward for individual goal_min: 0.0
episode_len_mean: 248.95
episode_reward_max: 2.0
episode_reward_mean: 0.04
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 132
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.02
  agent_1: 0.02
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 73.3900556564331
time_total_s: 146.7026722431183
timers:
  learn_throughput: 288.759
  learn_time_ms: 57141.167
  load_throughput: 8086231.933
  load_time_ms: 2.041
  training_iteration_time_ms: 73317.253
  update_time_ms: 3.196
timesteps_total: 33000
training_iteration: 2

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.04430379746835443
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.06
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 198
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.02
  agent_1: 0.04
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.19361877441406
time_total_s: 174.31791234016418
timers:
  learn_throughput: 370.81
  learn_time_ms: 44497.179
  load_throughput: 7532217.675
  load_time_ms: 2.191
  training_iteration_time_ms: 58067.784
  update_time_ms: 2.753
timesteps_total: 49500
training_iteration: 3

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 0.5
  reward for individual goal_mean: 0.14035087719298245
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.16
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 198
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.08
  agent_1: 0.08
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.229092836380005
time_total_s: 180.52877688407898
timers:
  learn_throughput: 355.464
  learn_time_ms: 46418.192
  load_throughput: 8383188.565
  load_time_ms: 1.968
  training_iteration_time_ms: 60028.054
  update_time_ms: 2.791
timesteps_total: 49500
training_iteration: 3

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.03896103896103896
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.04
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 198
policy_reward_max:
  agent_0: 1.0
  agent_1: 0.0
policy_reward_mean:
  agent_0: 0.04
  agent_1: 0.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.18321180343628
time_total_s: 177.0960304737091
timers:
  learn_throughput: 363.738
  learn_time_ms: 45362.356
  load_throughput: 7558267.429
  load_time_ms: 2.183
  training_iteration_time_ms: 58998.484
  update_time_ms: 2.916
timesteps_total: 49500
training_iteration: 3

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.14788732394366197
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.12
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 198
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.04
  agent_1: 0.08
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.9895339012146
time_total_s: 194.05232405662537
timers:
  learn_throughput: 329.799
  learn_time_ms: 50030.452
  load_throughput: 7004657.49
  load_time_ms: 2.356
  training_iteration_time_ms: 64650.75
  update_time_ms: 2.867
timesteps_total: 49500
training_iteration: 3

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 0.5
  reward for individual goal_mean: 0.12244897959183673
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.12
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 198
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.07
  agent_1: 0.05
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 66.70721364021301
time_total_s: 201.02966928482056
timers:
  learn_throughput: 317.015
  learn_time_ms: 52048.063
  load_throughput: 7461028.785
  load_time_ms: 2.211
  training_iteration_time_ms: 66962.751
  update_time_ms: 2.991
timesteps_total: 49500
training_iteration: 3

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 0.5
  reward for individual goal_mean: 0.09615384615384616
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.1
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 198
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.06
  agent_1: 0.04
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.2269811630249
time_total_s: 196.63848423957825
timers:
  learn_throughput: 324.238
  learn_time_ms: 50888.476
  load_throughput: 7070736.914
  load_time_ms: 2.334
  training_iteration_time_ms: 65514.415
  update_time_ms: 2.856
timesteps_total: 49500
training_iteration: 3

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.013333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.14666666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.17
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 198
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.07
  agent_1: 0.1
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 66.09845209121704
time_total_s: 198.2940275669098
timers:
  learn_throughput: 327.112
  learn_time_ms: 50441.453
  load_throughput: 7068089.058
  load_time_ms: 2.334
  training_iteration_time_ms: 65961.319
  update_time_ms: 3.046
timesteps_total: 49500
training_iteration: 3

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.09210526315789473
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.08
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 198
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.03
  agent_1: 0.05
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.75317716598511
time_total_s: 198.67760014533997
timers:
  learn_throughput: 329.068
  learn_time_ms: 50141.576
  load_throughput: 7343332.791
  load_time_ms: 2.247
  training_iteration_time_ms: 66191.367
  update_time_ms: 2.989
timesteps_total: 49500
training_iteration: 3

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.044444444444444446
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.13636363636363635
  reward for individual goal_min: 0.0
episode_len_mean: 248.15
episode_reward_max: 2.0
episode_reward_mean: 0.19
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 264
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.06
  agent_1: 0.13
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.04323410987854
time_total_s: 210.85352897644043
timers:
  learn_throughput: 411.643
  learn_time_ms: 40083.307
  load_throughput: 6918007.347
  load_time_ms: 2.385
  training_iteration_time_ms: 52664.322
  update_time_ms: 2.582
timesteps_total: 66000
training_iteration: 4

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.013157894736842105
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.1780821917808219
  reward for individual goal_min: 0.0
episode_len_mean: 248.26
episode_reward_max: 2.0
episode_reward_mean: 0.2
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 264
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.09
  agent_1: 0.11
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.93672251701355
time_total_s: 221.81640934944153
timers:
  learn_throughput: 388.558
  learn_time_ms: 42464.655
  load_throughput: 7080804.809
  load_time_ms: 2.33
  training_iteration_time_ms: 55420.438
  update_time_ms: 2.708
timesteps_total: 66000
training_iteration: 4

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.04375
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.04
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 198
policy_reward_max:
  agent_0: 0.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.0
  agent_1: 0.04
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.27696466445923
time_total_s: 190.1226601600647
timers:
  learn_throughput: 344.03
  learn_time_ms: 47960.968
  load_throughput: 7523210.784
  load_time_ms: 2.193
  training_iteration_time_ms: 63341.236
  update_time_ms: 2.936
timesteps_total: 49500
training_iteration: 3

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.0547945205479452
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.05
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 198
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.02
  agent_1: 0.03
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.588305711746216
time_total_s: 190.3593146800995
timers:
  learn_throughput: 344.146
  learn_time_ms: 47944.817
  load_throughput: 7256581.315
  load_time_ms: 2.274
  training_iteration_time_ms: 63412.941
  update_time_ms: 2.916
timesteps_total: 49500
training_iteration: 3

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.015151515151515152
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.08227848101265822
  reward for individual goal_min: 0.0
episode_len_mean: 249.08
episode_reward_max: 2.0
episode_reward_mean: 0.11
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 198
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.04
  agent_1: 0.07
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 71.40052437782288
time_total_s: 215.84884071350098
timers:
  learn_throughput: 295.256
  learn_time_ms: 55883.796
  load_throughput: 7090296.018
  load_time_ms: 2.327
  training_iteration_time_ms: 71770.289
  update_time_ms: 3.054
timesteps_total: 49500
training_iteration: 3

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.11038961038961038
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.12
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 264
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.05
  agent_1: 0.07
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.13878870010376
time_total_s: 231.45670104026794
timers:
  learn_throughput: 372.033
  learn_time_ms: 44350.868
  load_throughput: 6828249.525
  load_time_ms: 2.416
  training_iteration_time_ms: 57826.444
  update_time_ms: 2.713
timesteps_total: 66000
training_iteration: 4

custom_metrics:
  reward for collective goal_max: 0.5
  reward for collective goal_mean: 0.00625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.07462686567164178
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.09
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 264
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.07
  agent_1: 0.02
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.319451332092285
time_total_s: 235.4154818058014
timers:
  learn_throughput: 364.502
  learn_time_ms: 45267.289
  load_throughput: 6732921.416
  load_time_ms: 2.451
  training_iteration_time_ms: 58819.651
  update_time_ms: 2.851
timesteps_total: 66000
training_iteration: 4

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.09615384615384616
  reward for individual goal_min: 0.0
episode_len_mean: 248.95
episode_reward_max: 2.0
episode_reward_mean: 0.1
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 198
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.05
  agent_1: 0.05
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 71.55706691741943
time_total_s: 218.25973916053772
timers:
  learn_throughput: 290.914
  learn_time_ms: 56717.779
  load_throughput: 6861818.687
  load_time_ms: 2.405
  training_iteration_time_ms: 72717.814
  update_time_ms: 3.06
timesteps_total: 49500
training_iteration: 3

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.023809523809523808
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.1724137931034483
  reward for individual goal_min: 0.0
episode_len_mean: 248.47
episode_reward_max: 2.0
episode_reward_mean: 0.22
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 264
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.12
  agent_1: 0.1
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.37955832481384
time_total_s: 240.90833520889282
timers:
  learn_throughput: 354.744
  learn_time_ms: 46512.401
  load_throughput: 6847504.49
  load_time_ms: 2.41
  training_iteration_time_ms: 60107.342
  update_time_ms: 2.865
timesteps_total: 66000
training_iteration: 4

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.018518518518518517
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.17391304347826086
  reward for individual goal_min: 0.0
episode_len_mean: 248.32
episode_reward_max: 2.0
episode_reward_mean: 0.18
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 330
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.07
  agent_1: 0.11
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.10552167892456
time_total_s: 262.959050655365
timers:
  learn_throughput: 412.55
  learn_time_ms: 39995.123
  load_throughput: 6469182.075
  load_time_ms: 2.551
  training_iteration_time_ms: 52545.86
  update_time_ms: 2.548
timesteps_total: 82500
training_iteration: 5

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.16875
  reward for individual goal_min: 0.0
episode_len_mean: 249.93
episode_reward_max: 2.0
episode_reward_mean: 0.17
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 264
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.03
  agent_1: 0.14
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 64.56188464164734
time_total_s: 258.6142086982727
timers:
  learn_throughput: 329.992
  learn_time_ms: 50001.227
  load_throughput: 6378875.59
  load_time_ms: 2.587
  training_iteration_time_ms: 64619.816
  update_time_ms: 2.904
timesteps_total: 66000
training_iteration: 4

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.02
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.20512820512820512
  reward for individual goal_min: 0.0
episode_len_mean: 247.89
episode_reward_max: 2.0
episode_reward_mean: 0.25
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 330
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.09
  agent_1: 0.16
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.637914180755615
time_total_s: 276.45432353019714
timers:
  learn_throughput: 389.597
  learn_time_ms: 42351.45
  load_throughput: 6722685.732
  load_time_ms: 2.454
  training_iteration_time_ms: 55256.622
  update_time_ms: 2.705
timesteps_total: 82500
training_iteration: 5

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 0.5
  reward for individual goal_mean: 0.13953488372093023
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.12
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 264
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.09
  agent_1: 0.03
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 64.95227837562561
time_total_s: 261.59076261520386
timers:
  learn_throughput: 325.214
  learn_time_ms: 50735.813
  load_throughput: 6307798.934
  load_time_ms: 2.616
  training_iteration_time_ms: 65365.22
  update_time_ms: 2.904
timesteps_total: 66000
training_iteration: 4

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.013888888888888888
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.16666666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 248.23
episode_reward_max: 2.0
episode_reward_mean: 0.18
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 264
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.08
  agent_1: 0.1
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.6888473033905
time_total_s: 261.36644744873047
timers:
  learn_throughput: 331.703
  learn_time_ms: 49743.241
  load_throughput: 6507535.767
  load_time_ms: 2.536
  training_iteration_time_ms: 65306.385
  update_time_ms: 2.903
timesteps_total: 66000
training_iteration: 4

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.0196078431372549
  reward for collective goal_min: 0.0
  reward for individual goal_max: 0.5
  reward for individual goal_mean: 0.15306122448979592
  reward for individual goal_min: 0.0
episode_len_mean: 249.01
episode_reward_max: 2.0
episode_reward_mean: 0.17
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 264
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.11
  agent_1: 0.06
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 69.77413964271545
time_total_s: 270.803808927536
timers:
  learn_throughput: 313.321
  learn_time_ms: 52661.728
  load_throughput: 6436122.48
  load_time_ms: 2.564
  training_iteration_time_ms: 67656.926
  update_time_ms: 2.907
timesteps_total: 66000
training_iteration: 4

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.02857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.1962025316455696
  reward for individual goal_min: 0.0
episode_len_mean: 245.75
episode_reward_max: 2.0
episode_reward_mean: 0.24
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 264
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.16
  agent_1: 0.08
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.85665535926819
time_total_s: 264.150682926178
timers:
  learn_throughput: 327.085
  learn_time_ms: 50445.656
  load_throughput: 6417025.522
  load_time_ms: 2.571
  training_iteration_time_ms: 65924.632
  update_time_ms: 2.936
timesteps_total: 66000
training_iteration: 4

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.0875
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.09
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 264
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.03
  agent_1: 0.06
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.48819398880005
time_total_s: 250.61085414886475
timers:
  learn_throughput: 346.331
  learn_time_ms: 47642.296
  load_throughput: 6839383.916
  load_time_ms: 2.412
  training_iteration_time_ms: 62618.674
  update_time_ms: 2.82
timesteps_total: 66000
training_iteration: 4

custom_metrics:
  reward for collective goal_max: 0.5
  reward for collective goal_mean: 0.006493506493506494
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.11486486486486487
  reward for individual goal_min: 0.0
episode_len_mean: 249.48
episode_reward_max: 2.0
episode_reward_mean: 0.13
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 264
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.07
  agent_1: 0.06
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.552690267562866
time_total_s: 250.91200494766235
timers:
  learn_throughput: 346.349
  learn_time_ms: 47639.788
  load_throughput: 6565724.207
  load_time_ms: 2.513
  training_iteration_time_ms: 62682.526
  update_time_ms: 2.827
timesteps_total: 66000
training_iteration: 4

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.013888888888888888
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.22151898734177214
  reward for individual goal_min: 0.0
episode_len_mean: 248.95
episode_reward_max: 2.0
episode_reward_mean: 0.22
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 330
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.07
  agent_1: 0.15
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.41450238227844
time_total_s: 288.8712034225464
timers:
  learn_throughput: 372.524
  learn_time_ms: 44292.49
  load_throughput: 6512036.434
  load_time_ms: 2.534
  training_iteration_time_ms: 57736.749
  update_time_ms: 2.715
timesteps_total: 82500
training_iteration: 5

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.12666666666666668
  reward for individual goal_min: 0.0
episode_len_mean: 249.88
episode_reward_max: 2.0
episode_reward_mean: 0.15
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 330
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.07
  agent_1: 0.08
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.927791118621826
time_total_s: 292.3432729244232
timers:
  learn_throughput: 366.924
  learn_time_ms: 44968.427
  load_throughput: 6219088.426
  load_time_ms: 2.653
  training_iteration_time_ms: 58433.934
  update_time_ms: 2.844
timesteps_total: 82500
training_iteration: 5

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.041666666666666664
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.23076923076923078
  reward for individual goal_min: 0.0
episode_len_mean: 244.67
episode_reward_max: 2.0
episode_reward_mean: 0.28
episode_reward_min: 0.0
episodes_this_iter: 68
episodes_total: 332
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.13
  agent_1: 0.15
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.24171471595764
time_total_s: 301.15004992485046
timers:
  learn_throughput: 354.6
  learn_time_ms: 46531.3
  load_throughput: 6471601.863
  load_time_ms: 2.55
  training_iteration_time_ms: 60127.143
  update_time_ms: 2.789
timesteps_total: 82500
training_iteration: 5

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.02666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.11486486486486487
  reward for individual goal_min: 0.0
episode_len_mean: 249.25
episode_reward_max: 2.0
episode_reward_mean: 0.16
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 264
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.09
  agent_1: 0.07
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 72.17198729515076
time_total_s: 288.02082800865173
timers:
  learn_throughput: 295.038
  learn_time_ms: 55924.922
  load_throughput: 6146728.484
  load_time_ms: 2.684
  training_iteration_time_ms: 71861.443
  update_time_ms: 3.04
timesteps_total: 66000
training_iteration: 4

custom_metrics:
  reward for collective goal_max: 0.5
  reward for collective goal_mean: 0.00684931506849315
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.057692307692307696
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.09
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 264
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.03
  agent_1: 0.06
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 72.13721013069153
time_total_s: 290.39694929122925
timers:
  learn_throughput: 291.454
  learn_time_ms: 56612.642
  load_throughput: 6122123.626
  load_time_ms: 2.695
  training_iteration_time_ms: 72562.828
  update_time_ms: 2.951
timesteps_total: 66000
training_iteration: 4

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.034482758620689655
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.27380952380952384
  reward for individual goal_min: 0.0
episode_len_mean: 244.42
episode_reward_max: 2.0
episode_reward_mean: 0.27
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 397
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.15
  agent_1: 0.12
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.48037314414978
time_total_s: 315.43942379951477
timers:
  learn_throughput: 412.618
  learn_time_ms: 39988.608
  load_throughput: 6283934.321
  load_time_ms: 2.626
  training_iteration_time_ms: 52529.619
  update_time_ms: 2.55
timesteps_total: 99000
training_iteration: 6

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.013157894736842105
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.23076923076923078
  reward for individual goal_min: 0.0
episode_len_mean: 248.6
episode_reward_max: 2.0
episode_reward_mean: 0.24
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 397
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.1
  agent_1: 0.14
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.78979563713074
time_total_s: 331.2441191673279
timers:
  learn_throughput: 390.263
  learn_time_ms: 42279.133
  load_throughput: 6446864.507
  load_time_ms: 2.559
  training_iteration_time_ms: 55172.703
  update_time_ms: 2.688
timesteps_total: 99000
training_iteration: 6

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.012658227848101266
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.14814814814814814
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.15
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 330
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.02
  agent_1: 0.13
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.276336431503296
time_total_s: 321.890545129776
timers:
  learn_throughput: 331.616
  learn_time_ms: 49756.287
  load_throughput: 5941858.644
  load_time_ms: 2.777
  training_iteration_time_ms: 64343.651
  update_time_ms: 2.896
timesteps_total: 82500
training_iteration: 5

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.1518987341772152
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.16
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 330
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.1
  agent_1: 0.06
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.23236870765686
time_total_s: 324.59881615638733
timers:
  learn_throughput: 332.904
  learn_time_ms: 49563.855
  load_throughput: 6138985.913
  load_time_ms: 2.688
  training_iteration_time_ms: 64884.437
  update_time_ms: 2.861
timesteps_total: 82500
training_iteration: 5

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.14444444444444443
  reward for individual goal_min: 0.0
episode_len_mean: 249.45
episode_reward_max: 2.0
episode_reward_mean: 0.13
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 330
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.08
  agent_1: 0.05
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 64.52974009513855
time_total_s: 326.1205027103424
timers:
  learn_throughput: 325.923
  learn_time_ms: 50625.523
  load_throughput: 5901928.705
  load_time_ms: 2.796
  training_iteration_time_ms: 65191.417
  update_time_ms: 2.864
timesteps_total: 82500
training_iteration: 5

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.0273972602739726
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.26973684210526316
  reward for individual goal_min: 0.0
episode_len_mean: 243.92
episode_reward_max: 2.0
episode_reward_mean: 0.29
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 330
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.16
  agent_1: 0.13
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.77744913101196
time_total_s: 329.92813205718994
timers:
  learn_throughput: 326.651
  learn_time_ms: 50512.559
  load_throughput: 6013940.005
  load_time_ms: 2.744
  training_iteration_time_ms: 65887.784
  update_time_ms: 2.874
timesteps_total: 82500
training_iteration: 5

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.028169014084507043
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.17763157894736842
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.2
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 330
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.13
  agent_1: 0.07
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.7039110660553
time_total_s: 312.31476521492004
timers:
  learn_throughput: 346.522
  learn_time_ms: 47616.023
  load_throughput: 6310962.612
  load_time_ms: 2.614
  training_iteration_time_ms: 62428.364
  update_time_ms: 2.832
timesteps_total: 82500
training_iteration: 5

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.20408163265306123
  reward for individual goal_min: 0.0
episode_len_mean: 249.14
episode_reward_max: 2.0
episode_reward_mean: 0.2
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 330
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.11
  agent_1: 0.09
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 69.0526750087738
time_total_s: 339.8564839363098
timers:
  learn_throughput: 312.229
  learn_time_ms: 52845.826
  load_throughput: 5866507.527
  load_time_ms: 2.813
  training_iteration_time_ms: 67929.235
  update_time_ms: 2.939
timesteps_total: 82500
training_iteration: 5

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.11392405063291139
  reward for individual goal_min: 0.0
episode_len_mean: 249.48
episode_reward_max: 2.0
episode_reward_mean: 0.14
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 330
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.09
  agent_1: 0.05
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.44495105743408
time_total_s: 314.35695600509644
timers:
  learn_throughput: 343.983
  learn_time_ms: 47967.472
  load_throughput: 6134958.779
  load_time_ms: 2.69
  training_iteration_time_ms: 62827.406
  update_time_ms: 2.767
timesteps_total: 82500
training_iteration: 5

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.2323943661971831
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.22
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 396
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.06
  agent_1: 0.16
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.8213427066803
time_total_s: 344.6925461292267
timers:
  learn_throughput: 374.734
  learn_time_ms: 44031.213
  load_throughput: 6155202.206
  load_time_ms: 2.681
  training_iteration_time_ms: 57411.435
  update_time_ms: 2.722
timesteps_total: 99000
training_iteration: 6

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.18666666666666668
  reward for individual goal_min: 0.0
episode_len_mean: 244.94
episode_reward_max: 2.0
episode_reward_mean: 0.23
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 396
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.13
  agent_1: 0.1
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.6094069480896
time_total_s: 348.9526798725128
timers:
  learn_throughput: 368.853
  learn_time_ms: 44733.288
  load_throughput: 6015299.087
  load_time_ms: 2.743
  training_iteration_time_ms: 58123.163
  update_time_ms: 2.828
timesteps_total: 99000
training_iteration: 6

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.023809523809523808
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.25
  reward for individual goal_min: 0.0
episode_len_mean: 242.29
episode_reward_max: 2.0
episode_reward_mean: 0.31
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 398
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.15
  agent_1: 0.16
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.31130766868591
time_total_s: 361.4613575935364
timers:
  learn_throughput: 354.238
  learn_time_ms: 46578.801
  load_throughput: 6142818.409
  load_time_ms: 2.686
  training_iteration_time_ms: 60151.883
  update_time_ms: 2.772
timesteps_total: 99000
training_iteration: 6

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.11538461538461539
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.2708333333333333
  reward for individual goal_min: 0.0
episode_len_mean: 243.08
episode_reward_max: 2.0
episode_reward_mean: 0.38
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 464
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.2
  agent_1: 0.18
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.04999923706055
time_total_s: 367.4894230365753
timers:
  learn_throughput: 413.092
  learn_time_ms: 39942.641
  load_throughput: 6085650.369
  load_time_ms: 2.711
  training_iteration_time_ms: 52456.353
  update_time_ms: 2.552
timesteps_total: 115500
training_iteration: 7

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.024691358024691357
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.24666666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 248.35
episode_reward_max: 2.0
episode_reward_mean: 0.25
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 463
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.17
  agent_1: 0.08
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.073097944259644
time_total_s: 385.3172171115875
timers:
  learn_throughput: 391.346
  learn_time_ms: 42162.204
  load_throughput: 6226202.167
  load_time_ms: 2.65
  training_iteration_time_ms: 55010.645
  update_time_ms: 2.689
timesteps_total: 115500
training_iteration: 7

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.02531645569620253
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.1956521739130435
  reward for individual goal_min: 0.0
episode_len_mean: 248.21
episode_reward_max: 2.0
episode_reward_mean: 0.21
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 330
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.15
  agent_1: 0.06
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 70.45336174964905
time_total_s: 358.4741897583008
timers:
  learn_throughput: 296.289
  learn_time_ms: 55688.791
  load_throughput: 5755178.046
  load_time_ms: 2.867
  training_iteration_time_ms: 71571.993
  update_time_ms: 3.028
timesteps_total: 82500
training_iteration: 5

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.09420289855072464
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.07
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 330
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.04
  agent_1: 0.03
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 71.66938638687134
time_total_s: 362.0663356781006
timers:
  learn_throughput: 292.426
  learn_time_ms: 56424.577
  load_throughput: 5720545.554
  load_time_ms: 2.884
  training_iteration_time_ms: 72376.175
  update_time_ms: 2.924
timesteps_total: 82500
training_iteration: 5

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.018072289156626505
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.25675675675675674
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.24
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 396
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.07
  agent_1: 0.17
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.40925908088684
time_total_s: 384.29980421066284
timers:
  learn_throughput: 333.228
  learn_time_ms: 49515.63
  load_throughput: 5690270.319
  load_time_ms: 2.9
  training_iteration_time_ms: 64015.406
  update_time_ms: 2.837
timesteps_total: 99000
training_iteration: 6

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.176056338028169
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.17
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 396
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.14
  agent_1: 0.03
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.63990879058838
time_total_s: 386.2387249469757
timers:
  learn_throughput: 334.965
  learn_time_ms: 49258.931
  load_throughput: 5825258.775
  load_time_ms: 2.832
  training_iteration_time_ms: 64337.4
  update_time_ms: 2.816
timesteps_total: 99000
training_iteration: 6

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.022222222222222223
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.2909090909090909
  reward for individual goal_min: 0.0
episode_len_mean: 243.54
episode_reward_max: 2.0
episode_reward_mean: 0.34
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 397
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.19
  agent_1: 0.15
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.78419780731201
time_total_s: 389.9047005176544
timers:
  learn_throughput: 326.829
  learn_time_ms: 50485.122
  load_throughput: 5592781.952
  load_time_ms: 2.95
  training_iteration_time_ms: 64951.264
  update_time_ms: 2.876
timesteps_total: 99000
training_iteration: 6

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.0472972972972973
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.31690140845070425
  reward for individual goal_min: 0.0
episode_len_mean: 245.93
episode_reward_max: 2.0
episode_reward_mean: 0.36
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 396
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.22
  agent_1: 0.14
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.41707229614258
time_total_s: 392.3452043533325
timers:
  learn_throughput: 329.289
  learn_time_ms: 50107.934
  load_throughput: 5820767.568
  load_time_ms: 2.835
  training_iteration_time_ms: 65303.252
  update_time_ms: 2.836
timesteps_total: 99000
training_iteration: 6

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.06097560975609756
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.13333333333333333
  reward for individual goal_min: 0.0
episode_len_mean: 248.58
episode_reward_max: 2.0
episode_reward_mean: 0.2
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 396
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.09
  agent_1: 0.11
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.53751993179321
time_total_s: 375.85228514671326
timers:
  learn_throughput: 344.671
  learn_time_ms: 47871.765
  load_throughput: 5927625.53
  load_time_ms: 2.784
  training_iteration_time_ms: 62607.134
  update_time_ms: 2.789
timesteps_total: 99000
training_iteration: 6

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.19375
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.2
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 396
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.11
  agent_1: 0.09
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.742618560791016
time_total_s: 376.09957456588745
timers:
  learn_throughput: 344.697
  learn_time_ms: 47868.134
  load_throughput: 5836640.232
  load_time_ms: 2.827
  training_iteration_time_ms: 62640.291
  update_time_ms: 2.716
timesteps_total: 99000
training_iteration: 6

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.02
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.30625
  reward for individual goal_min: 0.0
episode_len_mean: 247.66
episode_reward_max: 2.0
episode_reward_mean: 0.35
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 463
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.12
  agent_1: 0.23
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.72388792037964
time_total_s: 398.4164340496063
timers:
  learn_throughput: 378.898
  learn_time_ms: 43547.396
  load_throughput: 5938318.832
  load_time_ms: 2.779
  training_iteration_time_ms: 56879.529
  update_time_ms: 2.733
timesteps_total: 115500
training_iteration: 7

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.06666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.35454545454545455
  reward for individual goal_min: 0.0
episode_len_mean: 245.72
episode_reward_max: 2.0
episode_reward_mean: 0.45
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 396
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.26
  agent_1: 0.19
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.5201997756958
time_total_s: 405.3766837120056
timers:
  learn_throughput: 314.555
  learn_time_ms: 52455.118
  load_throughput: 5589243.741
  load_time_ms: 2.952
  training_iteration_time_ms: 67521.866
  update_time_ms: 2.953
timesteps_total: 99000
training_iteration: 6

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.22560975609756098
  reward for individual goal_min: 0.0
episode_len_mean: 246.53
episode_reward_max: 2.0
episode_reward_mean: 0.28
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 462
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.16
  agent_1: 0.12
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.80309844017029
time_total_s: 403.7557783126831
timers:
  learn_throughput: 372.474
  learn_time_ms: 44298.335
  load_throughput: 5862148.768
  load_time_ms: 2.815
  training_iteration_time_ms: 57643.909
  update_time_ms: 2.786
timesteps_total: 115500
training_iteration: 7

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14285714285714285
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.36363636363636365
  reward for individual goal_min: 0.0
episode_len_mean: 241.23
episode_reward_max: 2.0
episode_reward_mean: 0.48
episode_reward_min: 0.0
episodes_this_iter: 68
episodes_total: 532
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.23
  agent_1: 0.25
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.60057067871094
time_total_s: 420.08999371528625
timers:
  learn_throughput: 412.701
  learn_time_ms: 39980.508
  load_throughput: 5991473.8
  load_time_ms: 2.754
  training_iteration_time_ms: 52470.104
  update_time_ms: 2.532
timesteps_total: 132000
training_iteration: 8

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.07272727272727272
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.3
  reward for individual goal_min: 0.0
episode_len_mean: 244.79
episode_reward_max: 2.0
episode_reward_mean: 0.35
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 465
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.18
  agent_1: 0.17
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.229714155197144
time_total_s: 420.6910717487335
timers:
  learn_throughput: 354.892
  learn_time_ms: 46493.073
  load_throughput: 5940649.098
  load_time_ms: 2.777
  training_iteration_time_ms: 60015.345
  update_time_ms: 2.745
timesteps_total: 115500
training_iteration: 7

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.0189873417721519
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.3092105263157895
  reward for individual goal_min: 0.0
episode_len_mean: 247.19
episode_reward_max: 2.0
episode_reward_mean: 0.3
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 530
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.18
  agent_1: 0.12
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.4690957069397
time_total_s: 439.7863128185272
timers:
  learn_throughput: 391.952
  learn_time_ms: 42097.045
  load_throughput: 6043929.61
  load_time_ms: 2.73
  training_iteration_time_ms: 54938.536
  update_time_ms: 2.701
timesteps_total: 132000
training_iteration: 8

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.012658227848101266
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.30405405405405406
  reward for individual goal_min: 0.0
episode_len_mean: 248.08
episode_reward_max: 2.0
episode_reward_mean: 0.28
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 397
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.17
  agent_1: 0.11
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 69.11745166778564
time_total_s: 427.5916414260864
timers:
  learn_throughput: 298.051
  learn_time_ms: 55359.561
  load_throughput: 5506817.888
  load_time_ms: 2.996
  training_iteration_time_ms: 71156.323
  update_time_ms: 3.004
timesteps_total: 99000
training_iteration: 6

custom_metrics:
  reward for collective goal_max: 0.5
  reward for collective goal_mean: 0.007042253521126761
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.21428571428571427
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.24
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 462
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.08
  agent_1: 0.16
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.550830602645874
time_total_s: 445.8506348133087
timers:
  learn_throughput: 335.603
  learn_time_ms: 49165.243
  load_throughput: 5525808.575
  load_time_ms: 2.986
  training_iteration_time_ms: 63658.088
  update_time_ms: 2.803
timesteps_total: 115500
training_iteration: 7

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.0136986301369863
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.25882352941176473
  reward for individual goal_min: 0.0
episode_len_mean: 249.07
episode_reward_max: 2.0
episode_reward_mean: 0.31
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 462
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.25
  agent_1: 0.06
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.79524278640747
time_total_s: 447.0339677333832
timers:
  learn_throughput: 337.497
  learn_time_ms: 48889.283
  load_throughput: 5595505.873
  load_time_ms: 2.949
  training_iteration_time_ms: 63826.01
  update_time_ms: 2.8
timesteps_total: 115500
training_iteration: 7

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.19736842105263158
  reward for individual goal_min: 0.0
episode_len_mean: 247.25
episode_reward_max: 2.0
episode_reward_mean: 0.2
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 396
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.1
  agent_1: 0.1
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 70.37351369857788
time_total_s: 432.43984937667847
timers:
  learn_throughput: 294.058
  learn_time_ms: 56111.465
  load_throughput: 5494939.537
  load_time_ms: 3.003
  training_iteration_time_ms: 72035.943
  update_time_ms: 2.884
timesteps_total: 99000
training_iteration: 6

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.06976744186046512
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.3684210526315789
  reward for individual goal_min: 0.0
episode_len_mean: 241.38
episode_reward_max: 2.0
episode_reward_mean: 0.48
episode_reward_min: 0.0
episodes_this_iter: 69
episodes_total: 466
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.27
  agent_1: 0.21
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.166971921920776
time_total_s: 451.0716724395752
timers:
  learn_throughput: 329.769
  learn_time_ms: 50035.085
  load_throughput: 5346571.075
  load_time_ms: 3.086
  training_iteration_time_ms: 64405.715
  update_time_ms: 2.878
timesteps_total: 115500
training_iteration: 7

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.06756756756756757
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.31547619047619047
  reward for individual goal_min: 0.0
episode_len_mean: 248.23
episode_reward_max: 2.0
episode_reward_mean: 0.44
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 529
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.17
  agent_1: 0.27
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.70695495605469
time_total_s: 452.123389005661
timers:
  learn_throughput: 381.676
  learn_time_ms: 43230.429
  load_throughput: 5754221.003
  load_time_ms: 2.867
  training_iteration_time_ms: 56478.617
  update_time_ms: 2.714
timesteps_total: 132000
training_iteration: 8

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.11486486486486487
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.40540540540540543
  reward for individual goal_min: 0.0
episode_len_mean: 241.37
episode_reward_max: 2.0
episode_reward_mean: 0.51
episode_reward_min: 0.0
episodes_this_iter: 70
episodes_total: 466
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.25
  agent_1: 0.26
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.37356901168823
time_total_s: 453.71877336502075
timers:
  learn_throughput: 331.931
  learn_time_ms: 49709.187
  load_throughput: 5656396.894
  load_time_ms: 2.917
  training_iteration_time_ms: 64736.245
  update_time_ms: 14.368
timesteps_total: 115500
training_iteration: 7

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.03571428571428571
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.28169014084507044
  reward for individual goal_min: 0.0
episode_len_mean: 247.39
episode_reward_max: 2.0
episode_reward_mean: 0.3
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 462
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.21
  agent_1: 0.09
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.84054493904114
time_total_s: 436.6928300857544
timers:
  learn_throughput: 345.941
  learn_time_ms: 47695.989
  load_throughput: 5782726.287
  load_time_ms: 2.853
  training_iteration_time_ms: 62349.401
  update_time_ms: 2.788
timesteps_total: 115500
training_iteration: 7

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.027777777777777776
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.26875
  reward for individual goal_min: 0.0
episode_len_mean: 247.57
episode_reward_max: 2.0
episode_reward_mean: 0.31
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 463
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.14
  agent_1: 0.17
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.626099824905396
time_total_s: 437.72567439079285
timers:
  learn_throughput: 345.399
  learn_time_ms: 47770.848
  load_throughput: 5751078.673
  load_time_ms: 2.869
  training_iteration_time_ms: 62489.702
  update_time_ms: 2.727
timesteps_total: 115500
training_iteration: 7

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.013333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.2532467532467532
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.28
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 528
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.14
  agent_1: 0.14
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.11188459396362
time_total_s: 459.86766290664673
timers:
  learn_throughput: 373.879
  learn_time_ms: 44131.887
  load_throughput: 5752546.943
  load_time_ms: 2.868
  training_iteration_time_ms: 57447.788
  update_time_ms: 2.748
timesteps_total: 132000
training_iteration: 8

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22807017543859648
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.43023255813953487
  reward for individual goal_min: 0.0
episode_len_mean: 231.56
episode_reward_max: 2.0
episode_reward_mean: 0.63
episode_reward_min: 0.0
episodes_this_iter: 72
episodes_total: 604
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.31
  agent_1: 0.32
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.876304388046265
time_total_s: 469.9662981033325
timers:
  learn_throughput: 415.249
  learn_time_ms: 39735.234
  load_throughput: 5877424.123
  load_time_ms: 2.807
  training_iteration_time_ms: 52178.178
  update_time_ms: 2.528
timesteps_total: 148500
training_iteration: 9

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.13953488372093023
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.34210526315789475
  reward for individual goal_min: 0.0
episode_len_mean: 240.28
episode_reward_max: 2.0
episode_reward_mean: 0.51
episode_reward_min: 0.0
episodes_this_iter: 69
episodes_total: 465
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.25
  agent_1: 0.26
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 67.1706383228302
time_total_s: 472.5473220348358
timers:
  learn_throughput: 314.987
  learn_time_ms: 52383.143
  load_throughput: 5454814.908
  load_time_ms: 3.025
  training_iteration_time_ms: 67466.478
  update_time_ms: 2.942
timesteps_total: 115500
training_iteration: 7

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.13333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.4
  reward for individual goal_min: 0.0
episode_len_mean: 231.3
episode_reward_max: 2.0
episode_reward_mean: 0.48
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 538
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.23
  agent_1: 0.25
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.91390609741211
time_total_s: 478.60497784614563
timers:
  learn_throughput: 356.581
  learn_time_ms: 46272.858
  load_throughput: 5802162.291
  load_time_ms: 2.844
  training_iteration_time_ms: 59748.52
  update_time_ms: 2.75
timesteps_total: 132000
training_iteration: 8

custom_metrics:
  reward for collective goal_max: 0.5
  reward for collective goal_mean: 0.012345679012345678
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.323943661971831
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.3
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 596
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.13
  agent_1: 0.17
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.64196181297302
time_total_s: 493.42827463150024
timers:
  learn_throughput: 393.361
  learn_time_ms: 41946.243
  load_throughput: 5933809.14
  load_time_ms: 2.781
  training_iteration_time_ms: 54790.644
  update_time_ms: 2.692
timesteps_total: 148500
training_iteration: 9

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.02857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.36627906976744184
  reward for individual goal_min: 0.0
episode_len_mean: 246.33
episode_reward_max: 2.0
episode_reward_mean: 0.46
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 529
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.29
  agent_1: 0.17
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.3386709690094
time_total_s: 505.3726387023926
timers:
  learn_throughput: 341.154
  learn_time_ms: 48365.231
  load_throughput: 5456058.971
  load_time_ms: 3.024
  training_iteration_time_ms: 63135.498
  update_time_ms: 2.764
timesteps_total: 132000
training_iteration: 8

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.29878048780487804
  reward for individual goal_min: 0.0
episode_len_mean: 249.01
episode_reward_max: 2.0
episode_reward_mean: 0.33
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 528
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.05
  agent_1: 0.28
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.526766300201416
time_total_s: 506.37740111351013
timers:
  learn_throughput: 337.784
  learn_time_ms: 48847.733
  load_throughput: 5315005.021
  load_time_ms: 3.104
  training_iteration_time_ms: 63261.914
  update_time_ms: 2.795
timesteps_total: 132000
training_iteration: 8
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.07236842105263158
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.3782051282051282
  reward for individual goal_min: 0.0
episode_len_mean: 242.46
episode_reward_max: 2.0
episode_reward_mean: 0.47
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 596
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.2
  agent_1: 0.27
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.84782958030701
time_total_s: 506.971218585968
timers:
  learn_throughput: 382.999
  learn_time_ms: 43081.102
  load_throughput: 5657681.388
  load_time_ms: 2.916
  training_iteration_time_ms: 56293.379
  update_time_ms: 2.691
timesteps_total: 148500
training_iteration: 9


custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.01948051948051948
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.3424657534246575
  reward for individual goal_min: 0.0
episode_len_mean: 245.18
episode_reward_max: 2.0
episode_reward_mean: 0.34
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 464
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.23
  agent_1: 0.11
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 69.83215737342834
time_total_s: 497.42379879951477
timers:
  learn_throughput: 299.27
  learn_time_ms: 55134.158
  load_throughput: 5331272.967
  load_time_ms: 3.095
  training_iteration_time_ms: 70961.653
  update_time_ms: 2.971
timesteps_total: 115500
training_iteration: 7

custom_metrics:
  reward for collective goal_max: 0.0
  reward for collective goal_mean: 0.0
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.29508196721311475
  reward for individual goal_min: 0.0
episode_len_mean: 247.29
episode_reward_max: 2.0
episode_reward_mean: 0.36
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 532
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.21
  agent_1: 0.15
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.96091318130493
time_total_s: 513.0325856208801
timers:
  learn_throughput: 331.588
  learn_time_ms: 49760.493
  load_throughput: 5237919.849
  load_time_ms: 3.15
  training_iteration_time_ms: 64095.948
  update_time_ms: 2.872
timesteps_total: 132000
training_iteration: 8

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36363636363636365
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.38392857142857145
  reward for individual goal_min: 0.0
episode_len_mean: 228.52
episode_reward_max: 2.0
episode_reward_mean: 0.75
episode_reward_min: 0.0
episodes_this_iter: 71
episodes_total: 675
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.37
  agent_1: 0.38
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.6736216545105
time_total_s: 519.639919757843
timers:
  learn_throughput: 417.322
  learn_time_ms: 39537.861
  load_throughput: 5831263.302
  load_time_ms: 2.83
  training_iteration_time_ms: 51924.456
  update_time_ms: 2.535
timesteps_total: 165000
training_iteration: 10

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.08571428571428572
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.325
  reward for individual goal_min: 0.0
episode_len_mean: 245.31
episode_reward_max: 2.0
episode_reward_mean: 0.43
episode_reward_min: 0.0
episodes_this_iter: 68
episodes_total: 596
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.22
  agent_1: 0.21
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.19632053375244
time_total_s: 515.0639834403992
timers:
  learn_throughput: 375.623
  learn_time_ms: 43927.056
  load_throughput: 5708654.293
  load_time_ms: 2.89
  training_iteration_time_ms: 57193.569
  update_time_ms: 2.73
timesteps_total: 148500
training_iteration: 9

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.06338028169014084
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.3223684210526316
  reward for individual goal_min: 0.0
episode_len_mean: 243.18
episode_reward_max: 2.0
episode_reward_mean: 0.41
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 530
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.2
  agent_1: 0.21
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.388062477111816
time_total_s: 497.11373686790466
timers:
  learn_throughput: 347.575
  learn_time_ms: 47471.79
  load_throughput: 5659172.132
  load_time_ms: 2.916
  training_iteration_time_ms: 62097.435
  update_time_ms: 2.694
timesteps_total: 132000
training_iteration: 8

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.0472972972972973
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.32894736842105265
  reward for individual goal_min: 0.0
episode_len_mean: 246.25
episode_reward_max: 2.0
episode_reward_mean: 0.37
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 529
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.26
  agent_1: 0.11
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.37942814826965
time_total_s: 497.07225823402405
timers:
  learn_throughput: 346.928
  learn_time_ms: 47560.238
  load_throughput: 5570180.874
  load_time_ms: 2.962
  training_iteration_time_ms: 62098.594
  update_time_ms: 2.778
timesteps_total: 132000
training_iteration: 8

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.07236842105263158
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.37349397590361444
  reward for individual goal_min: 0.0
episode_len_mean: 246.81
episode_reward_max: 2.0
episode_reward_mean: 0.47
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 533
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.19
  agent_1: 0.28
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.91950225830078
time_total_s: 517.6382756233215
timers:
  learn_throughput: 331.962
  learn_time_ms: 49704.421
  load_throughput: 5486934.264
  load_time_ms: 3.007
  training_iteration_time_ms: 64629.308
  update_time_ms: 12.9
timesteps_total: 132000
training_iteration: 8

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.012195121951219513
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.13043478260869565
  reward for individual goal_min: 0.0
episode_len_mean: 247.89
episode_reward_max: 2.0
episode_reward_mean: 0.14
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 462
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.04
  agent_1: 0.1
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 71.38416719436646
time_total_s: 503.8240165710449
timers:
  learn_throughput: 294.599
  learn_time_ms: 56008.425
  load_throughput: 5375821.029
  load_time_ms: 3.069
  training_iteration_time_ms: 71937.058
  update_time_ms: 2.862
timesteps_total: 115500
training_iteration: 7

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14814814814814814
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5
  reward for individual goal_min: 0.0
episode_len_mean: 229.21
episode_reward_max: 2.0
episode_reward_mean: 0.62
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 611
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.29
  agent_1: 0.33
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.08844065666199
time_total_s: 535.6934185028076
timers:
  learn_throughput: 358.27
  learn_time_ms: 46054.59
  load_throughput: 5764659.306
  load_time_ms: 2.862
  training_iteration_time_ms: 59436.019
  update_time_ms: 2.729
timesteps_total: 148500
training_iteration: 9

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.025974025974025976
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.2894736842105263
  reward for individual goal_min: 0.0
episode_len_mean: 247.48
episode_reward_max: 2.0
episode_reward_mean: 0.31
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 662
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.14
  agent_1: 0.17
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.56854581832886
time_total_s: 547.9968204498291
timers:
  learn_throughput: 393.386
  learn_time_ms: 41943.54
  load_throughput: 5822285.637
  load_time_ms: 2.834
  training_iteration_time_ms: 54764.934
  update_time_ms: 2.676
timesteps_total: 165000
training_iteration: 10

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2631578947368421
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.3709677419354839
  reward for individual goal_min: 0.0
episode_len_mean: 235.04
episode_reward_max: 2.0
episode_reward_mean: 0.66
episode_reward_min: 0.0
episodes_this_iter: 71
episodes_total: 536
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.38
  agent_1: 0.28
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.77800297737122
time_total_s: 538.325325012207
timers:
  learn_throughput: 316.243
  learn_time_ms: 52175.067
  load_throughput: 5442542.988
  load_time_ms: 3.032
  training_iteration_time_ms: 67250.641
  update_time_ms: 2.973
timesteps_total: 132000
training_iteration: 8

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.03289473684210526
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.47297297297297297
  reward for individual goal_min: 0.0
episode_len_mean: 243.1
episode_reward_max: 2.0
episode_reward_mean: 0.5
episode_reward_min: 0.0
episodes_this_iter: 68
episodes_total: 664
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.16
  agent_1: 0.34
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.22924089431763
time_total_s: 562.2004594802856
timers:
  learn_throughput: 383.523
  learn_time_ms: 43022.221
  load_throughput: 5576183.708
  load_time_ms: 2.959
  training_iteration_time_ms: 56183.336
  update_time_ms: 2.664
timesteps_total: 165000
training_iteration: 10

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.4
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.46
  reward for individual goal_min: 0.0
episode_len_mean: 224.59
episode_reward_max: 2.0
episode_reward_mean: 0.86
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 751
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.46
  agent_1: 0.4
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.56087851524353
time_total_s: 570.2007982730865
timers:
  learn_throughput: 419.181
  learn_time_ms: 39362.424
  load_throughput: 5338487.457
  load_time_ms: 3.091
  training_iteration_time_ms: 51665.229
  update_time_ms: 2.561
timesteps_total: 181500
training_iteration: 11

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.04929577464788732
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.4342105263157895
  reward for individual goal_min: 0.0
episode_len_mean: 243.14
episode_reward_max: 2.0
episode_reward_mean: 0.5
episode_reward_min: 0.0
episodes_this_iter: 69
episodes_total: 598
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.27
  agent_1: 0.23
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.27159023284912
time_total_s: 567.6442289352417
timers:
  learn_throughput: 341.478
  learn_time_ms: 48319.4
  load_throughput: 5397957.69
  load_time_ms: 3.057
  training_iteration_time_ms: 63035.362
  update_time_ms: 2.758
timesteps_total: 148500
training_iteration: 9

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.11428571428571428
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.3125
  reward for individual goal_min: 0.0
episode_len_mean: 243.16
episode_reward_max: 2.0
episode_reward_mean: 0.47
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 662
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.18
  agent_1: 0.29
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.36733651161194
time_total_s: 571.4313199520111
timers:
  learn_throughput: 376.24
  learn_time_ms: 43854.943
  load_throughput: 5657414.165
  load_time_ms: 2.917
  training_iteration_time_ms: 57107.485
  update_time_ms: 2.735
timesteps_total: 165000
training_iteration: 10

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.05333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.2602739726027397
  reward for individual goal_min: 0.0
episode_len_mean: 245.72
episode_reward_max: 2.0
episode_reward_mean: 0.32
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 594
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.08
  agent_1: 0.24
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.1039879322052
time_total_s: 571.4813890457153
timers:
  learn_throughput: 336.536
  learn_time_ms: 49028.924
  load_throughput: 5262725.97
  load_time_ms: 3.135
  training_iteration_time_ms: 63462.465
  update_time_ms: 2.781
timesteps_total: 148500
training_iteration: 9

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.08974358974358974
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.3835616438356164
  reward for individual goal_min: 0.0
episode_len_mean: 241.91
episode_reward_max: 2.0
episode_reward_mean: 0.45
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 596
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.25
  agent_1: 0.2
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.88514041900635
time_total_s: 554.9573986530304
timers:
  learn_throughput: 349.64
  learn_time_ms: 47191.385
  load_throughput: 5452202.348
  load_time_ms: 3.026
  training_iteration_time_ms: 61626.244
  update_time_ms: 2.75
timesteps_total: 148500
training_iteration: 9

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.09333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.38311688311688313
  reward for individual goal_min: 0.0
episode_len_mean: 243.7
episode_reward_max: 2.0
episode_reward_mean: 0.44
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 596
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.19
  agent_1: 0.25
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.795517683029175
time_total_s: 555.9092545509338
timers:
  learn_throughput: 349.669
  learn_time_ms: 47187.533
  load_throughput: 5515253.681
  load_time_ms: 2.992
  training_iteration_time_ms: 61726.194
  update_time_ms: 2.684
timesteps_total: 148500
training_iteration: 9

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.07692307692307693
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.3541666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 238.83
episode_reward_max: 2.0
episode_reward_mean: 0.42
episode_reward_min: 0.0
episodes_this_iter: 69
episodes_total: 601
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.27
  agent_1: 0.15
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 64.68007516860962
time_total_s: 577.7126607894897
timers:
  learn_throughput: 331.387
  learn_time_ms: 49790.722
  load_throughput: 5115929.165
  load_time_ms: 3.225
  training_iteration_time_ms: 64157.131
  update_time_ms: 2.826
timesteps_total: 148500
training_iteration: 9

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.05714285714285714
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.375
  reward for individual goal_min: 0.0
episode_len_mean: 240.94
episode_reward_max: 2.0
episode_reward_mean: 0.46
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 531
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.25
  agent_1: 0.21
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 68.06536817550659
time_total_s: 565.4891669750214
timers:
  learn_throughput: 301.16
  learn_time_ms: 54788.215
  load_throughput: 5223489.773
  load_time_ms: 3.159
  training_iteration_time_ms: 70594.8
  update_time_ms: 2.929
timesteps_total: 132000
training_iteration: 8

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.09868421052631579
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.4074074074074074
  reward for individual goal_min: 0.0
episode_len_mean: 243.7
episode_reward_max: 2.0
episode_reward_mean: 0.51
episode_reward_min: 0.0
episodes_this_iter: 69
episodes_total: 602
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.25
  agent_1: 0.26
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.12492370605469
time_total_s: 580.7631993293762
timers:
  learn_throughput: 332.954
  learn_time_ms: 49556.341
  load_throughput: 5386520.548
  load_time_ms: 3.063
  training_iteration_time_ms: 64457.625
  update_time_ms: 11.8
timesteps_total: 148500
training_iteration: 9

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.051470588235294115
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.31097560975609756
  reward for individual goal_min: 0.0
episode_len_mean: 245.01
episode_reward_max: 2.0
episode_reward_mean: 0.41
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 729
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.2
  agent_1: 0.21
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.80276322364807
time_total_s: 600.7995836734772
timers:
  learn_throughput: 395.644
  learn_time_ms: 41704.131
  load_throughput: 5278026.861
  load_time_ms: 3.126
  training_iteration_time_ms: 54377.665
  update_time_ms: 2.616
timesteps_total: 181500
training_iteration: 11

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30158730158730157
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5
  reward for individual goal_min: 0.0
episode_len_mean: 218.98
episode_reward_max: 2.0
episode_reward_mean: 0.75
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 687
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.32
  agent_1: 0.43
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.59938168525696
time_total_s: 592.2928001880646
timers:
  learn_throughput: 360.149
  learn_time_ms: 45814.35
  load_throughput: 5670950.867
  load_time_ms: 2.91
  training_iteration_time_ms: 59149.007
  update_time_ms: 2.725
timesteps_total: 165000
training_iteration: 10

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.05333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.15942028985507245
  reward for individual goal_min: 0.0
episode_len_mean: 250.0
episode_reward_max: 1.0
episode_reward_mean: 0.2
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 528
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.08
  agent_1: 0.12
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 70.10371088981628
time_total_s: 573.9277274608612
timers:
  learn_throughput: 295.759
  learn_time_ms: 55788.604
  load_throughput: 5202628.603
  load_time_ms: 3.171
  training_iteration_time_ms: 71703.068
  update_time_ms: 2.84
timesteps_total: 132000
training_iteration: 8

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36363636363636365
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.42857142857142855
  reward for individual goal_min: 0.0
episode_len_mean: 233.9
episode_reward_max: 2.0
episode_reward_mean: 0.8
episode_reward_min: 0.0
episodes_this_iter: 69
episodes_total: 605
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.47
  agent_1: 0.33
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 66.15861010551453
time_total_s: 604.4839351177216
timers:
  learn_throughput: 317.042
  learn_time_ms: 52043.651
  load_throughput: 5387545.576
  load_time_ms: 3.063
  training_iteration_time_ms: 67125.27
  update_time_ms: 2.963
timesteps_total: 148500
training_iteration: 9

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.5098039215686274
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5204081632653061
  reward for individual goal_min: 0.0
episode_len_mean: 211.23
episode_reward_max: 2.0
episode_reward_mean: 1.03
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 829
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.31981182098389
time_total_s: 616.5206100940704
timers:
  learn_throughput: 426.142
  learn_time_ms: 38719.503
  load_throughput: 5301396.168
  load_time_ms: 3.112
  training_iteration_time_ms: 50941.181
  update_time_ms: 2.544
timesteps_total: 198000
training_iteration: 12

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.04054054054054054
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.4866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 244.15
episode_reward_max: 2.0
episode_reward_mean: 0.55
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 731
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.14
  agent_1: 0.41
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.86907935142517
time_total_s: 617.0695388317108
timers:
  learn_throughput: 385.637
  learn_time_ms: 42786.327
  load_throughput: 5066289.119
  load_time_ms: 3.257
  training_iteration_time_ms: 55841.661
  update_time_ms: 2.597
timesteps_total: 181500
training_iteration: 11

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.08088235294117647
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.47530864197530864
  reward for individual goal_min: 0.0
episode_len_mean: 241.59
episode_reward_max: 2.0
episode_reward_mean: 0.6
episode_reward_min: 0.0
episodes_this_iter: 70
episodes_total: 732
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.32
  agent_1: 0.28
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.20469570159912
time_total_s: 624.6360156536102
timers:
  learn_throughput: 380.018
  learn_time_ms: 43418.974
  load_throughput: 5136492.348
  load_time_ms: 3.212
  training_iteration_time_ms: 56507.057
  update_time_ms: 2.64
timesteps_total: 181500
training_iteration: 11

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.0949367088607595
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.3958333333333333
  reward for individual goal_min: 0.0
episode_len_mean: 240.67
episode_reward_max: 2.0
episode_reward_mean: 0.51
episode_reward_min: 0.0
episodes_this_iter: 68
episodes_total: 666
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.3
  agent_1: 0.21
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.76899313926697
time_total_s: 627.4132220745087
timers:
  learn_throughput: 342.97
  learn_time_ms: 48109.171
  load_throughput: 5279597.199
  load_time_ms: 3.125
  training_iteration_time_ms: 62705.164
  update_time_ms: 2.763
timesteps_total: 165000
training_iteration: 10

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14935064935064934
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.4407894736842105
  reward for individual goal_min: 0.0
episode_len_mean: 240.87
episode_reward_max: 2.0
episode_reward_mean: 0.6
episode_reward_min: 0.0
episodes_this_iter: 69
episodes_total: 665
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.28
  agent_1: 0.32
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.720502614974976
time_total_s: 612.6297571659088
timers:
  learn_throughput: 352.34
  learn_time_ms: 46829.816
  load_throughput: 5429881.84
  load_time_ms: 3.039
  training_iteration_time_ms: 61221.991
  update_time_ms: 2.686
timesteps_total: 165000
training_iteration: 10

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16463414634146342
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.44366197183098594
  reward for individual goal_min: 0.0
episode_len_mean: 234.14
episode_reward_max: 2.0
episode_reward_mean: 0.62
episode_reward_min: 0.0
episodes_this_iter: 71
episodes_total: 667
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.36
  agent_1: 0.26
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.8738579750061
time_total_s: 612.8312566280365
timers:
  learn_throughput: 351.442
  learn_time_ms: 46949.393
  load_throughput: 5366263.405
  load_time_ms: 3.075
  training_iteration_time_ms: 61247.371
  update_time_ms: 2.736
timesteps_total: 165000
training_iteration: 10

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.043209876543209874
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.3904109589041096
  reward for individual goal_min: 0.0
episode_len_mean: 243.62
episode_reward_max: 2.0
episode_reward_mean: 0.41
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 661
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.14
  agent_1: 0.27
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.1069073677063
time_total_s: 634.5882964134216
timers:
  learn_throughput: 336.712
  learn_time_ms: 49003.343
  load_throughput: 5178773.067
  load_time_ms: 3.186
  training_iteration_time_ms: 63423.1
  update_time_ms: 2.768
timesteps_total: 165000
training_iteration: 10

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.07575757575757576
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.41025641025641024
  reward for individual goal_min: 0.0
episode_len_mean: 239.25
episode_reward_max: 2.0
episode_reward_mean: 0.52
episode_reward_min: 0.0
episodes_this_iter: 69
episodes_total: 798
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.17
  agent_1: 0.35
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.163175106048584
time_total_s: 652.9627587795258
timers:
  learn_throughput: 398.628
  learn_time_ms: 41391.982
  load_throughput: 5265738.09
  load_time_ms: 3.133
  training_iteration_time_ms: 54022.945
  update_time_ms: 2.62
timesteps_total: 198000
training_iteration: 12

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.48
  reward for individual goal_min: 0.0
episode_len_mean: 237.78
episode_reward_max: 2.0
episode_reward_mean: 0.62
episode_reward_min: 0.0
episodes_this_iter: 68
episodes_total: 669
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.35
  agent_1: 0.27
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.03236413002014
time_total_s: 640.7450249195099
timers:
  learn_throughput: 331.952
  learn_time_ms: 49706.047
  load_throughput: 5038184.664
  load_time_ms: 3.275
  training_iteration_time_ms: 64041.244
  update_time_ms: 2.827
timesteps_total: 165000
training_iteration: 10

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24489795918367346
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5686274509803921
  reward for individual goal_min: 0.0
episode_len_mean: 216.67
episode_reward_max: 2.0
episode_reward_mean: 0.82
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 763
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.36
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.57730269432068
time_total_s: 646.8701028823853
timers:
  learn_throughput: 363.383
  learn_time_ms: 45406.645
  load_throughput: 5183660.605
  load_time_ms: 3.183
  training_iteration_time_ms: 58593.256
  update_time_ms: 2.665
timesteps_total: 181500
training_iteration: 11

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17682926829268292
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.4609375
  reward for individual goal_min: 0.0
episode_len_mean: 228.63
episode_reward_max: 2.0
episode_reward_mean: 0.61
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 675
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.37
  agent_1: 0.24
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.734100341796875
time_total_s: 643.4972996711731
timers:
  learn_throughput: 333.599
  learn_time_ms: 49460.538
  load_throughput: 5340423.647
  load_time_ms: 3.09
  training_iteration_time_ms: 64281.337
  update_time_ms: 10.905
timesteps_total: 165000
training_iteration: 10

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.09090909090909091
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.375
  reward for individual goal_min: 0.0
episode_len_mean: 242.28
episode_reward_max: 2.0
episode_reward_mean: 0.48
episode_reward_min: 0.0
episodes_this_iter: 68
episodes_total: 599
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.28
  agent_1: 0.2
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 68.98925256729126
time_total_s: 634.4784195423126
timers:
  learn_throughput: 302.16
  learn_time_ms: 54606.876
  load_throughput: 5133892.814
  load_time_ms: 3.214
  training_iteration_time_ms: 70412.119
  update_time_ms: 2.902
timesteps_total: 148500
training_iteration: 9

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.5490196078431373
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5510204081632653
  reward for individual goal_min: 0.0
episode_len_mean: 205.76
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 909
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.76235795021057
time_total_s: 663.282968044281
timers:
  learn_throughput: 431.329
  learn_time_ms: 38253.833
  load_throughput: 5334290.339
  load_time_ms: 3.093
  training_iteration_time_ms: 50408.393
  update_time_ms: 2.551
timesteps_total: 214500
training_iteration: 13

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.06493506493506493
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.2635135135135135
  reward for individual goal_min: 0.0
episode_len_mean: 247.65
episode_reward_max: 2.0
episode_reward_mean: 0.32
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 595
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.11
  agent_1: 0.21
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 69.93057489395142
time_total_s: 643.8583023548126
timers:
  learn_throughput: 296.757
  learn_time_ms: 55600.987
  load_throughput: 5091964.127
  load_time_ms: 3.24
  training_iteration_time_ms: 71501.577
  update_time_ms: 2.832
timesteps_total: 148500
training_iteration: 9

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.45098039215686275
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.45918367346938777
  reward for individual goal_min: 0.0
episode_len_mean: 219.92
episode_reward_max: 2.0
episode_reward_mean: 0.91
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 682
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.41
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.44304823875427
time_total_s: 666.9269833564758
timers:
  learn_throughput: 319.483
  learn_time_ms: 51645.876
  load_throughput: 5277181.681
  load_time_ms: 3.127
  training_iteration_time_ms: 66653.299
  update_time_ms: 2.952
timesteps_total: 165000
training_iteration: 10

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.06578947368421052
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.538961038961039
  reward for individual goal_min: 0.0
episode_len_mean: 242.29
episode_reward_max: 2.0
episode_reward_mean: 0.6
episode_reward_min: 0.0
episodes_this_iter: 68
episodes_total: 799
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.23
  agent_1: 0.37
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.489871978759766
time_total_s: 669.5594108104706
timers:
  learn_throughput: 389.831
  learn_time_ms: 42326.035
  load_throughput: 5042810.321
  load_time_ms: 3.272
  training_iteration_time_ms: 55307.508
  update_time_ms: 2.585
timesteps_total: 198000
training_iteration: 12

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.13636363636363635
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5128205128205128
  reward for individual goal_min: 0.0
episode_len_mean: 230.78
episode_reward_max: 2.0
episode_reward_mean: 0.69
episode_reward_min: 0.0
episodes_this_iter: 71
episodes_total: 803
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.35
  agent_1: 0.34
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.058719635009766
time_total_s: 678.69473528862
timers:
  learn_throughput: 383.423
  learn_time_ms: 43033.442
  load_throughput: 5075913.218
  load_time_ms: 3.251
  training_iteration_time_ms: 56041.712
  update_time_ms: 2.629
timesteps_total: 198000
training_iteration: 12

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15753424657534246
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5126582278481012
  reward for individual goal_min: 0.0
episode_len_mean: 235.7
episode_reward_max: 2.0
episode_reward_mean: 0.72
episode_reward_min: 0.0
episodes_this_iter: 71
episodes_total: 737
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.41
  agent_1: 0.31
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.9389431476593
time_total_s: 685.352165222168
timers:
  learn_throughput: 347.14
  learn_time_ms: 47531.291
  load_throughput: 4821979.627
  load_time_ms: 3.422
  training_iteration_time_ms: 61540.019
  update_time_ms: 2.698
timesteps_total: 181500
training_iteration: 11

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5147058823529411
  reward for individual goal_min: 0.0
episode_len_mean: 231.26
episode_reward_max: 2.0
episode_reward_mean: 0.69
episode_reward_min: 0.0
episodes_this_iter: 70
episodes_total: 737
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.37
  agent_1: 0.32
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.382102251052856
time_total_s: 668.2133588790894
timers:
  learn_throughput: 355.245
  learn_time_ms: 46446.809
  load_throughput: 4930256.892
  load_time_ms: 3.347
  training_iteration_time_ms: 60261.817
  update_time_ms: 2.651
timesteps_total: 181500
training_iteration: 11

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15714285714285714
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.3380281690140845
  reward for individual goal_min: 0.0
episode_len_mean: 244.47
episode_reward_max: 2.0
episode_reward_mean: 0.51
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 731
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.25
  agent_1: 0.26
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.188379526138306
time_total_s: 669.8181366920471
timers:
  learn_throughput: 355.021
  learn_time_ms: 46476.156
  load_throughput: 4920232.056
  load_time_ms: 3.354
  training_iteration_time_ms: 60409.862
  update_time_ms: 2.615
timesteps_total: 181500
training_iteration: 11

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.12857142857142856
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.4857142857142857
  reward for individual goal_min: 0.0
episode_len_mean: 238.29
episode_reward_max: 2.0
episode_reward_mean: 0.61
episode_reward_min: 0.0
episodes_this_iter: 70
episodes_total: 868
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.29
  agent_1: 0.32
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.26870155334473
time_total_s: 707.2314603328705
timers:
  learn_throughput: 399.592
  learn_time_ms: 41292.108
  load_throughput: 5174126.83
  load_time_ms: 3.189
  training_iteration_time_ms: 53899.522
  update_time_ms: 2.634
timesteps_total: 214500
training_iteration: 13

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.06962025316455696
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.4246575342465753
  reward for individual goal_min: 0.0
episode_len_mean: 246.48
episode_reward_max: 2.0
episode_reward_mean: 0.48
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 728
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.17
  agent_1: 0.31
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.633695125579834
time_total_s: 695.2219915390015
timers:
  learn_throughput: 339.611
  learn_time_ms: 48585.045
  load_throughput: 4717712.79
  load_time_ms: 3.497
  training_iteration_time_ms: 62903.518
  update_time_ms: 2.711
timesteps_total: 181500
training_iteration: 11

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.4897959183673469
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5392156862745098
  reward for individual goal_min: 0.0
episode_len_mean: 210.03
episode_reward_max: 2.0
episode_reward_mean: 1.03
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 845
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.46
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.854740858078
time_total_s: 702.7248437404633
timers:
  learn_throughput: 365.715
  learn_time_ms: 45117.055
  load_throughput: 5048806.922
  load_time_ms: 3.268
  training_iteration_time_ms: 58270.264
  update_time_ms: 2.667
timesteps_total: 198000
training_iteration: 12

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.12195121951219512
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.4576271186440678
  reward for individual goal_min: 0.0
episode_len_mean: 233.38
episode_reward_max: 2.0
episode_reward_mean: 0.64
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 742
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.39
  agent_1: 0.25
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.12923860549927
time_total_s: 703.8742635250092
timers:
  learn_throughput: 333.255
  learn_time_ms: 49511.684
  load_throughput: 4600240.362
  load_time_ms: 3.587
  training_iteration_time_ms: 63783.579
  update_time_ms: 2.763
timesteps_total: 181500
training_iteration: 11

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.5294117647058824
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5408163265306123
  reward for individual goal_min: 0.0
episode_len_mean: 211.42
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 986
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.65462803840637
time_total_s: 709.9375960826874
timers:
  learn_throughput: 436.459
  learn_time_ms: 37804.265
  load_throughput: 5358244.631
  load_time_ms: 3.079
  training_iteration_time_ms: 49869.465
  update_time_ms: 2.544
timesteps_total: 231000
training_iteration: 14

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5945945945945946
  reward for individual goal_min: 0.0
episode_len_mean: 230.24
episode_reward_max: 2.0
episode_reward_mean: 0.84
episode_reward_min: 0.0
episodes_this_iter: 72
episodes_total: 747
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.34
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.63834547996521
time_total_s: 705.1356451511383
timers:
  learn_throughput: 334.859
  learn_time_ms: 49274.416
  load_throughput: 4800939.015
  load_time_ms: 3.437
  training_iteration_time_ms: 63810.57
  update_time_ms: 10.824
timesteps_total: 181500
training_iteration: 11

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.056962025316455694
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.42567567567567566
  reward for individual goal_min: 0.0
episode_len_mean: 246.63
episode_reward_max: 2.0
episode_reward_mean: 0.46
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 666
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.32
  agent_1: 0.14
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 67.08848071098328
time_total_s: 701.5669002532959
timers:
  learn_throughput: 304.085
  learn_time_ms: 54261.177
  load_throughput: 5056738.395
  load_time_ms: 3.263
  training_iteration_time_ms: 70076.026
  update_time_ms: 2.888
timesteps_total: 165000
training_iteration: 10

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.5681818181818182
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.4732142857142857
  reward for individual goal_min: 0.0
episode_len_mean: 213.91
episode_reward_max: 2.0
episode_reward_mean: 1.03
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 760
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.43
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.21019887924194
time_total_s: 727.1371822357178
timers:
  learn_throughput: 323.089
  learn_time_ms: 51069.46
  load_throughput: 4882465.29
  load_time_ms: 3.379
  training_iteration_time_ms: 65960.906
  update_time_ms: 2.896
timesteps_total: 181500
training_iteration: 11

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.12162162162162163
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.538961038961039
  reward for individual goal_min: 0.0
episode_len_mean: 233.46
episode_reward_max: 2.0
episode_reward_mean: 0.72
episode_reward_min: 0.0
episodes_this_iter: 72
episodes_total: 871
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.25
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.0526807308197
time_total_s: 722.6120915412903
timers:
  learn_throughput: 393.541
  learn_time_ms: 41926.966
  load_throughput: 4960791.364
  load_time_ms: 3.326
  training_iteration_time_ms: 54793.166
  update_time_ms: 2.612
timesteps_total: 214500
training_iteration: 13

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.07142857142857142
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.1858974358974359
  reward for individual goal_min: 0.0
episode_len_mean: 247.98
episode_reward_max: 2.0
episode_reward_mean: 0.25
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 661
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.13
  agent_1: 0.12
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 70.46166324615479
time_total_s: 714.3199656009674
timers:
  learn_throughput: 297.349
  learn_time_ms: 55490.306
  load_throughput: 4999784.421
  load_time_ms: 3.3
  training_iteration_time_ms: 71393.777
  update_time_ms: 2.828
timesteps_total: 165000
training_iteration: 10

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5705882352941176
  reward for individual goal_min: 0.0
episode_len_mean: 224.5
episode_reward_max: 2.0
episode_reward_mean: 0.86
episode_reward_min: 0.0
episodes_this_iter: 75
episodes_total: 878
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.45
  agent_1: 0.41
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.37160897254944
time_total_s: 732.0663442611694
timers:
  learn_throughput: 387.396
  learn_time_ms: 42592.022
  load_throughput: 5062953.377
  load_time_ms: 3.259
  training_iteration_time_ms: 55460.46
  update_time_ms: 2.631
timesteps_total: 214500
training_iteration: 13

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.08139534883720931
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.541095890410959
  reward for individual goal_min: 0.0
episode_len_mean: 239.88
episode_reward_max: 2.0
episode_reward_mean: 0.59
episode_reward_min: 0.0
episodes_this_iter: 69
episodes_total: 937
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.3
  agent_1: 0.29
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.36398124694824
time_total_s: 759.5954415798187
timers:
  learn_throughput: 401.085
  learn_time_ms: 41138.455
  load_throughput: 5109566.758
  load_time_ms: 3.229
  training_iteration_time_ms: 53742.397
  update_time_ms: 2.969
timesteps_total: 231000
training_iteration: 14

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.11538461538461539
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5324675324675324
  reward for individual goal_min: 0.0
episode_len_mean: 233.95
episode_reward_max: 2.0
episode_reward_mean: 0.67
episode_reward_min: 0.0
episodes_this_iter: 71
episodes_total: 808
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.37
  agent_1: 0.3
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.064675092697144
time_total_s: 743.4168403148651
timers:
  learn_throughput: 351.983
  learn_time_ms: 46877.238
  load_throughput: 4776979.72
  load_time_ms: 3.454
  training_iteration_time_ms: 60812.341
  update_time_ms: 2.682
timesteps_total: 198000
training_iteration: 12

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.13194444444444445
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.44
  reward for individual goal_min: 0.0
episode_len_mean: 235.35
episode_reward_max: 2.0
episode_reward_mean: 0.61
episode_reward_min: 0.0
episodes_this_iter: 70
episodes_total: 807
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.32
  agent_1: 0.29
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.72974395751953
time_total_s: 726.9431028366089
timers:
  learn_throughput: 356.921
  learn_time_ms: 46228.715
  load_throughput: 4917959.366
  load_time_ms: 3.355
  training_iteration_time_ms: 59973.195
  update_time_ms: 2.636
timesteps_total: 198000
training_iteration: 12

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.12837837837837837
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.3142857142857143
  reward for individual goal_min: 0.0
episode_len_mean: 245.84
episode_reward_max: 2.0
episode_reward_mean: 0.44
episode_reward_min: 0.0
episodes_this_iter: 68
episodes_total: 799
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.24
  agent_1: 0.2
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.73063135147095
time_total_s: 728.5487680435181
timers:
  learn_throughput: 356.984
  learn_time_ms: 46220.506
  load_throughput: 4907740.792
  load_time_ms: 3.362
  training_iteration_time_ms: 60137.552
  update_time_ms: 2.617
timesteps_total: 198000
training_iteration: 12

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6326530612244898
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6078431372549019
  reward for individual goal_min: 0.0
episode_len_mean: 184.95
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 1077
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.07082796096802
time_total_s: 755.0084240436554
timers:
  learn_throughput: 443.315
  learn_time_ms: 37219.603
  load_throughput: 5404862.079
  load_time_ms: 3.053
  training_iteration_time_ms: 49165.947
  update_time_ms: 2.539
timesteps_total: 247500
training_iteration: 15

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.5531914893617021
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6886792452830188
  reward for individual goal_min: 0.0
episode_len_mean: 194.82
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 928
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.7457070350647
time_total_s: 758.470550775528
timers:
  learn_throughput: 369.195
  learn_time_ms: 44691.837
  load_throughput: 4969590.188
  load_time_ms: 3.32
  training_iteration_time_ms: 57747.999
  update_time_ms: 2.673
timesteps_total: 214500
training_iteration: 13

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.04054054054054054
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6381578947368421
  reward for individual goal_min: 0.0
episode_len_mean: 239.32
episode_reward_max: 2.0
episode_reward_mean: 0.7
episode_reward_min: 0.0
episodes_this_iter: 69
episodes_total: 797
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.22
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.00795292854309
time_total_s: 753.2299444675446
timers:
  learn_throughput: 343.728
  learn_time_ms: 48003.099
  load_throughput: 4671347.688
  load_time_ms: 3.532
  training_iteration_time_ms: 62180.166
  update_time_ms: 2.708
timesteps_total: 198000
training_iteration: 12

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5961538461538461
  reward for individual goal_min: 0.0
episode_len_mean: 219.15
episode_reward_max: 2.0
episode_reward_mean: 0.8
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 819
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.43
  agent_1: 0.37
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.81067633628845
time_total_s: 764.6849398612976
timers:
  learn_throughput: 336.335
  learn_time_ms: 49058.269
  load_throughput: 4482429.11
  load_time_ms: 3.681
  training_iteration_time_ms: 63292.928
  update_time_ms: 2.779
timesteps_total: 198000
training_iteration: 12

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.55
  reward for individual goal_min: 0.0
episode_len_mean: 235.66
episode_reward_max: 2.0
episode_reward_mean: 0.69
episode_reward_min: 0.0
episodes_this_iter: 68
episodes_total: 815
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.43
  agent_1: 0.26
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.26538157463074
time_total_s: 766.401026725769
timers:
  learn_throughput: 336.597
  learn_time_ms: 49019.997
  load_throughput: 4736601.852
  load_time_ms: 3.484
  training_iteration_time_ms: 63381.599
  update_time_ms: 10.776
timesteps_total: 198000
training_iteration: 12

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1282051282051282
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6708860759493671
  reward for individual goal_min: 0.0
episode_len_mean: 228.95
episode_reward_max: 2.0
episode_reward_mean: 0.84
episode_reward_min: 0.0
episodes_this_iter: 72
episodes_total: 943
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.37
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.411399841308594
time_total_s: 774.0234913825989
timers:
  learn_throughput: 398.042
  learn_time_ms: 41452.925
  load_throughput: 4920092.137
  load_time_ms: 3.354
  training_iteration_time_ms: 54220.246
  update_time_ms: 2.629
timesteps_total: 231000
training_iteration: 14

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.06338028169014084
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.4305555555555556
  reward for individual goal_min: 0.0
episode_len_mean: 242.95
episode_reward_max: 2.0
episode_reward_mean: 0.51
episode_reward_min: 0.0
episodes_this_iter: 68
episodes_total: 734
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.27
  agent_1: 0.24
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 66.3773787021637
time_total_s: 767.9442789554596
timers:
  learn_throughput: 306.569
  learn_time_ms: 53821.498
  load_throughput: 4598039.758
  load_time_ms: 3.588
  training_iteration_time_ms: 69544.793
  update_time_ms: 2.829
timesteps_total: 181500
training_iteration: 11

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6204819277108434
  reward for individual goal_min: 0.0
episode_len_mean: 228.3
episode_reward_max: 2.0
episode_reward_mean: 0.9
episode_reward_min: 0.0
episodes_this_iter: 71
episodes_total: 949
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.49
  agent_1: 0.41
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.12415814399719
time_total_s: 782.1905024051666
timers:
  learn_throughput: 393.992
  learn_time_ms: 41878.971
  load_throughput: 5037597.886
  load_time_ms: 3.275
  training_iteration_time_ms: 54640.957
  update_time_ms: 2.614
timesteps_total: 231000
training_iteration: 14

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6037735849056604
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5212765957446809
  reward for individual goal_min: 0.0
episode_len_mean: 203.48
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 841
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.320865869522095
time_total_s: 789.4580481052399
timers:
  learn_throughput: 326.183
  learn_time_ms: 50585.078
  load_throughput: 4820938.329
  load_time_ms: 3.423
  training_iteration_time_ms: 65477.835
  update_time_ms: 2.891
timesteps_total: 198000
training_iteration: 12

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7708333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.625
  reward for individual goal_min: 0.0
episode_len_mean: 173.0
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 1172
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.60403347015381
time_total_s: 799.6124575138092
timers:
  learn_throughput: 451.406
  learn_time_ms: 36552.421
  load_throughput: 5385137.379
  load_time_ms: 3.064
  training_iteration_time_ms: 48378.074
  update_time_ms: 2.523
timesteps_total: 264000
training_iteration: 16

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.13636363636363635
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5753424657534246
  reward for individual goal_min: 0.0
episode_len_mean: 234.28
episode_reward_max: 2.0
episode_reward_mean: 0.72
episode_reward_min: 0.0
episodes_this_iter: 70
episodes_total: 1007
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.39
  agent_1: 0.33
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.53120279312134
time_total_s: 810.1266443729401
timers:
  learn_throughput: 404.513
  learn_time_ms: 40789.755
  load_throughput: 5010208.934
  load_time_ms: 3.293
  training_iteration_time_ms: 53331.84
  update_time_ms: 2.958
timesteps_total: 247500
training_iteration: 15

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.07432432432432433
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.2986111111111111
  reward for individual goal_min: 0.0
episode_len_mean: 244.43
episode_reward_max: 2.0
episode_reward_mean: 0.35
episode_reward_min: 0.0
episodes_this_iter: 69
episodes_total: 730
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.2
  agent_1: 0.15
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 69.33818197250366
time_total_s: 783.6581475734711
timers:
  learn_throughput: 299.104
  learn_time_ms: 55164.672
  load_throughput: 4531413.717
  load_time_ms: 3.641
  training_iteration_time_ms: 70995.468
  update_time_ms: 2.775
timesteps_total: 181500
training_iteration: 11

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3082191780821918
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5652173913043478
  reward for individual goal_min: 0.0
episode_len_mean: 221.27
episode_reward_max: 2.0
episode_reward_mean: 0.85
episode_reward_min: 0.0
episodes_this_iter: 75
episodes_total: 883
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.45
  agent_1: 0.4
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.65616250038147
time_total_s: 799.0730028152466
timers:
  learn_throughput: 357.271
  learn_time_ms: 46183.401
  load_throughput: 4760778.994
  load_time_ms: 3.466
  training_iteration_time_ms: 60002.705
  update_time_ms: 2.66
timesteps_total: 214500
training_iteration: 13

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16447368421052633
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.538961038961039
  reward for individual goal_min: 0.0
episode_len_mean: 230.72
episode_reward_max: 2.0
episode_reward_mean: 0.72
episode_reward_min: 0.0
episodes_this_iter: 71
episodes_total: 878
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.37
  agent_1: 0.35
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.90515375137329
time_total_s: 783.8482565879822
timers:
  learn_throughput: 361.314
  learn_time_ms: 45666.713
  load_throughput: 4852033.961
  load_time_ms: 3.401
  training_iteration_time_ms: 59335.974
  update_time_ms: 2.637
timesteps_total: 214500
training_iteration: 13

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6923076923076923
  reward for individual goal_min: 0.0
episode_len_mean: 180.78
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 1024
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.2536096572876
time_total_s: 812.7241604328156
timers:
  learn_throughput: 373.742
  learn_time_ms: 44148.108
  load_throughput: 5064731.893
  load_time_ms: 3.258
  training_iteration_time_ms: 57135.031
  update_time_ms: 2.605
timesteps_total: 231000
training_iteration: 14

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.11038961038961038
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.3783783783783784
  reward for individual goal_min: 0.0
episode_len_mean: 240.83
episode_reward_max: 2.0
episode_reward_mean: 0.5
episode_reward_min: 0.0
episodes_this_iter: 68
episodes_total: 867
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.24
  agent_1: 0.26
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.86980628967285
time_total_s: 787.4185743331909
timers:
  learn_throughput: 360.362
  learn_time_ms: 45787.279
  load_throughput: 4850503.652
  load_time_ms: 3.402
  training_iteration_time_ms: 59666.092
  update_time_ms: 2.601
timesteps_total: 214500
training_iteration: 13

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.06493506493506493
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6027397260273972
  reward for individual goal_min: 0.0
episode_len_mean: 241.73
episode_reward_max: 2.0
episode_reward_mean: 0.65
episode_reward_min: 0.0
episodes_this_iter: 68
episodes_total: 865
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.18
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.284990310668945
time_total_s: 811.5149347782135
timers:
  learn_throughput: 346.955
  learn_time_ms: 47556.585
  load_throughput: 4650504.388
  load_time_ms: 3.548
  training_iteration_time_ms: 61709.673
  update_time_ms: 2.691
timesteps_total: 214500
training_iteration: 13

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18867924528301888
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6170212765957447
  reward for individual goal_min: 0.0
episode_len_mean: 222.76
episode_reward_max: 2.0
episode_reward_mean: 0.78
episode_reward_min: 0.0
episodes_this_iter: 71
episodes_total: 890
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.44
  agent_1: 0.34
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.20316028594971
time_total_s: 825.8881001472473
timers:
  learn_throughput: 339.013
  learn_time_ms: 48670.671
  load_throughput: 4431113.445
  load_time_ms: 3.724
  training_iteration_time_ms: 62890.364
  update_time_ms: 2.768
timesteps_total: 214500
training_iteration: 13

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17142857142857143
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6231884057971014
  reward for individual goal_min: 0.0
episode_len_mean: 232.88
episode_reward_max: 2.0
episode_reward_mean: 0.8
episode_reward_min: 0.0
episodes_this_iter: 72
episodes_total: 887
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.42
  agent_1: 0.38
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.20803165435791
time_total_s: 824.609058380127
timers:
  learn_throughput: 340.846
  learn_time_ms: 48408.958
  load_throughput: 4755348.684
  load_time_ms: 3.47
  training_iteration_time_ms: 62592.307
  update_time_ms: 10.809
timesteps_total: 214500
training_iteration: 13

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15517241379310345
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7397260273972602
  reward for individual goal_min: 0.0
episode_len_mean: 225.14
episode_reward_max: 2.0
episode_reward_mean: 0.84
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 1017
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.36
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.80670714378357
time_total_s: 826.8301985263824
timers:
  learn_throughput: 401.748
  learn_time_ms: 41070.485
  load_throughput: 4840089.24
  load_time_ms: 3.409
  training_iteration_time_ms: 53759.229
  update_time_ms: 2.628
timesteps_total: 247500
training_iteration: 15

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21621621621621623
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6363636363636364
  reward for individual goal_min: 0.0
episode_len_mean: 231.23
episode_reward_max: 2.0
episode_reward_mean: 0.89
episode_reward_min: 0.0
episodes_this_iter: 71
episodes_total: 1020
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.49
  agent_1: 0.4
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.87623715400696
time_total_s: 834.0667395591736
timers:
  learn_throughput: 397.504
  learn_time_ms: 41509.024
  load_throughput: 5075727.079
  load_time_ms: 3.251
  training_iteration_time_ms: 54135.857
  update_time_ms: 2.604
timesteps_total: 247500
training_iteration: 15

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8260869565217391
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6296296296296297
  reward for individual goal_min: 0.5
episode_len_mean: 168.9
episode_reward_max: 2.0
episode_reward_mean: 1.44
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 1270
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.967700719833374
time_total_s: 843.5801582336426
timers:
  learn_throughput: 459.523
  learn_time_ms: 35906.819
  load_throughput: 5456596.704
  load_time_ms: 3.024
  training_iteration_time_ms: 47570.059
  update_time_ms: 2.531
timesteps_total: 280500
training_iteration: 17

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7142857142857143
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5113636363636364
  reward for individual goal_min: 0.5
episode_len_mean: 187.78
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 929
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.19956684112549
time_total_s: 850.6576149463654
timers:
  learn_throughput: 329.017
  learn_time_ms: 50149.457
  load_throughput: 4766287.371
  load_time_ms: 3.462
  training_iteration_time_ms: 64926.823
  update_time_ms: 2.887
timesteps_total: 214500
training_iteration: 13

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.10810810810810811
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5328947368421053
  reward for individual goal_min: 0.0
episode_len_mean: 238.08
episode_reward_max: 2.0
episode_reward_mean: 0.65
episode_reward_min: 0.0
episodes_this_iter: 70
episodes_total: 804
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.4
  agent_1: 0.25
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.34179997444153
time_total_s: 833.2860789299011
timers:
  learn_throughput: 310.314
  learn_time_ms: 53171.932
  load_throughput: 4494714.364
  load_time_ms: 3.671
  training_iteration_time_ms: 68845.475
  update_time_ms: 2.857
timesteps_total: 198000
training_iteration: 12

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18181818181818182
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5277777777777778
  reward for individual goal_min: 0.0
episode_len_mean: 230.67
episode_reward_max: 2.0
episode_reward_mean: 0.72
episode_reward_min: 0.0
episodes_this_iter: 71
episodes_total: 1078
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.33
  agent_1: 0.39
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.089940547943115
time_total_s: 861.2165849208832
timers:
  learn_throughput: 407.166
  learn_time_ms: 40524.057
  load_throughput: 4931591.939
  load_time_ms: 3.346
  training_iteration_time_ms: 52961.937
  update_time_ms: 2.954
timesteps_total: 264000
training_iteration: 16

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29605263157894735
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.58125
  reward for individual goal_min: 0.0
episode_len_mean: 224.49
episode_reward_max: 2.0
episode_reward_mean: 0.91
episode_reward_min: 0.0
episodes_this_iter: 72
episodes_total: 955
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.43
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.50981688499451
time_total_s: 854.5828197002411
timers:
  learn_throughput: 362.066
  learn_time_ms: 45571.774
  load_throughput: 4733362.242
  load_time_ms: 3.486
  training_iteration_time_ms: 59284.926
  update_time_ms: 2.678
timesteps_total: 231000
training_iteration: 14

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6486486486486487
  reward for individual goal_min: 0.0
episode_len_mean: 227.07
episode_reward_max: 2.0
episode_reward_mean: 0.8
episode_reward_min: 0.0
episodes_this_iter: 75
episodes_total: 953
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.41
  agent_1: 0.39
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.48999834060669
time_total_s: 837.3382549285889
timers:
  learn_throughput: 366.094
  learn_time_ms: 45070.412
  load_throughput: 4819528.257
  load_time_ms: 3.424
  training_iteration_time_ms: 58636.111
  update_time_ms: 2.642
timesteps_total: 231000
training_iteration: 14

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6851851851851852
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.75
  reward for individual goal_min: 0.0
episode_len_mean: 166.19
episode_reward_max: 2.0
episode_reward_mean: 1.43
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 1124
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.948697090148926
time_total_s: 867.6728575229645
timers:
  learn_throughput: 377.428
  learn_time_ms: 43716.954
  load_throughput: 5079564.311
  load_time_ms: 3.248
  training_iteration_time_ms: 56605.481
  update_time_ms: 2.605
timesteps_total: 247500
training_iteration: 15

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1144578313253012
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.43283582089552236
  reward for individual goal_min: 0.0
episode_len_mean: 238.85
episode_reward_max: 2.0
episode_reward_mean: 0.51
episode_reward_min: 0.0
episodes_this_iter: 68
episodes_total: 935
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.22
  agent_1: 0.29
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.584797859191895
time_total_s: 845.0033721923828
timers:
  learn_throughput: 362.33
  learn_time_ms: 45538.638
  load_throughput: 4796679.766
  load_time_ms: 3.44
  training_iteration_time_ms: 59371.898
  update_time_ms: 2.618
timesteps_total: 231000
training_iteration: 14

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.13013698630136986
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.42567567567567566
  reward for individual goal_min: 0.0
episode_len_mean: 242.39
episode_reward_max: 2.0
episode_reward_mean: 0.54
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 797
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.28
  agent_1: 0.26
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 66.94505405426025
time_total_s: 850.6032016277313
timers:
  learn_throughput: 302.447
  learn_time_ms: 54554.983
  load_throughput: 4500384.71
  load_time_ms: 3.666
  training_iteration_time_ms: 70350.863
  update_time_ms: 2.778
timesteps_total: 198000
training_iteration: 12

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.06707317073170732
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5454545454545454
  reward for individual goal_min: 0.0
episode_len_mean: 243.1
episode_reward_max: 2.0
episode_reward_mean: 0.55
episode_reward_min: 0.0
episodes_this_iter: 68
episodes_total: 933
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.15
  agent_1: 0.4
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.46667790412903
time_total_s: 870.9816126823425
timers:
  learn_throughput: 349.839
  learn_time_ms: 47164.61
  load_throughput: 4633597.086
  load_time_ms: 3.561
  training_iteration_time_ms: 61200.012
  update_time_ms: 2.654
timesteps_total: 231000
training_iteration: 14

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8561643835616438
  reward for individual goal_min: 0.0
episode_len_mean: 219.7
episode_reward_max: 2.0
episode_reward_mean: 0.98
episode_reward_min: 0.0
episodes_this_iter: 75
episodes_total: 1092
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.4
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.47870945930481
time_total_s: 876.3089079856873
timers:
  learn_throughput: 407.101
  learn_time_ms: 40530.469
  load_throughput: 4828135.818
  load_time_ms: 3.417
  training_iteration_time_ms: 53124.928
  update_time_ms: 2.604
timesteps_total: 264000
training_iteration: 16

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8775510204081632
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5784313725490197
  reward for individual goal_min: 0.0
episode_len_mean: 169.09
episode_reward_max: 2.0
episode_reward_mean: 1.45
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 1368
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.18073797225952
time_total_s: 886.7608962059021
timers:
  learn_throughput: 469.851
  learn_time_ms: 35117.519
  load_throughput: 5510866.771
  load_time_ms: 2.994
  training_iteration_time_ms: 46628.094
  update_time_ms: 2.523
timesteps_total: 297000
training_iteration: 18

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1953125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6036585365853658
  reward for individual goal_min: 0.0
episode_len_mean: 227.42
episode_reward_max: 2.0
episode_reward_mean: 0.89
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 960
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.37
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.91980338096619
time_total_s: 879.5288617610931
timers:
  learn_throughput: 347.039
  learn_time_ms: 47545.023
  load_throughput: 4756983.015
  load_time_ms: 3.469
  training_iteration_time_ms: 61499.155
  update_time_ms: 10.831
timesteps_total: 231000
training_iteration: 14

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26282051282051283
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7291666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 223.05
episode_reward_max: 2.0
episode_reward_mean: 0.99
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 1096
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.784382343292236
time_total_s: 884.8511219024658
timers:
  learn_throughput: 402.176
  learn_time_ms: 41026.836
  load_throughput: 5033347.831
  load_time_ms: 3.278
  training_iteration_time_ms: 53553.67
  update_time_ms: 2.587
timesteps_total: 264000
training_iteration: 16

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5777777777777777
  reward for individual goal_min: 0.0
episode_len_mean: 221.46
episode_reward_max: 2.0
episode_reward_mean: 0.82
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 968
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.47
  agent_1: 0.35
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.067898988723755
time_total_s: 885.9559991359711
timers:
  learn_throughput: 341.935
  learn_time_ms: 48254.855
  load_throughput: 4335374.457
  load_time_ms: 3.806
  training_iteration_time_ms: 62401.943
  update_time_ms: 2.733
timesteps_total: 231000
training_iteration: 14

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5416666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 228.07
episode_reward_max: 2.0
episode_reward_mean: 0.76
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 1151
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.42
  agent_1: 0.34
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.60626697540283
time_total_s: 910.822851896286
timers:
  learn_throughput: 410.851
  learn_time_ms: 40160.541
  load_throughput: 4918623.474
  load_time_ms: 3.355
  training_iteration_time_ms: 52515.256
  update_time_ms: 2.931
timesteps_total: 280500
training_iteration: 17

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5227272727272727
  reward for individual goal_min: 0.5
episode_len_mean: 186.87
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 1019
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.49226140975952
time_total_s: 911.1498763561249
timers:
  learn_throughput: 334.617
  learn_time_ms: 49310.151
  load_throughput: 4817045.848
  load_time_ms: 3.425
  training_iteration_time_ms: 63998.448
  update_time_ms: 2.885
timesteps_total: 231000
training_iteration: 14

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17391304347826086
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5493827160493827
  reward for individual goal_min: 0.0
episode_len_mean: 238.64
episode_reward_max: 2.0
episode_reward_mean: 0.75
episode_reward_min: 0.0
episodes_this_iter: 71
episodes_total: 875
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.45
  agent_1: 0.3
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 64.3560516834259
time_total_s: 897.642130613327
timers:
  learn_throughput: 314.073
  learn_time_ms: 52535.637
  load_throughput: 4411482.627
  load_time_ms: 3.74
  training_iteration_time_ms: 68141.31
  update_time_ms: 2.861
timesteps_total: 214500
training_iteration: 13

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3493150684931507
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5405405405405406
  reward for individual goal_min: 0.0
episode_len_mean: 219.67
episode_reward_max: 2.0
episode_reward_mean: 0.92
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 1033
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.46
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.422099113464355
time_total_s: 910.0049188137054
timers:
  learn_throughput: 367.036
  learn_time_ms: 44954.731
  load_throughput: 4743582.053
  load_time_ms: 3.478
  training_iteration_time_ms: 58503.944
  update_time_ms: 2.662
timesteps_total: 247500
training_iteration: 15

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6964285714285714
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7727272727272727
  reward for individual goal_min: 0.5
episode_len_mean: 162.94
episode_reward_max: 2.0
episode_reward_mean: 1.46
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 1224
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.8
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.709542989730835
time_total_s: 922.3824005126953
timers:
  learn_throughput: 381.451
  learn_time_ms: 43255.936
  load_throughput: 5061472.234
  load_time_ms: 3.26
  training_iteration_time_ms: 56045.215
  update_time_ms: 2.588
timesteps_total: 264000
training_iteration: 16

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6811594202898551
  reward for individual goal_min: 0.0
episode_len_mean: 220.33
episode_reward_max: 2.0
episode_reward_mean: 0.89
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 1027
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.48
  agent_1: 0.41
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.860028982162476
time_total_s: 896.1982839107513
timers:
  learn_throughput: 367.855
  learn_time_ms: 44854.616
  load_throughput: 4823525.607
  load_time_ms: 3.421
  training_iteration_time_ms: 58351.763
  update_time_ms: 2.632
timesteps_total: 247500
training_iteration: 15

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8518518518518519
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6372549019607843
  reward for individual goal_min: 0.5
episode_len_mean: 157.63809523809525
episode_reward_max: 2.0
episode_reward_mean: 1.4952380952380953
episode_reward_min: 0.0
episodes_this_iter: 105
episodes_total: 1473
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7619047619047619
  agent_1: 0.7333333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.67793655395508
time_total_s: 930.4388327598572
timers:
  learn_throughput: 477.348
  learn_time_ms: 34565.955
  load_throughput: 5602682.577
  load_time_ms: 2.945
  training_iteration_time_ms: 46008.19
  update_time_ms: 2.522
timesteps_total: 313500
training_iteration: 19

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.810126582278481
  reward for individual goal_min: 0.0
episode_len_mean: 209.76
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 1173
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.59041905403137
time_total_s: 925.8993270397186
timers:
  learn_throughput: 410.035
  learn_time_ms: 40240.486
  load_throughput: 4811085.111
  load_time_ms: 3.43
  training_iteration_time_ms: 52711.422
  update_time_ms: 2.587
timesteps_total: 280500
training_iteration: 17

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.375
  reward for individual goal_min: 0.0
episode_len_mean: 238.58
episode_reward_max: 2.0
episode_reward_mean: 0.54
episode_reward_min: 0.0
episodes_this_iter: 70
episodes_total: 1005
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.2
  agent_1: 0.34
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.7299964427948
time_total_s: 906.7333686351776
timers:
  learn_throughput: 363.737
  learn_time_ms: 45362.498
  load_throughput: 4796447.057
  load_time_ms: 3.44
  training_iteration_time_ms: 59200.415
  update_time_ms: 2.615
timesteps_total: 247500
training_iteration: 15

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.09146341463414634
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6015625
  reward for individual goal_min: 0.0
episode_len_mean: 242.28
episode_reward_max: 2.0
episode_reward_mean: 0.61
episode_reward_min: 0.0
episodes_this_iter: 68
episodes_total: 1001
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.24
  agent_1: 0.37
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.16397571563721
time_total_s: 928.1455883979797
timers:
  learn_throughput: 353.957
  learn_time_ms: 46615.847
  load_throughput: 4684818.716
  load_time_ms: 3.522
  training_iteration_time_ms: 60589.135
  update_time_ms: 2.621
timesteps_total: 247500
training_iteration: 15

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1518987341772152
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.3961038961038961
  reward for individual goal_min: 0.0
episode_len_mean: 245.53
episode_reward_max: 2.0
episode_reward_mean: 0.54
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 863
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.24
  agent_1: 0.3
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 66.56614422798157
time_total_s: 917.1693458557129
timers:
  learn_throughput: 305.145
  learn_time_ms: 54072.574
  load_throughput: 4401970.283
  load_time_ms: 3.748
  training_iteration_time_ms: 69851.949
  update_time_ms: 2.969
timesteps_total: 214500
training_iteration: 13

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6917808219178082
  reward for individual goal_min: 0.0
episode_len_mean: 215.24
episode_reward_max: 2.0
episode_reward_mean: 1.04
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 1172
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.626765727996826
time_total_s: 937.4778876304626
timers:
  learn_throughput: 403.217
  learn_time_ms: 40920.912
  load_throughput: 5027935.747
  load_time_ms: 3.282
  training_iteration_time_ms: 53335.749
  update_time_ms: 2.593
timesteps_total: 280500
training_iteration: 17

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21621621621621623
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6081081081081081
  reward for individual goal_min: 0.0
episode_len_mean: 229.77
episode_reward_max: 2.0
episode_reward_mean: 0.81
episode_reward_min: 0.0
episodes_this_iter: 70
episodes_total: 1030
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.45
  agent_1: 0.36
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.94672918319702
time_total_s: 938.4755909442902
timers:
  learn_throughput: 351.03
  learn_time_ms: 47004.475
  load_throughput: 4816408.886
  load_time_ms: 3.426
  training_iteration_time_ms: 60815.892
  update_time_ms: 10.832
timesteps_total: 247500
training_iteration: 15

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.10897435897435898
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6266666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 233.97
episode_reward_max: 2.0
episode_reward_mean: 0.74
episode_reward_min: 0.0
episodes_this_iter: 72
episodes_total: 1223
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.45
  agent_1: 0.29
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.253708362579346
time_total_s: 959.0765602588654
timers:
  learn_throughput: 416.542
  learn_time_ms: 39611.885
  load_throughput: 4919043.002
  load_time_ms: 3.354
  training_iteration_time_ms: 51893.578
  update_time_ms: 2.905
timesteps_total: 297000
training_iteration: 18

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3137254901960784
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6632653061224489
  reward for individual goal_min: 0.0
episode_len_mean: 202.5
episode_reward_max: 2.0
episode_reward_mean: 0.97
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 1051
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.43
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.12705373764038
time_total_s: 945.0830528736115
timers:
  learn_throughput: 345.051
  learn_time_ms: 47818.959
  load_throughput: 4299952.531
  load_time_ms: 3.837
  training_iteration_time_ms: 61861.537
  update_time_ms: 2.726
timesteps_total: 247500
training_iteration: 15

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7916666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 0.5
  reward for individual goal_mean: 0.5
  reward for individual goal_min: 0.5
episode_len_mean: 185.19
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 1107
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.134161710739136
time_total_s: 968.284038066864
timers:
  learn_throughput: 341.294
  learn_time_ms: 48345.346
  load_throughput: 4813360.505
  load_time_ms: 3.428
  training_iteration_time_ms: 62806.422
  update_time_ms: 2.862
timesteps_total: 247500
training_iteration: 15

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.58
  reward for individual goal_min: 0.0
episode_len_mean: 225.43
episode_reward_max: 2.0
episode_reward_mean: 0.89
episode_reward_min: 0.0
episodes_this_iter: 72
episodes_total: 1105
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.47
  agent_1: 0.42
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.82223343849182
time_total_s: 965.8271522521973
timers:
  learn_throughput: 371.302
  learn_time_ms: 44438.243
  load_throughput: 4745370.991
  load_time_ms: 3.477
  training_iteration_time_ms: 57922.259
  update_time_ms: 2.685
timesteps_total: 264000
training_iteration: 16

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7547169811320755
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7352941176470589
  reward for individual goal_min: 0.0
episode_len_mean: 159.08653846153845
episode_reward_max: 2.0
episode_reward_mean: 1.4903846153846154
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 1328
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7211538461538461
  agent_1: 0.7692307692307693
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.20038986206055
time_total_s: 977.5827903747559
timers:
  learn_throughput: 384.594
  learn_time_ms: 42902.406
  load_throughput: 5097147.908
  load_time_ms: 3.237
  training_iteration_time_ms: 55641.73
  update_time_ms: 2.596
timesteps_total: 280500
training_iteration: 17

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6865671641791045
  reward for individual goal_min: 0.0
episode_len_mean: 221.23
episode_reward_max: 2.0
episode_reward_mean: 0.87
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 1100
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.44
  agent_1: 0.43
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.280795097351074
time_total_s: 952.4790790081024
timers:
  learn_throughput: 373.833
  learn_time_ms: 44137.379
  load_throughput: 4867048.026
  load_time_ms: 3.39
  training_iteration_time_ms: 57614.417
  update_time_ms: 2.628
timesteps_total: 264000
training_iteration: 16

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22972972972972974
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.810126582278481
  reward for individual goal_min: 0.0
episode_len_mean: 214.11
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 1247
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.43
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.74829459190369
time_total_s: 974.6476216316223
timers:
  learn_throughput: 413.887
  learn_time_ms: 39865.993
  load_throughput: 4831911.302
  load_time_ms: 3.415
  training_iteration_time_ms: 52215.343
  update_time_ms: 2.593
timesteps_total: 297000
training_iteration: 18

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23417721518987342
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6736111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 224.74
episode_reward_max: 2.0
episode_reward_mean: 0.88
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 949
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.37
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 66.59156036376953
time_total_s: 964.2336909770966
timers:
  learn_throughput: 317.191
  learn_time_ms: 52019.168
  load_throughput: 4375834.846
  load_time_ms: 3.771
  training_iteration_time_ms: 67583.392
  update_time_ms: 2.85
timesteps_total: 231000
training_iteration: 14

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7333333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 143.0
episode_reward_max: 2.0
episode_reward_mean: 1.6333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7833333333333333
  agent_1: 0.85
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9310344827586207
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6428571428571429
  reward for individual goal_min: 0.0
episode_len_mean: 147.07894736842104
episode_reward_max: 2.0
episode_reward_mean: 1.5789473684210527
episode_reward_min: 0.0
episodes_this_iter: 114
episodes_total: 1587
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7719298245614035
  agent_1: 0.8070175438596491
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.421802282333374
time_total_s: 987.8606350421906
timers:
  learn_throughput: 484.722
  learn_time_ms: 34040.096
  load_throughput: 5627420.394
  load_time_ms: 2.932
  training_iteration_time_ms: 45386.477
  update_time_ms: 2.502
timesteps_total: 330000
training_iteration: 20

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-172kuwc_58/checkpoint_000020/checkpoint-20
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17088607594936708
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.48026315789473684
  reward for individual goal_min: 0.0
episode_len_mean: 233.23
episode_reward_max: 2.0
episode_reward_mean: 0.63
episode_reward_min: 0.0
episodes_this_iter: 72
episodes_total: 1077
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.26
  agent_1: 0.37
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.90133595466614
time_total_s: 962.6347045898438
timers:
  learn_throughput: 368.218
  learn_time_ms: 44810.472
  load_throughput: 4805105.745
  load_time_ms: 3.434
  training_iteration_time_ms: 58616.246
  update_time_ms: 2.631
timesteps_total: 264000
training_iteration: 16

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.06329113924050633
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.676829268292683
  reward for individual goal_min: 0.0
episode_len_mean: 241.37
episode_reward_max: 2.0
episode_reward_mean: 0.76
episode_reward_min: 0.0
episodes_this_iter: 70
episodes_total: 1071
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.27
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.45553803443909
time_total_s: 984.6011264324188
timers:
  learn_throughput: 357.465
  learn_time_ms: 46158.418
  load_throughput: 4656574.889
  load_time_ms: 3.543
  training_iteration_time_ms: 59993.685
  update_time_ms: 2.637
timesteps_total: 264000
training_iteration: 16

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22435897435897437
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6597222222222222
  reward for individual goal_min: 0.0
episode_len_mean: 225.55
episode_reward_max: 2.0
episode_reward_mean: 0.91
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 1246
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.39
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.83157014846802
time_total_s: 987.3094577789307
timers:
  learn_throughput: 408.04
  learn_time_ms: 40437.214
  load_throughput: 4998737.134
  load_time_ms: 3.301
  training_iteration_time_ms: 52707.888
  update_time_ms: 2.612
timesteps_total: 297000
training_iteration: 18

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21518987341772153
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.56
  reward for individual goal_min: 0.0
episode_len_mean: 228.35
episode_reward_max: 2.0
episode_reward_mean: 0.83
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 1296
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.48
  agent_1: 0.35
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.07408595085144
time_total_s: 1008.1506462097168
timers:
  learn_throughput: 420.441
  learn_time_ms: 39244.535
  load_throughput: 4903984.921
  load_time_ms: 3.365
  training_iteration_time_ms: 51436.61
  update_time_ms: 2.898
timesteps_total: 313500
training_iteration: 19

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2692307692307692
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6865671641791045
  reward for individual goal_min: 0.0
episode_len_mean: 216.07
episode_reward_max: 2.0
episode_reward_mean: 0.94
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 1107
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.49
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.23852825164795
time_total_s: 997.7141191959381
timers:
  learn_throughput: 352.896
  learn_time_ms: 46755.997
  load_throughput: 4786892.249
  load_time_ms: 3.447
  training_iteration_time_ms: 60497.547
  update_time_ms: 10.839
timesteps_total: 264000
training_iteration: 16

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.34415584415584416
  reward for individual goal_min: 0.0
episode_len_mean: 239.51
episode_reward_max: 2.0
episode_reward_mean: 0.55
episode_reward_min: 0.0
episodes_this_iter: 70
episodes_total: 933
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.28
  agent_1: 0.27
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 70.07042789459229
time_total_s: 987.2397737503052
timers:
  learn_throughput: 306.229
  learn_time_ms: 53881.19
  load_throughput: 4312438.684
  load_time_ms: 3.826
  training_iteration_time_ms: 69645.266
  update_time_ms: 2.984
timesteps_total: 231000
training_iteration: 14

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.46551724137931033
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6547619047619048
  reward for individual goal_min: 0.0
episode_len_mean: 199.57
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 1134
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.002800703048706
time_total_s: 1005.0858535766602
timers:
  learn_throughput: 347.9
  learn_time_ms: 47427.367
  load_throughput: 4296242.108
  load_time_ms: 3.841
  training_iteration_time_ms: 61483.438
  update_time_ms: 2.691
timesteps_total: 264000
training_iteration: 16

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6458333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5384615384615384
  reward for individual goal_min: 0.5
episode_len_mean: 191.5
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 1192
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.74515724182129
time_total_s: 1027.0291953086853
timers:
  learn_throughput: 345.811
  learn_time_ms: 47713.965
  load_throughput: 4808912.113
  load_time_ms: 3.431
  training_iteration_time_ms: 62129.125
  update_time_ms: 2.81
timesteps_total: 264000
training_iteration: 16

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3466666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6025641025641025
  reward for individual goal_min: 0.0
episode_len_mean: 217.28
episode_reward_max: 2.0
episode_reward_mean: 0.97
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 1183
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.48
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.308817863464355
time_total_s: 1020.1359701156616
timers:
  learn_throughput: 375.705
  learn_time_ms: 43917.423
  load_throughput: 4760353.281
  load_time_ms: 3.466
  training_iteration_time_ms: 57273.548
  update_time_ms: 2.674
timesteps_total: 280500
training_iteration: 17

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14383561643835616
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8614457831325302
  reward for individual goal_min: 0.0
episode_len_mean: 215.8
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 1325
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.47
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.973369121551514
time_total_s: 1021.6209907531738
timers:
  learn_throughput: 420.67
  learn_time_ms: 39223.105
  load_throughput: 4814766.97
  load_time_ms: 3.427
  training_iteration_time_ms: 51428.027
  update_time_ms: 2.613
timesteps_total: 313500
training_iteration: 19

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8653846153846154
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7
  reward for individual goal_min: 0.5
episode_len_mean: 152.18691588785046
episode_reward_max: 2.0
episode_reward_mean: 1.560747663551402
episode_reward_min: 0.0
episodes_this_iter: 107
episodes_total: 1694
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7663551401869159
  agent_1: 0.794392523364486
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.030147075653076
time_total_s: 1030.8907821178436
timers:
  learn_throughput: 493.816
  learn_time_ms: 33413.247
  load_throughput: 5652469.964
  load_time_ms: 2.919
  training_iteration_time_ms: 44638.686
  update_time_ms: 2.428
timesteps_total: 346500
training_iteration: 21

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7547169811320755
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7592592592592593
  reward for individual goal_min: 0.5
episode_len_mean: 151.2429906542056
episode_reward_max: 2.0
episode_reward_mean: 1.514018691588785
episode_reward_min: 0.0
episodes_this_iter: 107
episodes_total: 1435
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6728971962616822
  agent_1: 0.8411214953271028
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.52310609817505
time_total_s: 1031.105896472931
timers:
  learn_throughput: 388.212
  learn_time_ms: 42502.573
  load_throughput: 5093583.967
  load_time_ms: 3.239
  training_iteration_time_ms: 55202.239
  update_time_ms: 2.588
timesteps_total: 297000
training_iteration: 18

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5357142857142857
  reward for individual goal_min: 0.0
episode_len_mean: 231.77
episode_reward_max: 2.0
episode_reward_mean: 0.78
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 1173
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.45
  agent_1: 0.33
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.045390129089355
time_total_s: 1012.5244691371918
timers:
  learn_throughput: 374.33
  learn_time_ms: 44078.696
  load_throughput: 4824938.02
  load_time_ms: 3.42
  training_iteration_time_ms: 57535.075
  update_time_ms: 2.61
timesteps_total: 280500
training_iteration: 17

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16233766233766234
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.72
  reward for individual goal_min: 0.0
episode_len_mean: 220.96
episode_reward_max: 2.0
episode_reward_mean: 0.9
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 1319
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.46
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.456873655319214
time_total_s: 1036.7663314342499
timers:
  learn_throughput: 413.088
  learn_time_ms: 39943.072
  load_throughput: 4965560.945
  load_time_ms: 3.323
  training_iteration_time_ms: 52133.778
  update_time_ms: 2.611
timesteps_total: 313500
training_iteration: 19

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19753086419753085
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6233766233766234
  reward for individual goal_min: 0.0
episode_len_mean: 230.11
episode_reward_max: 2.0
episode_reward_mean: 0.82
episode_reward_min: 0.0
episodes_this_iter: 72
episodes_total: 1021
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.32
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.27314877510071
time_total_s: 1026.5068397521973
timers:
  learn_throughput: 321.686
  learn_time_ms: 51292.302
  load_throughput: 4360120.712
  load_time_ms: 3.784
  training_iteration_time_ms: 66765.666
  update_time_ms: 2.832
timesteps_total: 247500
training_iteration: 15

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.09210526315789473
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7168674698795181
  reward for individual goal_min: 0.0
episode_len_mean: 229.3
episode_reward_max: 2.0
episode_reward_mean: 0.87
episode_reward_min: 0.0
episodes_this_iter: 72
episodes_total: 1143
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.3
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.807024240493774
time_total_s: 1040.4081506729126
timers:
  learn_throughput: 361.136
  learn_time_ms: 45689.109
  load_throughput: 4665584.597
  load_time_ms: 3.537
  training_iteration_time_ms: 59419.336
  update_time_ms: 2.64
timesteps_total: 280500
training_iteration: 17

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22602739726027396
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5069444444444444
  reward for individual goal_min: 0.0
episode_len_mean: 223.41
episode_reward_max: 2.0
episode_reward_mean: 0.74
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 1150
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.28
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.53221035003662
time_total_s: 1024.1669149398804
timers:
  learn_throughput: 368.218
  learn_time_ms: 44810.448
  load_throughput: 4708020.354
  load_time_ms: 3.505
  training_iteration_time_ms: 58607.232
  update_time_ms: 2.615
timesteps_total: 280500
training_iteration: 17

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2621951219512195
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7123287671232876
  reward for individual goal_min: 0.0
episode_len_mean: 220.76
episode_reward_max: 2.0
episode_reward_mean: 0.97
episode_reward_min: 0.0
episodes_this_iter: 75
episodes_total: 1182
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.49
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.724526166915894
time_total_s: 1052.438645362854
timers:
  learn_throughput: 357.317
  learn_time_ms: 46177.443
  load_throughput: 4804672.068
  load_time_ms: 3.434
  training_iteration_time_ms: 59832.431
  update_time_ms: 2.761
timesteps_total: 280500
training_iteration: 17

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.4666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5166666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 201.7
episode_reward_max: 2.0
episode_reward_mean: 0.9833333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.43333333333333335
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5759493670886076
  reward for individual goal_min: 0.0
episode_len_mean: 221.3
episode_reward_max: 2.0
episode_reward_mean: 0.9
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 1370
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.37
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 69.17148518562317
time_total_s: 1077.32213139534
timers:
  learn_throughput: 424.645
  learn_time_ms: 38855.957
  load_throughput: 4909446.742
  load_time_ms: 3.361
  training_iteration_time_ms: 50959.338
  update_time_ms: 2.898
timesteps_total: 330000
training_iteration: 20

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-091_srff7m/checkpoint_000020/checkpoint-20
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.4878048780487805
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7033898305084746
  reward for individual goal_min: 0.0
episode_len_mean: 193.18
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 1222
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.03402519226074
time_total_s: 1063.119878768921
timers:
  learn_throughput: 350.219
  learn_time_ms: 47113.385
  load_throughput: 4305436.447
  load_time_ms: 3.832
  training_iteration_time_ms: 61170.032
  update_time_ms: 2.657
timesteps_total: 280500
training_iteration: 17

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9038461538461539
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6779661016949152
  reward for individual goal_min: 0.5
episode_len_mean: 147.2882882882883
episode_reward_max: 2.0
episode_reward_mean: 1.5675675675675675
episode_reward_min: 0.0
episodes_this_iter: 111
episodes_total: 1805
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8198198198198198
  agent_1: 0.7477477477477478
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.143858671188354
time_total_s: 1073.034640789032
timers:
  learn_throughput: 499.042
  learn_time_ms: 33063.319
  load_throughput: 5719363.652
  load_time_ms: 2.885
  training_iteration_time_ms: 44220.54
  update_time_ms: 2.417
timesteps_total: 363000
training_iteration: 22

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.45774647887323944
  reward for individual goal_min: 0.0
episode_len_mean: 234.33
episode_reward_max: 2.0
episode_reward_mean: 0.71
episode_reward_min: 0.0
episodes_this_iter: 70
episodes_total: 1003
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.31
  agent_1: 0.4
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 69.0744059085846
time_total_s: 1056.3141796588898
timers:
  learn_throughput: 307.44
  learn_time_ms: 53668.961
  load_throughput: 4275433.591
  load_time_ms: 3.859
  training_iteration_time_ms: 69386.064
  update_time_ms: 2.978
timesteps_total: 247500
training_iteration: 15

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6595744680851063
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5283018867924528
  reward for individual goal_min: 0.5
episode_len_mean: 192.65
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 1281
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.11816668510437
time_total_s: 1083.1473619937897
timers:
  learn_throughput: 352.367
  learn_time_ms: 46826.183
  load_throughput: 4761237.539
  load_time_ms: 3.465
  training_iteration_time_ms: 61023.735
  update_time_ms: 2.822
timesteps_total: 280500
training_iteration: 17

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3194444444444444
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5466666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 228.99
episode_reward_max: 2.0
episode_reward_mean: 0.82
episode_reward_min: 0.0
episodes_this_iter: 71
episodes_total: 1254
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.47
  agent_1: 0.35
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.955483198165894
time_total_s: 1076.0914533138275
timers:
  learn_throughput: 377.652
  learn_time_ms: 43691.013
  load_throughput: 4793822.325
  load_time_ms: 3.442
  training_iteration_time_ms: 57035.351
  update_time_ms: 2.675
timesteps_total: 297000
training_iteration: 18

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.803921568627451
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7545454545454545
  reward for individual goal_min: 0.5
episode_len_mean: 156.00943396226415
episode_reward_max: 2.0
episode_reward_mean: 1.5566037735849056
episode_reward_min: 0.0
episodes_this_iter: 106
episodes_total: 1541
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6886792452830188
  agent_1: 0.8679245283018868
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.955209493637085
time_total_s: 1086.061105966568
timers:
  learn_throughput: 390.411
  learn_time_ms: 42263.113
  load_throughput: 5101581.648
  load_time_ms: 3.234
  training_iteration_time_ms: 55000.209
  update_time_ms: 2.599
timesteps_total: 313500
training_iteration: 19

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.5333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8166666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 163.83333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6333333333333333
  agent_1: 0.7166666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2894736842105263
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9318181818181818
  reward for individual goal_min: 0.0
episode_len_mean: 201.31
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 1410
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.26173424720764
time_total_s: 1086.8827250003815
timers:
  learn_throughput: 427.451
  learn_time_ms: 38600.929
  load_throughput: 4843612.237
  load_time_ms: 3.407
  training_iteration_time_ms: 50729.278
  update_time_ms: 2.612
timesteps_total: 330000
training_iteration: 20

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19ecg2ejpn/checkpoint_000020/checkpoint-20
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6267605633802817
  reward for individual goal_min: 0.0
episode_len_mean: 229.39
episode_reward_max: 2.0
episode_reward_mean: 0.86
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 1246
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.47
  agent_1: 0.39
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.99817204475403
time_total_s: 1068.5226411819458
timers:
  learn_throughput: 377.918
  learn_time_ms: 43660.26
  load_throughput: 4833970.077
  load_time_ms: 3.413
  training_iteration_time_ms: 57096.665
  update_time_ms: 2.599
timesteps_total: 297000
training_iteration: 18

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16049382716049382
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.759493670886076
  reward for individual goal_min: 0.0
episode_len_mean: 229.27
episode_reward_max: 2.0
episode_reward_mean: 0.91
episode_reward_min: 0.0
episodes_this_iter: 72
episodes_total: 1215
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.37
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.78557538986206
time_total_s: 1097.1937260627747
timers:
  learn_throughput: 363.803
  learn_time_ms: 45354.216
  load_throughput: 4705331.52
  load_time_ms: 3.507
  training_iteration_time_ms: 59045.232
  update_time_ms: 2.636
timesteps_total: 297000
training_iteration: 18

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.65
  reward for individual goal_min: 0.0
episode_len_mean: 179.03333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.3166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6666666666666666
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3219178082191781
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6558441558441559
  reward for individual goal_min: 0.0
episode_len_mean: 210.48
episode_reward_max: 2.0
episode_reward_mean: 0.98
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 1399
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.20098233222961
time_total_s: 1101.9673137664795
timers:
  learn_throughput: 419.409
  learn_time_ms: 39341.067
  load_throughput: 4951917.342
  load_time_ms: 3.332
  training_iteration_time_ms: 51372.711
  update_time_ms: 2.608
timesteps_total: 330000
training_iteration: 20

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19istq_qwi/checkpoint_000020/checkpoint-20
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2391304347826087
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5342465753424658
  reward for individual goal_min: 0.0
episode_len_mean: 223.72
episode_reward_max: 2.0
episode_reward_mean: 0.82
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 1094
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.44
  agent_1: 0.38
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 64.5587706565857
time_total_s: 1091.065610408783
timers:
  learn_throughput: 324.746
  learn_time_ms: 50808.889
  load_throughput: 4325159.742
  load_time_ms: 3.815
  training_iteration_time_ms: 66310.033
  update_time_ms: 2.815
timesteps_total: 264000
training_iteration: 16

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18831168831168832
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.36666666666666664
  reward for individual goal_min: 0.0
episode_len_mean: 235.62
episode_reward_max: 2.0
episode_reward_mean: 0.57
episode_reward_min: 0.0
episodes_this_iter: 70
episodes_total: 1220
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.27
  agent_1: 0.3
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.07484793663025
time_total_s: 1084.2417628765106
timers:
  learn_throughput: 367.707
  learn_time_ms: 44872.627
  load_throughput: 4654633.11
  load_time_ms: 3.545
  training_iteration_time_ms: 58675.917
  update_time_ms: 2.642
timesteps_total: 297000
training_iteration: 18

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2361111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7792207792207793
  reward for individual goal_min: 0.0
episode_len_mean: 216.55
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 75
episodes_total: 1257
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.451255321502686
time_total_s: 1107.8899006843567
timers:
  learn_throughput: 363.176
  learn_time_ms: 45432.542
  load_throughput: 4891921.68
  load_time_ms: 3.373
  training_iteration_time_ms: 58985.814
  update_time_ms: 2.758
timesteps_total: 297000
training_iteration: 18

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2710843373493976
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6544117647058824
  reward for individual goal_min: 0.0
episode_len_mean: 217.46
episode_reward_max: 2.0
episode_reward_mean: 0.92
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 1446
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.36
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.56922936439514
time_total_s: 1126.891360759735
timers:
  learn_throughput: 427.979
  learn_time_ms: 38553.253
  load_throughput: 4913838.922
  load_time_ms: 3.358
  training_iteration_time_ms: 50636.031
  update_time_ms: 2.888
timesteps_total: 346500
training_iteration: 21

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8656716417910447
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7857142857142857
  reward for individual goal_min: 0.5
episode_len_mean: 127.56923076923077
episode_reward_max: 2.0
episode_reward_mean: 1.6538461538461537
episode_reward_min: 0.0
episodes_this_iter: 130
episodes_total: 1935
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8
  agent_1: 0.8538461538461538
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.14615297317505
time_total_s: 1117.180793762207
timers:
  learn_throughput: 501.93
  learn_time_ms: 32873.092
  load_throughput: 5750730.495
  load_time_ms: 2.869
  training_iteration_time_ms: 43948.324
  update_time_ms: 2.421
timesteps_total: 379500
training_iteration: 23

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.54
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.73
  reward for individual goal_min: 0.5
episode_len_mean: 186.35
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 1310
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.364269733428955
time_total_s: 1120.4841485023499
timers:
  learn_throughput: 353.28
  learn_time_ms: 46705.163
  load_throughput: 4272108.151
  load_time_ms: 3.862
  training_iteration_time_ms: 60710.225
  update_time_ms: 2.622
timesteps_total: 297000
training_iteration: 18

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2887323943661972
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.58125
  reward for individual goal_min: 0.0
episode_len_mean: 226.28
episode_reward_max: 2.0
episode_reward_mean: 0.89
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 1331
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.35
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.94716811180115
time_total_s: 1128.0386214256287
timers:
  learn_throughput: 385.003
  learn_time_ms: 42856.864
  load_throughput: 4784311.036
  load_time_ms: 3.449
  training_iteration_time_ms: 56002.74
  update_time_ms: 2.688
timesteps_total: 313500
training_iteration: 19

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2222222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8881578947368421
  reward for individual goal_min: 0.0
episode_len_mean: 210.43
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 1484
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.11005520820618
time_total_s: 1130.9927802085876
timers:
  learn_throughput: 437.228
  learn_time_ms: 37737.753
  load_throughput: 4794254.046
  load_time_ms: 3.442
  training_iteration_time_ms: 49653.617
  update_time_ms: 2.621
timesteps_total: 346500
training_iteration: 21

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6078431372549019
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5408163265306123
  reward for individual goal_min: 0.5
episode_len_mean: 186.43
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 1368
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.7028386592865
time_total_s: 1142.8502006530762
timers:
  learn_throughput: 356.342
  learn_time_ms: 46303.792
  load_throughput: 4692569.569
  load_time_ms: 3.516
  training_iteration_time_ms: 60416.359
  update_time_ms: 2.767
timesteps_total: 297000
training_iteration: 18

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3092105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5571428571428572
  reward for individual goal_min: 0.0
episode_len_mean: 232.91
episode_reward_max: 2.0
episode_reward_mean: 0.86
episode_reward_min: 0.0
episodes_this_iter: 71
episodes_total: 1074
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.4
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 68.36080121994019
time_total_s: 1124.67498087883
timers:
  learn_throughput: 308.312
  learn_time_ms: 53517.266
  load_throughput: 4220059.149
  load_time_ms: 3.91
  training_iteration_time_ms: 69184.939
  update_time_ms: 2.976
timesteps_total: 264000
training_iteration: 16

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21621621621621623
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6901408450704225
  reward for individual goal_min: 0.0
episode_len_mean: 221.44
episode_reward_max: 2.0
episode_reward_mean: 0.94
episode_reward_min: 0.0
episodes_this_iter: 75
episodes_total: 1321
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.49
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.49326229095459
time_total_s: 1125.0159034729004
timers:
  learn_throughput: 378.889
  learn_time_ms: 43548.349
  load_throughput: 4829045.439
  load_time_ms: 3.417
  training_iteration_time_ms: 56957.596
  update_time_ms: 2.606
timesteps_total: 313500
training_iteration: 19

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22972972972972974
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6604938271604939
  reward for individual goal_min: 0.0
episode_len_mean: 217.97
episode_reward_max: 2.0
episode_reward_mean: 0.93
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 1475
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.48
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.515329360961914
time_total_s: 1148.4826431274414
timers:
  learn_throughput: 425.509
  learn_time_ms: 38777.103
  load_throughput: 4985772.764
  load_time_ms: 3.309
  training_iteration_time_ms: 50703.848
  update_time_ms: 2.602
timesteps_total: 346500
training_iteration: 21

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8
  reward for individual goal_min: 0.5
episode_len_mean: 133.63333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.5666666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.8666666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8767123287671232
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7870370370370371
  reward for individual goal_min: 0.5
episode_len_mean: 132.03149606299212
episode_reward_max: 2.0
episode_reward_mean: 1.6771653543307086
episode_reward_min: 0.0
episodes_this_iter: 127
episodes_total: 1668
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7952755905511811
  agent_1: 0.8818897637795275
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 69.96072602272034
time_total_s: 1156.0218319892883
timers:
  learn_throughput: 392.069
  learn_time_ms: 42084.418
  load_throughput: 5095121.477
  load_time_ms: 3.238
  training_iteration_time_ms: 54855.419
  update_time_ms: 2.592
timesteps_total: 330000
training_iteration: 20

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-18g5xov65h/checkpoint_000020/checkpoint-20
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9016393442622951
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7295081967213115
  reward for individual goal_min: 0.5
episode_len_mean: 136.63114754098362
episode_reward_max: 2.0
episode_reward_mean: 1.6311475409836065
episode_reward_min: 0.0
episodes_this_iter: 122
episodes_total: 2057
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7622950819672131
  agent_1: 0.8688524590163934
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.46365189552307
time_total_s: 1160.64444565773
timers:
  learn_throughput: 505.939
  learn_time_ms: 32612.615
  load_throughput: 5721727.944
  load_time_ms: 2.884
  training_iteration_time_ms: 43628.617
  update_time_ms: 2.439
timesteps_total: 396000
training_iteration: 24

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.64
  reward for individual goal_min: 0.0
episode_len_mean: 231.05
episode_reward_max: 2.0
episode_reward_mean: 0.75
episode_reward_min: 0.0
episodes_this_iter: 70
episodes_total: 1285
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.3
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.08161759376526
time_total_s: 1155.27534365654
timers:
  learn_throughput: 368.891
  learn_time_ms: 44728.636
  load_throughput: 4698495.254
  load_time_ms: 3.512
  training_iteration_time_ms: 58343.191
  update_time_ms: 2.643
timesteps_total: 313500
training_iteration: 19

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6081081081081081
  reward for individual goal_min: 0.0
episode_len_mean: 231.45
episode_reward_max: 2.0
episode_reward_mean: 0.87
episode_reward_min: 0.0
episodes_this_iter: 71
episodes_total: 1517
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.32
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.817806243896484
time_total_s: 1177.7091670036316
timers:
  learn_throughput: 429.113
  learn_time_ms: 38451.442
  load_throughput: 4880743.614
  load_time_ms: 3.381
  training_iteration_time_ms: 50501.531
  update_time_ms: 2.902
timesteps_total: 363000
training_iteration: 22

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.13013698630136986
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.43037974683544306
  reward for individual goal_min: 0.0
episode_len_mean: 235.37
episode_reward_max: 2.0
episode_reward_mean: 0.62
episode_reward_min: 0.0
episodes_this_iter: 70
episodes_total: 1290
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.24
  agent_1: 0.38
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.336060523986816
time_total_s: 1143.5778234004974
timers:
  learn_throughput: 367.667
  learn_time_ms: 44877.514
  load_throughput: 4661970.252
  load_time_ms: 3.539
  training_iteration_time_ms: 58730.467
  update_time_ms: 2.633
timesteps_total: 313500
training_iteration: 19

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25675675675675674
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6438356164383562
  reward for individual goal_min: 0.0
episode_len_mean: 223.47
episode_reward_max: 2.0
episode_reward_mean: 0.88
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 1168
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.43
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.859402656555176
time_total_s: 1152.9250130653381
timers:
  learn_throughput: 329.132
  learn_time_ms: 50131.854
  load_throughput: 4294722.419
  load_time_ms: 3.842
  training_iteration_time_ms: 65512.74
  update_time_ms: 2.828
timesteps_total: 280500
training_iteration: 17

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30128205128205127
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8082191780821918
  reward for individual goal_min: 0.0
episode_len_mean: 219.31
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 1333
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.10774898529053
time_total_s: 1162.9976496696472
timers:
  learn_throughput: 368.659
  learn_time_ms: 44756.783
  load_throughput: 4964029.409
  load_time_ms: 3.324
  training_iteration_time_ms: 58184.244
  update_time_ms: 2.726
timesteps_total: 313500
training_iteration: 19

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7291666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6826923076923077
  reward for individual goal_min: 0.5
episode_len_mean: 178.56
episode_reward_max: 2.0
episode_reward_mean: 1.41
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 1403
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.77
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.08023118972778
time_total_s: 1178.5643796920776
timers:
  learn_throughput: 357.438
  learn_time_ms: 46161.87
  load_throughput: 4279690.306
  load_time_ms: 3.855
  training_iteration_time_ms: 60050.196
  update_time_ms: 2.655
timesteps_total: 313500
training_iteration: 19

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1987179487179487
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8441558441558441
  reward for individual goal_min: 0.0
episode_len_mean: 215.25
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 1562
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.48
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.99054574966431
time_total_s: 1178.983325958252
timers:
  learn_throughput: 440.817
  learn_time_ms: 37430.476
  load_throughput: 4839006.272
  load_time_ms: 3.41
  training_iteration_time_ms: 49203.62
  update_time_ms: 2.62
timesteps_total: 363000
training_iteration: 22

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6481481481481481
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.532608695652174
  reward for individual goal_min: 0.5
episode_len_mean: 179.49
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 1464
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.30333971977234
time_total_s: 1202.1535403728485
timers:
  learn_throughput: 360.589
  learn_time_ms: 45758.452
  load_throughput: 4700952.743
  load_time_ms: 3.51
  training_iteration_time_ms: 59731.102
  update_time_ms: 2.727
timesteps_total: 313500
training_iteration: 19

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.38961038961038963
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6544117647058824
  reward for individual goal_min: 0.0
episode_len_mean: 212.93
episode_reward_max: 2.0
episode_reward_mean: 1.03
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 1552
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.57260823249817
time_total_s: 1197.0552513599396
timers:
  learn_throughput: 431.009
  learn_time_ms: 38282.22
  load_throughput: 5009338.564
  load_time_ms: 3.294
  training_iteration_time_ms: 50155.393
  update_time_ms: 2.612
timesteps_total: 363000
training_iteration: 22

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6666666666666666
  reward for individual goal_min: 0.5
episode_len_mean: 164.48333333333332
episode_reward_max: 2.0
episode_reward_mean: 1.3333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6666666666666666
  agent_1: 0.6666666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34507042253521125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.55625
  reward for individual goal_min: 0.0
episode_len_mean: 219.72
episode_reward_max: 2.0
episode_reward_mean: 0.96
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 1405
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.41
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 68.43999218940735
time_total_s: 1196.478613615036
timers:
  learn_throughput: 390.956
  learn_time_ms: 42204.277
  load_throughput: 4886050.268
  load_time_ms: 3.377
  training_iteration_time_ms: 55280.847
  update_time_ms: 2.663
timesteps_total: 330000
training_iteration: 20

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19mcvnks1j/checkpoint_000020/checkpoint-20
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8688524590163934
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.75
  reward for individual goal_min: 0.5
episode_len_mean: 131.6
episode_reward_max: 2.0
episode_reward_mean: 1.616
episode_reward_min: 0.0
episodes_this_iter: 125
episodes_total: 2182
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.76
  agent_1: 0.856
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.854645013809204
time_total_s: 1204.4990906715393
timers:
  learn_throughput: 508.503
  learn_time_ms: 32448.197
  load_throughput: 5732581.425
  load_time_ms: 2.878
  training_iteration_time_ms: 43506.543
  update_time_ms: 2.44
timesteps_total: 412500
training_iteration: 25

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7132352941176471
  reward for individual goal_min: 0.5
episode_len_mean: 151.07407407407408
episode_reward_max: 2.0
episode_reward_mean: 1.5462962962962963
episode_reward_min: 0.0
episodes_this_iter: 108
episodes_total: 1776
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6666666666666666
  agent_1: 0.8796296296296297
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.45578360557556
time_total_s: 1205.477615594864
timers:
  learn_throughput: 396.373
  learn_time_ms: 41627.411
  load_throughput: 5073159.747
  load_time_ms: 3.252
  training_iteration_time_ms: 54342.684
  update_time_ms: 2.609
timesteps_total: 346500
training_iteration: 21

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2826086956521739
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5555555555555556
  reward for individual goal_min: 0.0
episode_len_mean: 236.22
episode_reward_max: 2.0
episode_reward_mean: 0.84
episode_reward_min: 0.0
episodes_this_iter: 70
episodes_total: 1144
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.36
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 64.63478088378906
time_total_s: 1189.309761762619
timers:
  learn_throughput: 311.547
  learn_time_ms: 52961.59
  load_throughput: 4175249.679
  load_time_ms: 3.952
  training_iteration_time_ms: 68510.356
  update_time_ms: 2.979
timesteps_total: 280500
training_iteration: 17

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2037037037037037
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7028985507246377
  reward for individual goal_min: 0.0
episode_len_mean: 233.05
episode_reward_max: 2.0
episode_reward_mean: 0.84
episode_reward_min: 0.0
episodes_this_iter: 72
episodes_total: 1589
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.32
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.51237487792969
time_total_s: 1226.2215418815613
timers:
  learn_throughput: 434.894
  learn_time_ms: 37940.271
  load_throughput: 4862568.225
  load_time_ms: 3.393
  training_iteration_time_ms: 49925.935
  update_time_ms: 2.91
timesteps_total: 379500
training_iteration: 23

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.5666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6666666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 180.85
episode_reward_max: 2.0
episode_reward_mean: 1.2333333333333334
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.5833333333333334
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22435897435897437
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6776315789473685
  reward for individual goal_min: 0.0
episode_len_mean: 219.91
episode_reward_max: 2.0
episode_reward_mean: 0.96
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 1394
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 77.76014351844788
time_total_s: 1202.7760469913483
timers:
  learn_throughput: 380.338
  learn_time_ms: 43382.515
  load_throughput: 4875929.376
  load_time_ms: 3.384
  training_iteration_time_ms: 56845.618
  update_time_ms: 2.629
timesteps_total: 330000
training_iteration: 20

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-28met_xp4k/checkpoint_000020/checkpoint-20
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7297297297297297
  reward for individual goal_min: 0.0
episode_len_mean: 220.99
episode_reward_max: 2.0
episode_reward_mean: 0.97
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 1241
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.26679754257202
time_total_s: 1215.1918106079102
timers:
  learn_throughput: 332.827
  learn_time_ms: 49575.338
  load_throughput: 4190342.222
  load_time_ms: 3.938
  training_iteration_time_ms: 64933.005
  update_time_ms: 2.852
timesteps_total: 297000
training_iteration: 18

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2662337662337662
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8896103896103896
  reward for individual goal_min: 0.0
episode_len_mean: 197.14
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 1646
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.044869899749756
time_total_s: 1226.0281958580017
timers:
  learn_throughput: 446.524
  learn_time_ms: 36952.089
  load_throughput: 4847955.279
  load_time_ms: 3.403
  training_iteration_time_ms: 48602.954
  update_time_ms: 2.589
timesteps_total: 379500
training_iteration: 23
{'agent_0': 2, 'agent_1': 2}

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.75
  reward for individual goal_min: 0.5
episode_len_mean: 199.01666666666668
episode_reward_max: 2.0
episode_reward_mean: 0.9833333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.36666666666666664
  agent_1: 0.6166666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.12857142857142856
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7948717948717948
  reward for individual goal_min: 0.0
episode_len_mean: 219.18
episode_reward_max: 2.0
episode_reward_mean: 0.97
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 1363
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.39
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 78.40878891944885
time_total_s: 1233.6841325759888
timers:
  learn_throughput: 373.884
  learn_time_ms: 44131.289
  load_throughput: 4736018.395
  load_time_ms: 3.484
  training_iteration_time_ms: 57669.492
  update_time_ms: 2.635
timesteps_total: 330000
training_iteration: 20

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-18pddilqzj/checkpoint_000020/checkpoint-20
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7166666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 166.41666666666666
episode_reward_max: 2.0
episode_reward_mean: 1.55
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.8
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3472222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7865853658536586
  reward for individual goal_min: 0.0
episode_len_mean: 198.7
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 1417
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 74.6799418926239
time_total_s: 1237.6775915622711
timers:
  learn_throughput: 372.572
  learn_time_ms: 44286.741
  load_throughput: 4919672.429
  load_time_ms: 3.354
  training_iteration_time_ms: 57618.353
  update_time_ms: 2.703
timesteps_total: 330000
training_iteration: 20

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19igysrqn6/checkpoint_000020/checkpoint-20
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9464285714285714
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7676056338028169
  reward for individual goal_min: 0.5
episode_len_mean: 130.40944881889763
episode_reward_max: 2.0
episode_reward_mean: 1.6929133858267718
episode_reward_min: 0.0
episodes_this_iter: 127
episodes_total: 2309
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7637795275590551
  agent_1: 0.9291338582677166
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.5573832988739
time_total_s: 1247.0564739704132
timers:
  learn_throughput: 511.137
  learn_time_ms: 32280.944
  load_throughput: 5757188.873
  load_time_ms: 2.866
  training_iteration_time_ms: 43301.203
  update_time_ms: 2.445
timesteps_total: 429000
training_iteration: 26

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.4166666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 222.9
episode_reward_max: 2.0
episode_reward_mean: 0.75
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.3333333333333333
  agent_1: 0.4166666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2054794520547945
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.38271604938271603
  reward for individual goal_min: 0.0
episode_len_mean: 236.08
episode_reward_max: 2.0
episode_reward_mean: 0.62
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 1357
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.27
  agent_1: 0.35
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 81.51710224151611
time_total_s: 1225.0949256420135
timers:
  learn_throughput: 368.284
  learn_time_ms: 44802.397
  load_throughput: 4651942.353
  load_time_ms: 3.547
  training_iteration_time_ms: 58754.398
  update_time_ms: 2.641
timesteps_total: 330000
training_iteration: 20

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-28ftt3iad9/checkpoint_000020/checkpoint-20
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2848101265822785
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6764705882352942
  reward for individual goal_min: 0.0
episode_len_mean: 204.52
episode_reward_max: 2.0
episode_reward_mean: 0.96
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 1631
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.49
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.79489183425903
time_total_s: 1245.8501431941986
timers:
  learn_throughput: 435.626
  learn_time_ms: 37876.524
  load_throughput: 5058327.681
  load_time_ms: 3.262
  training_iteration_time_ms: 49697.575
  update_time_ms: 2.61
timesteps_total: 379500
training_iteration: 23

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 153.83333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.5833333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.85
  agent_1: 0.7333333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7674418604651163
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6578947368421053
  reward for individual goal_min: 0.5
episode_len_mean: 181.41
episode_reward_max: 2.0
episode_reward_mean: 1.41
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 1495
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 71.29199743270874
time_total_s: 1249.8563771247864
timers:
  learn_throughput: 363.545
  learn_time_ms: 45386.355
  load_throughput: 4291233.871
  load_time_ms: 3.845
  training_iteration_time_ms: 59168.942
  update_time_ms: 2.627
timesteps_total: 330000
training_iteration: 20

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-184s1jdk42/checkpoint_000020/checkpoint-20
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.39473684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5845070422535211
  reward for individual goal_min: 0.0
episode_len_mean: 219.82
episode_reward_max: 2.0
episode_reward_mean: 0.98
episode_reward_min: 0.0
episodes_this_iter: 75
episodes_total: 1480
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.448304176330566
time_total_s: 1247.9269177913666
timers:
  learn_throughput: 395.635
  learn_time_ms: 41705.158
  load_throughput: 4938806.654
  load_time_ms: 3.341
  training_iteration_time_ms: 54631.988
  update_time_ms: 2.66
timesteps_total: 346500
training_iteration: 21

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8783783783783784
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7767857142857143
  reward for individual goal_min: 0.5
episode_len_mean: 127.04615384615384
episode_reward_max: 2.0
episode_reward_mean: 1.6692307692307693
episode_reward_min: 0.0
episodes_this_iter: 130
episodes_total: 1906
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7923076923076923
  agent_1: 0.8769230769230769
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.91536259651184
time_total_s: 1258.3929781913757
timers:
  learn_throughput: 399.143
  learn_time_ms: 41338.565
  load_throughput: 5171961.438
  load_time_ms: 3.19
  training_iteration_time_ms: 54047.823
  update_time_ms: 2.632
timesteps_total: 363000
training_iteration: 22

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23376623376623376
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6940298507462687
  reward for individual goal_min: 0.0
episode_len_mean: 223.49
episode_reward_max: 2.0
episode_reward_mean: 0.88
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 1662
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.38
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.589112758636475
time_total_s: 1275.8106546401978
timers:
  learn_throughput: 436.803
  learn_time_ms: 37774.466
  load_throughput: 4870267.63
  load_time_ms: 3.388
  training_iteration_time_ms: 49648.162
  update_time_ms: 2.602
timesteps_total: 396000
training_iteration: 24

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.55
  reward for individual goal_min: 0.5
episode_len_mean: 175.91666666666666
episode_reward_max: 2.0
episode_reward_mean: 1.2166666666666666
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6666666666666666
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6346153846153846
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5833333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 177.73
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 1550
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 75.72901153564453
time_total_s: 1277.882551908493
timers:
  learn_throughput: 363.439
  learn_time_ms: 45399.599
  load_throughput: 4687738.16
  load_time_ms: 3.52
  training_iteration_time_ms: 59300.158
  update_time_ms: 2.701
timesteps_total: 330000
training_iteration: 20

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-19wecnkw4d/checkpoint_000020/checkpoint-20
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22972972972972974
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8734177215189873
  reward for individual goal_min: 0.0
episode_len_mean: 199.73
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 1727
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.77191948890686
time_total_s: 1270.8001153469086
timers:
  learn_throughput: 452.984
  learn_time_ms: 36425.167
  load_throughput: 4891852.522
  load_time_ms: 3.373
  training_iteration_time_ms: 47939.042
  update_time_ms: 2.562
timesteps_total: 396000
training_iteration: 24
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2987012987012987
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5405405405405406
  reward for individual goal_min: 0.0
episode_len_mean: 230.88
episode_reward_max: 2.0
episode_reward_mean: 0.82
episode_reward_min: 0.0
episodes_this_iter: 71
episodes_total: 1215
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.35
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 66.96099519729614
time_total_s: 1256.2707569599152
timers:
  learn_throughput: 312.939
  learn_time_ms: 52725.863
  load_throughput: 4158415.613
  load_time_ms: 3.968
  training_iteration_time_ms: 68195.906
  update_time_ms: 2.978
timesteps_total: 297000
training_iteration: 18


custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2848101265822785
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7215189873417721
  reward for individual goal_min: 0.0
episode_len_mean: 213.84
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 1473
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.78033924102783
time_total_s: 1259.556386232376
timers:
  learn_throughput: 379.099
  learn_time_ms: 43524.239
  load_throughput: 4824635.291
  load_time_ms: 3.42
  training_iteration_time_ms: 56985.511
  update_time_ms: 2.644
timesteps_total: 346500
training_iteration: 21

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9523809523809523
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.82
  reward for individual goal_min: 0.5
episode_len_mean: 118.0
episode_reward_max: 2.0
episode_reward_mean: 1.7608695652173914
episode_reward_min: 0.0
episodes_this_iter: 138
episodes_total: 2447
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8405797101449275
  agent_1: 0.9202898550724637
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.46161413192749
time_total_s: 1289.5180881023407
timers:
  learn_throughput: 513.901
  learn_time_ms: 32107.339
  load_throughput: 5746146.68
  load_time_ms: 2.871
  training_iteration_time_ms: 43149.361
  update_time_ms: 2.409
timesteps_total: 445500
training_iteration: 27

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21621621621621623
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8227848101265823
  reward for individual goal_min: 0.0
episode_len_mean: 220.32
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 75
episodes_total: 1438
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.4
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.93875002861023
time_total_s: 1286.622882604599
timers:
  learn_throughput: 379.541
  learn_time_ms: 43473.545
  load_throughput: 4730288.714
  load_time_ms: 3.488
  training_iteration_time_ms: 56899.827
  update_time_ms: 2.632
timesteps_total: 346500
training_iteration: 21

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21052631578947367
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.71875
  reward for individual goal_min: 0.0
episode_len_mean: 218.11
episode_reward_max: 2.0
episode_reward_mean: 1.0
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 1317
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.48
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.419716119766235
time_total_s: 1278.6115267276764
timers:
  learn_throughput: 335.946
  learn_time_ms: 49115.04
  load_throughput: 4101777.836
  load_time_ms: 4.023
  training_iteration_time_ms: 64375.847
  update_time_ms: 2.876
timesteps_total: 313500
training_iteration: 19

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25316455696202533
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8653846153846154
  reward for individual goal_min: 0.0
episode_len_mean: 206.36
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 1498
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.296868085861206
time_total_s: 1292.9744596481323
timers:
  learn_throughput: 377.252
  learn_time_ms: 43737.386
  load_throughput: 5007924.859
  load_time_ms: 3.295
  training_iteration_time_ms: 56984.217
  update_time_ms: 2.707
timesteps_total: 346500
training_iteration: 21

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2785714285714286
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6486486486486487
  reward for individual goal_min: 0.0
episode_len_mean: 204.64
episode_reward_max: 2.0
episode_reward_mean: 1.0
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 1714
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.97030973434448
time_total_s: 1294.820452928543
timers:
  learn_throughput: 436.489
  learn_time_ms: 37801.655
  load_throughput: 5107681.226
  load_time_ms: 3.23
  training_iteration_time_ms: 49582.185
  update_time_ms: 2.625
timesteps_total: 396000
training_iteration: 24

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.38125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.569620253164557
  reward for individual goal_min: 0.0
episode_len_mean: 221.38
episode_reward_max: 2.0
episode_reward_mean: 0.97
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 1556
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.47
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.97360682487488
time_total_s: 1298.9005246162415
timers:
  learn_throughput: 400.883
  learn_time_ms: 41159.141
  load_throughput: 4965169.067
  load_time_ms: 3.323
  training_iteration_time_ms: 53922.697
  update_time_ms: 2.648
timesteps_total: 363000
training_iteration: 22

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8888888888888888
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7314814814814815
  reward for individual goal_min: 0.5
episode_len_mean: 152.24074074074073
episode_reward_max: 2.0
episode_reward_mean: 1.6203703703703705
episode_reward_min: 0.0
episodes_this_iter: 108
episodes_total: 1603
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8796296296296297
  agent_1: 0.7407407407407407
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.28881597518921
time_total_s: 1305.1451930999756
timers:
  learn_throughput: 368.857
  learn_time_ms: 44732.8
  load_throughput: 4280881.587
  load_time_ms: 3.854
  training_iteration_time_ms: 58384.754
  update_time_ms: 2.604
timesteps_total: 346500
training_iteration: 21

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16883116883116883
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.44594594594594594
  reward for individual goal_min: 0.0
episode_len_mean: 237.51
episode_reward_max: 2.0
episode_reward_mean: 0.61
episode_reward_min: 0.0
episodes_this_iter: 69
episodes_total: 1426
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.3
  agent_1: 0.31
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.81778573989868
time_total_s: 1284.9127113819122
timers:
  learn_throughput: 366.484
  learn_time_ms: 45022.42
  load_throughput: 4634341.773
  load_time_ms: 3.56
  training_iteration_time_ms: 59017.382
  update_time_ms: 2.645
timesteps_total: 346500
training_iteration: 21

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9230769230769231
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7615384615384615
  reward for individual goal_min: 0.5
episode_len_mean: 127.94615384615385
episode_reward_max: 2.0
episode_reward_mean: 1.6846153846153846
episode_reward_min: 0.0
episodes_this_iter: 130
episodes_total: 2036
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7384615384615385
  agent_1: 0.9461538461538461
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.85302972793579
time_total_s: 1310.2460079193115
timers:
  learn_throughput: 403.126
  learn_time_ms: 40930.112
  load_throughput: 5180246.115
  load_time_ms: 3.185
  training_iteration_time_ms: 53657.456
  update_time_ms: 2.622
timesteps_total: 379500
training_iteration: 23

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21794871794871795
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6907894736842105
  reward for individual goal_min: 0.0
episode_len_mean: 225.27
episode_reward_max: 2.0
episode_reward_mean: 0.94
episode_reward_min: 0.0
episodes_this_iter: 72
episodes_total: 1734
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.37
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.911561489105225
time_total_s: 1322.722216129303
timers:
  learn_throughput: 440.783
  learn_time_ms: 37433.375
  load_throughput: 4853531.198
  load_time_ms: 3.4
  training_iteration_time_ms: 49286.149
  update_time_ms: 2.596
timesteps_total: 412500
training_iteration: 25

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2708333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.881578947368421
  reward for individual goal_min: 0.0
episode_len_mean: 205.02
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 1808
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.42075991630554
time_total_s: 1316.220875263214
timers:
  learn_throughput: 460.342
  learn_time_ms: 35842.934
  load_throughput: 4964812.867
  load_time_ms: 3.323
  training_iteration_time_ms: 47200.642
  update_time_ms: 2.539
timesteps_total: 412500
training_iteration: 25

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9230769230769231
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8493150684931506
  reward for individual goal_min: 0.5
episode_len_mean: 109.17218543046357
episode_reward_max: 2.0
episode_reward_mean: 1.7748344370860927
episode_reward_min: 0.0
episodes_this_iter: 151
episodes_total: 2598
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8609271523178808
  agent_1: 0.9139072847682119
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.8114333152771
time_total_s: 1334.3295214176178
timers:
  learn_throughput: 512.273
  learn_time_ms: 32209.4
  load_throughput: 5738475.622
  load_time_ms: 2.875
  training_iteration_time_ms: 43311.216
  update_time_ms: 2.453
timesteps_total: 462000
training_iteration: 28

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.5769230769230769
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.59375
  reward for individual goal_min: 0.5
episode_len_mean: 178.18
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 1645
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.903589487075806
time_total_s: 1341.7861413955688
timers:
  learn_throughput: 360.724
  learn_time_ms: 45741.348
  load_throughput: 4563505.417
  load_time_ms: 3.616
  training_iteration_time_ms: 59669.252
  update_time_ms: 2.686
timesteps_total: 346500
training_iteration: 21

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2777777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7364864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 207.13
episode_reward_max: 2.0
episode_reward_mean: 1.04
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 1549
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.19253635406494
time_total_s: 1314.748922586441
timers:
  learn_throughput: 382.136
  learn_time_ms: 43178.357
  load_throughput: 4768849.167
  load_time_ms: 3.46
  training_iteration_time_ms: 56631.516
  update_time_ms: 2.627
timesteps_total: 363000
training_iteration: 22

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2922077922077922
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5208333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 228.77
episode_reward_max: 2.0
episode_reward_mean: 0.82
episode_reward_min: 0.0
episodes_this_iter: 71
episodes_total: 1286
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.39
  agent_1: 0.43
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.70711255073547
time_total_s: 1321.9778695106506
timers:
  learn_throughput: 314.98
  learn_time_ms: 52384.234
  load_throughput: 4157166.645
  load_time_ms: 3.969
  training_iteration_time_ms: 67773.765
  update_time_ms: 2.966
timesteps_total: 313500
training_iteration: 19

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7876712328767124
  reward for individual goal_min: 0.0
episode_len_mean: 218.09
episode_reward_max: 2.0
episode_reward_mean: 0.95
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 1512
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.37
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.78319454193115
time_total_s: 1340.4060771465302
timers:
  learn_throughput: 383.345
  learn_time_ms: 43042.117
  load_throughput: 4802504.858
  load_time_ms: 3.436
  training_iteration_time_ms: 56477.425
  update_time_ms: 2.615
timesteps_total: 363000
training_iteration: 22

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2972972972972973
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7094594594594594
  reward for individual goal_min: 0.0
episode_len_mean: 214.78
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 1791
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.00802946090698
time_total_s: 1341.82848238945
timers:
  learn_throughput: 441.605
  learn_time_ms: 37363.73
  load_throughput: 5131198.683
  load_time_ms: 3.216
  training_iteration_time_ms: 49095.434
  update_time_ms: 2.605
timesteps_total: 412500
training_iteration: 25

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16071428571428573
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6785714285714286
  reward for individual goal_min: 0.0
episode_len_mean: 231.18
episode_reward_max: 2.0
episode_reward_mean: 0.8
episode_reward_min: 0.0
episodes_this_iter: 71
episodes_total: 1805
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.44
  agent_1: 0.36
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.7948215007782
time_total_s: 1370.5170376300812
timers:
  learn_throughput: 444.393
  learn_time_ms: 37129.329
  load_throughput: 4912897.079
  load_time_ms: 3.359
  training_iteration_time_ms: 48956.768
  update_time_ms: 2.595
timesteps_total: 429000
training_iteration: 26

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8506493506493507
  reward for individual goal_min: 0.0
episode_len_mean: 204.48
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 1580
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.77948546409607
time_total_s: 1353.7539451122284
timers:
  learn_throughput: 377.368
  learn_time_ms: 43723.869
  load_throughput: 5083407.35
  load_time_ms: 3.246
  training_iteration_time_ms: 56935.25
  update_time_ms: 2.753
timesteps_total: 363000
training_iteration: 22

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3271604938271605
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5974025974025974
  reward for individual goal_min: 0.0
episode_len_mean: 216.69
episode_reward_max: 2.0
episode_reward_mean: 0.96
episode_reward_min: 0.0
episodes_this_iter: 75
episodes_total: 1631
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.39
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.88445711135864
time_total_s: 1353.7849817276
timers:
  learn_throughput: 401.907
  learn_time_ms: 41054.247
  load_throughput: 4966736.951
  load_time_ms: 3.322
  training_iteration_time_ms: 53845.448
  update_time_ms: 2.623
timesteps_total: 379500
training_iteration: 23

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9178082191780822
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8380281690140845
  reward for individual goal_min: 0.5
episode_len_mean: 114.27777777777777
episode_reward_max: 2.0
episode_reward_mean: 1.7569444444444444
episode_reward_min: 0.0
episodes_this_iter: 144
episodes_total: 2180
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8125
  agent_1: 0.9444444444444444
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.40983510017395
time_total_s: 1363.6558430194855
timers:
  learn_throughput: 403.394
  learn_time_ms: 40902.91
  load_throughput: 5156354.804
  load_time_ms: 3.2
  training_iteration_time_ms: 53571.896
  update_time_ms: 2.641
timesteps_total: 396000
training_iteration: 24

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8983050847457628
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.75
  reward for individual goal_min: 0.5
episode_len_mean: 149.56756756756758
episode_reward_max: 2.0
episode_reward_mean: 1.6576576576576576
episode_reward_min: 0.0
episodes_this_iter: 111
episodes_total: 1714
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8738738738738738
  agent_1: 0.7837837837837838
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.15367603302002
time_total_s: 1361.2988691329956
timers:
  learn_throughput: 371.974
  learn_time_ms: 44357.945
  load_throughput: 4329353.594
  load_time_ms: 3.811
  training_iteration_time_ms: 57906.604
  update_time_ms: 2.587
timesteps_total: 363000
training_iteration: 22

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2608695652173913
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.4268292682926829
  reward for individual goal_min: 0.0
episode_len_mean: 231.22
episode_reward_max: 2.0
episode_reward_mean: 0.71
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 1500
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.37
  agent_1: 0.34
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.09473633766174
time_total_s: 1343.007447719574
timers:
  learn_throughput: 367.11
  learn_time_ms: 44945.597
  load_throughput: 4634496.946
  load_time_ms: 3.56
  training_iteration_time_ms: 58953.828
  update_time_ms: 2.636
timesteps_total: 363000
training_iteration: 22

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9242424242424242
  reward for individual goal_min: 0.0
episode_len_mean: 195.53
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 1894
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.26213574409485
time_total_s: 1362.483011007309
timers:
  learn_throughput: 463.874
  learn_time_ms: 35570.002
  load_throughput: 5011007.038
  load_time_ms: 3.293
  training_iteration_time_ms: 46878.918
  update_time_ms: 2.535
timesteps_total: 429000
training_iteration: 26

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7166666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 167.0
episode_reward_max: 2.0
episode_reward_mean: 1.45
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.75
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7222222222222222
  reward for individual goal_min: 0.0
episode_len_mean: 216.62
episode_reward_max: 2.0
episode_reward_mean: 0.98
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 1390
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.47
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 83.19995260238647
time_total_s: 1361.8114793300629
timers:
  learn_throughput: 338.992
  learn_time_ms: 48673.715
  load_throughput: 4071588.782
  load_time_ms: 4.052
  training_iteration_time_ms: 63813.616
  update_time_ms: 3.049
timesteps_total: 330000
training_iteration: 20

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-29ws591pkw/checkpoint_000020/checkpoint-20
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9473684210526315
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7928571428571428
  reward for individual goal_min: 0.5
episode_len_mean: 113.95890410958904
episode_reward_max: 2.0
episode_reward_mean: 1.7465753424657535
episode_reward_min: 0.0
episodes_this_iter: 146
episodes_total: 2744
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7945205479452054
  agent_1: 0.952054794520548
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.78665542602539
time_total_s: 1378.1161768436432
timers:
  learn_throughput: 511.803
  learn_time_ms: 32238.944
  load_throughput: 5659912.655
  load_time_ms: 2.915
  training_iteration_time_ms: 43321.111
  update_time_ms: 2.446
timesteps_total: 478500
training_iteration: 29

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32051282051282054
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7364864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 202.73
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 1873
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.46994471549988
time_total_s: 1389.29842710495
timers:
  learn_throughput: 444.986
  learn_time_ms: 37079.782
  load_throughput: 5188129.512
  load_time_ms: 3.18
  training_iteration_time_ms: 48763.989
  update_time_ms: 2.6
timesteps_total: 429000
training_iteration: 26

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32098765432098764
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7162162162162162
  reward for individual goal_min: 0.0
episode_len_mean: 210.0
episode_reward_max: 2.0
episode_reward_mean: 1.04
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 1631
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.48
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.651610136032104
time_total_s: 1372.4005327224731
timers:
  learn_throughput: 381.437
  learn_time_ms: 43257.424
  load_throughput: 4748952.919
  load_time_ms: 3.474
  training_iteration_time_ms: 56705.947
  update_time_ms: 2.628
timesteps_total: 379500
training_iteration: 23

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1987179487179487
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8533333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 219.73
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 1588
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.38
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.73910570144653
time_total_s: 1394.1451828479767
timers:
  learn_throughput: 385.673
  learn_time_ms: 42782.407
  load_throughput: 4801471.953
  load_time_ms: 3.436
  training_iteration_time_ms: 56022.843
  update_time_ms: 2.607
timesteps_total: 379500
training_iteration: 23

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.62
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.59
  reward for individual goal_min: 0.5
episode_len_mean: 175.94
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 1739
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.315898180007935
time_total_s: 1404.1020395755768
timers:
  learn_throughput: 360.848
  learn_time_ms: 45725.655
  load_throughput: 4464328.216
  load_time_ms: 3.696
  training_iteration_time_ms: 59668.627
  update_time_ms: 2.714
timesteps_total: 363000
training_iteration: 22

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14705882352941177
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6794871794871795
  reward for individual goal_min: 0.0
episode_len_mean: 220.05
episode_reward_max: 2.0
episode_reward_mean: 0.9
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 1881
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.4
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.2145836353302
time_total_s: 1418.7316212654114
timers:
  learn_throughput: 445.82
  learn_time_ms: 37010.468
  load_throughput: 4862909.903
  load_time_ms: 3.393
  training_iteration_time_ms: 48817.438
  update_time_ms: 2.627
timesteps_total: 445500
training_iteration: 27

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29605263157894735
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 210.67
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 1711
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.456912994384766
time_total_s: 1404.2418947219849
timers:
  learn_throughput: 405.641
  learn_time_ms: 40676.325
  load_throughput: 5004701.697
  load_time_ms: 3.297
  training_iteration_time_ms: 53340.106
  update_time_ms: 2.59
timesteps_total: 396000
training_iteration: 24

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2916666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9044117647058824
  reward for individual goal_min: 0.0
episode_len_mean: 209.21
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 1975
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.18695640563965
time_total_s: 1407.6699674129486
timers:
  learn_throughput: 468.873
  learn_time_ms: 35190.755
  load_throughput: 5058253.739
  load_time_ms: 3.262
  training_iteration_time_ms: 46438.674
  update_time_ms: 2.517
timesteps_total: 445500
training_iteration: 27

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2911392405063291
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8947368421052632
  reward for individual goal_min: 0.0
episode_len_mean: 200.9
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 1662
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.130149602890015
time_total_s: 1407.8840947151184
timers:
  learn_throughput: 380.504
  learn_time_ms: 43363.523
  load_throughput: 5073345.698
  load_time_ms: 3.252
  training_iteration_time_ms: 56526.884
  update_time_ms: 2.707
timesteps_total: 379500
training_iteration: 23

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8888888888888888
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8227848101265823
  reward for individual goal_min: 0.5
episode_len_mean: 117.07042253521126
episode_reward_max: 2.0
episode_reward_mean: 1.704225352112676
episode_reward_min: 0.0
episodes_this_iter: 142
episodes_total: 2322
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7535211267605634
  agent_1: 0.9507042253521126
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.481186866760254
time_total_s: 1417.1370298862457
timers:
  learn_throughput: 405.259
  learn_time_ms: 40714.655
  load_throughput: 5112964.227
  load_time_ms: 3.227
  training_iteration_time_ms: 53424.21
  update_time_ms: 2.651
timesteps_total: 412500
training_iteration: 25

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9879518072289156
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7681159420289855
  reward for individual goal_min: 0.5
episode_len_mean: 109.86184210526316
episode_reward_max: 2.0
episode_reward_mean: 1.7763157894736843
episode_reward_min: 0.0
episodes_this_iter: 152
episodes_total: 2896
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8026315789473685
  agent_1: 0.9736842105263158
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.64726901054382
time_total_s: 1420.763445854187
timers:
  learn_throughput: 512.741
  learn_time_ms: 32179.977
  load_throughput: 5663293.754
  load_time_ms: 2.913
  training_iteration_time_ms: 43238.632
  update_time_ms: 2.431
timesteps_total: 495000
training_iteration: 30

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9242424242424242
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.77
  reward for individual goal_min: 0.5
episode_len_mean: 142.2155172413793
episode_reward_max: 2.0
episode_reward_mean: 1.7155172413793103
episode_reward_min: 0.0
episodes_this_iter: 116
episodes_total: 1830
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9137931034482759
  agent_1: 0.8017241379310345
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.83450770378113
time_total_s: 1418.1333768367767
timers:
  learn_throughput: 374.584
  learn_time_ms: 44048.894
  load_throughput: 4329109.857
  load_time_ms: 3.811
  training_iteration_time_ms: 57469.526
  update_time_ms: 2.571
timesteps_total: 379500
training_iteration: 23

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21232876712328766
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.4788732394366197
  reward for individual goal_min: 0.0
episode_len_mean: 240.38
episode_reward_max: 2.0
episode_reward_mean: 0.66
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 1567
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.27
  agent_1: 0.39
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.0109179019928
time_total_s: 1401.0183656215668
timers:
  learn_throughput: 367.678
  learn_time_ms: 44876.204
  load_throughput: 4591968.469
  load_time_ms: 3.593
  training_iteration_time_ms: 58867.582
  update_time_ms: 2.641
timesteps_total: 379500
training_iteration: 23

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6166666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 196.96666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.3166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6833333333333333
  agent_1: 0.6333333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2236842105263158
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6392405063291139
  reward for individual goal_min: 0.0
episode_len_mean: 227.59
episode_reward_max: 2.0
episode_reward_mean: 0.91
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 1360
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.39
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 85.86511421203613
time_total_s: 1407.8429837226868
timers:
  learn_throughput: 320.557
  learn_time_ms: 51472.837
  load_throughput: 4146183.147
  load_time_ms: 3.98
  training_iteration_time_ms: 66665.35
  update_time_ms: 2.971
timesteps_total: 330000
training_iteration: 20

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-2933yh83nn/checkpoint_000020/checkpoint-20
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23026315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6708860759493671
  reward for individual goal_min: 0.0
episode_len_mean: 215.01
episode_reward_max: 2.0
episode_reward_mean: 0.96
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 1468
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.45
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.00479078292847
time_total_s: 1423.8162701129913
timers:
  learn_throughput: 341.562
  learn_time_ms: 48307.464
  load_throughput: 4078571.445
  load_time_ms: 4.046
  training_iteration_time_ms: 63376.156
  update_time_ms: 3.039
timesteps_total: 346500
training_iteration: 21

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32608695652173914
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7012195121951219
  reward for individual goal_min: 0.0
episode_len_mean: 200.21
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 1955
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.83901619911194
time_total_s: 1435.137443304062
timers:
  learn_throughput: 452.388
  learn_time_ms: 36473.131
  load_throughput: 5205455.927
  load_time_ms: 3.17
  training_iteration_time_ms: 48085.209
  update_time_ms: 2.589
timesteps_total: 445500
training_iteration: 27

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2867647058823529
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6964285714285714
  reward for individual goal_min: 0.0
episode_len_mean: 211.11
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 1709
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.37641143798828
time_total_s: 1427.7769441604614
timers:
  learn_throughput: 380.442
  learn_time_ms: 43370.563
  load_throughput: 4732714.851
  load_time_ms: 3.486
  training_iteration_time_ms: 56894.634
  update_time_ms: 2.629
timesteps_total: 396000
training_iteration: 24

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9044117647058824
  reward for individual goal_min: 0.0
episode_len_mean: 213.3
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 1665
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.49
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.1539249420166
time_total_s: 1448.2991077899933
timers:
  learn_throughput: 390.12
  learn_time_ms: 42294.67
  load_throughput: 4819125.53
  load_time_ms: 3.424
  training_iteration_time_ms: 55491.629
  update_time_ms: 2.595
timesteps_total: 396000
training_iteration: 24

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21621621621621623
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6733333333333333
  reward for individual goal_min: 0.0
episode_len_mean: 222.45
episode_reward_max: 2.0
episode_reward_mean: 0.9
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 1957
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.36
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.4564790725708
time_total_s: 1465.1881003379822
timers:
  learn_throughput: 447.047
  learn_time_ms: 36908.862
  load_throughput: 4800672.591
  load_time_ms: 3.437
  training_iteration_time_ms: 48637.89
  update_time_ms: 2.643
timesteps_total: 462000
training_iteration: 28

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3037974683544304
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8972602739726028
  reward for individual goal_min: 0.0
episode_len_mean: 203.13
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 2056
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.514633893966675
time_total_s: 1451.1846013069153
timers:
  learn_throughput: 475.51
  learn_time_ms: 34699.561
  load_throughput: 5102108.197
  load_time_ms: 3.234
  training_iteration_time_ms: 45915.317
  update_time_ms: 2.492
timesteps_total: 462000
training_iteration: 28

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9863013698630136
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8148148148148148
  reward for individual goal_min: 0.5
episode_len_mean: 107.16883116883118
episode_reward_max: 2.0
episode_reward_mean: 1.7922077922077921
episode_reward_min: 0.0
episodes_this_iter: 154
episodes_total: 3050
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8116883116883117
  agent_1: 0.9805194805194806
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.62546968460083
time_total_s: 1462.3889155387878
timers:
  learn_throughput: 514.757
  learn_time_ms: 32053.959
  load_throughput: 5664777.153
  load_time_ms: 2.913
  training_iteration_time_ms: 43096.56
  update_time_ms: 2.42
timesteps_total: 511500
training_iteration: 31

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.38405797101449274
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6883116883116883
  reward for individual goal_min: 0.0
episode_len_mean: 213.37
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 1788
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.68658208847046
time_total_s: 1456.9284768104553
timers:
  learn_throughput: 408.191
  learn_time_ms: 40422.284
  load_throughput: 4988108.576
  load_time_ms: 3.308
  training_iteration_time_ms: 53066.526
  update_time_ms: 2.585
timesteps_total: 412500
training_iteration: 25

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.5932203389830508
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5609756097560976
  reward for individual goal_min: 0.5
episode_len_mean: 176.64
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 1830
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 66.11637115478516
time_total_s: 1470.218410730362
timers:
  learn_throughput: 358.497
  learn_time_ms: 46025.446
  load_throughput: 4377966.32
  load_time_ms: 3.769
  training_iteration_time_ms: 60160.134
  update_time_ms: 2.709
timesteps_total: 379500
training_iteration: 23

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23376623376623376
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8846153846153846
  reward for individual goal_min: 0.0
episode_len_mean: 202.91
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 1745
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.80156064033508
time_total_s: 1461.6856553554535
timers:
  learn_throughput: 382.115
  learn_time_ms: 43180.709
  load_throughput: 5059917.966
  load_time_ms: 3.261
  training_iteration_time_ms: 56414.545
  update_time_ms: 2.677
timesteps_total: 396000
training_iteration: 24

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9772727272727273
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8511904761904762
  reward for individual goal_min: 0.5
episode_len_mean: 96.65697674418605
episode_reward_max: 2.0
episode_reward_mean: 1.8313953488372092
episode_reward_min: 0.0
episodes_this_iter: 172
episodes_total: 2494
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8430232558139535
  agent_1: 0.9883720930232558
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.00514316558838
time_total_s: 1470.142173051834
timers:
  learn_throughput: 407.124
  learn_time_ms: 40528.22
  load_throughput: 5100491.285
  load_time_ms: 3.235
  training_iteration_time_ms: 53252.048
  update_time_ms: 2.669
timesteps_total: 429000
training_iteration: 26

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9090909090909091
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.71
  reward for individual goal_min: 0.5
episode_len_mean: 156.31428571428572
episode_reward_max: 2.0
episode_reward_mean: 1.6285714285714286
episode_reward_min: 0.0
episodes_this_iter: 105
episodes_total: 1935
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8666666666666667
  agent_1: 0.7619047619047619
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.361231565475464
time_total_s: 1472.4946084022522
timers:
  learn_throughput: 378.885
  learn_time_ms: 43548.781
  load_throughput: 4370253.036
  load_time_ms: 3.776
  training_iteration_time_ms: 56898.49
  update_time_ms: 2.549
timesteps_total: 396000
training_iteration: 24

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15753424657534246
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.49324324324324326
  reward for individual goal_min: 0.0
episode_len_mean: 245.79
episode_reward_max: 2.0
episode_reward_mean: 0.65
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 1633
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.26
  agent_1: 0.39
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.42080545425415
time_total_s: 1459.439171075821
timers:
  learn_throughput: 367.522
  learn_time_ms: 44895.245
  load_throughput: 4601280.268
  load_time_ms: 3.586
  training_iteration_time_ms: 58951.139
  update_time_ms: 2.64
timesteps_total: 396000
training_iteration: 24

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34415584415584416
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6445783132530121
  reward for individual goal_min: 0.0
episode_len_mean: 208.98
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 2034
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.555867195129395
time_total_s: 1480.6933104991913
timers:
  learn_throughput: 456.804
  learn_time_ms: 36120.497
  load_throughput: 5277423.133
  load_time_ms: 3.127
  training_iteration_time_ms: 47657.566
  update_time_ms: 2.569
timesteps_total: 462000
training_iteration: 28

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1962025316455696
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6756756756756757
  reward for individual goal_min: 0.0
episode_len_mean: 224.07
episode_reward_max: 2.0
episode_reward_mean: 0.92
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 1433
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.44
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.87618041038513
time_total_s: 1468.719164133072
timers:
  learn_throughput: 324.938
  learn_time_ms: 50778.881
  load_throughput: 4149365.118
  load_time_ms: 3.977
  training_iteration_time_ms: 65819.066
  update_time_ms: 2.96
timesteps_total: 346500
training_iteration: 21

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27631578947368424
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.671875
  reward for individual goal_min: 0.0
episode_len_mean: 212.12
episode_reward_max: 2.0
episode_reward_mean: 0.93
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 2035
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.46
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.02695631980896
time_total_s: 1512.2150566577911
timers:
  learn_throughput: 449.124
  learn_time_ms: 36738.182
  load_throughput: 4776485.171
  load_time_ms: 3.454
  training_iteration_time_ms: 48433.318
  update_time_ms: 2.638
timesteps_total: 478500
training_iteration: 29

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22151898734177214
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7101449275362319
  reward for individual goal_min: 0.0
episode_len_mean: 222.61
episode_reward_max: 2.0
episode_reward_mean: 0.94
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 1542
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.49
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.92505884170532
time_total_s: 1485.7413289546967
timers:
  learn_throughput: 343.731
  learn_time_ms: 48002.71
  load_throughput: 4113675.951
  load_time_ms: 4.011
  training_iteration_time_ms: 63034.337
  update_time_ms: 2.987
timesteps_total: 363000
training_iteration: 22

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 193.77
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 2140
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.15663003921509
time_total_s: 1497.3412313461304
timers:
  learn_throughput: 477.043
  learn_time_ms: 34588.041
  load_throughput: 5167944.8
  load_time_ms: 3.193
  training_iteration_time_ms: 45822.604
  update_time_ms: 2.494
timesteps_total: 478500
training_iteration: 29

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16233766233766234
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9333333333333333
  reward for individual goal_min: 0.0
episode_len_mean: 217.91
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 1741
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.48
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.256887912750244
time_total_s: 1498.5559957027435
timers:
  learn_throughput: 394.736
  learn_time_ms: 41800.095
  load_throughput: 4793589.893
  load_time_ms: 3.442
  training_iteration_time_ms: 54800.605
  update_time_ms: 2.603
timesteps_total: 412500
training_iteration: 25

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7865853658536586
  reward for individual goal_min: 0.0
episode_len_mean: 209.0
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 1785
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.53023099899292
time_total_s: 1479.3071751594543
timers:
  learn_throughput: 385.647
  learn_time_ms: 42785.289
  load_throughput: 4717262.59
  load_time_ms: 3.498
  training_iteration_time_ms: 56161.311
  update_time_ms: 2.638
timesteps_total: 412500
training_iteration: 25

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9305555555555556
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7857142857142857
  reward for individual goal_min: 0.5
episode_len_mean: 109.3758389261745
episode_reward_max: 2.0
episode_reward_mean: 1.7114093959731544
episode_reward_min: 0.0
episodes_this_iter: 149
episodes_total: 3199
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7718120805369127
  agent_1: 0.9395973154362416
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.66460967063904
time_total_s: 1505.0535252094269
timers:
  learn_throughput: 513.845
  learn_time_ms: 32110.872
  load_throughput: 5615684.9
  load_time_ms: 2.938
  training_iteration_time_ms: 43148.106
  update_time_ms: 2.44
timesteps_total: 528000
training_iteration: 32

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3246753246753247
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6818181818181818
  reward for individual goal_min: 0.0
episode_len_mean: 209.45
episode_reward_max: 2.0
episode_reward_mean: 1.03
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 1866
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.21908903121948
time_total_s: 1509.1475658416748
timers:
  learn_throughput: 411.345
  learn_time_ms: 40112.317
  load_throughput: 5006729.269
  load_time_ms: 3.296
  training_iteration_time_ms: 52706.404
  update_time_ms: 2.554
timesteps_total: 429000
training_iteration: 26

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9393939393939394
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8026315789473685
  reward for individual goal_min: 0.5
episode_len_mean: 116.45774647887323
episode_reward_max: 2.0
episode_reward_mean: 1.732394366197183
episode_reward_min: 0.0
episodes_this_iter: 142
episodes_total: 2636
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7887323943661971
  agent_1: 0.9436619718309859
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.15417838096619
time_total_s: 1522.2963514328003
timers:
  learn_throughput: 410.376
  learn_time_ms: 40207.023
  load_throughput: 5075317.62
  load_time_ms: 3.251
  training_iteration_time_ms: 52946.653
  update_time_ms: 2.688
timesteps_total: 445500
training_iteration: 27

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22151898734177214
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.910958904109589
  reward for individual goal_min: 0.0
episode_len_mean: 208.19
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 75
episodes_total: 1820
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.365190267562866
time_total_s: 1518.0508456230164
timers:
  learn_throughput: 384.034
  learn_time_ms: 42964.919
  load_throughput: 4959547.087
  load_time_ms: 3.327
  training_iteration_time_ms: 56156.363
  update_time_ms: 2.683
timesteps_total: 412500
training_iteration: 25

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.627906976744186
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5614035087719298
  reward for individual goal_min: 0.5
episode_len_mean: 191.94
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 1916
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.57126259803772
time_total_s: 1531.7896733283997
timers:
  learn_throughput: 358.24
  learn_time_ms: 46058.551
  load_throughput: 4240798.573
  load_time_ms: 3.891
  training_iteration_time_ms: 60268.055
  update_time_ms: 2.726
timesteps_total: 396000
training_iteration: 24

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9344262295081968
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7090909090909091
  reward for individual goal_min: 0.5
episode_len_mean: 142.69827586206895
episode_reward_max: 2.0
episode_reward_mean: 1.6551724137931034
episode_reward_min: 0.0
episodes_this_iter: 116
episodes_total: 2051
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.896551724137931
  agent_1: 0.7586206896551724
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.6684205532074
time_total_s: 1526.1630289554596
timers:
  learn_throughput: 383.277
  learn_time_ms: 43049.858
  load_throughput: 4390297.525
  load_time_ms: 3.758
  training_iteration_time_ms: 56352.359
  update_time_ms: 2.578
timesteps_total: 412500
training_iteration: 25

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30434782608695654
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7012987012987013
  reward for individual goal_min: 0.0
episode_len_mean: 209.19
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 2114
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.56224465370178
time_total_s: 1525.255555152893
timers:
  learn_throughput: 461.518
  learn_time_ms: 35751.604
  load_throughput: 5316423.863
  load_time_ms: 3.104
  training_iteration_time_ms: 47168.497
  update_time_ms: 2.555
timesteps_total: 478500
training_iteration: 29

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17647058823529413
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.4759036144578313
  reward for individual goal_min: 0.0
episode_len_mean: 235.02
episode_reward_max: 2.0
episode_reward_mean: 0.7
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 1706
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.33
  agent_1: 0.37
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.91920351982117
time_total_s: 1519.358374595642
timers:
  learn_throughput: 368.438
  learn_time_ms: 44783.674
  load_throughput: 4580813.619
  load_time_ms: 3.602
  training_iteration_time_ms: 58770.416
  update_time_ms: 2.66
timesteps_total: 412500
training_iteration: 25

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6025641025641025
  reward for individual goal_min: 0.0
episode_len_mean: 218.98
episode_reward_max: 2.0
episode_reward_mean: 0.92
episode_reward_min: 0.0
episodes_this_iter: 71
episodes_total: 2106
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.41
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.506436586380005
time_total_s: 1556.7214932441711
timers:
  learn_throughput: 455.015
  learn_time_ms: 36262.569
  load_throughput: 4755217.986
  load_time_ms: 3.47
  training_iteration_time_ms: 47904.304
  update_time_ms: 2.647
timesteps_total: 495000
training_iteration: 30

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9782608695652174
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7790697674418605
  reward for individual goal_min: 0.5
episode_len_mean: 122.81060606060606
episode_reward_max: 2.0
episode_reward_mean: 1.696969696969697
episode_reward_min: 0.0
episodes_this_iter: 132
episodes_total: 3331
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7575757575757576
  agent_1: 0.9393939393939394
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.555888414382935
time_total_s: 1545.6094136238098
timers:
  learn_throughput: 518.85
  learn_time_ms: 31801.081
  load_throughput: 5623944.871
  load_time_ms: 2.934
  training_iteration_time_ms: 42799.024
  update_time_ms: 2.427
timesteps_total: 544500
training_iteration: 33

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9375
  reward for individual goal_min: 0.0
episode_len_mean: 194.1
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 2223
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.835447788238525
time_total_s: 1541.176679134369
timers:
  learn_throughput: 481.96
  learn_time_ms: 34235.227
  load_throughput: 5186030.109
  load_time_ms: 3.182
  training_iteration_time_ms: 45381.966
  update_time_ms: 2.509
timesteps_total: 495000
training_iteration: 30

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19736842105263158
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7635135135135135
  reward for individual goal_min: 0.0
episode_len_mean: 220.86
episode_reward_max: 2.0
episode_reward_mean: 1.01
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 1509
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.35992765426636
time_total_s: 1528.0790917873383
timers:
  learn_throughput: 329.06
  learn_time_ms: 50142.841
  load_throughput: 4143998.371
  load_time_ms: 3.982
  training_iteration_time_ms: 65060.717
  update_time_ms: 2.946
timesteps_total: 363000
training_iteration: 22

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9276315789473685
  reward for individual goal_min: 0.0
episode_len_mean: 204.55
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 1823
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.38996171951294
time_total_s: 1551.9459574222565
timers:
  learn_throughput: 397.473
  learn_time_ms: 41512.286
  load_throughput: 4860826.409
  load_time_ms: 3.394
  training_iteration_time_ms: 54493.979
  update_time_ms: 2.599
timesteps_total: 429000
training_iteration: 26

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7784810126582279
  reward for individual goal_min: 0.0
episode_len_mean: 208.03
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 1865
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.1772141456604
time_total_s: 1535.4843893051147
timers:
  learn_throughput: 384.92
  learn_time_ms: 42866.045
  load_throughput: 4711514.021
  load_time_ms: 3.502
  training_iteration_time_ms: 56162.638
  update_time_ms: 2.67
timesteps_total: 429000
training_iteration: 26

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23717948717948717
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6818181818181818
  reward for individual goal_min: 0.0
episode_len_mean: 219.14
episode_reward_max: 2.0
episode_reward_mean: 0.95
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 1620
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.47
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.75284552574158
time_total_s: 1546.4941744804382
timers:
  learn_throughput: 346.124
  learn_time_ms: 47670.84
  load_throughput: 4121564.629
  load_time_ms: 4.003
  training_iteration_time_ms: 62673.53
  update_time_ms: 3.002
timesteps_total: 379500
training_iteration: 23

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2230769230769231
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.68125
  reward for individual goal_min: 0.0
episode_len_mean: 214.31
episode_reward_max: 2.0
episode_reward_mean: 1.01
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 1944
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.58067178726196
time_total_s: 1562.7282376289368
timers:
  learn_throughput: 411.865
  learn_time_ms: 40061.718
  load_throughput: 5015801.009
  load_time_ms: 3.29
  training_iteration_time_ms: 52633.802
  update_time_ms: 2.572
timesteps_total: 445500
training_iteration: 27

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35064935064935066
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6381578947368421
  reward for individual goal_min: 0.0
episode_len_mean: 214.33
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 2194
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.57417964935303
time_total_s: 1570.829734802246
timers:
  learn_throughput: 465.506
  learn_time_ms: 35445.332
  load_throughput: 5346405.859
  load_time_ms: 3.086
  training_iteration_time_ms: 46849.953
  update_time_ms: 2.529
timesteps_total: 495000
training_iteration: 30

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9444444444444444
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.831081081081081
  reward for individual goal_min: 0.5
episode_len_mean: 100.65243902439025
episode_reward_max: 2.0
episode_reward_mean: 1.7865853658536586
episode_reward_min: 0.0
episodes_this_iter: 164
episodes_total: 2800
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.823170731707317
  agent_1: 0.9634146341463414
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.32118844985962
time_total_s: 1576.61753988266
timers:
  learn_throughput: 409.878
  learn_time_ms: 40255.871
  load_throughput: 5097560.897
  load_time_ms: 3.237
  training_iteration_time_ms: 53025.037
  update_time_ms: 2.67
timesteps_total: 462000
training_iteration: 28

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19642857142857142
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9513888888888888
  reward for individual goal_min: 0.0
episode_len_mean: 205.95
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 1903
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.8174774646759
time_total_s: 1570.8683230876923
timers:
  learn_throughput: 389.034
  learn_time_ms: 42412.717
  load_throughput: 4991670.405
  load_time_ms: 3.306
  training_iteration_time_ms: 55514.527
  update_time_ms: 2.707
timesteps_total: 429000
training_iteration: 26

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9148936170212766
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7333333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 153.52336448598132
episode_reward_max: 2.0
episode_reward_mean: 1.6261682242990654
episode_reward_min: 0.0
episodes_this_iter: 107
episodes_total: 2158
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8504672897196262
  agent_1: 0.7757009345794392
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.07016944885254
time_total_s: 1579.2331984043121
timers:
  learn_throughput: 388.341
  learn_time_ms: 42488.384
  load_throughput: 4392220.1
  load_time_ms: 3.757
  training_iteration_time_ms: 55658.687
  update_time_ms: 2.575
timesteps_total: 429000
training_iteration: 26

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9523809523809523
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8181818181818182
  reward for individual goal_min: 0.5
episode_len_mean: 104.62732919254658
episode_reward_max: 2.0
episode_reward_mean: 1.7763975155279503
episode_reward_min: 0.0
episodes_this_iter: 161
episodes_total: 3492
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8322981366459627
  agent_1: 0.9440993788819876
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.650139808654785
time_total_s: 1590.2595534324646
timers:
  learn_throughput: 517.296
  learn_time_ms: 31896.651
  load_throughput: 5694140.646
  load_time_ms: 2.898
  training_iteration_time_ms: 42916.878
  update_time_ms: 2.432
timesteps_total: 561000
training_iteration: 34

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2887323943661972
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8972602739726028
  reward for individual goal_min: 0.0
episode_len_mean: 179.53
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 2314
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.76096487045288
time_total_s: 1585.9376440048218
timers:
  learn_throughput: 481.724
  learn_time_ms: 34252.004
  load_throughput: 5331742.373
  load_time_ms: 3.095
  training_iteration_time_ms: 45447.154
  update_time_ms: 2.493
timesteps_total: 511500
training_iteration: 31

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7090909090909091
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5555555555555556
  reward for individual goal_min: 0.5
episode_len_mean: 172.05
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 2013
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.62041401863098
time_total_s: 1595.4100873470306
timers:
  learn_throughput: 354.72
  learn_time_ms: 46515.56
  load_throughput: 4118327.105
  load_time_ms: 4.006
  training_iteration_time_ms: 60916.449
  update_time_ms: 2.716
timesteps_total: 412500
training_iteration: 25

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6392405063291139
  reward for individual goal_min: 0.0
episode_len_mean: 221.12
episode_reward_max: 2.0
episode_reward_mean: 0.83
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 2180
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.45
  agent_1: 0.38
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.607125997543335
time_total_s: 1603.3286192417145
timers:
  learn_throughput: 458.294
  learn_time_ms: 36003.127
  load_throughput: 4765827.853
  load_time_ms: 3.462
  training_iteration_time_ms: 47608.113
  update_time_ms: 2.626
timesteps_total: 511500
training_iteration: 31

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2028985507246377
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.48
  reward for individual goal_min: 0.0
episode_len_mean: 231.23
episode_reward_max: 2.0
episode_reward_mean: 0.7
episode_reward_min: 0.0
episodes_this_iter: 69
episodes_total: 1775
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.34
  agent_1: 0.36
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.982720375061035
time_total_s: 1578.3410949707031
timers:
  learn_throughput: 366.033
  learn_time_ms: 45077.856
  load_throughput: 4515389.223
  load_time_ms: 3.654
  training_iteration_time_ms: 59078.621
  update_time_ms: 2.654
timesteps_total: 429000
training_iteration: 26

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20270270270270271
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8963414634146342
  reward for individual goal_min: 0.0
episode_len_mean: 203.62
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 1902
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.944929361343384
time_total_s: 1601.8908867835999
timers:
  learn_throughput: 401.429
  learn_time_ms: 41103.109
  load_throughput: 4868725.799
  load_time_ms: 3.389
  training_iteration_time_ms: 53907.713
  update_time_ms: 2.61
timesteps_total: 445500
training_iteration: 27

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28169014084507044
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7014925373134329
  reward for individual goal_min: 0.0
episode_len_mean: 222.48
episode_reward_max: 2.0
episode_reward_mean: 1.01
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 1582
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.282472372055054
time_total_s: 1589.3615641593933
timers:
  learn_throughput: 331.734
  learn_time_ms: 49738.625
  load_throughput: 4152028.798
  load_time_ms: 3.974
  training_iteration_time_ms: 64531.807
  update_time_ms: 2.765
timesteps_total: 379500
training_iteration: 23

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3223684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7662337662337663
  reward for individual goal_min: 0.0
episode_len_mean: 207.41
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 1946
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.608856439590454
time_total_s: 1589.0932457447052
timers:
  learn_throughput: 389.894
  learn_time_ms: 42319.226
  load_throughput: 4723347.552
  load_time_ms: 3.493
  training_iteration_time_ms: 55518.78
  update_time_ms: 2.682
timesteps_total: 445500
training_iteration: 27

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18243243243243243
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7784810126582279
  reward for individual goal_min: 0.0
episode_len_mean: 211.47
episode_reward_max: 2.0
episode_reward_mean: 1.03
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 1696
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.49
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.138922929763794
time_total_s: 1605.633097410202
timers:
  learn_throughput: 350.634
  learn_time_ms: 47057.622
  load_throughput: 4140031.945
  load_time_ms: 3.985
  training_iteration_time_ms: 61927.959
  update_time_ms: 3.013
timesteps_total: 396000
training_iteration: 24

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3076923076923077
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7333333333333333
  reward for individual goal_min: 0.0
episode_len_mean: 201.11
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 2274
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.133742332458496
time_total_s: 1616.9634771347046
timers:
  learn_throughput: 466.008
  learn_time_ms: 35407.149
  load_throughput: 5367220.611
  load_time_ms: 3.074
  training_iteration_time_ms: 46811.78
  update_time_ms: 2.518
timesteps_total: 511500
training_iteration: 31

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7207792207792207
  reward for individual goal_min: 0.0
episode_len_mean: 213.21
episode_reward_max: 2.0
episode_reward_mean: 0.97
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 2021
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.6401641368866
time_total_s: 1617.3684017658234
timers:
  learn_throughput: 412.632
  learn_time_ms: 39987.172
  load_throughput: 5008685.986
  load_time_ms: 3.294
  training_iteration_time_ms: 52501.689
  update_time_ms: 2.604
timesteps_total: 462000
training_iteration: 28

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.935064935064935
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8012820512820513
  reward for individual goal_min: 0.5
episode_len_mean: 107.05806451612904
episode_reward_max: 2.0
episode_reward_mean: 1.735483870967742
episode_reward_min: 0.0
episodes_this_iter: 155
episodes_total: 2955
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7741935483870968
  agent_1: 0.9612903225806452
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.88674879074097
time_total_s: 1628.5042886734009
timers:
  learn_throughput: 412.009
  learn_time_ms: 40047.632
  load_throughput: 4987317.748
  load_time_ms: 3.308
  training_iteration_time_ms: 52717.193
  update_time_ms: 2.651
timesteps_total: 478500
training_iteration: 29

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9025974025974026
  reward for individual goal_min: 0.0
episode_len_mean: 194.01
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 1987
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.75146555900574
time_total_s: 1625.619788646698
timers:
  learn_throughput: 389.046
  learn_time_ms: 42411.43
  load_throughput: 4995778.213
  load_time_ms: 3.303
  training_iteration_time_ms: 55517.121
  update_time_ms: 2.693
timesteps_total: 445500
training_iteration: 27

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9764705882352941
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 96.53488372093024
episode_reward_max: 2.0
episode_reward_mean: 1.808139534883721
episode_reward_min: 0.0
episodes_this_iter: 172
episodes_total: 3664
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8430232558139535
  agent_1: 0.9651162790697675
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.3269989490509
time_total_s: 1633.5865523815155
timers:
  learn_throughput: 517.175
  learn_time_ms: 31904.086
  load_throughput: 5689927.238
  load_time_ms: 2.9
  training_iteration_time_ms: 42862.846
  update_time_ms: 2.419
timesteps_total: 577500
training_iteration: 35

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7714285714285715
  reward for individual goal_min: 0.5
episode_len_mean: 122.15328467153284
episode_reward_max: 2.0
episode_reward_mean: 1.7664233576642336
episode_reward_min: 1.0
episodes_this_iter: 137
episodes_total: 2295
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8978102189781022
  agent_1: 0.8686131386861314
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.139302253723145
time_total_s: 1632.3725006580353
timers:
  learn_throughput: 391.398
  learn_time_ms: 42156.59
  load_throughput: 4402894.461
  load_time_ms: 3.748
  training_iteration_time_ms: 55168.228
  update_time_ms: 2.59
timesteps_total: 445500
training_iteration: 27

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23076923076923078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8924050632911392
  reward for individual goal_min: 0.0
episode_len_mean: 197.29
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 2398
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.48
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.724215507507324
time_total_s: 1630.661859512329
timers:
  learn_throughput: 487.187
  learn_time_ms: 33867.89
  load_throughput: 5335277.302
  load_time_ms: 3.093
  training_iteration_time_ms: 45120.298
  update_time_ms: 2.517
timesteps_total: 528000
training_iteration: 32

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1891891891891892
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6481481481481481
  reward for individual goal_min: 0.0
episode_len_mean: 218.69
episode_reward_max: 2.0
episode_reward_mean: 0.91
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 2259
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.38
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.27964735031128
time_total_s: 1648.6082665920258
timers:
  learn_throughput: 464.395
  learn_time_ms: 35530.072
  load_throughput: 4795250.62
  load_time_ms: 3.441
  training_iteration_time_ms: 47054.337
  update_time_ms: 2.605
timesteps_total: 528000
training_iteration: 32

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7608695652173914
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6481481481481481
  reward for individual goal_min: 0.5
episode_len_mean: 165.56
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 2113
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.81196665763855
time_total_s: 1657.2220540046692
timers:
  learn_throughput: 352.642
  learn_time_ms: 46789.67
  load_throughput: 4099469.603
  load_time_ms: 4.025
  training_iteration_time_ms: 61222.7
  update_time_ms: 2.744
timesteps_total: 429000
training_iteration: 26

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8975903614457831
  reward for individual goal_min: 0.0
episode_len_mean: 211.18
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 1979
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.44
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.92045736312866
time_total_s: 1653.8113441467285
timers:
  learn_throughput: 405.051
  learn_time_ms: 40735.595
  load_throughput: 4911920.735
  load_time_ms: 3.359
  training_iteration_time_ms: 53421.355
  update_time_ms: 2.604
timesteps_total: 462000
training_iteration: 28

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27611940298507465
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.564935064935065
  reward for individual goal_min: 0.0
episode_len_mean: 228.43
episode_reward_max: 2.0
episode_reward_mean: 0.86
episode_reward_min: 0.0
episodes_this_iter: 71
episodes_total: 1846
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.38
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.10609006881714
time_total_s: 1636.4471850395203
timers:
  learn_throughput: 368.059
  learn_time_ms: 44829.759
  load_throughput: 4471597.229
  load_time_ms: 3.69
  training_iteration_time_ms: 58736.046
  update_time_ms: 2.672
timesteps_total: 445500
training_iteration: 27

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2328767123287671
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6626506024096386
  reward for individual goal_min: 0.0
episode_len_mean: 216.92
episode_reward_max: 2.0
episode_reward_mean: 0.95
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 2348
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.48
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.69616341590881
time_total_s: 1662.6596405506134
timers:
  learn_throughput: 468.58
  learn_time_ms: 35212.803
  load_throughput: 5357456.513
  load_time_ms: 3.08
  training_iteration_time_ms: 46523.943
  update_time_ms: 2.526
timesteps_total: 528000
training_iteration: 32

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.38461538461538464
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8214285714285714
  reward for individual goal_min: 0.0
episode_len_mean: 190.79
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 2030
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.172502279281616
time_total_s: 1646.2657480239868
timers:
  learn_throughput: 388.745
  learn_time_ms: 42444.227
  load_throughput: 4721414.118
  load_time_ms: 3.495
  training_iteration_time_ms: 55636.26
  update_time_ms: 2.687
timesteps_total: 462000
training_iteration: 28

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29850746268656714
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7039473684210527
  reward for individual goal_min: 0.0
episode_len_mean: 212.89
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 1663
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.573389291763306
time_total_s: 1651.9349534511566
timers:
  learn_throughput: 335.866
  learn_time_ms: 49126.714
  load_throughput: 4203577.346
  load_time_ms: 3.925
  training_iteration_time_ms: 63782.242
  update_time_ms: 2.758
timesteps_total: 396000
training_iteration: 24

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2682926829268293
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.75
  reward for individual goal_min: 0.0
episode_len_mean: 217.62
episode_reward_max: 2.0
episode_reward_mean: 0.98
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 2098
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.48
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.661001205444336
time_total_s: 1669.0294029712677
timers:
  learn_throughput: 412.877
  learn_time_ms: 39963.435
  load_throughput: 4983116.193
  load_time_ms: 3.311
  training_iteration_time_ms: 52473.211
  update_time_ms: 2.618
timesteps_total: 478500
training_iteration: 29

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.98989898989899
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8433734939759037
  reward for individual goal_min: 0.5
episode_len_mean: 90.96153846153847
episode_reward_max: 2.0
episode_reward_mean: 1.8461538461538463
episode_reward_min: 0.0
episodes_this_iter: 182
episodes_total: 3846
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8571428571428571
  agent_1: 0.989010989010989
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.62819218635559
time_total_s: 1677.214744567871
timers:
  learn_throughput: 515.905
  learn_time_ms: 31982.622
  load_throughput: 5696437.238
  load_time_ms: 2.897
  training_iteration_time_ms: 42968.604
  update_time_ms: 2.403
timesteps_total: 594000
training_iteration: 36

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15492957746478872
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7823529411764706
  reward for individual goal_min: 0.0
episode_len_mean: 218.58
episode_reward_max: 2.0
episode_reward_mean: 1.06
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 1769
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.366085052490234
time_total_s: 1663.9991824626923
timers:
  learn_throughput: 352.995
  learn_time_ms: 46742.868
  load_throughput: 4113553.694
  load_time_ms: 4.011
  training_iteration_time_ms: 61537.149
  update_time_ms: 3.04
timesteps_total: 412500
training_iteration: 25

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8734177215189873
  reward for individual goal_min: 0.0
episode_len_mean: 201.98
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 2478
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.336463928222656
time_total_s: 1673.9983234405518
timers:
  learn_throughput: 492.484
  learn_time_ms: 33503.596
  load_throughput: 5361025.633
  load_time_ms: 3.078
  training_iteration_time_ms: 44749.548
  update_time_ms: 2.518
timesteps_total: 544500
training_iteration: 33

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9418604651162791
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8452380952380952
  reward for individual goal_min: 0.5
episode_len_mean: 97.39411764705882
episode_reward_max: 2.0
episode_reward_mean: 1.7882352941176471
episode_reward_min: 0.0
episodes_this_iter: 170
episodes_total: 3125
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8235294117647058
  agent_1: 0.9647058823529412
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.43919110298157
time_total_s: 1681.9434797763824
timers:
  learn_throughput: 413.467
  learn_time_ms: 39906.44
  load_throughput: 5028629.89
  load_time_ms: 3.281
  training_iteration_time_ms: 52543.713
  update_time_ms: 2.64
timesteps_total: 495000
training_iteration: 30

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7098765432098766
  reward for individual goal_min: 0.0
episode_len_mean: 205.74
episode_reward_max: 2.0
episode_reward_mean: 0.99
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 2340
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.43
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.67083501815796
time_total_s: 1694.2791016101837
timers:
  learn_throughput: 467.386
  learn_time_ms: 35302.732
  load_throughput: 4824500.756
  load_time_ms: 3.42
  training_iteration_time_ms: 46770.324
  update_time_ms: 2.571
timesteps_total: 544500
training_iteration: 33

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20833333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8988095238095238
  reward for individual goal_min: 0.0
episode_len_mean: 197.58
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 2073
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.51013517379761
time_total_s: 1680.1299238204956
timers:
  learn_throughput: 389.321
  learn_time_ms: 42381.497
  load_throughput: 4950181.753
  load_time_ms: 3.333
  training_iteration_time_ms: 55422.783
  update_time_ms: 2.695
timesteps_total: 462000
training_iteration: 28

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9857142857142858
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7692307692307693
  reward for individual goal_min: 0.5
episode_len_mean: 122.74074074074075
episode_reward_max: 2.0
episode_reward_mean: 1.762962962962963
episode_reward_min: 0.0
episodes_this_iter: 135
episodes_total: 2430
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9111111111111111
  agent_1: 0.8518518518518519
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.32882022857666
time_total_s: 1686.701320886612
timers:
  learn_throughput: 393.752
  learn_time_ms: 41904.498
  load_throughput: 4431907.988
  load_time_ms: 3.723
  training_iteration_time_ms: 54863.636
  update_time_ms: 2.593
timesteps_total: 462000
training_iteration: 28

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23076923076923078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9533333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 192.9
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 2068
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.27212309837341
time_total_s: 1706.083467245102
timers:
  learn_throughput: 409.481
  learn_time_ms: 40294.861
  load_throughput: 4921001.749
  load_time_ms: 3.353
  training_iteration_time_ms: 52840.118
  update_time_ms: 2.586
timesteps_total: 478500
training_iteration: 29

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36764705882352944
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7066666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 193.57
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 2436
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.56068444252014
time_total_s: 1710.2203249931335
timers:
  learn_throughput: 469.791
  learn_time_ms: 35122.03
  load_throughput: 5344258.973
  load_time_ms: 3.087
  training_iteration_time_ms: 46400.69
  update_time_ms: 2.524
timesteps_total: 544500
training_iteration: 33

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7966101694915254
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.63
  reward for individual goal_min: 0.5
episode_len_mean: 152.38532110091742
episode_reward_max: 2.0
episode_reward_mean: 1.4403669724770642
episode_reward_min: 0.0
episodes_this_iter: 109
episodes_total: 2222
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7614678899082569
  agent_1: 0.6788990825688074
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.32435441017151
time_total_s: 1720.5464084148407
timers:
  learn_throughput: 349.218
  learn_time_ms: 47248.382
  load_throughput: 4036277.616
  load_time_ms: 4.088
  training_iteration_time_ms: 61943.225
  update_time_ms: 2.712
timesteps_total: 445500
training_iteration: 27

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1893939393939394
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5602409638554217
  reward for individual goal_min: 0.0
episode_len_mean: 234.83
episode_reward_max: 2.0
episode_reward_mean: 0.79
episode_reward_min: 0.0
episodes_this_iter: 70
episodes_total: 1916
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.36
  agent_1: 0.43
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.43148994445801
time_total_s: 1694.8786749839783
timers:
  learn_throughput: 369.187
  learn_time_ms: 44692.854
  load_throughput: 4491097.498
  load_time_ms: 3.674
  training_iteration_time_ms: 58571.709
  update_time_ms: 2.66
timesteps_total: 462000
training_iteration: 28

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9882352941176471
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7931034482758621
  reward for individual goal_min: 0.5
episode_len_mean: 96.34883720930233
episode_reward_max: 2.0
episode_reward_mean: 1.7790697674418605
episode_reward_min: 0.0
episodes_this_iter: 172
episodes_total: 4018
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8081395348837209
  agent_1: 0.9709302325581395
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.250006675720215
time_total_s: 1719.4647512435913
timers:
  learn_throughput: 516.458
  learn_time_ms: 31948.411
  load_throughput: 5673600.866
  load_time_ms: 2.908
  training_iteration_time_ms: 42946.787
  update_time_ms: 2.401
timesteps_total: 610500
training_iteration: 37

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7705882352941177
  reward for individual goal_min: 0.0
episode_len_mean: 199.72
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 2112
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.547332763671875
time_total_s: 1696.8130807876587
timers:
  learn_throughput: 393.61
  learn_time_ms: 41919.688
  load_throughput: 4699548.149
  load_time_ms: 3.511
  training_iteration_time_ms: 55041.275
  update_time_ms: 2.689
timesteps_total: 478500
training_iteration: 29

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26973684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9166666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 193.02
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 2564
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.52130913734436
time_total_s: 1720.5196325778961
timers:
  learn_throughput: 490.779
  learn_time_ms: 33620.0
  load_throughput: 5315933.818
  load_time_ms: 3.104
  training_iteration_time_ms: 44924.394
  update_time_ms: 2.529
timesteps_total: 561000
training_iteration: 34

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23026315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6363636363636364
  reward for individual goal_min: 0.0
episode_len_mean: 218.09
episode_reward_max: 2.0
episode_reward_mean: 0.89
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 2414
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.39
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.42580580711365
time_total_s: 1739.7049074172974
timers:
  learn_throughput: 472.726
  learn_time_ms: 34903.956
  load_throughput: 4790603.481
  load_time_ms: 3.444
  training_iteration_time_ms: 46353.984
  update_time_ms: 2.536
timesteps_total: 561000
training_iteration: 34

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36666666666666664
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6558441558441559
  reward for individual goal_min: 0.0
episode_len_mean: 205.61
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 2180
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.51730537414551
time_total_s: 1724.5467083454132
timers:
  learn_throughput: 410.585
  learn_time_ms: 40186.564
  load_throughput: 4897945.872
  load_time_ms: 3.369
  training_iteration_time_ms: 52769.658
  update_time_ms: 2.635
timesteps_total: 495000
training_iteration: 30

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.740506329113924
  reward for individual goal_min: 0.0
episode_len_mean: 206.91
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 1745
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.55131196975708
time_total_s: 1713.4862654209137
timers:
  learn_throughput: 340.399
  learn_time_ms: 48472.471
  load_throughput: 4230274.149
  load_time_ms: 3.9
  training_iteration_time_ms: 63029.746
  update_time_ms: 2.755
timesteps_total: 412500
training_iteration: 25

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9895833333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8804347826086957
  reward for individual goal_min: 0.5
episode_len_mean: 86.13297872340425
episode_reward_max: 2.0
episode_reward_mean: 1.872340425531915
episode_reward_min: 0.0
episodes_this_iter: 188
episodes_total: 3313
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8829787234042553
  agent_1: 0.9893617021276596
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.75736117362976
time_total_s: 1735.7008409500122
timers:
  learn_throughput: 409.83
  learn_time_ms: 40260.554
  load_throughput: 5018456.161
  load_time_ms: 3.288
  training_iteration_time_ms: 52971.888
  update_time_ms: 2.609
timesteps_total: 511500
training_iteration: 31

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.75
  reward for individual goal_min: 0.0
episode_len_mean: 210.28
episode_reward_max: 2.0
episode_reward_mean: 0.98
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 1850
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.4
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.268559217453
time_total_s: 1725.2677416801453
timers:
  learn_throughput: 354.381
  learn_time_ms: 46560.076
  load_throughput: 4131825.786
  load_time_ms: 3.993
  training_iteration_time_ms: 61207.923
  update_time_ms: 3.05
timesteps_total: 429000
training_iteration: 26

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23648648648648649
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8703703703703703
  reward for individual goal_min: 0.0
episode_len_mean: 200.39
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 2157
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.361010789871216
time_total_s: 1735.4909346103668
timers:
  learn_throughput: 388.983
  learn_time_ms: 42418.321
  load_throughput: 4913943.594
  load_time_ms: 3.358
  training_iteration_time_ms: 55447.943
  update_time_ms: 2.697
timesteps_total: 478500
training_iteration: 29

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9871794871794872
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7692307692307693
  reward for individual goal_min: 0.5
episode_len_mean: 114.33566433566433
episode_reward_max: 2.0
episode_reward_mean: 1.7762237762237763
episode_reward_min: 0.0
episodes_this_iter: 143
episodes_total: 2573
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8811188811188811
  agent_1: 0.8951048951048951
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.5192494392395
time_total_s: 1740.2205703258514
timers:
  learn_throughput: 397.228
  learn_time_ms: 41537.853
  load_throughput: 4409234.121
  load_time_ms: 3.742
  training_iteration_time_ms: 54406.162
  update_time_ms: 2.584
timesteps_total: 478500
training_iteration: 29

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9876543209876543
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8409090909090909
  reward for individual goal_min: 0.5
episode_len_mean: 96.76331360946746
episode_reward_max: 2.0
episode_reward_mean: 1.8224852071005917
episode_reward_min: 0.0
episodes_this_iter: 169
episodes_total: 4187
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.834319526627219
  agent_1: 0.9881656804733728
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.34816312789917
time_total_s: 1762.8129143714905
timers:
  learn_throughput: 517.866
  learn_time_ms: 31861.501
  load_throughput: 5638745.57
  load_time_ms: 2.926
  training_iteration_time_ms: 42800.052
  update_time_ms: 2.375
timesteps_total: 627000
training_iteration: 38

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9733333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 190.44
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 2154
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.939334869384766
time_total_s: 1759.0228021144867
timers:
  learn_throughput: 412.201
  learn_time_ms: 40028.981
  load_throughput: 4944982.28
  load_time_ms: 3.337
  training_iteration_time_ms: 52496.967
  update_time_ms: 2.599
timesteps_total: 495000
training_iteration: 30

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.39864864864864863
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.765625
  reward for individual goal_min: 0.0
episode_len_mean: 195.32
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 2519
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.449912786483765
time_total_s: 1759.6702377796173
timers:
  learn_throughput: 469.084
  learn_time_ms: 35174.914
  load_throughput: 5312913.865
  load_time_ms: 3.106
  training_iteration_time_ms: 46448.555
  update_time_ms: 2.514
timesteps_total: 561000
training_iteration: 34

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9487179487179487
  reward for individual goal_min: 0.0
episode_len_mean: 186.16
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 2652
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.93433856964111
time_total_s: 1768.4539711475372
timers:
  learn_throughput: 488.168
  learn_time_ms: 33799.818
  load_throughput: 5263775.594
  load_time_ms: 3.135
  training_iteration_time_ms: 45175.675
  update_time_ms: 2.532
timesteps_total: 577500
training_iteration: 35

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21604938271604937
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6690140845070423
  reward for individual goal_min: 0.0
episode_len_mean: 220.84
episode_reward_max: 2.0
episode_reward_mean: 0.86
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 2488
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.33
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.931007385253906
time_total_s: 1785.6359148025513
timers:
  learn_throughput: 473.337
  learn_time_ms: 34858.877
  load_throughput: 4844290.324
  load_time_ms: 3.406
  training_iteration_time_ms: 46255.889
  update_time_ms: 2.523
timesteps_total: 577500
training_iteration: 35

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2077922077922078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6351351351351351
  reward for individual goal_min: 0.0
episode_len_mean: 227.96
episode_reward_max: 2.0
episode_reward_mean: 0.87
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 1990
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.36
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.748146295547485
time_total_s: 1753.6268212795258
timers:
  learn_throughput: 369.509
  learn_time_ms: 44653.888
  load_throughput: 4444859.376
  load_time_ms: 3.712
  training_iteration_time_ms: 58512.75
  update_time_ms: 2.663
timesteps_total: 478500
training_iteration: 29

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2597402597402597
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7590361445783133
  reward for individual goal_min: 0.0
episode_len_mean: 211.54
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 2189
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.7010498046875
time_total_s: 1752.5141305923462
timers:
  learn_throughput: 394.724
  learn_time_ms: 41801.355
  load_throughput: 4673619.039
  load_time_ms: 3.53
  training_iteration_time_ms: 54935.893
  update_time_ms: 2.671
timesteps_total: 495000
training_iteration: 30

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8548387096774194
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6875
  reward for individual goal_min: 0.5
episode_len_mean: 139.82203389830508
episode_reward_max: 2.0
episode_reward_mean: 1.5508474576271187
episode_reward_min: 0.0
episodes_this_iter: 118
episodes_total: 2340
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8135593220338984
  agent_1: 0.7372881355932204
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.62357258796692
time_total_s: 1782.1699810028076
timers:
  learn_throughput: 348.087
  learn_time_ms: 47402.006
  load_throughput: 3876088.851
  load_time_ms: 4.257
  training_iteration_time_ms: 62134.724
  update_time_ms: 2.705
timesteps_total: 462000
training_iteration: 28

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8058823529411765
  reward for individual goal_min: 0.5
episode_len_mean: 100.70186335403727
episode_reward_max: 2.0
episode_reward_mean: 1.795031055900621
episode_reward_min: 1.0
episodes_this_iter: 161
episodes_total: 3474
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7950310559006211
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 50.52682137489319
time_total_s: 1786.2276623249054
timers:
  learn_throughput: 411.893
  learn_time_ms: 40058.934
  load_throughput: 4908680.659
  load_time_ms: 3.361
  training_iteration_time_ms: 52732.695
  update_time_ms: 2.581
timesteps_total: 528000
training_iteration: 32

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3359375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6604938271604939
  reward for individual goal_min: 0.0
episode_len_mean: 196.9
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 2261
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.71548557281494
time_total_s: 1780.2621939182281
timers:
  learn_throughput: 407.397
  learn_time_ms: 40500.999
  load_throughput: 4838904.769
  load_time_ms: 3.41
  training_iteration_time_ms: 53196.286
  update_time_ms: 2.647
timesteps_total: 511500
training_iteration: 31

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36486486486486486
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8278688524590164
  reward for individual goal_min: 0.0
episode_len_mean: 202.68
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 1824
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.20501184463501
time_total_s: 1773.6912772655487
timers:
  learn_throughput: 345.458
  learn_time_ms: 47762.626
  load_throughput: 4215560.645
  load_time_ms: 3.914
  training_iteration_time_ms: 62213.952
  update_time_ms: 2.782
timesteps_total: 429000
training_iteration: 26

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7794117647058824
  reward for individual goal_min: 0.5
episode_len_mean: 109.75510204081633
episode_reward_max: 2.0
episode_reward_mean: 1.7959183673469388
episode_reward_min: 1.0
episodes_this_iter: 147
episodes_total: 2720
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9387755102040817
  agent_1: 0.8571428571428571
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.55223560333252
time_total_s: 1793.772805929184
timers:
  learn_throughput: 397.471
  learn_time_ms: 41512.504
  load_throughput: 4403426.739
  load_time_ms: 3.747
  training_iteration_time_ms: 54338.098
  update_time_ms: 2.594
timesteps_total: 495000
training_iteration: 30

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8857142857142857
  reward for individual goal_min: 0.0
episode_len_mean: 196.41
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 2242
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.6582612991333
time_total_s: 1791.1491959095001
timers:
  learn_throughput: 390.812
  learn_time_ms: 42219.837
  load_throughput: 4976916.594
  load_time_ms: 3.315
  training_iteration_time_ms: 55306.161
  update_time_ms: 2.71
timesteps_total: 495000
training_iteration: 30

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19863013698630136
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8
  reward for individual goal_min: 0.0
episode_len_mean: 215.31
episode_reward_max: 2.0
episode_reward_mean: 1.04
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 1926
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.36889028549194
time_total_s: 1782.6366319656372
timers:
  learn_throughput: 356.812
  learn_time_ms: 46242.794
  load_throughput: 4129976.487
  load_time_ms: 3.995
  training_iteration_time_ms: 60758.952
  update_time_ms: 3.03
timesteps_total: 445500
training_iteration: 27

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.989247311827957
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8037974683544303
  reward for individual goal_min: 0.5
episode_len_mean: 95.1686046511628
episode_reward_max: 2.0
episode_reward_mean: 1.808139534883721
episode_reward_min: 0.0
episodes_this_iter: 172
episodes_total: 4359
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8313953488372093
  agent_1: 0.9767441860465116
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.81283950805664
time_total_s: 1805.6257538795471
timers:
  learn_throughput: 518.899
  learn_time_ms: 31798.094
  load_throughput: 5685533.219
  load_time_ms: 2.902
  training_iteration_time_ms: 42701.975
  update_time_ms: 2.37
timesteps_total: 643500
training_iteration: 39

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34285714285714286
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.19
episode_reward_max: 2.0
episode_reward_mean: 1.45
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 2248
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.75
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.92507243156433
time_total_s: 1808.947874546051
timers:
  learn_throughput: 414.839
  learn_time_ms: 39774.513
  load_throughput: 4980211.569
  load_time_ms: 3.313
  training_iteration_time_ms: 52195.452
  update_time_ms: 2.599
timesteps_total: 511500
training_iteration: 31

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.325
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.671875
  reward for individual goal_min: 0.0
episode_len_mean: 204.17
episode_reward_max: 2.0
episode_reward_mean: 0.99
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 2599
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.45
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.00681161880493
time_total_s: 1810.6770493984222
timers:
  learn_throughput: 465.211
  learn_time_ms: 35467.805
  load_throughput: 5267180.857
  load_time_ms: 3.133
  training_iteration_time_ms: 46848.532
  update_time_ms: 2.527
timesteps_total: 577500
training_iteration: 35

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9466666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 187.35
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 2738
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.36761689186096
time_total_s: 1814.8215880393982
timers:
  learn_throughput: 487.678
  learn_time_ms: 33833.826
  load_throughput: 5261494.53
  load_time_ms: 3.136
  training_iteration_time_ms: 45186.18
  update_time_ms: 2.531
timesteps_total: 594000
training_iteration: 36

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6081081081081081
  reward for individual goal_min: 0.0
episode_len_mean: 221.3
episode_reward_max: 2.0
episode_reward_mean: 0.92
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 2566
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.36
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.369818449020386
time_total_s: 1832.0057332515717
timers:
  learn_throughput: 475.394
  learn_time_ms: 34708.044
  load_throughput: 4836774.18
  load_time_ms: 3.411
  training_iteration_time_ms: 46113.065
  update_time_ms: 2.518
timesteps_total: 594000
training_iteration: 36

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31756756756756754
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7638888888888888
  reward for individual goal_min: 0.0
episode_len_mean: 206.54
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 2270
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.31176710128784
time_total_s: 1806.825897693634
timers:
  learn_throughput: 397.626
  learn_time_ms: 41496.309
  load_throughput: 4705331.52
  load_time_ms: 3.507
  training_iteration_time_ms: 54688.915
  update_time_ms: 2.655
timesteps_total: 511500
training_iteration: 31

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6571428571428571
  reward for individual goal_min: 0.0
episode_len_mean: 227.99
episode_reward_max: 2.0
episode_reward_mean: 0.86
episode_reward_min: 0.0
episodes_this_iter: 72
episodes_total: 2062
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.44
  agent_1: 0.42
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.08416771888733
time_total_s: 1809.710988998413
timers:
  learn_throughput: 369.555
  learn_time_ms: 44648.331
  load_throughput: 4410133.248
  load_time_ms: 3.741
  training_iteration_time_ms: 58425.087
  update_time_ms: 2.682
timesteps_total: 495000
training_iteration: 30

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9714285714285714
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7797619047619048
  reward for individual goal_min: 0.5
episode_len_mean: 108.54545454545455
episode_reward_max: 2.0
episode_reward_mean: 1.7337662337662338
episode_reward_min: 0.0
episodes_this_iter: 154
episodes_total: 3628
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7532467532467533
  agent_1: 0.9805194805194806
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.38781261444092
time_total_s: 1837.6154749393463
timers:
  learn_throughput: 411.287
  learn_time_ms: 40117.962
  load_throughput: 4897911.208
  load_time_ms: 3.369
  training_iteration_time_ms: 52685.648
  update_time_ms: 2.597
timesteps_total: 544500
training_iteration: 33

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.864406779661017
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6636363636363637
  reward for individual goal_min: 0.5
episode_len_mean: 142.25438596491227
episode_reward_max: 2.0
episode_reward_mean: 1.5350877192982457
episode_reward_min: 0.0
episodes_this_iter: 114
episodes_total: 2454
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7456140350877193
  agent_1: 0.7894736842105263
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.44650340080261
time_total_s: 1843.6164844036102
timers:
  learn_throughput: 347.495
  learn_time_ms: 47482.66
  load_throughput: 3828740.498
  load_time_ms: 4.31
  training_iteration_time_ms: 62348.202
  update_time_ms: 2.795
timesteps_total: 478500
training_iteration: 29

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29850746268656714
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.675
  reward for individual goal_min: 0.0
episode_len_mean: 206.45
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 2343
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.47381591796875
time_total_s: 1834.736009836197
timers:
  learn_throughput: 404.976
  learn_time_ms: 40743.14
  load_throughput: 4807408.879
  load_time_ms: 3.432
  training_iteration_time_ms: 53546.459
  update_time_ms: 2.676
timesteps_total: 528000
training_iteration: 32

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.726027397260274
  reward for individual goal_min: 0.5
episode_len_mean: 128.4351145038168
episode_reward_max: 2.0
episode_reward_mean: 1.6946564885496183
episode_reward_min: 1.0
episodes_this_iter: 131
episodes_total: 2851
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8778625954198473
  agent_1: 0.816793893129771
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.48300218582153
time_total_s: 1846.2558081150055
timers:
  learn_throughput: 399.539
  learn_time_ms: 41297.634
  load_throughput: 4417283.096
  load_time_ms: 3.735
  training_iteration_time_ms: 54056.889
  update_time_ms: 2.61
timesteps_total: 511500
training_iteration: 31

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.38513513513513514
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7727272727272727
  reward for individual goal_min: 0.0
episode_len_mean: 205.04
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 1906
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.98580002784729
time_total_s: 1832.677077293396
timers:
  learn_throughput: 349.182
  learn_time_ms: 47253.361
  load_throughput: 4239317.848
  load_time_ms: 3.892
  training_iteration_time_ms: 61648.814
  update_time_ms: 2.795
timesteps_total: 445500
training_iteration: 27

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2323943661971831
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9415584415584416
  reward for individual goal_min: 0.0
episode_len_mean: 188.02
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 2328
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.66108536720276
time_total_s: 1845.8102812767029
timers:
  learn_throughput: 392.848
  learn_time_ms: 42001.025
  load_throughput: 4939088.632
  load_time_ms: 3.341
  training_iteration_time_ms: 55242.485
  update_time_ms: 2.703
timesteps_total: 511500
training_iteration: 31

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8
  reward for individual goal_min: 0.5
episode_len_mean: 95.95
episode_reward_max: 2.0
episode_reward_mean: 1.8
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8333333333333334
  agent_1: 0.9666666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.845360824742268
  reward for individual goal_min: 0.5
episode_len_mean: 90.04864864864865
episode_reward_max: 2.0
episode_reward_mean: 1.837837837837838
episode_reward_min: 1.0
episodes_this_iter: 185
episodes_total: 4544
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8540540540540541
  agent_1: 0.9837837837837838
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.149871826171875
time_total_s: 1855.775625705719
timers:
  learn_throughput: 519.837
  learn_time_ms: 31740.692
  load_throughput: 5673228.787
  load_time_ms: 2.908
  training_iteration_time_ms: 42661.943
  update_time_ms: 2.382
timesteps_total: 660000
training_iteration: 40

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-172kuwc_58/checkpoint_000040/checkpoint-40
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2972972972972973
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8698630136986302
  reward for individual goal_min: 0.0
episode_len_mean: 201.29
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 2010
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.741904735565186
time_total_s: 1844.3785367012024
timers:
  learn_throughput: 356.511
  learn_time_ms: 46281.933
  load_throughput: 4225779.656
  load_time_ms: 3.905
  training_iteration_time_ms: 60706.421
  update_time_ms: 3.048
timesteps_total: 462000
training_iteration: 28

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15384615384615385
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9825581395348837
  reward for individual goal_min: 0.0
episode_len_mean: 182.58
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 2337
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.77842831611633
time_total_s: 1857.7263028621674
timers:
  learn_throughput: 418.188
  learn_time_ms: 39455.984
  load_throughput: 4953618.69
  load_time_ms: 3.331
  training_iteration_time_ms: 51694.938
  update_time_ms: 2.619
timesteps_total: 528000
training_iteration: 32

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31690140845070425
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6756756756756757
  reward for individual goal_min: 0.0
episode_len_mean: 205.27
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 2645
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.551762104034424
time_total_s: 1874.557495355606
timers:
  learn_throughput: 482.349
  learn_time_ms: 34207.606
  load_throughput: 4890262.44
  load_time_ms: 3.374
  training_iteration_time_ms: 45546.931
  update_time_ms: 2.485
timesteps_total: 610500
training_iteration: 37

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28846153846153844
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7
  reward for individual goal_min: 0.0
episode_len_mean: 211.02
episode_reward_max: 2.0
episode_reward_mean: 0.97
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 2680
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.567527532577515
time_total_s: 1861.2445769309998
timers:
  learn_throughput: 461.968
  learn_time_ms: 35716.788
  load_throughput: 5217110.635
  load_time_ms: 3.163
  training_iteration_time_ms: 47158.219
  update_time_ms: 2.533
timesteps_total: 594000
training_iteration: 36

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3961038961038961
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9776119402985075
  reward for individual goal_min: 0.5
episode_len_mean: 182.89
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 2828
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.33190870285034
time_total_s: 1862.1534967422485
timers:
  learn_throughput: 485.19
  learn_time_ms: 34007.305
  load_throughput: 5258376.276
  load_time_ms: 3.138
  training_iteration_time_ms: 45400.655
  update_time_ms: 2.54
timesteps_total: 610500
training_iteration: 37

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2945205479452055
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8376623376623377
  reward for individual goal_min: 0.0
episode_len_mean: 190.96
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 2358
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.01797342300415
time_total_s: 1860.8438711166382
timers:
  learn_throughput: 398.391
  learn_time_ms: 41416.565
  load_throughput: 4730644.392
  load_time_ms: 3.488
  training_iteration_time_ms: 54571.417
  update_time_ms: 2.659
timesteps_total: 528000
training_iteration: 32

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9746835443037974
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7976190476190477
  reward for individual goal_min: 0.5
episode_len_mean: 103.08588957055214
episode_reward_max: 2.0
episode_reward_mean: 1.7668711656441718
episode_reward_min: 0.0
episodes_this_iter: 163
episodes_total: 3791
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7791411042944786
  agent_1: 0.9877300613496932
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.30807685852051
time_total_s: 1887.9235517978668
timers:
  learn_throughput: 413.842
  learn_time_ms: 39870.305
  load_throughput: 4880192.934
  load_time_ms: 3.381
  training_iteration_time_ms: 52375.046
  update_time_ms: 2.61
timesteps_total: 561000
training_iteration: 34

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24025974025974026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6538461538461539
  reward for individual goal_min: 0.0
episode_len_mean: 222.84
episode_reward_max: 2.0
episode_reward_mean: 0.94
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 2135
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.38
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.87730526924133
time_total_s: 1867.5882942676544
timers:
  learn_throughput: 371.016
  learn_time_ms: 44472.442
  load_throughput: 4448373.528
  load_time_ms: 3.709
  training_iteration_time_ms: 58231.024
  update_time_ms: 2.676
timesteps_total: 511500
training_iteration: 31

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33766233766233766
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6907894736842105
  reward for individual goal_min: 0.0
episode_len_mean: 207.78
episode_reward_max: 2.0
episode_reward_mean: 1.04
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 2422
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.45456075668335
time_total_s: 1887.1905705928802
timers:
  learn_throughput: 406.675
  learn_time_ms: 40572.897
  load_throughput: 4753062.506
  load_time_ms: 3.471
  training_iteration_time_ms: 53303.369
  update_time_ms: 2.691
timesteps_total: 544500
training_iteration: 33

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8823529411764706
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6612903225806451
  reward for individual goal_min: 0.5
episode_len_mean: 147.39823008849558
episode_reward_max: 2.0
episode_reward_mean: 1.5221238938053097
episode_reward_min: 0.0
episodes_this_iter: 113
episodes_total: 2567
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7522123893805309
  agent_1: 0.7699115044247787
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.19785189628601
time_total_s: 1903.8143362998962
timers:
  learn_throughput: 346.901
  learn_time_ms: 47563.974
  load_throughput: 3834255.765
  load_time_ms: 4.303
  training_iteration_time_ms: 62554.398
  update_time_ms: 2.805
timesteps_total: 495000
training_iteration: 30

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9805825242718447
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8148148148148148
  reward for individual goal_min: 0.5
episode_len_mean: 89.7336956521739
episode_reward_max: 2.0
episode_reward_mean: 1.815217391304348
episode_reward_min: 0.0
episodes_this_iter: 184
episodes_total: 4728
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8369565217391305
  agent_1: 0.9782608695652174
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.719802379608154
time_total_s: 1898.4954280853271
timers:
  learn_throughput: 518.043
  learn_time_ms: 31850.642
  load_throughput: 5617553.817
  load_time_ms: 2.937
  training_iteration_time_ms: 42770.925
  update_time_ms: 2.37
timesteps_total: 676500
training_iteration: 41

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8066666666666666
  reward for individual goal_min: 0.5
episode_len_mean: 104.31645569620254
episode_reward_max: 2.0
episode_reward_mean: 1.8164556962025316
episode_reward_min: 1.0
episodes_this_iter: 158
episodes_total: 3009
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9367088607594937
  agent_1: 0.879746835443038
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.1136953830719
time_total_s: 1900.3695034980774
timers:
  learn_throughput: 401.333
  learn_time_ms: 41112.99
  load_throughput: 4422533.534
  load_time_ms: 3.731
  training_iteration_time_ms: 53864.247
  update_time_ms: 2.654
timesteps_total: 528000
training_iteration: 32

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18382352941176472
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9329268292682927
  reward for individual goal_min: 0.0
episode_len_mean: 186.15
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 2418
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.75
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.197592973709106
time_total_s: 1897.007874250412
timers:
  learn_throughput: 401.68
  learn_time_ms: 41077.436
  load_throughput: 4904645.259
  load_time_ms: 3.364
  training_iteration_time_ms: 54284.627
  update_time_ms: 2.68
timesteps_total: 528000
training_iteration: 32

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2468354430379747
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6842105263157895
  reward for individual goal_min: 0.0
episode_len_mean: 219.63
episode_reward_max: 2.0
episode_reward_mean: 0.93
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 2721
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.4
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.27573776245117
time_total_s: 1918.8332331180573
timers:
  learn_throughput: 485.617
  learn_time_ms: 33977.412
  load_throughput: 4968341.494
  load_time_ms: 3.321
  training_iteration_time_ms: 45328.923
  update_time_ms: 2.455
timesteps_total: 627000
training_iteration: 38

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7847222222222222
  reward for individual goal_min: 0.0
episode_len_mean: 215.66
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 1979
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.47
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.98061728477478
time_total_s: 1890.6576945781708
timers:
  learn_throughput: 355.308
  learn_time_ms: 46438.537
  load_throughput: 4257207.465
  load_time_ms: 3.876
  training_iteration_time_ms: 60750.986
  update_time_ms: 2.797
timesteps_total: 462000
training_iteration: 28

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.94
  reward for individual goal_min: 0.0
episode_len_mean: 200.43
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 2420
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.764739751815796
time_total_s: 1907.4910426139832
timers:
  learn_throughput: 422.225
  learn_time_ms: 39078.686
  load_throughput: 5006186.008
  load_time_ms: 3.296
  training_iteration_time_ms: 51297.505
  update_time_ms: 2.62
timesteps_total: 544500
training_iteration: 33

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3013698630136986
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9743589743589743
  reward for individual goal_min: 0.0
episode_len_mean: 174.47
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 2922
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.78049159049988
time_total_s: 1906.9339883327484
timers:
  learn_throughput: 482.885
  learn_time_ms: 34169.619
  load_throughput: 5207962.976
  load_time_ms: 3.168
  training_iteration_time_ms: 45527.058
  update_time_ms: 2.561
timesteps_total: 627000
training_iteration: 38

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3263888888888889
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6466666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 204.5
episode_reward_max: 2.0
episode_reward_mean: 1.03
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 2761
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.26068925857544
time_total_s: 1908.5052661895752
timers:
  learn_throughput: 460.423
  learn_time_ms: 35836.598
  load_throughput: 5193268.548
  load_time_ms: 3.177
  training_iteration_time_ms: 47300.623
  update_time_ms: 2.533
timesteps_total: 610500
training_iteration: 37

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21428571428571427
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 198.66
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 2092
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.48
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.90112257003784
time_total_s: 1901.2796592712402
timers:
  learn_throughput: 360.536
  learn_time_ms: 45765.157
  load_throughput: 4271738.977
  load_time_ms: 3.863
  training_iteration_time_ms: 60054.722
  update_time_ms: 3.074
timesteps_total: 478500
training_iteration: 29

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27702702702702703
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8544303797468354
  reward for individual goal_min: 0.0
episode_len_mean: 196.57
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 2439
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.87308096885681
time_total_s: 1913.716952085495
timers:
  learn_throughput: 402.316
  learn_time_ms: 41012.529
  load_throughput: 4721027.621
  load_time_ms: 3.495
  training_iteration_time_ms: 54093.609
  update_time_ms: 2.651
timesteps_total: 544500
training_iteration: 33

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9805825242718447
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8511904761904762
  reward for individual goal_min: 0.5
episode_len_mean: 86.0909090909091
episode_reward_max: 2.0
episode_reward_mean: 1.8449197860962567
episode_reward_min: 0.0
episodes_this_iter: 187
episodes_total: 3978
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8663101604278075
  agent_1: 0.9786096256684492
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.153059244155884
time_total_s: 1940.0766110420227
timers:
  learn_throughput: 414.953
  learn_time_ms: 39763.528
  load_throughput: 4862294.916
  load_time_ms: 3.393
  training_iteration_time_ms: 52240.971
  update_time_ms: 2.627
timesteps_total: 577500
training_iteration: 35

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9879518072289156
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8157894736842105
  reward for individual goal_min: 0.5
episode_len_mean: 93.06179775280899
episode_reward_max: 2.0
episode_reward_mean: 1.7921348314606742
episode_reward_min: 0.0
episodes_this_iter: 178
episodes_total: 4906
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8033707865168539
  agent_1: 0.9887640449438202
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.853278160095215
time_total_s: 1940.3487062454224
timers:
  learn_throughput: 519.446
  learn_time_ms: 31764.6
  load_throughput: 5598286.361
  load_time_ms: 2.947
  training_iteration_time_ms: 42688.974
  update_time_ms: 2.367
timesteps_total: 693000
training_iteration: 42

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3246753246753247
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.68
  reward for individual goal_min: 0.0
episode_len_mean: 217.4
episode_reward_max: 2.0
episode_reward_mean: 1.02
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 2213
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.44
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.26286840438843
time_total_s: 1925.8511626720428
timers:
  learn_throughput: 371.339
  learn_time_ms: 44433.769
  load_throughput: 4375530.519
  load_time_ms: 3.771
  training_iteration_time_ms: 58247.812
  update_time_ms: 2.692
timesteps_total: 528000
training_iteration: 32

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.410958904109589
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.65
  reward for individual goal_min: 0.0
episode_len_mean: 186.51
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 2513
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.922858238220215
time_total_s: 1943.1134288311005
timers:
  learn_throughput: 402.507
  learn_time_ms: 40993.077
  load_throughput: 4736374.935
  load_time_ms: 3.484
  training_iteration_time_ms: 53849.98
  update_time_ms: 2.694
timesteps_total: 561000
training_iteration: 34

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9846153846153847
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7941176470588235
  reward for individual goal_min: 0.5
episode_len_mean: 111.39333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.7533333333333334
episode_reward_min: 0.0
episodes_this_iter: 150
episodes_total: 3159
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.94
  agent_1: 0.8133333333333334
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.446727991104126
time_total_s: 1950.8162314891815
timers:
  learn_throughput: 406.917
  learn_time_ms: 40548.767
  load_throughput: 4402810.428
  load_time_ms: 3.748
  training_iteration_time_ms: 53224.525
  update_time_ms: 2.655
timesteps_total: 544500
training_iteration: 33

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6458333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 224.74
episode_reward_max: 2.0
episode_reward_mean: 0.87
episode_reward_min: 0.0
episodes_this_iter: 75
episodes_total: 2796
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.35
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.01510691642761
time_total_s: 1964.8483400344849
timers:
  learn_throughput: 487.145
  learn_time_ms: 33870.792
  load_throughput: 5002892.751
  load_time_ms: 3.298
  training_iteration_time_ms: 45227.507
  update_time_ms: 2.478
timesteps_total: 643500
training_iteration: 39

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2692307692307692
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9102564102564102
  reward for individual goal_min: 0.0
episode_len_mean: 191.95
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 2505
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.52385115623474
time_total_s: 1952.5317254066467
timers:
  learn_throughput: 401.346
  learn_time_ms: 41111.672
  load_throughput: 4920476.932
  load_time_ms: 3.353
  training_iteration_time_ms: 54424.743
  update_time_ms: 2.689
timesteps_total: 544500
training_iteration: 33

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9552238805970149
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6825396825396826
  reward for individual goal_min: 0.5
episode_len_mean: 128.1076923076923
episode_reward_max: 2.0
episode_reward_mean: 1.646153846153846
episode_reward_min: 0.0
episodes_this_iter: 130
episodes_total: 2697
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8384615384615385
  agent_1: 0.8076923076923077
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.84634709358215
time_total_s: 1963.6606833934784
timers:
  learn_throughput: 349.565
  learn_time_ms: 47201.47
  load_throughput: 3825671.563
  load_time_ms: 4.313
  training_iteration_time_ms: 62148.098
  update_time_ms: 2.805
timesteps_total: 511500
training_iteration: 31

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2839506172839506
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9487179487179487
  reward for individual goal_min: 0.0
episode_len_mean: 180.19
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 3013
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.95300531387329
time_total_s: 1953.8869936466217
timers:
  learn_throughput: 481.251
  learn_time_ms: 34285.626
  load_throughput: 5201856.27
  load_time_ms: 3.172
  training_iteration_time_ms: 45617.623
  update_time_ms: 2.545
timesteps_total: 643500
training_iteration: 39

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21232876712328766
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9657534246575342
  reward for individual goal_min: 0.0
episode_len_mean: 192.91
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 2507
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.75084185600281
time_total_s: 1957.241884469986
timers:
  learn_throughput: 426.874
  learn_time_ms: 38653.061
  load_throughput: 5006620.608
  load_time_ms: 3.296
  training_iteration_time_ms: 50857.045
  update_time_ms: 2.612
timesteps_total: 561000
training_iteration: 34

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2986111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.620253164556962
  reward for individual goal_min: 0.0
episode_len_mean: 206.51
episode_reward_max: 2.0
episode_reward_mean: 1.0
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 2838
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.42
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.235066652297974
time_total_s: 1958.7403328418732
timers:
  learn_throughput: 456.06
  learn_time_ms: 36179.46
  load_throughput: 5185913.526
  load_time_ms: 3.182
  training_iteration_time_ms: 47768.608
  update_time_ms: 2.529
timesteps_total: 627000
training_iteration: 38

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3765432098765432
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.85
  reward for individual goal_min: 0.0
episode_len_mean: 183.53
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 2070
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.39311480522156
time_total_s: 1950.0508093833923
timers:
  learn_throughput: 359.637
  learn_time_ms: 45879.607
  load_throughput: 4262635.336
  load_time_ms: 3.871
  training_iteration_time_ms: 60119.492
  update_time_ms: 2.789
timesteps_total: 478500
training_iteration: 29

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2721518987341772
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8623188405797102
  reward for individual goal_min: 0.0
episode_len_mean: 204.33
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 2176
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.732441663742065
time_total_s: 1960.0121009349823
timers:
  learn_throughput: 362.24
  learn_time_ms: 45549.961
  load_throughput: 4280219.682
  load_time_ms: 3.855
  training_iteration_time_ms: 59781.084
  update_time_ms: 2.898
timesteps_total: 495000
training_iteration: 30

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.975
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8076923076923077
  reward for individual goal_min: 0.5
episode_len_mean: 97.60233918128655
episode_reward_max: 2.0
episode_reward_mean: 1.7719298245614035
episode_reward_min: 0.0
episodes_this_iter: 171
episodes_total: 5077
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7953216374269005
  agent_1: 0.9766081871345029
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.70796179771423
time_total_s: 1982.0566680431366
timers:
  learn_throughput: 517.645
  learn_time_ms: 31875.142
  load_throughput: 5575285.265
  load_time_ms: 2.959
  training_iteration_time_ms: 42803.399
  update_time_ms: 2.373
timesteps_total: 709500
training_iteration: 43

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8484848484848485
  reward for individual goal_min: 0.5
episode_len_mean: 85.98969072164948
episode_reward_max: 2.0
episode_reward_mean: 1.8453608247422681
episode_reward_min: 1.0
episodes_this_iter: 194
episodes_total: 4172
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8505154639175257
  agent_1: 0.9948453608247423
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.32176113128662
time_total_s: 1991.3983721733093
timers:
  learn_throughput: 416.281
  learn_time_ms: 39636.652
  load_throughput: 4901171.788
  load_time_ms: 3.367
  training_iteration_time_ms: 52071.975
  update_time_ms: 2.612
timesteps_total: 594000
training_iteration: 36

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31097560975609756
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8246753246753247
  reward for individual goal_min: 0.0
episode_len_mean: 194.19
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 2524
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.231982946395874
time_total_s: 1966.9489350318909
timers:
  learn_throughput: 404.283
  learn_time_ms: 40812.989
  load_throughput: 4708148.471
  load_time_ms: 3.505
  training_iteration_time_ms: 53879.191
  update_time_ms: 2.668
timesteps_total: 561000
training_iteration: 34

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2054794520547945
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7162162162162162
  reward for individual goal_min: 0.0
episode_len_mean: 225.85
episode_reward_max: 2.0
episode_reward_mean: 0.93
episode_reward_min: 0.0
episodes_this_iter: 70
episodes_total: 2283
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.36
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.71382975578308
time_total_s: 1979.564992427826
timers:
  learn_throughput: 374.68
  learn_time_ms: 44037.579
  load_throughput: 4343210.307
  load_time_ms: 3.799
  training_iteration_time_ms: 57818.254
  update_time_ms: 2.69
timesteps_total: 544500
training_iteration: 33

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.42105263157894735
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6691176470588235
  reward for individual goal_min: 0.0
episode_len_mean: 199.56
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 2590
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.40901708602905
time_total_s: 1996.5224459171295
timers:
  learn_throughput: 401.962
  learn_time_ms: 41048.661
  load_throughput: 4697028.37
  load_time_ms: 3.513
  training_iteration_time_ms: 53922.061
  update_time_ms: 2.703
timesteps_total: 577500
training_iteration: 35

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7014925373134329
  reward for individual goal_min: 0.5
episode_len_mean: 122.82835820895522
episode_reward_max: 2.0
episode_reward_mean: 1.7014925373134329
episode_reward_min: 1.0
episodes_this_iter: 134
episodes_total: 3293
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9029850746268657
  agent_1: 0.7985074626865671
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.36596941947937
time_total_s: 2001.182200908661
timers:
  learn_throughput: 409.477
  learn_time_ms: 40295.308
  load_throughput: 4453698.179
  load_time_ms: 3.705
  training_iteration_time_ms: 52824.256
  update_time_ms: 2.67
timesteps_total: 561000
training_iteration: 34

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23972602739726026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9177215189873418
  reward for individual goal_min: 0.0
episode_len_mean: 196.87
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 2587
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.660057067871094
time_total_s: 2003.1917824745178
timers:
  learn_throughput: 403.957
  learn_time_ms: 40845.9
  load_throughput: 4901241.209
  load_time_ms: 3.366
  training_iteration_time_ms: 54110.762
  update_time_ms: 2.709
timesteps_total: 561000
training_iteration: 34

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3402777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6644736842105263
  reward for individual goal_min: 0.0
episode_len_mean: 208.05
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 2918
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.2577006816864
time_total_s: 2005.9980335235596
timers:
  learn_throughput: 453.184
  learn_time_ms: 36409.016
  load_throughput: 5167404.576
  load_time_ms: 3.193
  training_iteration_time_ms: 48037.751
  update_time_ms: 2.538
timesteps_total: 643500
training_iteration: 39

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.98
  reward for individual goal_min: 0.0
episode_len_mean: 185.21
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 2598
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.4558527469635
time_total_s: 2007.6977372169495
timers:
  learn_throughput: 427.221
  learn_time_ms: 38621.665
  load_throughput: 5018856.496
  load_time_ms: 3.288
  training_iteration_time_ms: 50876.627
  update_time_ms: 2.636
timesteps_total: 577500
training_iteration: 35

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.65
  reward for individual goal_min: 0.0
episode_len_mean: 172.88333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6645569620253164
  reward for individual goal_min: 0.0
episode_len_mean: 218.34
episode_reward_max: 2.0
episode_reward_mean: 0.93
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 2870
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.37
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.2718768119812
time_total_s: 2026.120216846466
timers:
  learn_throughput: 485.594
  learn_time_ms: 33979.02
  load_throughput: 4978097.985
  load_time_ms: 3.315
  training_iteration_time_ms: 45302.696
  update_time_ms: 2.484
timesteps_total: 660000
training_iteration: 40

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-091_srff7m/checkpoint_000040/checkpoint-40
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.94
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6911764705882353
  reward for individual goal_min: 0.5
episode_len_mean: 137.98305084745763
episode_reward_max: 2.0
episode_reward_mean: 1.5932203389830508
episode_reward_min: 0.0
episodes_this_iter: 118
episodes_total: 2815
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7457627118644068
  agent_1: 0.847457627118644
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.60180115699768
time_total_s: 2023.262484550476
timers:
  learn_throughput: 351.215
  learn_time_ms: 46979.773
  load_throughput: 3790801.86
  load_time_ms: 4.353
  training_iteration_time_ms: 61876.413
  update_time_ms: 2.789
timesteps_total: 528000
training_iteration: 32

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9333333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 114.0
episode_reward_max: 2.0
episode_reward_mean: 1.6666666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8166666666666667
  agent_1: 0.85
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.38
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9726027397260274
  reward for individual goal_min: 0.0
episode_len_mean: 167.59
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 3113
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.139779806137085
time_total_s: 2015.0267734527588
timers:
  learn_throughput: 477.102
  learn_time_ms: 34583.788
  load_throughput: 5143019.701
  load_time_ms: 3.208
  training_iteration_time_ms: 46009.734
  update_time_ms: 2.547
timesteps_total: 660000
training_iteration: 40

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19ecg2ejpn/checkpoint_000040/checkpoint-40
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1927710843373494
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9146341463414634
  reward for individual goal_min: 0.0
episode_len_mean: 207.16
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 72
episodes_total: 2142
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.92786645889282
time_total_s: 2005.9786758422852
timers:
  learn_throughput: 362.043
  learn_time_ms: 45574.639
  load_throughput: 4280087.326
  load_time_ms: 3.855
  training_iteration_time_ms: 59774.084
  update_time_ms: 2.788
timesteps_total: 495000
training_iteration: 30

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8796296296296297
  reward for individual goal_min: 0.5
episode_len_mean: 82.43877551020408
episode_reward_max: 2.0
episode_reward_mean: 1.8673469387755102
episode_reward_min: 1.0
episodes_this_iter: 196
episodes_total: 5273
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8724489795918368
  agent_1: 0.9948979591836735
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.99181842803955
time_total_s: 2025.0484864711761
timers:
  learn_throughput: 520.104
  learn_time_ms: 31724.421
  load_throughput: 5542689.092
  load_time_ms: 2.977
  training_iteration_time_ms: 42636.653
  update_time_ms: 2.371
timesteps_total: 726000
training_iteration: 44

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21153846153846154
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8611111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 205.19
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 2256
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.929322957992554
time_total_s: 2019.9414238929749
timers:
  learn_throughput: 363.646
  learn_time_ms: 45373.859
  load_throughput: 4231515.693
  load_time_ms: 3.899
  training_iteration_time_ms: 59573.335
  update_time_ms: 2.903
timesteps_total: 511500
training_iteration: 31

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8350515463917526
  reward for individual goal_min: 0.5
episode_len_mean: 88.27272727272727
episode_reward_max: 2.0
episode_reward_mean: 1.8288770053475936
episode_reward_min: 1.0
episodes_this_iter: 187
episodes_total: 4359
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.839572192513369
  agent_1: 0.9893048128342246
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.64375424385071
time_total_s: 2041.04212641716
timers:
  learn_throughput: 417.479
  learn_time_ms: 39522.964
  load_throughput: 4881569.867
  load_time_ms: 3.38
  training_iteration_time_ms: 51820.013
  update_time_ms: 2.582
timesteps_total: 610500
training_iteration: 37

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2631578947368421
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8636363636363636
  reward for individual goal_min: 0.0
episode_len_mean: 197.3
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 2607
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.40015983581543
time_total_s: 2019.3490948677063
timers:
  learn_throughput: 403.796
  learn_time_ms: 40862.22
  load_throughput: 4723186.372
  load_time_ms: 3.493
  training_iteration_time_ms: 53966.532
  update_time_ms: 2.641
timesteps_total: 577500
training_iteration: 35

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7640449438202247
  reward for individual goal_min: 0.5
episode_len_mean: 117.93571428571428
episode_reward_max: 2.0
episode_reward_mean: 1.7
episode_reward_min: 1.0
episodes_this_iter: 140
episodes_total: 3433
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9285714285714286
  agent_1: 0.7714285714285715
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.68980407714844
time_total_s: 2050.8720049858093
timers:
  learn_throughput: 412.484
  learn_time_ms: 40001.584
  load_throughput: 4459351.646
  load_time_ms: 3.7
  training_iteration_time_ms: 52425.489
  update_time_ms: 2.654
timesteps_total: 577500
training_iteration: 35

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3116883116883117
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6158536585365854
  reward for individual goal_min: 0.0
episode_len_mean: 215.71
episode_reward_max: 2.0
episode_reward_mean: 1.01
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 2669
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.031736612319946
time_total_s: 2049.5541825294495
timers:
  learn_throughput: 401.214
  learn_time_ms: 41125.166
  load_throughput: 4669299.059
  load_time_ms: 3.534
  training_iteration_time_ms: 54003.024
  update_time_ms: 2.703
timesteps_total: 594000
training_iteration: 36

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21232876712328766
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7261904761904762
  reward for individual goal_min: 0.0
episode_len_mean: 214.95
episode_reward_max: 2.0
episode_reward_mean: 1.01
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 2361
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.904741525650024
time_total_s: 2036.469733953476
timers:
  learn_throughput: 375.847
  learn_time_ms: 43900.881
  load_throughput: 4337113.313
  load_time_ms: 3.804
  training_iteration_time_ms: 57666.454
  update_time_ms: 2.688
timesteps_total: 561000
training_iteration: 34

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20512820512820512
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6233766233766234
  reward for individual goal_min: 0.0
episode_len_mean: 217.14
episode_reward_max: 2.0
episode_reward_mean: 0.91
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 2949
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.35
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.77133750915527
time_total_s: 2073.8915543556213
timers:
  learn_throughput: 484.336
  learn_time_ms: 34067.293
  load_throughput: 4952803.315
  load_time_ms: 3.331
  training_iteration_time_ms: 45419.217
  update_time_ms: 2.519
timesteps_total: 676500
training_iteration: 41

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9113924050632911
  reward for individual goal_min: 0.0
episode_len_mean: 192.97
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 2673
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.106406450271606
time_total_s: 2057.2981889247894
timers:
  learn_throughput: 406.642
  learn_time_ms: 40576.195
  load_throughput: 4979745.71
  load_time_ms: 3.313
  training_iteration_time_ms: 53884.781
  update_time_ms: 2.72
timesteps_total: 577500
training_iteration: 35

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2468354430379747
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 191.14
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 3200
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.3399715423584
time_total_s: 2059.366744995117
timers:
  learn_throughput: 477.174
  learn_time_ms: 34578.607
  load_throughput: 5092609.441
  load_time_ms: 3.24
  training_iteration_time_ms: 45967.428
  update_time_ms: 2.549
timesteps_total: 676500
training_iteration: 41

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29577464788732394
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.28
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 2694
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.760658502578735
time_total_s: 2061.458395719528
timers:
  learn_throughput: 426.841
  learn_time_ms: 38656.123
  load_throughput: 5029653.188
  load_time_ms: 3.281
  training_iteration_time_ms: 50913.504
  update_time_ms: 2.63
timesteps_total: 594000
training_iteration: 36

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8440860215053764
  reward for individual goal_min: 0.5
episode_len_mean: 83.8391959798995
episode_reward_max: 2.0
episode_reward_mean: 1.8542713567839195
episode_reward_min: 1.0
episodes_this_iter: 199
episodes_total: 5472
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.864321608040201
  agent_1: 0.9899497487437185
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.8944456577301
time_total_s: 2067.9429321289062
timers:
  learn_throughput: 520.504
  learn_time_ms: 31700.055
  load_throughput: 5528299.397
  load_time_ms: 2.985
  training_iteration_time_ms: 42592.851
  update_time_ms: 2.389
timesteps_total: 742500
training_iteration: 45

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.65
  reward for individual goal_min: 0.5
episode_len_mean: 147.28333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.5833333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8
  agent_1: 0.7833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6776315789473685
  reward for individual goal_min: 0.0
episode_len_mean: 207.38
episode_reward_max: 2.0
episode_reward_mean: 1.04
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 2997
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.72209978103638
time_total_s: 2068.720133304596
timers:
  learn_throughput: 448.905
  learn_time_ms: 36756.092
  load_throughput: 5139734.866
  load_time_ms: 3.21
  training_iteration_time_ms: 48400.025
  update_time_ms: 2.563
timesteps_total: 660000
training_iteration: 40

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19istq_qwi/checkpoint_000040/checkpoint-40
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6594202898550725
  reward for individual goal_min: 0.5
episode_len_mean: 133.016
episode_reward_max: 2.0
episode_reward_mean: 1.624
episode_reward_min: 1.0
episodes_this_iter: 125
episodes_total: 2940
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.84
  agent_1: 0.784
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.762489795684814
time_total_s: 2081.024974346161
timers:
  learn_throughput: 356.327
  learn_time_ms: 46305.819
  load_throughput: 3809477.508
  load_time_ms: 4.331
  training_iteration_time_ms: 61040.338
  update_time_ms: 2.876
timesteps_total: 544500
training_iteration: 33

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2236842105263158
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9166666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 197.95
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 2231
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.17387795448303
time_total_s: 2066.152553796768
timers:
  learn_throughput: 362.231
  learn_time_ms: 45551.002
  load_throughput: 4336868.702
  load_time_ms: 3.805
  training_iteration_time_ms: 59703.717
  update_time_ms: 2.771
timesteps_total: 511500
training_iteration: 31

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8627450980392157
  reward for individual goal_min: 0.5
episode_len_mean: 83.45226130653266
episode_reward_max: 2.0
episode_reward_mean: 1.8592964824120604
episode_reward_min: 1.0
episodes_this_iter: 199
episodes_total: 4558
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8592964824120602
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 51.56802678108215
time_total_s: 2092.610153198242
timers:
  learn_throughput: 419.714
  learn_time_ms: 39312.443
  load_throughput: 4826519.559
  load_time_ms: 3.419
  training_iteration_time_ms: 51543.712
  update_time_ms: 2.6
timesteps_total: 627000
training_iteration: 38

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2602739726027397
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8289473684210527
  reward for individual goal_min: 0.0
episode_len_mean: 204.89
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 2336
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.33632159233093
time_total_s: 2079.277745485306
timers:
  learn_throughput: 365.342
  learn_time_ms: 45163.213
  load_throughput: 4234933.697
  load_time_ms: 3.896
  training_iteration_time_ms: 59314.495
  update_time_ms: 2.928
timesteps_total: 528000
training_iteration: 32

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9013157894736842
  reward for individual goal_min: 0.0
episode_len_mean: 203.21
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 2687
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.31966996192932
time_total_s: 2071.6687648296356
timers:
  learn_throughput: 406.908
  learn_time_ms: 40549.692
  load_throughput: 4718259.577
  load_time_ms: 3.497
  training_iteration_time_ms: 53580.852
  update_time_ms: 2.602
timesteps_total: 594000
training_iteration: 36

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7884615384615384
  reward for individual goal_min: 0.5
episode_len_mean: 101.7888198757764
episode_reward_max: 2.0
episode_reward_mean: 1.795031055900621
episode_reward_min: 1.0
episodes_this_iter: 161
episodes_total: 3594
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9565217391304348
  agent_1: 0.8385093167701864
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.99669551849365
time_total_s: 2102.868700504303
timers:
  learn_throughput: 413.234
  learn_time_ms: 39928.991
  load_throughput: 4503869.998
  load_time_ms: 3.664
  training_iteration_time_ms: 52316.908
  update_time_ms: 2.644
timesteps_total: 594000
training_iteration: 36

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2792207792207792
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9683544303797469
  reward for individual goal_min: 0.0
episode_len_mean: 179.24
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 3289
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.198718786239624
time_total_s: 2102.565463781357
timers:
  learn_throughput: 478.316
  learn_time_ms: 34496.016
  load_throughput: 5094633.873
  load_time_ms: 3.239
  training_iteration_time_ms: 45814.952
  update_time_ms: 2.523
timesteps_total: 693000
training_iteration: 42

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6597222222222222
  reward for individual goal_min: 0.0
episode_len_mean: 201.73
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 2750
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.65119385719299
time_total_s: 2103.2053763866425
timers:
  learn_throughput: 401.453
  learn_time_ms: 41100.667
  load_throughput: 4659867.084
  load_time_ms: 3.541
  training_iteration_time_ms: 54009.885
  update_time_ms: 2.728
timesteps_total: 610500
training_iteration: 37

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3014705882352941
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7134146341463414
  reward for individual goal_min: 0.0
episode_len_mean: 210.27
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 3026
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.42
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.63724970817566
time_total_s: 2120.528804063797
timers:
  learn_throughput: 482.821
  learn_time_ms: 34174.155
  load_throughput: 4911816.149
  load_time_ms: 3.359
  training_iteration_time_ms: 45554.989
  update_time_ms: 2.538
timesteps_total: 693000
training_iteration: 42

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8181818181818182
  reward for individual goal_min: 0.5
episode_len_mean: 98.7710843373494
episode_reward_max: 2.0
episode_reward_mean: 1.8072289156626506
episode_reward_min: 1.0
episodes_this_iter: 166
episodes_total: 5638
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8253012048192772
  agent_1: 0.9819277108433735
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.974000215530396
time_total_s: 2109.9169323444366
timers:
  learn_throughput: 522.411
  learn_time_ms: 31584.315
  load_throughput: 5485662.106
  load_time_ms: 3.008
  training_iteration_time_ms: 42427.828
  update_time_ms: 2.393
timesteps_total: 759000
training_iteration: 46

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.98
  reward for individual goal_min: 0.0
episode_len_mean: 196.09
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 2758
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.338702917099
time_total_s: 2109.6368918418884
timers:
  learn_throughput: 406.608
  learn_time_ms: 40579.633
  load_throughput: 4917295.438
  load_time_ms: 3.356
  training_iteration_time_ms: 53836.235
  update_time_ms: 2.697
timesteps_total: 594000
training_iteration: 36

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30246913580246915
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 190.93
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 2776
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.11489963531494
time_total_s: 2112.573295354843
timers:
  learn_throughput: 426.851
  learn_time_ms: 38655.215
  load_throughput: 5021988.593
  load_time_ms: 3.286
  training_iteration_time_ms: 51030.288
  update_time_ms: 2.616
timesteps_total: 610500
training_iteration: 37

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24358974358974358
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6301369863013698
  reward for individual goal_min: 0.0
episode_len_mean: 220.14
episode_reward_max: 2.0
episode_reward_mean: 0.85
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 2435
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.36
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.464561223983765
time_total_s: 2095.9342951774597
timers:
  learn_throughput: 377.129
  learn_time_ms: 43751.582
  load_throughput: 4358582.954
  load_time_ms: 3.786
  training_iteration_time_ms: 57620.679
  update_time_ms: 2.68
timesteps_total: 577500
training_iteration: 35

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6688311688311688
  reward for individual goal_min: 0.0
episode_len_mean: 207.79
episode_reward_max: 2.0
episode_reward_mean: 0.97
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 3078
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.41
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.703773736953735
time_total_s: 2115.4239070415497
timers:
  learn_throughput: 448.583
  learn_time_ms: 36782.501
  load_throughput: 5096997.746
  load_time_ms: 3.237
  training_iteration_time_ms: 48456.826
  update_time_ms: 2.567
timesteps_total: 676500
training_iteration: 41

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9850746268656716
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6666666666666666
  reward for individual goal_min: 0.5
episode_len_mean: 124.6015037593985
episode_reward_max: 2.0
episode_reward_mean: 1.6541353383458646
episode_reward_min: 0.0
episodes_this_iter: 133
episodes_total: 3073
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.849624060150376
  agent_1: 0.8045112781954887
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.81548070907593
time_total_s: 2139.840455055237
timers:
  learn_throughput: 358.011
  learn_time_ms: 46087.909
  load_throughput: 3890557.561
  load_time_ms: 4.241
  training_iteration_time_ms: 60763.726
  update_time_ms: 2.862
timesteps_total: 561000
training_iteration: 34

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9887640449438202
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8440860215053764
  reward for individual goal_min: 0.5
episode_len_mean: 89.47252747252747
episode_reward_max: 2.0
episode_reward_mean: 1.8296703296703296
episode_reward_min: 0.0
episodes_this_iter: 182
episodes_total: 4740
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8351648351648352
  agent_1: 0.9945054945054945
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.104342460632324
time_total_s: 2142.7144956588745
timers:
  learn_throughput: 421.782
  learn_time_ms: 39119.753
  load_throughput: 4830697.11
  load_time_ms: 3.416
  training_iteration_time_ms: 51364.744
  update_time_ms: 2.612
timesteps_total: 643500
training_iteration: 39

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3160919540229885
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8636363636363636
  reward for individual goal_min: 0.0
episode_len_mean: 202.58
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 75
episodes_total: 2306
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.00605630874634
time_total_s: 2126.1586101055145
timers:
  learn_throughput: 361.651
  learn_time_ms: 45624.125
  load_throughput: 4294695.767
  load_time_ms: 3.842
  training_iteration_time_ms: 59768.283
  update_time_ms: 2.766
timesteps_total: 528000
training_iteration: 32

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2926829268292683
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8611111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 199.97
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 2771
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.485384464263916
time_total_s: 2124.1541492938995
timers:
  learn_throughput: 407.749
  learn_time_ms: 40466.103
  load_throughput: 4731000.123
  load_time_ms: 3.488
  training_iteration_time_ms: 53468.533
  update_time_ms: 2.589
timesteps_total: 610500
training_iteration: 37

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9879518072289156
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8111111111111111
  reward for individual goal_min: 0.5
episode_len_mean: 92.72254335260115
episode_reward_max: 2.0
episode_reward_mean: 1.7919075144508672
episode_reward_min: 0.0
episodes_this_iter: 173
episodes_total: 5811
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8092485549132948
  agent_1: 0.9826589595375722
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.140069246292114
time_total_s: 2152.0570015907288
timers:
  learn_throughput: 522.666
  learn_time_ms: 31568.945
  load_throughput: 5507270.716
  load_time_ms: 2.996
  training_iteration_time_ms: 42416.466
  update_time_ms: 2.409
timesteps_total: 775500
training_iteration: 47

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7876712328767124
  reward for individual goal_min: 0.0
episode_len_mean: 210.77
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 2413
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.158369302749634
time_total_s: 2140.4361147880554
timers:
  learn_throughput: 365.276
  learn_time_ms: 45171.294
  load_throughput: 4251558.319
  load_time_ms: 3.881
  training_iteration_time_ms: 59355.218
  update_time_ms: 2.927
timesteps_total: 544500
training_iteration: 33

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2847222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9746835443037974
  reward for individual goal_min: 0.0
episode_len_mean: 176.19
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 3386
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.80793285369873
time_total_s: 2149.3733966350555
timers:
  learn_throughput: 473.284
  learn_time_ms: 34862.754
  load_throughput: 5079862.591
  load_time_ms: 3.248
  training_iteration_time_ms: 46161.915
  update_time_ms: 2.517
timesteps_total: 709500
training_iteration: 43

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8024691358024691
  reward for individual goal_min: 0.5
episode_len_mean: 102.89808917197452
episode_reward_max: 2.0
episode_reward_mean: 1.7961783439490446
episode_reward_min: 1.0
episodes_this_iter: 157
episodes_total: 3751
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9681528662420382
  agent_1: 0.8280254777070064
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.70625185966492
time_total_s: 2154.574952363968
timers:
  learn_throughput: 415.208
  learn_time_ms: 39739.079
  load_throughput: 4498483.259
  load_time_ms: 3.668
  training_iteration_time_ms: 52173.283
  update_time_ms: 2.638
timesteps_total: 610500
training_iteration: 37

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2571428571428571
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 196.98
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 3111
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.5519163608551
time_total_s: 2169.080720424652
timers:
  learn_throughput: 479.707
  learn_time_ms: 34396.008
  load_throughput: 4883154.3
  load_time_ms: 3.379
  training_iteration_time_ms: 45843.018
  update_time_ms: 2.559
timesteps_total: 709500
training_iteration: 43

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.41875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6623376623376623
  reward for individual goal_min: 0.0
episode_len_mean: 196.67
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 2833
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.62647581100464
time_total_s: 2156.831852197647
timers:
  learn_throughput: 402.353
  learn_time_ms: 41008.737
  load_throughput: 4638162.053
  load_time_ms: 3.557
  training_iteration_time_ms: 53908.817
  update_time_ms: 2.706
timesteps_total: 627000
training_iteration: 38

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2848101265822785
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.5
episode_len_mean: 189.54
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 2863
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.95012354850769
time_total_s: 2162.523418903351
timers:
  learn_throughput: 428.496
  learn_time_ms: 38506.743
  load_throughput: 5028301.062
  load_time_ms: 3.281
  training_iteration_time_ms: 50833.216
  update_time_ms: 2.621
timesteps_total: 627000
training_iteration: 38

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34558823529411764
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7102272727272727
  reward for individual goal_min: 0.0
episode_len_mean: 191.86
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 3162
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.341124534606934
time_total_s: 2162.7650315761566
timers:
  learn_throughput: 447.288
  learn_time_ms: 36888.994
  load_throughput: 5065992.431
  load_time_ms: 3.257
  training_iteration_time_ms: 48621.159
  update_time_ms: 2.552
timesteps_total: 693000
training_iteration: 42

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19480519480519481
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9605263157894737
  reward for individual goal_min: 0.0
episode_len_mean: 203.16
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 2839
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.85439157485962
time_total_s: 2161.491283416748
timers:
  learn_throughput: 409.38
  learn_time_ms: 40304.838
  load_throughput: 4913699.367
  load_time_ms: 3.358
  training_iteration_time_ms: 53546.777
  update_time_ms: 2.704
timesteps_total: 610500
training_iteration: 37

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2905405405405405
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6319444444444444
  reward for individual goal_min: 0.0
episode_len_mean: 229.05
episode_reward_max: 2.0
episode_reward_mean: 0.9
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 2508
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.36
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.61868166923523
time_total_s: 2150.552976846695
timers:
  learn_throughput: 380.211
  learn_time_ms: 43397.012
  load_throughput: 4411876.351
  load_time_ms: 3.74
  training_iteration_time_ms: 57184.244
  update_time_ms: 2.689
timesteps_total: 594000
training_iteration: 36

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6
  reward for individual goal_min: 0.5
episode_len_mean: 140.51282051282053
episode_reward_max: 2.0
episode_reward_mean: 1.5897435897435896
episode_reward_min: 1.0
episodes_this_iter: 117
episodes_total: 3190
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8632478632478633
  agent_1: 0.7264957264957265
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.98864388465881
time_total_s: 2195.8290989398956
timers:
  learn_throughput: 362.657
  learn_time_ms: 45497.489
  load_throughput: 3977631.561
  load_time_ms: 4.148
  training_iteration_time_ms: 60000.338
  update_time_ms: 2.851
timesteps_total: 577500
training_iteration: 35

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.82
  reward for individual goal_min: 0.5
episode_len_mean: 93.07821229050279
episode_reward_max: 2.0
episode_reward_mean: 1.7988826815642458
episode_reward_min: 1.0
episodes_this_iter: 179
episodes_total: 5990
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8044692737430168
  agent_1: 0.994413407821229
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.87880206108093
time_total_s: 2192.9358036518097
timers:
  learn_throughput: 526.113
  learn_time_ms: 31362.077
  load_throughput: 5483271.612
  load_time_ms: 3.009
  training_iteration_time_ms: 42169.187
  update_time_ms: 2.395
timesteps_total: 792000
training_iteration: 48

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3235294117647059
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.5
episode_len_mean: 172.26
episode_reward_max: 2.0
episode_reward_mean: 1.41
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 3479
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.75
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.28331422805786
time_total_s: 2193.6567108631134
timers:
  learn_throughput: 475.219
  learn_time_ms: 34720.843
  load_throughput: 5074647.739
  load_time_ms: 3.251
  training_iteration_time_ms: 45938.439
  update_time_ms: 2.514
timesteps_total: 726000
training_iteration: 44

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2654320987654321
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8266666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 208.4
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 2850
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.65223979949951
time_total_s: 2175.806389093399
timers:
  learn_throughput: 412.187
  learn_time_ms: 40030.363
  load_throughput: 4788714.01
  load_time_ms: 3.446
  training_iteration_time_ms: 52916.703
  update_time_ms: 2.601
timesteps_total: 627000
training_iteration: 38

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.85
  reward for individual goal_min: 0.5
episode_len_mean: 95.36666666666666
episode_reward_max: 2.0
episode_reward_mean: 1.8166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8333333333333334
  agent_1: 0.9833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8404255319148937
  reward for individual goal_min: 0.5
episode_len_mean: 93.69101123595506
episode_reward_max: 2.0
episode_reward_mean: 1.8314606741573034
episode_reward_min: 1.0
episodes_this_iter: 178
episodes_total: 4918
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8370786516853933
  agent_1: 0.9943820224719101
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.112751722335815
time_total_s: 2201.8272473812103
timers:
  learn_throughput: 424.359
  learn_time_ms: 38882.172
  load_throughput: 4811118.557
  load_time_ms: 3.43
  training_iteration_time_ms: 51021.999
  update_time_ms: 2.756
timesteps_total: 660000
training_iteration: 40

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-18g5xov65h/checkpoint_000040/checkpoint-40
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.875
  reward for individual goal_min: 0.0
episode_len_mean: 201.22
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 2391
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.470229387283325
time_total_s: 2185.628839492798
timers:
  learn_throughput: 363.184
  learn_time_ms: 45431.506
  load_throughput: 4276860.365
  load_time_ms: 3.858
  training_iteration_time_ms: 59587.31
  update_time_ms: 2.74
timesteps_total: 544500
training_iteration: 33

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33088235294117646
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7133333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 193.05
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 3192
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.91008138656616
time_total_s: 2215.9908018112183
timers:
  learn_throughput: 477.416
  learn_time_ms: 34561.043
  load_throughput: 4873594.456
  load_time_ms: 3.386
  training_iteration_time_ms: 45991.359
  update_time_ms: 2.59
timesteps_total: 726000
training_iteration: 44

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7828947368421053
  reward for individual goal_min: 0.5
episode_len_mean: 101.52095808383234
episode_reward_max: 2.0
episode_reward_mean: 1.8023952095808384
episode_reward_min: 1.0
episodes_this_iter: 167
episodes_total: 3918
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9640718562874252
  agent_1: 0.8383233532934131
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.52974772453308
time_total_s: 2207.104700088501
timers:
  learn_throughput: 416.481
  learn_time_ms: 39617.699
  load_throughput: 4490776.928
  load_time_ms: 3.674
  training_iteration_time_ms: 51992.668
  update_time_ms: 2.655
timesteps_total: 627000
training_iteration: 38

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.38311688311688313
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7278481012658228
  reward for individual goal_min: 0.0
episode_len_mean: 205.67
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 2914
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.70914435386658
time_total_s: 2207.5409965515137
timers:
  learn_throughput: 403.741
  learn_time_ms: 40867.775
  load_throughput: 4665962.069
  load_time_ms: 3.536
  training_iteration_time_ms: 53813.632
  update_time_ms: 2.683
timesteps_total: 643500
training_iteration: 39

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21052631578947367
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9736842105263158
  reward for individual goal_min: 0.0
episode_len_mean: 191.72
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 2946
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.54313659667969
time_total_s: 2211.0665555000305
timers:
  learn_throughput: 432.364
  learn_time_ms: 38162.285
  load_throughput: 5059732.998
  load_time_ms: 3.261
  training_iteration_time_ms: 50460.419
  update_time_ms: 2.621
timesteps_total: 643500
training_iteration: 39

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.39156626506024095
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7160493827160493
  reward for individual goal_min: 0.0
episode_len_mean: 202.71
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 3243
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.67328476905823
time_total_s: 2211.438316345215
timers:
  learn_throughput: 446.721
  learn_time_ms: 36935.802
  load_throughput: 5040973.727
  load_time_ms: 3.273
  training_iteration_time_ms: 48732.889
  update_time_ms: 2.585
timesteps_total: 709500
training_iteration: 43

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21518987341772153
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8873239436619719
  reward for individual goal_min: 0.0
episode_len_mean: 205.43
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 2493
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.17784595489502
time_total_s: 2201.6139607429504
timers:
  learn_throughput: 363.846
  learn_time_ms: 45348.867
  load_throughput: 4260195.016
  load_time_ms: 3.873
  training_iteration_time_ms: 59559.031
  update_time_ms: 2.904
timesteps_total: 561000
training_iteration: 34

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.925
  reward for individual goal_min: 0.0
episode_len_mean: 191.69
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 2928
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.072606801986694
time_total_s: 2217.5638902187347
timers:
  learn_throughput: 407.983
  learn_time_ms: 40442.905
  load_throughput: 4906905.657
  load_time_ms: 3.363
  training_iteration_time_ms: 53703.008
  update_time_ms: 2.721
timesteps_total: 627000
training_iteration: 38

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30666666666666664
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7289156626506024
  reward for individual goal_min: 0.0
episode_len_mean: 215.75
episode_reward_max: 2.0
episode_reward_mean: 1.04
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 2585
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.38
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.07887053489685
time_total_s: 2205.631847381592
timers:
  learn_throughput: 382.876
  learn_time_ms: 43094.934
  load_throughput: 4459983.889
  load_time_ms: 3.7
  training_iteration_time_ms: 56881.472
  update_time_ms: 2.677
timesteps_total: 610500
training_iteration: 37

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7857142857142857
  reward for individual goal_min: 0.5
episode_len_mean: 96.0
episode_reward_max: 2.0
episode_reward_mean: 1.7705882352941176
episode_reward_min: 1.0
episodes_this_iter: 170
episodes_total: 6160
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7705882352941177
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 41.20551323890686
time_total_s: 2234.1413168907166
timers:
  learn_throughput: 528.334
  learn_time_ms: 31230.217
  load_throughput: 5489229.988
  load_time_ms: 3.006
  training_iteration_time_ms: 42008.455
  update_time_ms: 2.379
timesteps_total: 808500
training_iteration: 49

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22435897435897437
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9459459459459459
  reward for individual goal_min: 0.0
episode_len_mean: 194.84
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 3561
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.175708293914795
time_total_s: 2240.832419157028
timers:
  learn_throughput: 476.154
  learn_time_ms: 34652.626
  load_throughput: 5120075.759
  load_time_ms: 3.223
  training_iteration_time_ms: 45862.748
  update_time_ms: 2.528
timesteps_total: 742500
training_iteration: 45

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6484375
  reward for individual goal_min: 0.5
episode_len_mean: 126.70542635658914
episode_reward_max: 2.0
episode_reward_mean: 1.6511627906976745
episode_reward_min: 1.0
episodes_this_iter: 129
episodes_total: 3319
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7984496124031008
  agent_1: 0.8527131782945736
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.59475755691528
time_total_s: 2253.423856496811
timers:
  learn_throughput: 365.496
  learn_time_ms: 45144.096
  load_throughput: 3971126.681
  load_time_ms: 4.155
  training_iteration_time_ms: 59578.149
  update_time_ms: 2.832
timesteps_total: 594000
training_iteration: 36

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9565217391304348
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8585858585858586
  reward for individual goal_min: 0.5
episode_len_mean: 86.08376963350786
episode_reward_max: 2.0
episode_reward_mean: 1.8115183246073299
episode_reward_min: 0.0
episodes_this_iter: 191
episodes_total: 5109
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8324607329842932
  agent_1: 0.9790575916230366
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.75553870201111
time_total_s: 2250.5827860832214
timers:
  learn_throughput: 428.132
  learn_time_ms: 38539.481
  load_throughput: 4843781.741
  load_time_ms: 3.406
  training_iteration_time_ms: 50521.959
  update_time_ms: 2.767
timesteps_total: 676500
training_iteration: 41

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8857142857142857
  reward for individual goal_min: 0.0
episode_len_mean: 190.9
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 2938
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.03486347198486
time_total_s: 2228.841252565384
timers:
  learn_throughput: 409.357
  learn_time_ms: 40307.163
  load_throughput: 4827697.974
  load_time_ms: 3.418
  training_iteration_time_ms: 53165.718
  update_time_ms: 2.629
timesteps_total: 643500
training_iteration: 39

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20945945945945946
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6617647058823529
  reward for individual goal_min: 0.0
episode_len_mean: 215.98
episode_reward_max: 2.0
episode_reward_mean: 0.88
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 3268
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.34
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.36339473724365
time_total_s: 2264.354196548462
timers:
  learn_throughput: 474.867
  learn_time_ms: 34746.584
  load_throughput: 4866945.343
  load_time_ms: 3.39
  training_iteration_time_ms: 46234.995
  update_time_ms: 2.967
timesteps_total: 742500
training_iteration: 45

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8372093023255814
  reward for individual goal_min: 0.5
episode_len_mean: 91.21111111111111
episode_reward_max: 2.0
episode_reward_mean: 1.8444444444444446
episode_reward_min: 1.0
episodes_this_iter: 180
episodes_total: 4098
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9611111111111111
  agent_1: 0.8833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.9966094493866
time_total_s: 2260.1013095378876
timers:
  learn_throughput: 417.206
  learn_time_ms: 39548.818
  load_throughput: 4530553.439
  load_time_ms: 3.642
  training_iteration_time_ms: 51939.667
  update_time_ms: 2.661
timesteps_total: 643500
training_iteration: 39

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31690140845070425
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8924050632911392
  reward for individual goal_min: 0.0
episode_len_mean: 186.93
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 2482
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.68459153175354
time_total_s: 2245.3134310245514
timers:
  learn_throughput: 365.638
  learn_time_ms: 45126.641
  load_throughput: 4272978.6
  load_time_ms: 3.861
  training_iteration_time_ms: 59298.188
  update_time_ms: 2.761
timesteps_total: 561000
training_iteration: 34

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35135135135135137
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7191780821917808
  reward for individual goal_min: 0.0
episode_len_mean: 202.21
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 3324
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.618332386016846
time_total_s: 2260.0566487312317
timers:
  learn_throughput: 448.164
  learn_time_ms: 36816.907
  load_throughput: 5050243.806
  load_time_ms: 3.267
  training_iteration_time_ms: 48649.944
  update_time_ms: 2.586
timesteps_total: 726000
training_iteration: 44

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22857142857142856
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8026315789473685
  reward for individual goal_min: 0.0
episode_len_mean: 205.25
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 2574
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.043811559677124
time_total_s: 2260.6577723026276
timers:
  learn_throughput: 363.39
  learn_time_ms: 45405.735
  load_throughput: 4231696.812
  load_time_ms: 3.899
  training_iteration_time_ms: 59626.647
  update_time_ms: 2.876
timesteps_total: 577500
training_iteration: 35

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.918918918918919
  reward for individual goal_min: 0.0
episode_len_mean: 186.2
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 3014
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.69275379180908
time_total_s: 2269.256644010544
timers:
  learn_throughput: 411.866
  learn_time_ms: 40061.592
  load_throughput: 4923872.73
  load_time_ms: 3.351
  training_iteration_time_ms: 53335.79
  update_time_ms: 2.72
timesteps_total: 643500
training_iteration: 39

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8
  reward for individual goal_min: 0.5
episode_len_mean: 94.5195530726257
episode_reward_max: 2.0
episode_reward_mean: 1.7877094972067038
episode_reward_min: 1.0
episodes_this_iter: 179
episodes_total: 6339
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7988826815642458
  agent_1: 0.9888268156424581
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.374608755111694
time_total_s: 2275.5159256458282
timers:
  learn_throughput: 528.871
  learn_time_ms: 31198.559
  load_throughput: 5509024.303
  load_time_ms: 2.995
  training_iteration_time_ms: 41920.664
  update_time_ms: 2.374
timesteps_total: 825000
training_iteration: 50

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.65
  reward for individual goal_min: 0.5
episode_len_mean: 149.65
episode_reward_max: 2.0
episode_reward_mean: 1.55
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7833333333333333
  agent_1: 0.7666666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.38235294117647056
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6959459459459459
  reward for individual goal_min: 0.0
episode_len_mean: 211.43
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 2991
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 66.424631357193
time_total_s: 2273.9656279087067
timers:
  learn_throughput: 406.392
  learn_time_ms: 40601.176
  load_throughput: 4702262.325
  load_time_ms: 3.509
  training_iteration_time_ms: 53434.94
  update_time_ms: 2.673
timesteps_total: 660000
training_iteration: 40

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19mcvnks1j/checkpoint_000040/checkpoint-40
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2785714285714286
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.75
  reward for individual goal_min: 0.0
episode_len_mean: 204.33
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 2665
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.43
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.20277690887451
time_total_s: 2260.8346242904663
timers:
  learn_throughput: 385.531
  learn_time_ms: 42798.062
  load_throughput: 4444973.57
  load_time_ms: 3.712
  training_iteration_time_ms: 56558.473
  update_time_ms: 2.676
timesteps_total: 627000
training_iteration: 38

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 121.58333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.6
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8166666666666667
  agent_1: 0.7833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3291139240506329
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 180.34
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 3039
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 68.20554304122925
time_total_s: 2279.2720985412598
timers:
  learn_throughput: 433.21
  learn_time_ms: 38087.721
  load_throughput: 5078856.036
  load_time_ms: 3.249
  training_iteration_time_ms: 50385.237
  update_time_ms: 2.614
timesteps_total: 660000
training_iteration: 40

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-18pddilqzj/checkpoint_000040/checkpoint-40
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2152777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9662162162162162
  reward for individual goal_min: 0.0
episode_len_mean: 189.08
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 3651
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.66366124153137
time_total_s: 2287.4960803985596
timers:
  learn_throughput: 475.598
  learn_time_ms: 34693.193
  load_throughput: 5128118.915
  load_time_ms: 3.218
  training_iteration_time_ms: 45892.402
  update_time_ms: 2.524
timesteps_total: 759000
training_iteration: 46

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7307692307692307
  reward for individual goal_min: 0.0
episode_len_mean: 216.07
episode_reward_max: 2.0
episode_reward_mean: 0.99
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 3345
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.36
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.370073556900024
time_total_s: 2310.724270105362
timers:
  learn_throughput: 474.776
  learn_time_ms: 34753.218
  load_throughput: 4793623.096
  load_time_ms: 3.442
  training_iteration_time_ms: 46235.233
  update_time_ms: 2.964
timesteps_total: 759000
training_iteration: 46

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9791666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8387096774193549
  reward for individual goal_min: 0.5
episode_len_mean: 87.4021164021164
episode_reward_max: 2.0
episode_reward_mean: 1.82010582010582
episode_reward_min: 0.0
episodes_this_iter: 189
episodes_total: 5298
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8359788359788359
  agent_1: 0.9841269841269841
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.24041533470154
time_total_s: 2301.823201417923
timers:
  learn_throughput: 427.562
  learn_time_ms: 38590.916
  load_throughput: 4842731.007
  load_time_ms: 3.407
  training_iteration_time_ms: 50592.294
  update_time_ms: 2.798
timesteps_total: 693000
training_iteration: 42

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6666666666666666
  reward for individual goal_min: 0.5
episode_len_mean: 122.7910447761194
episode_reward_max: 2.0
episode_reward_mean: 1.6567164179104477
episode_reward_min: 1.0
episodes_this_iter: 134
episodes_total: 3453
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7910447761194029
  agent_1: 0.8656716417910447
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.19271636009216
time_total_s: 2310.616572856903
timers:
  learn_throughput: 368.961
  learn_time_ms: 44720.147
  load_throughput: 4078451.266
  load_time_ms: 4.046
  training_iteration_time_ms: 58963.985
  update_time_ms: 2.837
timesteps_total: 610500
training_iteration: 37

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.352112676056338
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6811594202898551
  reward for individual goal_min: 0.0
episode_len_mean: 206.83
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 3400
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.26362228393555
time_total_s: 2307.3202710151672
timers:
  learn_throughput: 451.642
  learn_time_ms: 36533.373
  load_throughput: 5082362.065
  load_time_ms: 3.247
  training_iteration_time_ms: 48275.72
  update_time_ms: 2.567
timesteps_total: 742500
training_iteration: 45

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 120.35
episode_reward_max: 2.0
episode_reward_mean: 1.7166666666666666
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9166666666666666
  agent_1: 0.8
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35526315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8767123287671232
  reward for individual goal_min: 0.0
episode_len_mean: 180.76
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 3032
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 64.74605321884155
time_total_s: 2293.5873057842255
timers:
  learn_throughput: 412.398
  learn_time_ms: 40009.925
  load_throughput: 4818689.319
  load_time_ms: 3.424
  training_iteration_time_ms: 52744.964
  update_time_ms: 2.633
timesteps_total: 660000
training_iteration: 40

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-28met_xp4k/checkpoint_000040/checkpoint-40
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8484848484848485
  reward for individual goal_min: 0.5
episode_len_mean: 82.58883248730965
episode_reward_max: 2.0
episode_reward_mean: 1.8477157360406091
episode_reward_min: 1.0
episodes_this_iter: 197
episodes_total: 6536
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8527918781725888
  agent_1: 0.9949238578680203
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.816696643829346
time_total_s: 2317.3326222896576
timers:
  learn_throughput: 530.366
  learn_time_ms: 31110.603
  load_throughput: 5572098.131
  load_time_ms: 2.961
  training_iteration_time_ms: 41829.884
  update_time_ms: 2.374
timesteps_total: 841500
training_iteration: 51

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2535211267605634
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.875
  reward for individual goal_min: 0.0
episode_len_mean: 190.03
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 2569
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.08995985984802
time_total_s: 2304.4033908843994
timers:
  learn_throughput: 367.425
  learn_time_ms: 44907.115
  load_throughput: 4253622.702
  load_time_ms: 3.879
  training_iteration_time_ms: 59052.133
  update_time_ms: 2.757
timesteps_total: 577500
training_iteration: 35

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 94.06666666666666
episode_reward_max: 2.0
episode_reward_mean: 1.8333333333333333
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9833333333333333
  agent_1: 0.85
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8630952380952381
  reward for individual goal_min: 0.5
episode_len_mean: 86.56451612903226
episode_reward_max: 2.0
episode_reward_mean: 1.8763440860215055
episode_reward_min: 1.0
episodes_this_iter: 186
episodes_total: 4284
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.978494623655914
  agent_1: 0.8978494623655914
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.82988667488098
time_total_s: 2321.9311962127686
timers:
  learn_throughput: 418.035
  learn_time_ms: 39470.385
  load_throughput: 4497693.897
  load_time_ms: 3.669
  training_iteration_time_ms: 51882.394
  update_time_ms: 2.659
timesteps_total: 660000
training_iteration: 40

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-184s1jdk42/checkpoint_000040/checkpoint-40
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6917808219178082
  reward for individual goal_min: 0.0
episode_len_mean: 212.52
episode_reward_max: 2.0
episode_reward_mean: 0.99
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 3071
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.61688995361328
time_total_s: 2325.58251786232
timers:
  learn_throughput: 410.405
  learn_time_ms: 40204.205
  load_throughput: 4706771.585
  load_time_ms: 3.506
  training_iteration_time_ms: 53024.654
  update_time_ms: 2.644
timesteps_total: 676500
training_iteration: 41

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8717948717948718
  reward for individual goal_min: 0.0
episode_len_mean: 185.96
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 2661
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.98378658294678
time_total_s: 2319.6415588855743
timers:
  learn_throughput: 365.024
  learn_time_ms: 45202.457
  load_throughput: 4215047.141
  load_time_ms: 3.915
  training_iteration_time_ms: 59398.274
  update_time_ms: 2.856
timesteps_total: 594000
training_iteration: 36

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20270270270270271
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.958904109589041
  reward for individual goal_min: 0.0
episode_len_mean: 191.61
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 3124
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.93394923210144
time_total_s: 2330.206047773361
timers:
  learn_throughput: 432.541
  learn_time_ms: 38146.702
  load_throughput: 5050022.694
  load_time_ms: 3.267
  training_iteration_time_ms: 50486.354
  update_time_ms: 2.596
timesteps_total: 676500
training_iteration: 41

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26973684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7432432432432432
  reward for individual goal_min: 0.0
episode_len_mean: 206.65
episode_reward_max: 2.0
episode_reward_mean: 1.04
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 2742
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.44
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.45345664024353
time_total_s: 2316.28808093071
timers:
  learn_throughput: 387.743
  learn_time_ms: 42553.934
  load_throughput: 4519517.525
  load_time_ms: 3.651
  training_iteration_time_ms: 56228.858
  update_time_ms: 2.695
timesteps_total: 643500
training_iteration: 39

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2945205479452055
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9794520547945206
  reward for individual goal_min: 0.0
episode_len_mean: 177.43
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 3742
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.469000816345215
time_total_s: 2332.965081214905
timers:
  learn_throughput: 477.31
  learn_time_ms: 34568.764
  load_throughput: 5126675.359
  load_time_ms: 3.218
  training_iteration_time_ms: 45706.191
  update_time_ms: 2.515
timesteps_total: 775500
training_iteration: 47

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 100.16666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.85
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9333333333333333
  agent_1: 0.9166666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9487179487179487
  reward for individual goal_min: 0.0
episode_len_mean: 196.28
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 3097
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.45639848709106
time_total_s: 2334.713042497635
timers:
  learn_throughput: 412.801
  learn_time_ms: 39970.837
  load_throughput: 4916841.275
  load_time_ms: 3.356
  training_iteration_time_ms: 53111.002
  update_time_ms: 2.699
timesteps_total: 660000
training_iteration: 40

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19igysrqn6/checkpoint_000040/checkpoint-40
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20422535211267606
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.722972972972973
  reward for individual goal_min: 0.0
episode_len_mean: 217.36
episode_reward_max: 2.0
episode_reward_mean: 0.96
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 3423
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.37
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.77189064025879
time_total_s: 2357.4961607456207
timers:
  learn_throughput: 469.94
  learn_time_ms: 35110.858
  load_throughput: 4786494.958
  load_time_ms: 3.447
  training_iteration_time_ms: 46657.037
  update_time_ms: 2.979
timesteps_total: 775500
training_iteration: 47

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9876543209876543
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.828125
  reward for individual goal_min: 0.5
episode_len_mean: 92.96610169491525
episode_reward_max: 2.0
episode_reward_mean: 1.8022598870056497
episode_reward_min: 0.0
episodes_this_iter: 177
episodes_total: 5475
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8192090395480226
  agent_1: 0.9830508474576272
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.93576502799988
time_total_s: 2350.758966445923
timers:
  learn_throughput: 430.435
  learn_time_ms: 38333.327
  load_throughput: 4802338.23
  load_time_ms: 3.436
  training_iteration_time_ms: 50346.452
  update_time_ms: 2.809
timesteps_total: 709500
training_iteration: 43

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2923076923076923
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6623376623376623
  reward for individual goal_min: 0.0
episode_len_mean: 209.74
episode_reward_max: 2.0
episode_reward_mean: 1.04
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 3480
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.91807675361633
time_total_s: 2355.2383477687836
timers:
  learn_throughput: 454.418
  learn_time_ms: 36310.185
  load_throughput: 5095909.342
  load_time_ms: 3.238
  training_iteration_time_ms: 48011.19
  update_time_ms: 2.563
timesteps_total: 759000
training_iteration: 46

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8523809523809524
  reward for individual goal_min: 0.5
episode_len_mean: 87.91005291005291
episode_reward_max: 2.0
episode_reward_mean: 1.835978835978836
episode_reward_min: 1.0
episodes_this_iter: 189
episodes_total: 6725
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8412698412698413
  agent_1: 0.9947089947089947
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.23368239402771
time_total_s: 2359.5663046836853
timers:
  learn_throughput: 528.75
  learn_time_ms: 31205.69
  load_throughput: 5580185.291
  load_time_ms: 2.957
  training_iteration_time_ms: 41867.595
  update_time_ms: 2.364
timesteps_total: 858000
training_iteration: 52

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6811594202898551
  reward for individual goal_min: 0.5
episode_len_mean: 118.24113475177305
episode_reward_max: 2.0
episode_reward_mean: 1.6879432624113475
episode_reward_min: 1.0
episodes_this_iter: 141
episodes_total: 3594
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8581560283687943
  agent_1: 0.8297872340425532
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.37441921234131
time_total_s: 2369.9909920692444
timers:
  learn_throughput: 370.49
  learn_time_ms: 44535.565
  load_throughput: 4163143.502
  load_time_ms: 3.963
  training_iteration_time_ms: 58738.586
  update_time_ms: 2.843
timesteps_total: 627000
training_iteration: 38

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3108108108108108
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7662337662337663
  reward for individual goal_min: 0.0
episode_len_mean: 202.51
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 3108
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.00429034233093
time_total_s: 2346.5915961265564
timers:
  learn_throughput: 412.942
  learn_time_ms: 39957.195
  load_throughput: 4803004.81
  load_time_ms: 3.435
  training_iteration_time_ms: 52614.217
  update_time_ms: 2.645
timesteps_total: 676500
training_iteration: 41

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.810126582278481
  reward for individual goal_min: 0.5
episode_len_mean: 100.12426035502959
episode_reward_max: 2.0
episode_reward_mean: 1.8224852071005917
episode_reward_min: 1.0
episodes_this_iter: 169
episodes_total: 4453
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9940828402366864
  agent_1: 0.8284023668639053
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.661694288253784
time_total_s: 2374.5928905010223
timers:
  learn_throughput: 417.699
  learn_time_ms: 39502.137
  load_throughput: 4453898.818
  load_time_ms: 3.705
  training_iteration_time_ms: 51899.513
  update_time_ms: 2.663
timesteps_total: 676500
training_iteration: 41

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2916666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8511904761904762
  reward for individual goal_min: 0.0
episode_len_mean: 198.56
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 2654
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.180192947387695
time_total_s: 2362.583583831787
timers:
  learn_throughput: 368.974
  learn_time_ms: 44718.593
  load_throughput: 4298723.912
  load_time_ms: 3.838
  training_iteration_time_ms: 58849.811
  update_time_ms: 2.726
timesteps_total: 594000
training_iteration: 36

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30666666666666664
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7621951219512195
  reward for individual goal_min: 0.0
episode_len_mean: 192.31
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 3153
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.12936758995056
time_total_s: 2376.7118854522705
timers:
  learn_throughput: 413.001
  learn_time_ms: 39951.509
  load_throughput: 4696486.492
  load_time_ms: 3.513
  training_iteration_time_ms: 52690.215
  update_time_ms: 2.65
timesteps_total: 693000
training_iteration: 42

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2785714285714286
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.96875
  reward for individual goal_min: 0.0
episode_len_mean: 177.99
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 3831
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.19406533241272
time_total_s: 2378.1591465473175
timers:
  learn_throughput: 477.877
  learn_time_ms: 34527.743
  load_throughput: 5163934.397
  load_time_ms: 3.195
  training_iteration_time_ms: 45747.567
  update_time_ms: 2.496
timesteps_total: 792000
training_iteration: 48

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2152777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.972972972972973
  reward for individual goal_min: 0.5
episode_len_mean: 194.06
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 3208
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.9330198764801
time_total_s: 2381.1390676498413
timers:
  learn_throughput: 431.029
  learn_time_ms: 38280.47
  load_throughput: 5059917.966
  load_time_ms: 3.261
  training_iteration_time_ms: 50701.779
  update_time_ms: 2.595
timesteps_total: 693000
training_iteration: 42

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2236842105263158
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7536231884057971
  reward for individual goal_min: 0.0
episode_len_mean: 216.41
episode_reward_max: 2.0
episode_reward_mean: 0.99
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 3499
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.4
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.77222514152527
time_total_s: 2403.268385887146
timers:
  learn_throughput: 467.968
  learn_time_ms: 35258.812
  load_throughput: 4735759.127
  load_time_ms: 3.484
  training_iteration_time_ms: 46806.652
  update_time_ms: 3.012
timesteps_total: 792000
training_iteration: 48

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.225
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9513888888888888
  reward for individual goal_min: 0.0
episode_len_mean: 202.52
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 2742
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.74195218086243
time_total_s: 2378.3835110664368
timers:
  learn_throughput: 364.402
  learn_time_ms: 45279.611
  load_throughput: 4236489.162
  load_time_ms: 3.895
  training_iteration_time_ms: 59535.686
  update_time_ms: 2.845
timesteps_total: 610500
training_iteration: 37

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9142857142857143
  reward for individual goal_min: 0.0
episode_len_mean: 197.03
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 3184
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.863404512405396
time_total_s: 2391.5764470100403
timers:
  learn_throughput: 410.358
  learn_time_ms: 40208.784
  load_throughput: 4944876.282
  load_time_ms: 3.337
  training_iteration_time_ms: 53330.969
  update_time_ms: 2.714
timesteps_total: 676500
training_iteration: 41

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8160919540229885
  reward for individual goal_min: 0.5
episode_len_mean: 92.42134831460675
episode_reward_max: 2.0
episode_reward_mean: 1.8202247191011236
episode_reward_min: 1.0
episodes_this_iter: 178
episodes_total: 5653
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8202247191011236
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.59751033782959
time_total_s: 2399.3564767837524
timers:
  learn_throughput: 432.06
  learn_time_ms: 38189.181
  load_throughput: 4841342.026
  load_time_ms: 3.408
  training_iteration_time_ms: 50175.189
  update_time_ms: 2.799
timesteps_total: 726000
training_iteration: 44

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8085106382978723
  reward for individual goal_min: 0.5
episode_len_mean: 82.875
episode_reward_max: 2.0
episode_reward_mean: 1.82
episode_reward_min: 1.0
episodes_this_iter: 200
episodes_total: 6925
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.82
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 42.354925870895386
time_total_s: 2401.9212305545807
timers:
  learn_throughput: 528.124
  learn_time_ms: 31242.657
  load_throughput: 5552116.039
  load_time_ms: 2.972
  training_iteration_time_ms: 41931.34
  update_time_ms: 2.353
timesteps_total: 874500
training_iteration: 53

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2692307692307692
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7394366197183099
  reward for individual goal_min: 0.0
episode_len_mean: 209.63
episode_reward_max: 2.0
episode_reward_mean: 1.01
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 3556
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.75454878807068
time_total_s: 2403.9928965568542
timers:
  learn_throughput: 452.834
  learn_time_ms: 36437.167
  load_throughput: 5149678.62
  load_time_ms: 3.204
  training_iteration_time_ms: 48160.653
  update_time_ms: 2.561
timesteps_total: 775500
training_iteration: 47

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6166666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 165.8
episode_reward_max: 2.0
episode_reward_mean: 1.3833333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7166666666666667
  agent_1: 0.6666666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18292682926829268
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.717391304347826
  reward for individual goal_min: 0.0
episode_len_mean: 224.96
episode_reward_max: 2.0
episode_reward_mean: 0.87
episode_reward_min: 0.0
episodes_this_iter: 72
episodes_total: 2814
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.35
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 78.93039464950562
time_total_s: 2395.2184755802155
timers:
  learn_throughput: 386.626
  learn_time_ms: 42676.891
  load_throughput: 4547552.355
  load_time_ms: 3.628
  training_iteration_time_ms: 56462.573
  update_time_ms: 2.655
timesteps_total: 660000
training_iteration: 40

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-28ftt3iad9/checkpoint_000040/checkpoint-40
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7083333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 115.8943661971831
episode_reward_max: 2.0
episode_reward_mean: 1.704225352112676
episode_reward_min: 1.0
episodes_this_iter: 142
episodes_total: 3736
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.852112676056338
  agent_1: 0.852112676056338
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.15735912322998
time_total_s: 2428.1483511924744
timers:
  learn_throughput: 372.397
  learn_time_ms: 44307.507
  load_throughput: 4062434.901
  load_time_ms: 4.062
  training_iteration_time_ms: 58408.998
  update_time_ms: 2.768
timesteps_total: 643500
training_iteration: 39

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29605263157894735
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7714285714285715
  reward for individual goal_min: 0.0
episode_len_mean: 196.63
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 3194
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.83591628074646
time_total_s: 2402.427512407303
timers:
  learn_throughput: 411.747
  learn_time_ms: 40073.19
  load_throughput: 4803771.605
  load_time_ms: 3.435
  training_iteration_time_ms: 52796.258
  update_time_ms: 2.649
timesteps_total: 693000
training_iteration: 42

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8023255813953488
  reward for individual goal_min: 0.5
episode_len_mean: 100.44444444444444
episode_reward_max: 2.0
episode_reward_mean: 1.7901234567901234
episode_reward_min: 1.0
episodes_this_iter: 162
episodes_total: 4615
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9938271604938271
  agent_1: 0.7962962962962963
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.42141628265381
time_total_s: 2424.014306783676
timers:
  learn_throughput: 422.355
  learn_time_ms: 39066.707
  load_throughput: 4447515.906
  load_time_ms: 3.71
  training_iteration_time_ms: 51429.851
  update_time_ms: 2.626
timesteps_total: 693000
training_iteration: 42

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2987012987012987
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 179.28
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 3924
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.08375310897827
time_total_s: 2423.242899656296
timers:
  learn_throughput: 480.228
  learn_time_ms: 34358.679
  load_throughput: 5151250.186
  load_time_ms: 3.203
  training_iteration_time_ms: 45560.297
  update_time_ms: 2.495
timesteps_total: 808500
training_iteration: 49

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3357142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6513157894736842
  reward for individual goal_min: 0.0
episode_len_mean: 203.26
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 3231
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.79128932952881
time_total_s: 2427.5031747817993
timers:
  learn_throughput: 414.212
  learn_time_ms: 39834.693
  load_throughput: 4719546.636
  load_time_ms: 3.496
  training_iteration_time_ms: 52524.145
  update_time_ms: 2.653
timesteps_total: 709500
training_iteration: 43

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1643835616438356
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9683544303797469
  reward for individual goal_min: 0.0
episode_len_mean: 198.61
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 3294
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.26485347747803
time_total_s: 2429.4039211273193
timers:
  learn_throughput: 432.33
  learn_time_ms: 38165.322
  load_throughput: 5078036.174
  load_time_ms: 3.249
  training_iteration_time_ms: 50551.529
  update_time_ms: 2.624
timesteps_total: 709500
training_iteration: 43

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28378378378378377
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.684931506849315
  reward for individual goal_min: 0.0
episode_len_mean: 213.88
episode_reward_max: 2.0
episode_reward_mean: 0.98
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 3576
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.33
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.27752137184143
time_total_s: 2449.5459072589874
timers:
  learn_throughput: 467.217
  learn_time_ms: 35315.459
  load_throughput: 4683265.279
  load_time_ms: 3.523
  training_iteration_time_ms: 46833.034
  update_time_ms: 3.004
timesteps_total: 808500
training_iteration: 49

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3223684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8375
  reward for individual goal_min: 0.0
episode_len_mean: 194.38
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 2739
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.69088649749756
time_total_s: 2420.2744703292847
timers:
  learn_throughput: 369.609
  learn_time_ms: 44641.722
  load_throughput: 4287511.91
  load_time_ms: 3.848
  training_iteration_time_ms: 58720.407
  update_time_ms: 2.723
timesteps_total: 610500
training_iteration: 37

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.82
  reward for individual goal_min: 0.5
episode_len_mean: 93.2090395480226
episode_reward_max: 2.0
episode_reward_mean: 1.7966101694915255
episode_reward_min: 1.0
episodes_this_iter: 177
episodes_total: 7102
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8022598870056498
  agent_1: 0.9943502824858758
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.59969353675842
time_total_s: 2442.520924091339
timers:
  learn_throughput: 531.34
  learn_time_ms: 31053.592
  load_throughput: 5536126.968
  load_time_ms: 2.98
  training_iteration_time_ms: 41692.497
  update_time_ms: 2.343
timesteps_total: 891000
training_iteration: 54

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8068181818181818
  reward for individual goal_min: 0.5
episode_len_mean: 88.63736263736264
episode_reward_max: 2.0
episode_reward_mean: 1.8131868131868132
episode_reward_min: 1.0
episodes_this_iter: 182
episodes_total: 5835
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8131868131868132
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.91972255706787
time_total_s: 2446.2761993408203
timers:
  learn_throughput: 436.303
  learn_time_ms: 37817.771
  load_throughput: 4876547.817
  load_time_ms: 3.384
  training_iteration_time_ms: 49651.968
  update_time_ms: 2.786
timesteps_total: 742500
training_iteration: 45

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2564102564102564
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95
  reward for individual goal_min: 0.0
episode_len_mean: 198.46
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 3266
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.47156000137329
time_total_s: 2443.0480070114136
timers:
  learn_throughput: 409.374
  learn_time_ms: 40305.42
  load_throughput: 4954540.743
  load_time_ms: 3.33
  training_iteration_time_ms: 53358.221
  update_time_ms: 2.733
timesteps_total: 693000
training_iteration: 42

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2894736842105263
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8589743589743589
  reward for individual goal_min: 0.0
episode_len_mean: 185.91
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 2832
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.74575853347778
time_total_s: 2436.1292695999146
timers:
  learn_throughput: 367.061
  learn_time_ms: 44951.707
  load_throughput: 4198120.473
  load_time_ms: 3.93
  training_iteration_time_ms: 59135.994
  update_time_ms: 2.816
timesteps_total: 627000
training_iteration: 38

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.38666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.773972602739726
  reward for individual goal_min: 0.0
episode_len_mean: 184.57
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 3647
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.29963421821594
time_total_s: 2450.29253077507
timers:
  learn_throughput: 456.231
  learn_time_ms: 36165.92
  load_throughput: 5149142.207
  load_time_ms: 3.204
  training_iteration_time_ms: 47767.037
  update_time_ms: 2.557
timesteps_total: 792000
training_iteration: 48

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23943661971830985
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6838235294117647
  reward for individual goal_min: 0.0
episode_len_mean: 222.59
episode_reward_max: 2.0
episode_reward_mean: 0.91
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 2891
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.38
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.7964448928833
time_total_s: 2450.0149204730988
timers:
  learn_throughput: 389.344
  learn_time_ms: 42379.015
  load_throughput: 4475616.864
  load_time_ms: 3.687
  training_iteration_time_ms: 56154.566
  update_time_ms: 2.652
timesteps_total: 676500
training_iteration: 41

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2564102564102564
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 187.62
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 4013
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.19058609008789
time_total_s: 2467.4334857463837
timers:
  learn_throughput: 483.718
  learn_time_ms: 34110.763
  load_throughput: 5222859.04
  load_time_ms: 3.159
  training_iteration_time_ms: 45203.873
  update_time_ms: 2.49
timesteps_total: 825000
training_iteration: 50

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29577464788732394
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8263888888888888
  reward for individual goal_min: 0.0
episode_len_mean: 199.51
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 3277
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.07280659675598
time_total_s: 2453.500319004059
timers:
  learn_throughput: 413.283
  learn_time_ms: 39924.208
  load_throughput: 4803671.574
  load_time_ms: 3.435
  training_iteration_time_ms: 52616.191
  update_time_ms: 2.669
timesteps_total: 709500
training_iteration: 43

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8473684210526315
  reward for individual goal_min: 0.5
episode_len_mean: 90.89071038251366
episode_reward_max: 2.0
episode_reward_mean: 1.8415300546448088
episode_reward_min: 1.0
episodes_this_iter: 183
episodes_total: 4798
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9890710382513661
  agent_1: 0.8524590163934426
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.99230241775513
time_total_s: 2475.0066092014313
timers:
  learn_throughput: 421.365
  learn_time_ms: 39158.464
  load_throughput: 4459524.058
  load_time_ms: 3.7
  training_iteration_time_ms: 51483.624
  update_time_ms: 2.644
timesteps_total: 709500
training_iteration: 43

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2894736842105263
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.5
episode_len_mean: 183.5
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 3386
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.49781823158264
time_total_s: 2477.901739358902
timers:
  learn_throughput: 432.441
  learn_time_ms: 38155.499
  load_throughput: 5094333.856
  load_time_ms: 3.239
  training_iteration_time_ms: 50426.156
  update_time_ms: 2.64
timesteps_total: 726000
training_iteration: 44

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2876712328767123
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7278481012658228
  reward for individual goal_min: 0.0
episode_len_mean: 191.53
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 3320
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.426562547683716
time_total_s: 2477.929737329483
timers:
  learn_throughput: 418.962
  learn_time_ms: 39383.09
  load_throughput: 4725218.044
  load_time_ms: 3.492
  training_iteration_time_ms: 51974.218
  update_time_ms: 2.667
timesteps_total: 726000
training_iteration: 44

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25675675675675674
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8289473684210527
  reward for individual goal_min: 0.0
episode_len_mean: 200.97
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 3659
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.81144976615906
time_total_s: 2496.3573570251465
timers:
  learn_throughput: 465.797
  learn_time_ms: 35423.132
  load_throughput: 4690915.598
  load_time_ms: 3.517
  training_iteration_time_ms: 46987.937
  update_time_ms: 2.978
timesteps_total: 825000
training_iteration: 50

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.835
  reward for individual goal_min: 0.5
episode_len_mean: 80.71497584541063
episode_reward_max: 2.0
episode_reward_mean: 1.8405797101449275
episode_reward_min: 1.0
episodes_this_iter: 207
episodes_total: 7309
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8405797101449275
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 42.48666834831238
time_total_s: 2485.0075924396515
timers:
  learn_throughput: 531.85
  learn_time_ms: 31023.806
  load_throughput: 5542289.6
  load_time_ms: 2.977
  training_iteration_time_ms: 41651.243
  update_time_ms: 2.334
timesteps_total: 907500
training_iteration: 55

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9761904761904762
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7848837209302325
  reward for individual goal_min: 0.5
episode_len_mean: 98.52352941176471
episode_reward_max: 2.0
episode_reward_mean: 1.7588235294117647
episode_reward_min: 0.0
episodes_this_iter: 170
episodes_total: 6005
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7705882352941177
  agent_1: 0.9882352941176471
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.795920848846436
time_total_s: 2494.0721201896667
timers:
  learn_throughput: 439.751
  learn_time_ms: 37521.205
  load_throughput: 4872599.362
  load_time_ms: 3.386
  training_iteration_time_ms: 49299.956
  update_time_ms: 2.783
timesteps_total: 759000
training_iteration: 46

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6666666666666666
  reward for individual goal_min: 0.5
episode_len_mean: 126.75
episode_reward_max: 2.0
episode_reward_mean: 1.6333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8166666666666667
  agent_1: 0.8166666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9857142857142858
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6370967741935484
  reward for individual goal_min: 0.5
episode_len_mean: 126.03030303030303
episode_reward_max: 2.0
episode_reward_mean: 1.643939393939394
episode_reward_min: 0.0
episodes_this_iter: 132
episodes_total: 3868
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8560606060606061
  agent_1: 0.7878787878787878
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 72.56747722625732
time_total_s: 2500.7158284187317
timers:
  learn_throughput: 373.8
  learn_time_ms: 44141.252
  load_throughput: 4003263.417
  load_time_ms: 4.122
  training_iteration_time_ms: 58175.457
  update_time_ms: 2.817
timesteps_total: 660000
training_iteration: 40

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-19wecnkw4d/checkpoint_000040/checkpoint-40
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8642857142857143
  reward for individual goal_min: 0.0
episode_len_mean: 206.32
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 2819
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.08884382247925
time_total_s: 2480.363314151764
timers:
  learn_throughput: 368.071
  learn_time_ms: 44828.313
  load_throughput: 4305945.421
  load_time_ms: 3.832
  training_iteration_time_ms: 58931.26
  update_time_ms: 2.721
timesteps_total: 627000
training_iteration: 38

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2631578947368421
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9782608695652174
  reward for individual goal_min: 0.5
episode_len_mean: 189.58
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 3356
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.5900022983551
time_total_s: 2495.6380093097687
timers:
  learn_throughput: 412.344
  learn_time_ms: 40015.125
  load_throughput: 4966487.449
  load_time_ms: 3.322
  training_iteration_time_ms: 53064.939
  update_time_ms: 2.727
timesteps_total: 709500
training_iteration: 43

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.38028169014084506
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7121212121212122
  reward for individual goal_min: 0.0
episode_len_mean: 201.75
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 3729
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.838332176208496
time_total_s: 2497.1308629512787
timers:
  learn_throughput: 456.875
  learn_time_ms: 36114.95
  load_throughput: 5124435.658
  load_time_ms: 3.22
  training_iteration_time_ms: 47725.324
  update_time_ms: 2.546
timesteps_total: 808500
training_iteration: 49

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35714285714285715
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8785714285714286
  reward for individual goal_min: 0.0
episode_len_mean: 173.42
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 2925
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.78207349777222
time_total_s: 2495.9113430976868
timers:
  learn_throughput: 365.619
  learn_time_ms: 45128.969
  load_throughput: 4204011.445
  load_time_ms: 3.925
  training_iteration_time_ms: 59423.954
  update_time_ms: 2.793
timesteps_total: 643500
training_iteration: 39

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2357142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.03
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 4111
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.215078353881836
time_total_s: 2511.6485641002655
timers:
  learn_throughput: 483.632
  learn_time_ms: 34116.836
  load_throughput: 5242641.698
  load_time_ms: 3.147
  training_iteration_time_ms: 45191.41
  update_time_ms: 2.517
timesteps_total: 841500
training_iteration: 51

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.75
  reward for individual goal_min: 0.0
episode_len_mean: 222.74
episode_reward_max: 2.0
episode_reward_mean: 0.96
episode_reward_min: 0.0
episodes_this_iter: 75
episodes_total: 2966
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.4
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.651875257492065
time_total_s: 2503.666795730591
timers:
  learn_throughput: 392.422
  learn_time_ms: 42046.568
  load_throughput: 4545342.152
  load_time_ms: 3.63
  training_iteration_time_ms: 55693.563
  update_time_ms: 2.631
timesteps_total: 693000
training_iteration: 42

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3888888888888889
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8071428571428572
  reward for individual goal_min: 0.0
episode_len_mean: 184.02
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 3365
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.57168626785278
time_total_s: 2505.0720052719116
timers:
  learn_throughput: 414.503
  learn_time_ms: 39806.675
  load_throughput: 4819293.325
  load_time_ms: 3.424
  training_iteration_time_ms: 52450.006
  update_time_ms: 2.659
timesteps_total: 726000
training_iteration: 44

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8368421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 87.10526315789474
episode_reward_max: 2.0
episode_reward_mean: 1.8368421052631578
episode_reward_min: 1.0
episodes_this_iter: 190
episodes_total: 4988
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8368421052631579
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 51.79320168495178
time_total_s: 2526.799810886383
timers:
  learn_throughput: 420.117
  learn_time_ms: 39274.735
  load_throughput: 4420725.524
  load_time_ms: 3.732
  training_iteration_time_ms: 51625.071
  update_time_ms: 2.655
timesteps_total: 726000
training_iteration: 44

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.83
  reward for individual goal_min: 0.5
episode_len_mean: 85.94818652849742
episode_reward_max: 2.0
episode_reward_mean: 1.8238341968911918
episode_reward_min: 1.0
episodes_this_iter: 193
episodes_total: 7502
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8238341968911918
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 42.07593107223511
time_total_s: 2527.0835235118866
timers:
  learn_throughput: 532.174
  learn_time_ms: 31004.879
  load_throughput: 5609858.226
  load_time_ms: 2.941
  training_iteration_time_ms: 41660.831
  update_time_ms: 2.339
timesteps_total: 924000
training_iteration: 56

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2357142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7195121951219512
  reward for individual goal_min: 0.0
episode_len_mean: 204.28
episode_reward_max: 2.0
episode_reward_mean: 1.03
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 3743
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.39
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.53115177154541
time_total_s: 2542.888508796692
timers:
  learn_throughput: 467.533
  learn_time_ms: 35291.606
  load_throughput: 4703924.309
  load_time_ms: 3.508
  training_iteration_time_ms: 46863.817
  update_time_ms: 2.98
timesteps_total: 841500
training_iteration: 51

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3271604938271605
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6901408450704225
  reward for individual goal_min: 0.0
episode_len_mean: 207.09
episode_reward_max: 2.0
episode_reward_mean: 1.04
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 3400
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.198832988739014
time_total_s: 2529.128570318222
timers:
  learn_throughput: 421.809
  learn_time_ms: 39117.255
  load_throughput: 4788482.072
  load_time_ms: 3.446
  training_iteration_time_ms: 51753.209
  update_time_ms: 2.665
timesteps_total: 742500
training_iteration: 45

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3888888888888889
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9848484848484849
  reward for individual goal_min: 0.0
episode_len_mean: 168.58
episode_reward_max: 2.0
episode_reward_mean: 1.41
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 3484
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.76
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.285948514938354
time_total_s: 2530.1876878738403
timers:
  learn_throughput: 430.744
  learn_time_ms: 38305.799
  load_throughput: 5095008.945
  load_time_ms: 3.238
  training_iteration_time_ms: 50609.637
  update_time_ms: 2.611
timesteps_total: 742500
training_iteration: 45

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8296703296703297
  reward for individual goal_min: 0.5
episode_len_mean: 88.46524064171123
episode_reward_max: 2.0
episode_reward_mean: 1.8342245989304813
episode_reward_min: 1.0
episodes_this_iter: 187
episodes_total: 6192
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8342245989304813
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.522316694259644
time_total_s: 2541.5944368839264
timers:
  learn_throughput: 442.048
  learn_time_ms: 37326.26
  load_throughput: 4908819.929
  load_time_ms: 3.361
  training_iteration_time_ms: 49087.638
  update_time_ms: 2.8
timesteps_total: 775500
training_iteration: 47

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3732394366197183
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7534246575342466
  reward for individual goal_min: 0.0
episode_len_mean: 204.35
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 3806
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.33682703971863
time_total_s: 2543.4676899909973
timers:
  learn_throughput: 460.07
  learn_time_ms: 35864.118
  load_throughput: 5136873.608
  load_time_ms: 3.212
  training_iteration_time_ms: 47439.701
  update_time_ms: 2.512
timesteps_total: 825000
training_iteration: 50

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6470588235294118
  reward for individual goal_min: 0.5
episode_len_mean: 125.68702290076335
episode_reward_max: 2.0
episode_reward_mean: 1.633587786259542
episode_reward_min: 1.0
episodes_this_iter: 131
episodes_total: 3999
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8091603053435115
  agent_1: 0.8244274809160306
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.69162559509277
time_total_s: 2557.4074540138245
timers:
  learn_throughput: 376.335
  learn_time_ms: 43843.964
  load_throughput: 4100125.363
  load_time_ms: 4.024
  training_iteration_time_ms: 57859.788
  update_time_ms: 2.864
timesteps_total: 676500
training_iteration: 41

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2986111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.94
  reward for individual goal_min: 0.0
episode_len_mean: 177.16
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 3447
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.65589475631714
time_total_s: 2549.293904066086
timers:
  learn_throughput: 408.055
  learn_time_ms: 40435.705
  load_throughput: 4974877.328
  load_time_ms: 3.317
  training_iteration_time_ms: 53364.714
  update_time_ms: 2.702
timesteps_total: 726000
training_iteration: 44

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30405405405405406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8986486486486487
  reward for individual goal_min: 0.0
episode_len_mean: 190.01
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 2905
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.98422908782959
time_total_s: 2538.3475432395935
timers:
  learn_throughput: 368.69
  learn_time_ms: 44753.01
  load_throughput: 4243320.784
  load_time_ms: 3.888
  training_iteration_time_ms: 58790.452
  update_time_ms: 2.754
timesteps_total: 643500
training_iteration: 39

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.310126582278481
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9791666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 183.96
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 4200
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.84032154083252
time_total_s: 2557.488885641098
timers:
  learn_throughput: 480.939
  learn_time_ms: 34307.906
  load_throughput: 5241569.608
  load_time_ms: 3.148
  training_iteration_time_ms: 45455.364
  update_time_ms: 2.528
timesteps_total: 858000
training_iteration: 52

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8669724770642202
  reward for individual goal_min: 0.5
episode_len_mean: 74.13636363636364
episode_reward_max: 2.0
episode_reward_mean: 1.8681818181818182
episode_reward_min: 1.0
episodes_this_iter: 220
episodes_total: 7722
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8681818181818182
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 43.23120069503784
time_total_s: 2570.3147242069244
timers:
  learn_throughput: 530.269
  learn_time_ms: 31116.294
  load_throughput: 5584282.74
  load_time_ms: 2.955
  training_iteration_time_ms: 41769.103
  update_time_ms: 2.339
timesteps_total: 940500
training_iteration: 57

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.76
  reward for individual goal_min: 0.0
episode_len_mean: 213.41
episode_reward_max: 2.0
episode_reward_mean: 1.01
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 3817
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.39
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.81871461868286
time_total_s: 2588.7072234153748
timers:
  learn_throughput: 467.899
  learn_time_ms: 35264.05
  load_throughput: 4712733.216
  load_time_ms: 3.501
  training_iteration_time_ms: 46781.858
  update_time_ms: 2.997
timesteps_total: 858000
training_iteration: 52

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 102.13333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.8333333333333333
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.85
  agent_1: 0.9833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2894736842105263
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9027777777777778
  reward for individual goal_min: 0.0
episode_len_mean: 189.98
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 3013
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 68.88934183120728
time_total_s: 2564.800684928894
timers:
  learn_throughput: 367.138
  learn_time_ms: 44942.244
  load_throughput: 4188642.986
  load_time_ms: 3.939
  training_iteration_time_ms: 59155.743
  update_time_ms: 2.791
timesteps_total: 660000
training_iteration: 40

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-29ws591pkw/checkpoint_000040/checkpoint-40
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23684210526315788
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7432432432432432
  reward for individual goal_min: 0.0
episode_len_mean: 215.35
episode_reward_max: 2.0
episode_reward_mean: 0.99
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 3044
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.44
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.40613603591919
time_total_s: 2559.07293176651
timers:
  learn_throughput: 391.182
  learn_time_ms: 42179.857
  load_throughput: 4602198.223
  load_time_ms: 3.585
  training_iteration_time_ms: 55862.991
  update_time_ms: 2.655
timesteps_total: 709500
training_iteration: 43

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3181818181818182
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8888888888888888
  reward for individual goal_min: 0.0
episode_len_mean: 184.09
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 3456
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.5817334651947
time_total_s: 2557.6537387371063
timers:
  learn_throughput: 415.076
  learn_time_ms: 39751.781
  load_throughput: 4813427.461
  load_time_ms: 3.428
  training_iteration_time_ms: 52467.716
  update_time_ms: 2.656
timesteps_total: 742500
training_iteration: 45

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9903846153846154
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8686868686868687
  reward for individual goal_min: 0.5
episode_len_mean: 80.56650246305419
episode_reward_max: 2.0
episode_reward_mean: 1.8620689655172413
episode_reward_min: 0.0
episodes_this_iter: 203
episodes_total: 5191
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9852216748768473
  agent_1: 0.8768472906403941
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.11898875236511
time_total_s: 2578.918799638748
timers:
  learn_throughput: 418.484
  learn_time_ms: 39428.042
  load_throughput: 4394953.609
  load_time_ms: 3.754
  training_iteration_time_ms: 51866.336
  update_time_ms: 2.638
timesteps_total: 742500
training_iteration: 45

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2671232876712329
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 179.69
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 3575
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.98804569244385
time_total_s: 2579.175733566284
timers:
  learn_throughput: 435.327
  learn_time_ms: 37902.561
  load_throughput: 5054633.206
  load_time_ms: 3.264
  training_iteration_time_ms: 50132.603
  update_time_ms: 2.607
timesteps_total: 759000
training_iteration: 46

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3028169014084507
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6301369863013698
  reward for individual goal_min: 0.0
episode_len_mean: 203.15
episode_reward_max: 2.0
episode_reward_mean: 1.0
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 3482
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.48
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.912543296813965
time_total_s: 2583.041113615036
timers:
  learn_throughput: 421.415
  learn_time_ms: 39153.824
  load_throughput: 4813159.648
  load_time_ms: 3.428
  training_iteration_time_ms: 51841.667
  update_time_ms: 2.676
timesteps_total: 759000
training_iteration: 46

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3194444444444444
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7746478873239436
  reward for individual goal_min: 0.0
episode_len_mean: 205.07
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 3888
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.48714327812195
time_total_s: 2586.9548332691193
timers:
  learn_throughput: 462.986
  learn_time_ms: 35638.209
  load_throughput: 5189802.475
  load_time_ms: 3.179
  training_iteration_time_ms: 47118.29
  update_time_ms: 2.504
timesteps_total: 841500
training_iteration: 51

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9611650485436893
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.865979381443299
  reward for individual goal_min: 0.5
episode_len_mean: 82.18
episode_reward_max: 2.0
episode_reward_mean: 1.83
episode_reward_min: 0.0
episodes_this_iter: 200
episodes_total: 6392
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.85
  agent_1: 0.98
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.829713344573975
time_total_s: 2591.4241502285004
timers:
  learn_throughput: 443.539
  learn_time_ms: 37200.786
  load_throughput: 4951315.061
  load_time_ms: 3.332
  training_iteration_time_ms: 48913.924
  update_time_ms: 2.787
timesteps_total: 792000
training_iteration: 48

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22142857142857142
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9473684210526315
  reward for individual goal_min: 0.0
episode_len_mean: 184.38
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 3538
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.792155265808105
time_total_s: 2600.086059331894
timers:
  learn_throughput: 410.115
  learn_time_ms: 40232.584
  load_throughput: 4926922.436
  load_time_ms: 3.349
  training_iteration_time_ms: 53033.231
  update_time_ms: 2.695
timesteps_total: 742500
training_iteration: 45

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6301369863013698
  reward for individual goal_min: 0.5
episode_len_mean: 148.5132743362832
episode_reward_max: 2.0
episode_reward_mean: 1.5221238938053097
episode_reward_min: 1.0
episodes_this_iter: 113
episodes_total: 4112
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8053097345132744
  agent_1: 0.7168141592920354
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.01514768600464
time_total_s: 2611.422601699829
timers:
  learn_throughput: 380.13
  learn_time_ms: 43406.151
  load_throughput: 4196745.743
  load_time_ms: 3.932
  training_iteration_time_ms: 57301.325
  update_time_ms: 2.846
timesteps_total: 693000
training_iteration: 42

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29411764705882354
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9746835443037974
  reward for individual goal_min: 0.0
episode_len_mean: 171.4
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 4295
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.85236096382141
time_total_s: 2601.3412466049194
timers:
  learn_throughput: 485.471
  learn_time_ms: 33987.634
  load_throughput: 5262014.599
  load_time_ms: 3.136
  training_iteration_time_ms: 45159.866
  update_time_ms: 2.527
timesteps_total: 874500
training_iteration: 53

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8282828282828283
  reward for individual goal_min: 0.5
episode_len_mean: 84.70918367346938
episode_reward_max: 2.0
episode_reward_mean: 1.8265306122448979
episode_reward_min: 1.0
episodes_this_iter: 196
episodes_total: 7918
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8367346938775511
  agent_1: 0.9897959183673469
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.387646198272705
time_total_s: 2612.702370405197
timers:
  learn_throughput: 528.396
  learn_time_ms: 31226.57
  load_throughput: 5566092.894
  load_time_ms: 2.964
  training_iteration_time_ms: 41919.516
  update_time_ms: 2.335
timesteps_total: 957000
training_iteration: 58

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2565789473684211
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8125
  reward for individual goal_min: 0.0
episode_len_mean: 203.63
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 3902
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.81259560585022
time_total_s: 2635.519819021225
timers:
  learn_throughput: 469.391
  learn_time_ms: 35151.931
  load_throughput: 4740495.243
  load_time_ms: 3.481
  training_iteration_time_ms: 46607.713
  update_time_ms: 2.984
timesteps_total: 874500
training_iteration: 53

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8
  reward for individual goal_min: 0.5
episode_len_mean: 113.56666666666666
episode_reward_max: 2.0
episode_reward_mean: 1.7666666666666666
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9
  agent_1: 0.8666666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.323943661971831
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8819444444444444
  reward for individual goal_min: 0.0
episode_len_mean: 186.38
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 2991
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 70.78801417350769
time_total_s: 2609.135557413101
timers:
  learn_throughput: 367.679
  learn_time_ms: 44876.11
  load_throughput: 4260352.372
  load_time_ms: 3.873
  training_iteration_time_ms: 58910.493
  update_time_ms: 2.768
timesteps_total: 660000
training_iteration: 40

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-2933yh83nn/checkpoint_000040/checkpoint-40
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2631578947368421
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 188.2
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 3663
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.86050486564636
time_total_s: 2627.0362384319305
timers:
  learn_throughput: 437.696
  learn_time_ms: 37697.433
  load_throughput: 5100754.433
  load_time_ms: 3.235
  training_iteration_time_ms: 49807.443
  update_time_ms: 2.599
timesteps_total: 775500
training_iteration: 47

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33098591549295775
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8851351351351351
  reward for individual goal_min: 0.0
episode_len_mean: 179.77
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 3551
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.2386109828949
time_total_s: 2609.892349720001
timers:
  learn_throughput: 415.254
  learn_time_ms: 39734.67
  load_throughput: 4814264.567
  load_time_ms: 3.427
  training_iteration_time_ms: 52459.643
  update_time_ms: 2.764
timesteps_total: 759000
training_iteration: 46

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.989247311827957
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8473684210526315
  reward for individual goal_min: 0.5
episode_len_mean: 88.88297872340425
episode_reward_max: 2.0
episode_reward_mean: 1.8351063829787233
episode_reward_min: 0.0
episodes_this_iter: 188
episodes_total: 5379
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9840425531914894
  agent_1: 0.851063829787234
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.014968156814575
time_total_s: 2630.9337677955627
timers:
  learn_throughput: 419.032
  learn_time_ms: 39376.432
  load_throughput: 4357293.172
  load_time_ms: 3.787
  training_iteration_time_ms: 51867.014
  update_time_ms: 2.648
timesteps_total: 759000
training_iteration: 46

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33098591549295775
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9240506329113924
  reward for individual goal_min: 0.0
episode_len_mean: 178.03
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 3106
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.88483691215515
time_total_s: 2622.685521841049
timers:
  learn_throughput: 368.005
  learn_time_ms: 44836.342
  load_throughput: 4174645.216
  load_time_ms: 3.952
  training_iteration_time_ms: 58951.29
  update_time_ms: 2.781
timesteps_total: 676500
training_iteration: 41

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6447368421052632
  reward for individual goal_min: 0.0
episode_len_mean: 215.77
episode_reward_max: 2.0
episode_reward_mean: 0.98
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 3117
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.44
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.47429871559143
time_total_s: 2615.5472304821014
timers:
  learn_throughput: 391.783
  learn_time_ms: 42115.195
  load_throughput: 4586095.531
  load_time_ms: 3.598
  training_iteration_time_ms: 55819.869
  update_time_ms: 2.66
timesteps_total: 726000
training_iteration: 44

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3493150684931507
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7088607594936709
  reward for individual goal_min: 0.0
episode_len_mean: 203.43
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 3561
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.642202615737915
time_total_s: 2632.683316230774
timers:
  learn_throughput: 424.966
  learn_time_ms: 38826.615
  load_throughput: 4830528.52
  load_time_ms: 3.416
  training_iteration_time_ms: 51440.627
  update_time_ms: 2.639
timesteps_total: 775500
training_iteration: 47

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.38
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7307692307692307
  reward for individual goal_min: 0.0
episode_len_mean: 193.26
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 3970
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.78752136230469
time_total_s: 2633.742354631424
timers:
  learn_throughput: 463.192
  learn_time_ms: 35622.36
  load_throughput: 5243237.493
  load_time_ms: 3.147
  training_iteration_time_ms: 47063.198
  update_time_ms: 2.505
timesteps_total: 858000
training_iteration: 52

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9893617021276596
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8351063829787234
  reward for individual goal_min: 0.5
episode_len_mean: 86.76595744680851
episode_reward_max: 2.0
episode_reward_mean: 1.824468085106383
episode_reward_min: 0.0
episodes_this_iter: 188
episodes_total: 6580
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8297872340425532
  agent_1: 0.9946808510638298
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.78425121307373
time_total_s: 2640.208401441574
timers:
  learn_throughput: 444.29
  learn_time_ms: 37137.926
  load_throughput: 4987281.807
  load_time_ms: 3.308
  training_iteration_time_ms: 48781.911
  update_time_ms: 2.804
timesteps_total: 808500
training_iteration: 49

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17073170731707318
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9746835443037974
  reward for individual goal_min: 0.0
episode_len_mean: 206.44
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 4375
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.740123987197876
time_total_s: 2645.0813705921173
timers:
  learn_throughput: 486.388
  learn_time_ms: 33923.567
  load_throughput: 5313036.228
  load_time_ms: 3.106
  training_iteration_time_ms: 45105.486
  update_time_ms: 2.528
timesteps_total: 891000
training_iteration: 54

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23076923076923078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9683544303797469
  reward for individual goal_min: 0.0
episode_len_mean: 188.97
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 3625
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.97966718673706
time_total_s: 2653.065726518631
timers:
  learn_throughput: 410.127
  learn_time_ms: 40231.443
  load_throughput: 5004918.858
  load_time_ms: 3.297
  training_iteration_time_ms: 53097.495
  update_time_ms: 2.693
timesteps_total: 759000
training_iteration: 46

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8052631578947368
  reward for individual goal_min: 0.5
episode_len_mean: 87.94594594594595
episode_reward_max: 2.0
episode_reward_mean: 1.8
episode_reward_min: 1.0
episodes_this_iter: 185
episodes_total: 8103
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 41.46993279457092
time_total_s: 2654.172303199768
timers:
  learn_throughput: 528.311
  learn_time_ms: 31231.61
  load_throughput: 5560815.408
  load_time_ms: 2.967
  training_iteration_time_ms: 41945.732
  update_time_ms: 2.342
timesteps_total: 973500
training_iteration: 59

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6884057971014492
  reward for individual goal_min: 0.5
episode_len_mean: 105.98709677419355
episode_reward_max: 2.0
episode_reward_mean: 1.7225806451612904
episode_reward_min: 1.0
episodes_this_iter: 155
episodes_total: 4267
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8451612903225807
  agent_1: 0.8774193548387097
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.93233585357666
time_total_s: 2669.3549375534058
timers:
  learn_throughput: 379.733
  learn_time_ms: 43451.55
  load_throughput: 4238616.812
  load_time_ms: 3.893
  training_iteration_time_ms: 57317.456
  update_time_ms: 2.75
timesteps_total: 709500
training_iteration: 43

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1917808219178082
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7302631578947368
  reward for individual goal_min: 0.0
episode_len_mean: 204.04
episode_reward_max: 2.0
episode_reward_mean: 0.97
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 3982
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.296082496643066
time_total_s: 2684.815901517868
timers:
  learn_throughput: 467.119
  learn_time_ms: 35322.922
  load_throughput: 4747193.843
  load_time_ms: 3.476
  training_iteration_time_ms: 46846.473
  update_time_ms: 2.965
timesteps_total: 891000
training_iteration: 54

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3150684931506849
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9788732394366197
  reward for individual goal_min: 0.0
episode_len_mean: 172.4
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 3757
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.76
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.39809727668762
time_total_s: 2676.434335708618
timers:
  learn_throughput: 438.837
  learn_time_ms: 37599.423
  load_throughput: 5088153.867
  load_time_ms: 3.243
  training_iteration_time_ms: 49752.172
  update_time_ms: 2.581
timesteps_total: 792000
training_iteration: 48

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23972602739726026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8875
  reward for individual goal_min: 0.0
episode_len_mean: 201.02
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 3074
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.353856801986694
time_total_s: 2665.489414215088
timers:
  learn_throughput: 370.768
  learn_time_ms: 44502.168
  load_throughput: 4187274.454
  load_time_ms: 3.941
  training_iteration_time_ms: 58528.535
  update_time_ms: 2.793
timesteps_total: 676500
training_iteration: 41

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35443037974683544
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6923076923076923
  reward for individual goal_min: 0.0
episode_len_mean: 194.74
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 4053
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.89125156402588
time_total_s: 2681.63360619545
timers:
  learn_throughput: 463.837
  learn_time_ms: 35572.818
  load_throughput: 5263455.325
  load_time_ms: 3.135
  training_iteration_time_ms: 46984.681
  update_time_ms: 2.481
timesteps_total: 874500
training_iteration: 53

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8556701030927835
  reward for individual goal_min: 0.5
episode_len_mean: 83.16326530612245
episode_reward_max: 2.0
episode_reward_mean: 1.8571428571428572
episode_reward_min: 1.0
episodes_this_iter: 196
episodes_total: 5575
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9948979591836735
  agent_1: 0.8622448979591837
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.1822829246521
time_total_s: 2683.116050720215
timers:
  learn_throughput: 418.433
  learn_time_ms: 39432.836
  load_throughput: 4334451.257
  load_time_ms: 3.807
  training_iteration_time_ms: 51913.69
  update_time_ms: 2.645
timesteps_total: 775500
training_iteration: 47

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.37012987012987014
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8357142857142857
  reward for individual goal_min: 0.0
episode_len_mean: 180.86
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 3643
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.587905168533325
time_total_s: 2663.4802548885345
timers:
  learn_throughput: 414.47
  learn_time_ms: 39809.864
  load_throughput: 4808344.114
  load_time_ms: 3.432
  training_iteration_time_ms: 52569.934
  update_time_ms: 2.788
timesteps_total: 775500
training_iteration: 47

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.37333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6075949367088608
  reward for individual goal_min: 0.0
episode_len_mean: 218.07
episode_reward_max: 2.0
episode_reward_mean: 1.0
episode_reward_min: 0.0
episodes_this_iter: 75
episodes_total: 3636
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.16715931892395
time_total_s: 2682.850475549698
timers:
  learn_throughput: 428.335
  learn_time_ms: 38521.245
  load_throughput: 4845375.659
  load_time_ms: 3.405
  training_iteration_time_ms: 51095.074
  update_time_ms: 2.658
timesteps_total: 792000
training_iteration: 48

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2708333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6688311688311688
  reward for individual goal_min: 0.0
episode_len_mean: 216.56
episode_reward_max: 2.0
episode_reward_mean: 1.0
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 3194
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.47
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.487693786621094
time_total_s: 2669.0349242687225
timers:
  learn_throughput: 396.077
  learn_time_ms: 41658.608
  load_throughput: 4520934.681
  load_time_ms: 3.65
  training_iteration_time_ms: 55222.103
  update_time_ms: 2.676
timesteps_total: 742500
training_iteration: 45

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8247422680412371
  reward for individual goal_min: 0.5
episode_len_mean: 87.35078534031413
episode_reward_max: 2.0
episode_reward_mean: 1.8219895287958114
episode_reward_min: 1.0
episodes_this_iter: 191
episodes_total: 6771
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8219895287958116
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.93932509422302
time_total_s: 2689.147726535797
timers:
  learn_throughput: 445.656
  learn_time_ms: 37024.062
  load_throughput: 4999639.942
  load_time_ms: 3.3
  training_iteration_time_ms: 48662.782
  update_time_ms: 2.664
timesteps_total: 825000
training_iteration: 50

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2077922077922078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9683544303797469
  reward for individual goal_min: 0.0
episode_len_mean: 192.49
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 3191
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.44359993934631
time_total_s: 2678.1291217803955
timers:
  learn_throughput: 370.436
  learn_time_ms: 44542.086
  load_throughput: 4111378.737
  load_time_ms: 4.013
  training_iteration_time_ms: 58562.13
  update_time_ms: 2.774
timesteps_total: 693000
training_iteration: 42

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21951219512195122
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935897435897436
  reward for individual goal_min: 0.5
episode_len_mean: 194.45
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 4461
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.924267053604126
time_total_s: 2689.0056376457214
timers:
  learn_throughput: 489.862
  learn_time_ms: 33682.946
  load_throughput: 5321615.724
  load_time_ms: 3.101
  training_iteration_time_ms: 44780.302
  update_time_ms: 2.525
timesteps_total: 907500
training_iteration: 55

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 78.53333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.8333333333333333
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8333333333333334
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8632075471698113
  reward for individual goal_min: 0.5
episode_len_mean: 82.49751243781094
episode_reward_max: 2.0
episode_reward_mean: 1.855721393034826
episode_reward_min: 1.0
episodes_this_iter: 201
episodes_total: 8304
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8606965174129353
  agent_1: 0.9950248756218906
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.195486545562744
time_total_s: 2703.367789745331
timers:
  learn_throughput: 527.307
  learn_time_ms: 31291.074
  load_throughput: 5559251.976
  load_time_ms: 2.968
  training_iteration_time_ms: 42084.967
  update_time_ms: 2.358
timesteps_total: 990000
training_iteration: 60

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-172kuwc_58/checkpoint_000060/checkpoint-60
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3782051282051282
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.94
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 3717
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.48155617713928
time_total_s: 2704.5472826957703
timers:
  learn_throughput: 410.143
  learn_time_ms: 40229.894
  load_throughput: 5024431.425
  load_time_ms: 3.284
  training_iteration_time_ms: 53060.104
  update_time_ms: 2.695
timesteps_total: 775500
training_iteration: 47

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26973684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8355263157894737
  reward for individual goal_min: 0.0
episode_len_mean: 197.74
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 4067
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.43
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.48425817489624
time_total_s: 2729.3001596927643
timers:
  learn_throughput: 471.084
  learn_time_ms: 35025.598
  load_throughput: 4736666.689
  load_time_ms: 3.483
  training_iteration_time_ms: 46458.211
  update_time_ms: 2.595
timesteps_total: 907500
training_iteration: 55

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6917808219178082
  reward for individual goal_min: 0.5
episode_len_mean: 113.33561643835617
episode_reward_max: 2.0
episode_reward_mean: 1.6917808219178083
episode_reward_min: 1.0
episodes_this_iter: 146
episodes_total: 4413
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8287671232876712
  agent_1: 0.863013698630137
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.34667372703552
time_total_s: 2726.7016112804413
timers:
  learn_throughput: 380.486
  learn_time_ms: 43365.595
  load_throughput: 4240226.943
  load_time_ms: 3.891
  training_iteration_time_ms: 57170.299
  update_time_ms: 2.736
timesteps_total: 726000
training_iteration: 44

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17567567567567569
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9882352941176471
  reward for individual goal_min: 0.0
episode_len_mean: 185.09
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 3845
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.42728924751282
time_total_s: 2723.861624956131
timers:
  learn_throughput: 439.884
  learn_time_ms: 37509.867
  load_throughput: 5051792.135
  load_time_ms: 3.266
  training_iteration_time_ms: 49640.465
  update_time_ms: 2.585
timesteps_total: 808500
training_iteration: 49

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2847222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7094594594594594
  reward for individual goal_min: 0.0
episode_len_mean: 196.92
episode_reward_max: 2.0
episode_reward_mean: 1.06
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 4135
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.04930138587952
time_total_s: 2729.6829075813293
timers:
  learn_throughput: 464.503
  learn_time_ms: 35521.842
  load_throughput: 5249202.903
  load_time_ms: 3.143
  training_iteration_time_ms: 46927.868
  update_time_ms: 2.485
timesteps_total: 891000
training_iteration: 54

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8380281690140845
  reward for individual goal_min: 0.0
episode_len_mean: 191.89
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 3729
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.953267097473145
time_total_s: 2713.4335219860077
timers:
  learn_throughput: 416.539
  learn_time_ms: 39612.101
  load_throughput: 4760811.744
  load_time_ms: 3.466
  training_iteration_time_ms: 52399.896
  update_time_ms: 2.777
timesteps_total: 792000
training_iteration: 48

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8111111111111111
  reward for individual goal_min: 0.5
episode_len_mean: 89.75675675675676
episode_reward_max: 2.0
episode_reward_mean: 1.8162162162162163
episode_reward_min: 1.0
episodes_this_iter: 185
episodes_total: 5760
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9891891891891892
  agent_1: 0.827027027027027
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.70065760612488
time_total_s: 2733.8167083263397
timers:
  learn_throughput: 420.071
  learn_time_ms: 39279.071
  load_throughput: 4316473.274
  load_time_ms: 3.823
  training_iteration_time_ms: 51730.282
  update_time_ms: 2.632
timesteps_total: 792000
training_iteration: 48

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8523809523809524
  reward for individual goal_min: 0.5
episode_len_mean: 82.525
episode_reward_max: 2.0
episode_reward_mean: 1.845
episode_reward_min: 1.0
episodes_this_iter: 200
episodes_total: 6971
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.845
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.481698751449585
time_total_s: 2736.6294252872467
timers:
  learn_throughput: 447.145
  learn_time_ms: 36900.785
  load_throughput: 4975521.126
  load_time_ms: 3.316
  training_iteration_time_ms: 48535.309
  update_time_ms: 2.675
timesteps_total: 841500
training_iteration: 51

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15714285714285714
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.66
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 4554
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.42117261886597
time_total_s: 2732.4268102645874
timers:
  learn_throughput: 494.618
  learn_time_ms: 33359.053
  load_throughput: 5337005.367
  load_time_ms: 3.092
  training_iteration_time_ms: 44456.249
  update_time_ms: 2.524
timesteps_total: 924000
training_iteration: 56

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30985915492957744
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6481481481481481
  reward for individual goal_min: 0.0
episode_len_mean: 202.01
episode_reward_max: 2.0
episode_reward_mean: 1.03
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 3717
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.624884605407715
time_total_s: 2734.4753601551056
timers:
  learn_throughput: 426.845
  learn_time_ms: 38655.682
  load_throughput: 4844629.439
  load_time_ms: 3.406
  training_iteration_time_ms: 51186.662
  update_time_ms: 2.699
timesteps_total: 808500
training_iteration: 49

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3670886075949367
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9090909090909091
  reward for individual goal_min: 0.0
episode_len_mean: 190.03
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 3161
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.77556896209717
time_total_s: 2725.264983177185
timers:
  learn_throughput: 370.993
  learn_time_ms: 44475.25
  load_throughput: 4261506.669
  load_time_ms: 3.872
  training_iteration_time_ms: 58505.214
  update_time_ms: 2.803
timesteps_total: 693000
training_iteration: 42

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22794117647058823
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.782051282051282
  reward for individual goal_min: 0.0
episode_len_mean: 211.47
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 3270
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.47
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.406407594680786
time_total_s: 2723.4413318634033
timers:
  learn_throughput: 395.954
  learn_time_ms: 41671.538
  load_throughput: 4500209.124
  load_time_ms: 3.666
  training_iteration_time_ms: 55200.934
  update_time_ms: 2.664
timesteps_total: 759000
training_iteration: 46

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23809523809523808
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9315068493150684
  reward for individual goal_min: 0.0
episode_len_mean: 202.11
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 3271
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.08247494697571
time_total_s: 2734.211596727371
timers:
  learn_throughput: 373.828
  learn_time_ms: 44137.921
  load_throughput: 4109498.887
  load_time_ms: 4.015
  training_iteration_time_ms: 58054.847
  update_time_ms: 2.769
timesteps_total: 709500
training_iteration: 43

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8613445378151261
  reward for individual goal_min: 0.5
episode_len_mean: 80.09615384615384
episode_reward_max: 2.0
episode_reward_mean: 1.8413461538461537
episode_reward_min: 1.0
episodes_this_iter: 208
episodes_total: 8512
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8413461538461539
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 42.015117168426514
time_total_s: 2745.3829069137573
timers:
  learn_throughput: 527.169
  learn_time_ms: 31299.251
  load_throughput: 5504817.569
  load_time_ms: 2.997
  training_iteration_time_ms: 42104.766
  update_time_ms: 2.362
timesteps_total: 1006500
training_iteration: 61

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30120481927710846
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9848484848484849
  reward for individual goal_min: 0.0
episode_len_mean: 190.13
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 3806
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.38219141960144
time_total_s: 2755.9294741153717
timers:
  learn_throughput: 415.045
  learn_time_ms: 39754.766
  load_throughput: 5019657.358
  load_time_ms: 3.287
  training_iteration_time_ms: 52591.041
  update_time_ms: 2.682
timesteps_total: 792000
training_iteration: 48

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.80625
  reward for individual goal_min: 0.0
episode_len_mean: 200.11
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 4144
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.11103010177612
time_total_s: 2775.4111897945404
timers:
  learn_throughput: 471.251
  learn_time_ms: 35013.196
  load_throughput: 4824298.969
  load_time_ms: 3.42
  training_iteration_time_ms: 46432.378
  update_time_ms: 2.591
timesteps_total: 924000
training_iteration: 56

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 192.46
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 3932
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.156394243240356
time_total_s: 2774.0180191993713
timers:
  learn_throughput: 441.902
  learn_time_ms: 37338.559
  load_throughput: 5022243.703
  load_time_ms: 3.285
  training_iteration_time_ms: 49437.496
  update_time_ms: 2.602
timesteps_total: 825000
training_iteration: 50

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.676923076923077
  reward for individual goal_min: 0.5
episode_len_mean: 106.3157894736842
episode_reward_max: 2.0
episode_reward_mean: 1.7236842105263157
episode_reward_min: 1.0
episodes_this_iter: 152
episodes_total: 4565
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.875
  agent_1: 0.8486842105263158
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.74799370765686
time_total_s: 2785.449604988098
timers:
  learn_throughput: 378.885
  learn_time_ms: 43548.875
  load_throughput: 4329109.857
  load_time_ms: 3.811
  training_iteration_time_ms: 57445.308
  update_time_ms: 2.8
timesteps_total: 742500
training_iteration: 45

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3472222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7215189873417721
  reward for individual goal_min: 0.0
episode_len_mean: 202.22
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 4219
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.1864070892334
time_total_s: 2776.8693146705627
timers:
  learn_throughput: 464.373
  learn_time_ms: 35531.773
  load_throughput: 5224791.14
  load_time_ms: 3.158
  training_iteration_time_ms: 46919.746
  update_time_ms: 2.514
timesteps_total: 907500
training_iteration: 55

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18421052631578946
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9551282051282052
  reward for individual goal_min: 0.0
episode_len_mean: 196.17
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 4638
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.47859025001526
time_total_s: 2775.9054005146027
timers:
  learn_throughput: 497.589
  learn_time_ms: 33159.929
  load_throughput: 5391094.25
  load_time_ms: 3.061
  training_iteration_time_ms: 44257.092
  update_time_ms: 2.526
timesteps_total: 940500
training_iteration: 57

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8
  reward for individual goal_min: 0.5
episode_len_mean: 101.63975155279503
episode_reward_max: 2.0
episode_reward_mean: 1.763975155279503
episode_reward_min: 1.0
episodes_this_iter: 161
episodes_total: 5921
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9937888198757764
  agent_1: 0.7701863354037267
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.60396671295166
time_total_s: 2783.4206750392914
timers:
  learn_throughput: 423.149
  learn_time_ms: 38993.334
  load_throughput: 4284751.203
  load_time_ms: 3.851
  training_iteration_time_ms: 51390.956
  update_time_ms: 2.612
timesteps_total: 808500
training_iteration: 49

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9893617021276596
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8490566037735849
  reward for individual goal_min: 0.5
episode_len_mean: 83.94
episode_reward_max: 2.0
episode_reward_mean: 1.83
episode_reward_min: 0.0
episodes_this_iter: 200
episodes_total: 7171
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.835
  agent_1: 0.995
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.48220872879028
time_total_s: 2786.111634016037
timers:
  learn_throughput: 448.34
  learn_time_ms: 36802.401
  load_throughput: 5025598.989
  load_time_ms: 3.283
  training_iteration_time_ms: 48359.372
  update_time_ms: 2.663
timesteps_total: 858000
training_iteration: 52

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3767123287671233
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8717948717948718
  reward for individual goal_min: 0.0
episode_len_mean: 179.15
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 3823
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.69869136810303
time_total_s: 2765.1322133541107
timers:
  learn_throughput: 418.141
  learn_time_ms: 39460.385
  load_throughput: 4771282.136
  load_time_ms: 3.458
  training_iteration_time_ms: 52266.336
  update_time_ms: 2.741
timesteps_total: 808500
training_iteration: 49

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34177215189873417
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.608433734939759
  reward for individual goal_min: 0.0
episode_len_mean: 210.69
episode_reward_max: 2.0
episode_reward_mean: 0.99
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 3795
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.41472005844116
time_total_s: 2783.8900802135468
timers:
  learn_throughput: 429.351
  learn_time_ms: 38430.107
  load_throughput: 4841104.963
  load_time_ms: 3.408
  training_iteration_time_ms: 50955.169
  update_time_ms: 2.691
timesteps_total: 825000
training_iteration: 50

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8818181818181818
  reward for individual goal_min: 0.5
episode_len_mean: 75.25462962962963
episode_reward_max: 2.0
episode_reward_mean: 1.8796296296296295
episode_reward_min: 1.0
episodes_this_iter: 216
episodes_total: 8728
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8842592592592593
  agent_1: 0.9953703703703703
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.57007384300232
time_total_s: 2788.9529807567596
timers:
  learn_throughput: 526.073
  learn_time_ms: 31364.468
  load_throughput: 5538918.404
  load_time_ms: 2.979
  training_iteration_time_ms: 42227.9
  update_time_ms: 2.378
timesteps_total: 1023000
training_iteration: 62

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17567567567567569
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.782051282051282
  reward for individual goal_min: 0.0
episode_len_mean: 218.11
episode_reward_max: 2.0
episode_reward_mean: 1.01
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 3346
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.42
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.684499740600586
time_total_s: 2778.125831604004
timers:
  learn_throughput: 395.92
  learn_time_ms: 41675.106
  load_throughput: 4507360.688
  load_time_ms: 3.661
  training_iteration_time_ms: 55161.61
  update_time_ms: 2.652
timesteps_total: 775500
training_iteration: 47

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32142857142857145
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8618421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 184.28
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 3246
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.14449596405029
time_total_s: 2783.4094791412354
timers:
  learn_throughput: 371.633
  learn_time_ms: 44398.61
  load_throughput: 4304418.861
  load_time_ms: 3.833
  training_iteration_time_ms: 58372.848
  update_time_ms: 2.802
timesteps_total: 709500
training_iteration: 43

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23214285714285715
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9066666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 201.2
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 3353
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.04162931442261
time_total_s: 2792.253226041794
timers:
  learn_throughput: 376.197
  learn_time_ms: 43859.974
  load_throughput: 4104453.776
  load_time_ms: 4.02
  training_iteration_time_ms: 57741.287
  update_time_ms: 2.759
timesteps_total: 726000
training_iteration: 44

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23717948717948717
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7682926829268293
  reward for individual goal_min: 0.0
episode_len_mean: 203.47
episode_reward_max: 2.0
episode_reward_mean: 1.04
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 4226
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.41
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.23237085342407
time_total_s: 2822.6435606479645
timers:
  learn_throughput: 470.813
  learn_time_ms: 35045.773
  load_throughput: 4805406.028
  load_time_ms: 3.434
  training_iteration_time_ms: 46478.423
  update_time_ms: 2.594
timesteps_total: 940500
training_iteration: 57

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2534246575342466
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9671052631578947
  reward for individual goal_min: 0.0
episode_len_mean: 181.76
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 3896
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.358681201934814
time_total_s: 2806.2881553173065
timers:
  learn_throughput: 414.424
  learn_time_ms: 39814.249
  load_throughput: 4985449.516
  load_time_ms: 3.31
  training_iteration_time_ms: 52458.12
  update_time_ms: 2.68
timesteps_total: 808500
training_iteration: 49

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25301204819277107
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9714285714285714
  reward for individual goal_min: 0.0
episode_len_mean: 196.04
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 4724
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.6436231136322
time_total_s: 2820.549023628235
timers:
  learn_throughput: 498.154
  learn_time_ms: 33122.278
  load_throughput: 5390674.321
  load_time_ms: 3.061
  training_iteration_time_ms: 44202.342
  update_time_ms: 2.531
timesteps_total: 957000
training_iteration: 58

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3116883116883117
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6688311688311688
  reward for individual goal_min: 0.0
episode_len_mean: 213.74
episode_reward_max: 2.0
episode_reward_mean: 0.99
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 4293
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.47
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.967392921447754
time_total_s: 2822.8367075920105
timers:
  learn_throughput: 467.014
  learn_time_ms: 35330.86
  load_throughput: 5244071.835
  load_time_ms: 3.146
  training_iteration_time_ms: 46724.526
  update_time_ms: 2.517
timesteps_total: 924000
training_iteration: 56

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23376623376623376
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 184.45
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 4021
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.119019508361816
time_total_s: 2824.137038707733
timers:
  learn_throughput: 442.169
  learn_time_ms: 37316.04
  load_throughput: 5082474.039
  load_time_ms: 3.246
  training_iteration_time_ms: 49355.914
  update_time_ms: 2.593
timesteps_total: 841500
training_iteration: 51

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8177083333333334
  reward for individual goal_min: 0.5
episode_len_mean: 88.80748663101605
episode_reward_max: 2.0
episode_reward_mean: 1.8128342245989304
episode_reward_min: 1.0
episodes_this_iter: 187
episodes_total: 7358
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8128342245989305
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.535356283187866
time_total_s: 2833.646990299225
timers:
  learn_throughput: 449.798
  learn_time_ms: 36683.133
  load_throughput: 5063805.426
  load_time_ms: 3.258
  training_iteration_time_ms: 48218.869
  update_time_ms: 2.675
timesteps_total: 874500
training_iteration: 53

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8398058252427184
  reward for individual goal_min: 0.5
episode_len_mean: 83.275
episode_reward_max: 2.0
episode_reward_mean: 1.835
episode_reward_min: 1.0
episodes_this_iter: 200
episodes_total: 8928
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.835
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 42.294893741607666
time_total_s: 2831.2478744983673
timers:
  learn_throughput: 526.022
  learn_time_ms: 31367.517
  load_throughput: 5521462.901
  load_time_ms: 2.988
  training_iteration_time_ms: 42221.947
  update_time_ms: 2.379
timesteps_total: 1039500
training_iteration: 63

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6690140845070423
  reward for individual goal_min: 0.5
episode_len_mean: 116.61538461538461
episode_reward_max: 2.0
episode_reward_mean: 1.6713286713286712
episode_reward_min: 1.0
episodes_this_iter: 143
episodes_total: 4708
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8671328671328671
  agent_1: 0.8041958041958042
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.69090676307678
time_total_s: 2843.140511751175
timers:
  learn_throughput: 378.641
  learn_time_ms: 43576.919
  load_throughput: 4326619.903
  load_time_ms: 3.814
  training_iteration_time_ms: 57454.406
  update_time_ms: 2.932
timesteps_total: 759000
training_iteration: 46

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8048780487804879
  reward for individual goal_min: 0.5
episode_len_mean: 95.44571428571429
episode_reward_max: 2.0
episode_reward_mean: 1.8171428571428572
episode_reward_min: 1.0
episodes_this_iter: 175
episodes_total: 6096
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9942857142857143
  agent_1: 0.8228571428571428
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.854158878326416
time_total_s: 2834.274833917618
timers:
  learn_throughput: 424.682
  learn_time_ms: 38852.571
  load_throughput: 4345337.394
  load_time_ms: 3.797
  training_iteration_time_ms: 51177.591
  update_time_ms: 2.609
timesteps_total: 825000
training_iteration: 50

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3819444444444444
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6410256410256411
  reward for individual goal_min: 0.0
episode_len_mean: 201.59
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 3876
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.865596771240234
time_total_s: 2833.755676984787
timers:
  learn_throughput: 430.375
  learn_time_ms: 38338.676
  load_throughput: 4826283.945
  load_time_ms: 3.419
  training_iteration_time_ms: 50780.31
  update_time_ms: 2.691
timesteps_total: 841500
training_iteration: 51

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3013698630136986
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8493150684931506
  reward for individual goal_min: 0.0
episode_len_mean: 177.79
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 3912
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.49448800086975
time_total_s: 2818.6267013549805
timers:
  learn_throughput: 417.019
  learn_time_ms: 39566.583
  load_throughput: 4774442.122
  load_time_ms: 3.456
  training_iteration_time_ms: 52466.305
  update_time_ms: 2.737
timesteps_total: 825000
training_iteration: 50

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25316455696202533
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7196969696969697
  reward for individual goal_min: 0.0
episode_len_mean: 218.6
episode_reward_max: 2.0
episode_reward_mean: 0.95
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 3423
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.4
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.41157245635986
time_total_s: 2833.5374040603638
timers:
  learn_throughput: 395.562
  learn_time_ms: 41712.816
  load_throughput: 4539260.27
  load_time_ms: 3.635
  training_iteration_time_ms: 55182.51
  update_time_ms: 2.643
timesteps_total: 792000
training_iteration: 48

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.4240506329113924
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7337662337662337
  reward for individual goal_min: 0.0
episode_len_mean: 195.04
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 4312
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.58853363990784
time_total_s: 2869.2320942878723
timers:
  learn_throughput: 469.788
  learn_time_ms: 35122.259
  load_throughput: 4814398.531
  load_time_ms: 3.427
  training_iteration_time_ms: 46560.074
  update_time_ms: 2.582
timesteps_total: 957000
training_iteration: 58

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8947368421052632
  reward for individual goal_min: 0.0
episode_len_mean: 183.4
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 3337
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.81038808822632
time_total_s: 2840.2198672294617
timers:
  learn_throughput: 373.116
  learn_time_ms: 44222.201
  load_throughput: 4299471.683
  load_time_ms: 3.838
  training_iteration_time_ms: 58085.446
  update_time_ms: 2.791
timesteps_total: 726000
training_iteration: 44

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22857142857142856
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9743589743589743
  reward for individual goal_min: 0.0
episode_len_mean: 181.71
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 3987
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.93306827545166
time_total_s: 2856.221223592758
timers:
  learn_throughput: 418.277
  learn_time_ms: 39447.507
  load_throughput: 4980677.515
  load_time_ms: 3.313
  training_iteration_time_ms: 52110.142
  update_time_ms: 2.7
timesteps_total: 825000
training_iteration: 50

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29605263157894735
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9155844155844156
  reward for individual goal_min: 0.0
episode_len_mean: 185.54
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 3443
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.46074366569519
time_total_s: 2849.713969707489
timers:
  learn_throughput: 376.908
  learn_time_ms: 43777.218
  load_throughput: 4107206.334
  load_time_ms: 4.017
  training_iteration_time_ms: 57583.026
  update_time_ms: 2.764
timesteps_total: 742500
training_iteration: 45

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18181818181818182
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9753086419753086
  reward for individual goal_min: 0.0
episode_len_mean: 178.09
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 4815
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.956000328063965
time_total_s: 2864.505023956299
timers:
  learn_throughput: 499.055
  learn_time_ms: 33062.512
  load_throughput: 5370177.619
  load_time_ms: 3.073
  training_iteration_time_ms: 44089.956
  update_time_ms: 2.542
timesteps_total: 973500
training_iteration: 59

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30405405405405406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7066666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 208.64
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 4375
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.61097526550293
time_total_s: 2868.4476828575134
timers:
  learn_throughput: 470.892
  learn_time_ms: 35039.913
  load_throughput: 5223529.199
  load_time_ms: 3.159
  training_iteration_time_ms: 46410.192
  update_time_ms: 2.51
timesteps_total: 940500
training_iteration: 57

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2647058823529412
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.967948717948718
  reward for individual goal_min: 0.0
episode_len_mean: 172.87
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 4115
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.0523567199707
time_total_s: 2874.189395427704
timers:
  learn_throughput: 442.573
  learn_time_ms: 37281.963
  load_throughput: 5073978.034
  load_time_ms: 3.252
  training_iteration_time_ms: 49267.765
  update_time_ms: 2.6
timesteps_total: 858000
training_iteration: 52

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8428571428571429
  reward for individual goal_min: 0.5
episode_len_mean: 81.10344827586206
episode_reward_max: 2.0
episode_reward_mean: 1.8374384236453203
episode_reward_min: 1.0
episodes_this_iter: 203
episodes_total: 9131
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8374384236453202
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 41.83309030532837
time_total_s: 2873.0809648036957
timers:
  learn_throughput: 524.417
  learn_time_ms: 31463.49
  load_throughput: 5545531.588
  load_time_ms: 2.975
  training_iteration_time_ms: 42344.513
  update_time_ms: 2.36
timesteps_total: 1056000
training_iteration: 64

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8349056603773585
  reward for individual goal_min: 0.5
episode_len_mean: 86.36125654450262
episode_reward_max: 2.0
episode_reward_mean: 1.8167539267015707
episode_reward_min: 1.0
episodes_this_iter: 191
episodes_total: 7549
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8167539267015707
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.66264724731445
time_total_s: 2880.3096375465393
timers:
  learn_throughput: 451.758
  learn_time_ms: 36523.958
  load_throughput: 5059104.207
  load_time_ms: 3.261
  training_iteration_time_ms: 48025.097
  update_time_ms: 2.658
timesteps_total: 891000
training_iteration: 54

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.4
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6975308641975309
  reward for individual goal_min: 0.0
episode_len_mean: 196.99
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 3964
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.421144247055054
time_total_s: 2884.176821231842
timers:
  learn_throughput: 431.181
  learn_time_ms: 38266.985
  load_throughput: 4869273.894
  load_time_ms: 3.389
  training_iteration_time_ms: 50709.363
  update_time_ms: 2.666
timesteps_total: 858000
training_iteration: 52

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8045977011494253
  reward for individual goal_min: 0.5
episode_len_mean: 88.9945652173913
episode_reward_max: 2.0
episode_reward_mean: 1.815217391304348
episode_reward_min: 1.0
episodes_this_iter: 184
episodes_total: 6280
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9945652173913043
  agent_1: 0.8206521739130435
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.206894397735596
time_total_s: 2885.4817283153534
timers:
  learn_throughput: 426.637
  learn_time_ms: 38674.593
  load_throughput: 4376194.559
  load_time_ms: 3.77
  training_iteration_time_ms: 51031.475
  update_time_ms: 2.602
timesteps_total: 841500
training_iteration: 51

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6544117647058824
  reward for individual goal_min: 0.5
episode_len_mean: 121.31111111111112
episode_reward_max: 2.0
episode_reward_mean: 1.651851851851852
episode_reward_min: 1.0
episodes_this_iter: 135
episodes_total: 4843
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8074074074074075
  agent_1: 0.8444444444444444
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.02693176269531
time_total_s: 2899.1674435138702
timers:
  learn_throughput: 379.543
  learn_time_ms: 43473.29
  load_throughput: 4262714.103
  load_time_ms: 3.871
  training_iteration_time_ms: 57338.429
  update_time_ms: 2.936
timesteps_total: 775500
training_iteration: 47

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8602941176470589
  reward for individual goal_min: 0.0
episode_len_mean: 192.81
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 3996
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.82490515708923
time_total_s: 2871.4516065120697
timers:
  learn_throughput: 417.098
  learn_time_ms: 39559.088
  load_throughput: 4760124.083
  load_time_ms: 3.466
  training_iteration_time_ms: 52448.218
  update_time_ms: 2.727
timesteps_total: 841500
training_iteration: 51

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28313253012048195
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7285714285714285
  reward for individual goal_min: 0.0
episode_len_mean: 211.0
episode_reward_max: 2.0
episode_reward_mean: 1.03
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 4389
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.192025899887085
time_total_s: 2915.4241201877594
timers:
  learn_throughput: 470.04
  learn_time_ms: 35103.365
  load_throughput: 4834105.14
  load_time_ms: 3.413
  training_iteration_time_ms: 46551.437
  update_time_ms: 2.591
timesteps_total: 973500
training_iteration: 59

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23529411764705882
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8175675675675675
  reward for individual goal_min: 0.0
episode_len_mean: 213.6
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 3499
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.48
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.87896728515625
time_total_s: 2884.41637134552
timers:
  learn_throughput: 399.064
  learn_time_ms: 41346.778
  load_throughput: 4461594.043
  load_time_ms: 3.698
  training_iteration_time_ms: 54724.76
  update_time_ms: 2.617
timesteps_total: 808500
training_iteration: 49

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30303030303030304
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95625
  reward for individual goal_min: 0.0
episode_len_mean: 159.85714285714286
episode_reward_max: 2.0
episode_reward_mean: 1.3714285714285714
episode_reward_min: 0.0
episodes_this_iter: 105
episodes_total: 4092
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7333333333333333
  agent_1: 0.638095238095238
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.77830410003662
time_total_s: 2907.999527692795
timers:
  learn_throughput: 422.855
  learn_time_ms: 39020.503
  load_throughput: 4963530.973
  load_time_ms: 3.324
  training_iteration_time_ms: 51601.345
  update_time_ms: 2.701
timesteps_total: 841500
training_iteration: 51

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36363636363636365
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9071428571428571
  reward for individual goal_min: 0.0
episode_len_mean: 177.89
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 3424
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.45170450210571
time_total_s: 2898.6715717315674
timers:
  learn_throughput: 373.381
  learn_time_ms: 44190.77
  load_throughput: 4307902.072
  load_time_ms: 3.83
  training_iteration_time_ms: 58021.388
  update_time_ms: 2.797
timesteps_total: 742500
training_iteration: 45

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3269230769230769
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8108108108108109
  reward for individual goal_min: 0.0
episode_len_mean: 196.77
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 4459
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.57595133781433
time_total_s: 2912.0236341953278
timers:
  learn_throughput: 474.816
  learn_time_ms: 34750.269
  load_throughput: 5213809.065
  load_time_ms: 3.165
  training_iteration_time_ms: 46137.754
  update_time_ms: 2.525
timesteps_total: 957000
training_iteration: 58

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2631578947368421
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9733333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 185.51
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 3532
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.48258829116821
time_total_s: 2906.196557998657
timers:
  learn_throughput: 378.633
  learn_time_ms: 43577.833
  load_throughput: 4100416.879
  load_time_ms: 4.024
  training_iteration_time_ms: 57332.938
  update_time_ms: 2.763
timesteps_total: 759000
training_iteration: 46

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8783783783783784
  reward for individual goal_min: 0.5
episode_len_mean: 76.06018518518519
episode_reward_max: 2.0
episode_reward_mean: 1.875
episode_reward_min: 1.0
episodes_this_iter: 216
episodes_total: 9347
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.875
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 42.90410041809082
time_total_s: 2915.9850652217865
timers:
  learn_throughput: 523.987
  learn_time_ms: 31489.355
  load_throughput: 5524239.565
  load_time_ms: 2.987
  training_iteration_time_ms: 42386.07
  update_time_ms: 2.376
timesteps_total: 1072500
training_iteration: 65

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 185.8
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 4205
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.13015913963318
time_total_s: 2923.319554567337
timers:
  learn_throughput: 441.784
  learn_time_ms: 37348.588
  load_throughput: 5015328.468
  load_time_ms: 3.29
  training_iteration_time_ms: 49354.508
  update_time_ms: 2.581
timesteps_total: 874500
training_iteration: 53

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 113.53333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.5666666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7833333333333333
  agent_1: 0.7833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2777777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 175.85
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 4910
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.50889706611633
time_total_s: 2923.013921022415
timers:
  learn_throughput: 499.198
  learn_time_ms: 33053.014
  load_throughput: 5298636.103
  load_time_ms: 3.114
  training_iteration_time_ms: 44149.477
  update_time_ms: 2.55
timesteps_total: 990000
training_iteration: 60

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8085106382978723
  reward for individual goal_min: 0.5
episode_len_mean: 86.84126984126983
episode_reward_max: 2.0
episode_reward_mean: 1.8095238095238095
episode_reward_min: 1.0
episodes_this_iter: 189
episodes_total: 7738
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8095238095238095
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.00773859024048
time_total_s: 2927.31737613678
timers:
  learn_throughput: 451.811
  learn_time_ms: 36519.662
  load_throughput: 5076806.878
  load_time_ms: 3.25
  training_iteration_time_ms: 48034.137
  update_time_ms: 2.657
timesteps_total: 907500
training_iteration: 55

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.37142857142857144
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7368421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 190.77
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 4048
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.26134276390076
time_total_s: 2934.438163995743
timers:
  learn_throughput: 431.935
  learn_time_ms: 38200.225
  load_throughput: 4869136.859
  load_time_ms: 3.389
  training_iteration_time_ms: 50656.378
  update_time_ms: 2.655
timesteps_total: 874500
training_iteration: 53

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8636363636363636
  reward for individual goal_min: 0.5
episode_len_mean: 79.40975609756097
episode_reward_max: 2.0
episode_reward_mean: 1.8682926829268294
episode_reward_min: 1.0
episodes_this_iter: 205
episodes_total: 6485
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8682926829268293
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 51.58924913406372
time_total_s: 2937.070977449417
timers:
  learn_throughput: 424.024
  learn_time_ms: 38912.873
  load_throughput: 4359818.567
  load_time_ms: 3.785
  training_iteration_time_ms: 51247.077
  update_time_ms: 2.609
timesteps_total: 858000
training_iteration: 52

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21686746987951808
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8618421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 203.26
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 4077
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.61729574203491
time_total_s: 2921.0689022541046
timers:
  learn_throughput: 422.738
  learn_time_ms: 39031.28
  load_throughput: 4744037.291
  load_time_ms: 3.478
  training_iteration_time_ms: 51826.067
  update_time_ms: 2.722
timesteps_total: 858000
training_iteration: 52

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6838235294117647
  reward for individual goal_min: 0.5
episode_len_mean: 105.25
episode_reward_max: 2.0
episode_reward_mean: 1.7243589743589745
episode_reward_min: 1.0
episodes_this_iter: 156
episodes_total: 4999
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8589743589743589
  agent_1: 0.8653846153846154
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.89367628097534
time_total_s: 2959.0611197948456
timers:
  learn_throughput: 379.026
  learn_time_ms: 43532.626
  load_throughput: 4389155.922
  load_time_ms: 3.759
  training_iteration_time_ms: 57389.563
  update_time_ms: 2.958
timesteps_total: 792000
training_iteration: 48

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8166666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 138.65
episode_reward_max: 2.0
episode_reward_mean: 1.45
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2792207792207792
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7887323943661971
  reward for individual goal_min: 0.0
episode_len_mean: 208.01
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 4467
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.43
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.05660581588745
time_total_s: 2972.480726003647
timers:
  learn_throughput: 472.633
  learn_time_ms: 34910.823
  load_throughput: 4868212.072
  load_time_ms: 3.389
  training_iteration_time_ms: 46286.215
  update_time_ms: 2.625
timesteps_total: 990000
training_iteration: 60

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-091_srff7m/checkpoint_000060/checkpoint-60
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35507246376811596
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7662337662337663
  reward for individual goal_min: 0.0
episode_len_mean: 193.54
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 4541
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.669978857040405
time_total_s: 2956.693613052368
timers:
  learn_throughput: 477.174
  learn_time_ms: 34578.592
  load_throughput: 5264175.985
  load_time_ms: 3.134
  training_iteration_time_ms: 45921.124
  update_time_ms: 2.526
timesteps_total: 973500
training_iteration: 59

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2642857142857143
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8181818181818182
  reward for individual goal_min: 0.0
episode_len_mean: 206.24
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 3578
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.61264252662659
time_total_s: 2941.0290138721466
timers:
  learn_throughput: 399.96
  learn_time_ms: 41254.172
  load_throughput: 4481558.307
  load_time_ms: 3.682
  training_iteration_time_ms: 54544.136
  update_time_ms: 2.628
timesteps_total: 825000
training_iteration: 50

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8836206896551724
  reward for individual goal_min: 0.5
episode_len_mean: 75.43055555555556
episode_reward_max: 2.0
episode_reward_mean: 1.875
episode_reward_min: 1.0
episodes_this_iter: 216
episodes_total: 9563
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.875
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 43.24376201629639
time_total_s: 2959.228827238083
timers:
  learn_throughput: 522.019
  learn_time_ms: 31608.033
  load_throughput: 5484314.481
  load_time_ms: 3.009
  training_iteration_time_ms: 42502.158
  update_time_ms: 2.371
timesteps_total: 1089000
training_iteration: 66

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32432432432432434
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9246575342465754
  reward for individual goal_min: 0.0
episode_len_mean: 171.88
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 4185
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.30783462524414
time_total_s: 2961.307362318039
timers:
  learn_throughput: 421.395
  learn_time_ms: 39155.666
  load_throughput: 4942863.183
  load_time_ms: 3.338
  training_iteration_time_ms: 51785.215
  update_time_ms: 2.668
timesteps_total: 858000
training_iteration: 52

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.93125
  reward for individual goal_min: 0.0
episode_len_mean: 182.17
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 3623
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.12212157249451
time_total_s: 2961.3186795711517
timers:
  learn_throughput: 381.012
  learn_time_ms: 43305.729
  load_throughput: 4066851.344
  load_time_ms: 4.057
  training_iteration_time_ms: 56970.443
  update_time_ms: 2.783
timesteps_total: 775500
training_iteration: 47

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.42948717948717946
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7875
  reward for individual goal_min: 0.0
episode_len_mean: 183.87
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 3518
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.67974901199341
time_total_s: 2957.351320743561
timers:
  learn_throughput: 372.552
  learn_time_ms: 44289.125
  load_throughput: 4264947.032
  load_time_ms: 3.869
  training_iteration_time_ms: 58071.073
  update_time_ms: 2.816
timesteps_total: 759000
training_iteration: 46

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29012345679012347
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9632352941176471
  reward for individual goal_min: 0.0
episode_len_mean: 187.75
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 5001
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.08207988739014
time_total_s: 2969.0960009098053
timers:
  learn_throughput: 497.187
  learn_time_ms: 33186.733
  load_throughput: 5254583.39
  load_time_ms: 3.14
  training_iteration_time_ms: 44336.172
  update_time_ms: 2.532
timesteps_total: 1006500
training_iteration: 61

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8105263157894737
  reward for individual goal_min: 0.5
episode_len_mean: 90.55367231638418
episode_reward_max: 2.0
episode_reward_mean: 1.7966101694915255
episode_reward_min: 1.0
episodes_this_iter: 177
episodes_total: 7915
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7966101694915254
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.960503578186035
time_total_s: 2974.277879714966
timers:
  learn_throughput: 451.895
  learn_time_ms: 36512.921
  load_throughput: 5097823.743
  load_time_ms: 3.237
  training_iteration_time_ms: 47950.481
  update_time_ms: 2.646
timesteps_total: 924000
training_iteration: 56

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9930555555555556
  reward for individual goal_min: 0.5
episode_len_mean: 190.3
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 4293
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.461259841918945
time_total_s: 2974.780814409256
timers:
  learn_throughput: 439.058
  learn_time_ms: 37580.493
  load_throughput: 4986311.603
  load_time_ms: 3.309
  training_iteration_time_ms: 49651.004
  update_time_ms: 2.57
timesteps_total: 891000
training_iteration: 54

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3188405797101449
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.65
  reward for individual goal_min: 0.0
episode_len_mean: 202.82
episode_reward_max: 2.0
episode_reward_mean: 1.04
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 4128
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.1887412071228
time_total_s: 2986.6269052028656
timers:
  learn_throughput: 431.395
  learn_time_ms: 38248.003
  load_throughput: 4843544.438
  load_time_ms: 3.407
  training_iteration_time_ms: 50832.881
  update_time_ms: 2.663
timesteps_total: 891000
training_iteration: 54

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.40131578947368424
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8642857142857143
  reward for individual goal_min: 0.0
episode_len_mean: 179.5
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 4166
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.25917458534241
time_total_s: 2970.328076839447
timers:
  learn_throughput: 425.012
  learn_time_ms: 38822.414
  load_throughput: 4794054.78
  load_time_ms: 3.442
  training_iteration_time_ms: 51644.922
  update_time_ms: 2.717
timesteps_total: 874500
training_iteration: 53

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8899082568807339
  reward for individual goal_min: 0.5
episode_len_mean: 72.41880341880342
episode_reward_max: 2.0
episode_reward_mean: 1.8974358974358974
episode_reward_min: 1.0
episodes_this_iter: 234
episodes_total: 6719
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9957264957264957
  agent_1: 0.9017094017094017
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.12887120246887
time_total_s: 2992.199848651886
timers:
  learn_throughput: 421.142
  learn_time_ms: 39179.215
  load_throughput: 4380626.654
  load_time_ms: 3.767
  training_iteration_time_ms: 51659.326
  update_time_ms: 2.628
timesteps_total: 874500
training_iteration: 53

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8490566037735849
  reward for individual goal_min: 0.5
episode_len_mean: 84.51530612244898
episode_reward_max: 2.0
episode_reward_mean: 1.836734693877551
episode_reward_min: 1.0
episodes_this_iter: 196
episodes_total: 9759
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8367346938775511
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 41.55487632751465
time_total_s: 3000.7837035655975
timers:
  learn_throughput: 523.76
  learn_time_ms: 31503.001
  load_throughput: 5471739.656
  load_time_ms: 3.015
  training_iteration_time_ms: 42334.814
  update_time_ms: 2.368
timesteps_total: 1105500
training_iteration: 67

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2635135135135135
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.84
  reward for individual goal_min: 0.0
episode_len_mean: 184.63
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 4556
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.442566871643066
time_total_s: 3019.92329287529
timers:
  learn_throughput: 471.383
  learn_time_ms: 35003.383
  load_throughput: 4884843.197
  load_time_ms: 3.378
  training_iteration_time_ms: 46377.142
  update_time_ms: 2.606
timesteps_total: 1006500
training_iteration: 61

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7051282051282052
  reward for individual goal_min: 0.5
episode_len_mean: 112.78231292517007
episode_reward_max: 2.0
episode_reward_mean: 1.6870748299319729
episode_reward_min: 1.0
episodes_this_iter: 147
episodes_total: 5146
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8639455782312925
  agent_1: 0.8231292517006803
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.01819920539856
time_total_s: 3016.079319000244
timers:
  learn_throughput: 379.854
  learn_time_ms: 43437.783
  load_throughput: 4550692.145
  load_time_ms: 3.626
  training_iteration_time_ms: 57275.858
  update_time_ms: 2.969
timesteps_total: 808500
training_iteration: 49

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24050632911392406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9513888888888888
  reward for individual goal_min: 0.0
episode_len_mean: 193.03
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 4270
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.70659065246582
time_total_s: 3010.0139529705048
timers:
  learn_throughput: 424.049
  learn_time_ms: 38910.574
  load_throughput: 4886947.336
  load_time_ms: 3.376
  training_iteration_time_ms: 51396.697
  update_time_ms: 2.665
timesteps_total: 874500
training_iteration: 53

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2948717948717949
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7777777777777778
  reward for individual goal_min: 0.0
episode_len_mean: 206.39
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 3660
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.43
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.587934732437134
time_total_s: 2995.6169486045837
timers:
  learn_throughput: 399.466
  learn_time_ms: 41305.159
  load_throughput: 4534798.671
  load_time_ms: 3.639
  training_iteration_time_ms: 54523.198
  update_time_ms: 2.631
timesteps_total: 841500
training_iteration: 51

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7
  reward for individual goal_min: 0.5
episode_len_mean: 134.38333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.6666666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8666666666666667
  agent_1: 0.8
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3141025641025641
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7605633802816901
  reward for individual goal_min: 0.0
episode_len_mean: 208.5
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 4622
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.899805545806885
time_total_s: 3013.593418598175
timers:
  learn_throughput: 478.281
  learn_time_ms: 34498.578
  load_throughput: 5308186.783
  load_time_ms: 3.108
  training_iteration_time_ms: 45839.917
  update_time_ms: 2.542
timesteps_total: 990000
training_iteration: 60

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19istq_qwi/checkpoint_000060/checkpoint-60
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9772727272727273
  reward for individual goal_min: 0.0
episode_len_mean: 183.82
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 5092
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.59745979309082
time_total_s: 3013.693460702896
timers:
  learn_throughput: 497.912
  learn_time_ms: 33138.393
  load_throughput: 5218369.477
  load_time_ms: 3.162
  training_iteration_time_ms: 44211.874
  update_time_ms: 2.526
timesteps_total: 1023000
training_iteration: 62

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8316831683168316
  reward for individual goal_min: 0.5
episode_len_mean: 83.2128712871287
episode_reward_max: 2.0
episode_reward_mean: 1.8316831683168318
episode_reward_min: 1.0
episodes_this_iter: 202
episodes_total: 8117
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8316831683168316
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.84844136238098
time_total_s: 3021.126321077347
timers:
  learn_throughput: 452.579
  learn_time_ms: 36457.709
  load_throughput: 5111302.678
  load_time_ms: 3.228
  training_iteration_time_ms: 47882.812
  update_time_ms: 2.629
timesteps_total: 940500
training_iteration: 57

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33116883116883117
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.875
  reward for individual goal_min: 0.0
episode_len_mean: 195.89
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 3603
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.9333553314209
time_total_s: 3012.2846760749817
timers:
  learn_throughput: 374.603
  learn_time_ms: 44046.584
  load_throughput: 4247148.828
  load_time_ms: 3.885
  training_iteration_time_ms: 57795.076
  update_time_ms: 2.803
timesteps_total: 775500
training_iteration: 47

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29878048780487804
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9558823529411765
  reward for individual goal_min: 0.0
episode_len_mean: 195.12
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 4375
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.18306589126587
time_total_s: 3025.963880300522
timers:
  learn_throughput: 440.5
  learn_time_ms: 37457.395
  load_throughput: 4942898.487
  load_time_ms: 3.338
  training_iteration_time_ms: 49540.84
  update_time_ms: 2.58
timesteps_total: 907500
training_iteration: 55

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2887323943661972
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8986486486486487
  reward for individual goal_min: 0.0
episode_len_mean: 190.71
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 3709
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.893688917160034
time_total_s: 3017.2123684883118
timers:
  learn_throughput: 382.42
  learn_time_ms: 43146.331
  load_throughput: 4088860.949
  load_time_ms: 4.035
  training_iteration_time_ms: 56785.22
  update_time_ms: 2.769
timesteps_total: 792000
training_iteration: 48

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6666666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 206.38
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 4209
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.01167106628418
time_total_s: 3035.63857626915
timers:
  learn_throughput: 432.842
  learn_time_ms: 38120.163
  load_throughput: 4793457.084
  load_time_ms: 3.442
  training_iteration_time_ms: 50614.485
  update_time_ms: 3.0
timesteps_total: 907500
training_iteration: 55

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28378378378378377
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9166666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 187.35
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 4253
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.286489486694336
time_total_s: 3021.6145663261414
timers:
  learn_throughput: 425.421
  learn_time_ms: 38785.066
  load_throughput: 4827462.245
  load_time_ms: 3.418
  training_iteration_time_ms: 51616.3
  update_time_ms: 2.725
timesteps_total: 891000
training_iteration: 54

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.859375
  reward for individual goal_min: 0.5
episode_len_mean: 77.74881516587678
episode_reward_max: 2.0
episode_reward_mean: 1.872037914691943
episode_reward_min: 1.0
episodes_this_iter: 211
episodes_total: 6930
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8720379146919431
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 52.452780961990356
time_total_s: 3044.6526296138763
timers:
  learn_throughput: 421.083
  learn_time_ms: 39184.64
  load_throughput: 4357101.143
  load_time_ms: 3.787
  training_iteration_time_ms: 51724.886
  update_time_ms: 2.601
timesteps_total: 891000
training_iteration: 54

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9016393442622951
  reward for individual goal_min: 0.5
episode_len_mean: 73.05194805194805
episode_reward_max: 2.0
episode_reward_mean: 1.896103896103896
episode_reward_min: 1.0
episodes_this_iter: 231
episodes_total: 9990
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9004329004329005
  agent_1: 0.9956709956709957
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.659340381622314
time_total_s: 3044.44304394722
timers:
  learn_throughput: 522.118
  learn_time_ms: 31602.041
  load_throughput: 5488228.773
  load_time_ms: 3.006
  training_iteration_time_ms: 42461.379
  update_time_ms: 2.382
timesteps_total: 1122000
training_iteration: 68

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2222222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8355263157894737
  reward for individual goal_min: 0.0
episode_len_mean: 196.27
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 4640
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.38
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.50744819641113
time_total_s: 3065.430741071701
timers:
  learn_throughput: 472.059
  learn_time_ms: 34953.238
  load_throughput: 4895936.16
  load_time_ms: 3.37
  training_iteration_time_ms: 46345.983
  update_time_ms: 2.59
timesteps_total: 1023000
training_iteration: 62

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22560975609756098
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.782051282051282
  reward for individual goal_min: 0.0
episode_len_mean: 218.74
episode_reward_max: 2.0
episode_reward_mean: 1.01
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 4695
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.47
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.799448013305664
time_total_s: 3058.3928666114807
timers:
  learn_throughput: 477.034
  learn_time_ms: 34588.726
  load_throughput: 5284676.991
  load_time_ms: 3.122
  training_iteration_time_ms: 45971.006
  update_time_ms: 2.549
timesteps_total: 1006500
training_iteration: 61

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2777777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9166666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 197.67
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 5175
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.72399663925171
time_total_s: 3056.417457342148
timers:
  learn_throughput: 499.171
  learn_time_ms: 33054.804
  load_throughput: 5235106.66
  load_time_ms: 3.152
  training_iteration_time_ms: 44099.224
  update_time_ms: 2.535
timesteps_total: 1039500
training_iteration: 63

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3219178082191781
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9785714285714285
  reward for individual goal_min: 0.0
episode_len_mean: 169.41
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 4368
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.51726174354553
time_total_s: 3061.5312147140503
timers:
  learn_throughput: 427.154
  learn_time_ms: 38627.784
  load_throughput: 4918798.269
  load_time_ms: 3.354
  training_iteration_time_ms: 51182.879
  update_time_ms: 2.681
timesteps_total: 891000
training_iteration: 54

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6640625
  reward for individual goal_min: 0.5
episode_len_mean: 101.96273291925466
episode_reward_max: 2.0
episode_reward_mean: 1.7329192546583851
episode_reward_min: 1.0
episodes_this_iter: 161
episodes_total: 5307
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8571428571428571
  agent_1: 0.8757763975155279
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.69798469543457
time_total_s: 3074.7773036956787
timers:
  learn_throughput: 378.72
  learn_time_ms: 43567.824
  load_throughput: 4522707.376
  load_time_ms: 3.648
  training_iteration_time_ms: 57357.694
  update_time_ms: 2.895
timesteps_total: 825000
training_iteration: 50

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24358974358974358
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8648648648648649
  reward for individual goal_min: 0.0
episode_len_mean: 205.31
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 3738
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.49
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.37437438964844
time_total_s: 3048.991322994232
timers:
  learn_throughput: 399.558
  learn_time_ms: 41295.65
  load_throughput: 4529901.032
  load_time_ms: 3.642
  training_iteration_time_ms: 54495.471
  update_time_ms: 2.63
timesteps_total: 858000
training_iteration: 52

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8365384615384616
  reward for individual goal_min: 0.5
episode_len_mean: 92.19662921348315
episode_reward_max: 2.0
episode_reward_mean: 1.8089887640449438
episode_reward_min: 1.0
episodes_this_iter: 178
episodes_total: 8295
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8089887640449438
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.21058535575867
time_total_s: 3068.3369064331055
timers:
  learn_throughput: 455.215
  learn_time_ms: 36246.638
  load_throughput: 5031371.802
  load_time_ms: 3.279
  training_iteration_time_ms: 47621.49
  update_time_ms: 2.638
timesteps_total: 957000
training_iteration: 58

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3588235294117647
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9776119402985075
  reward for individual goal_min: 0.0
episode_len_mean: 177.97
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 4469
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.46388483047485
time_total_s: 3077.4277651309967
timers:
  learn_throughput: 437.629
  learn_time_ms: 37703.179
  load_throughput: 4947810.57
  load_time_ms: 3.335
  training_iteration_time_ms: 49788.258
  update_time_ms: 2.594
timesteps_total: 924000
training_iteration: 56

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30434782608695654
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9518072289156626
  reward for individual goal_min: 0.0
episode_len_mean: 166.79
episode_reward_max: 2.0
episode_reward_mean: 1.43
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 3808
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.75
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.799460649490356
time_total_s: 3070.011829137802
timers:
  learn_throughput: 387.36
  learn_time_ms: 42596.042
  load_throughput: 4134886.928
  load_time_ms: 3.99
  training_iteration_time_ms: 56087.086
  update_time_ms: 2.755
timesteps_total: 808500
training_iteration: 49

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3108108108108108
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8506493506493507
  reward for individual goal_min: 0.0
episode_len_mean: 191.44
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 3691
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.673598527908325
time_total_s: 3066.95827460289
timers:
  learn_throughput: 378.207
  learn_time_ms: 43626.863
  load_throughput: 4279240.439
  load_time_ms: 3.856
  training_iteration_time_ms: 57253.418
  update_time_ms: 2.797
timesteps_total: 792000
training_iteration: 48

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3873239436619718
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.668918918918919
  reward for individual goal_min: 0.0
episode_len_mean: 205.51
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 4288
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.354066371917725
time_total_s: 3085.9926426410675
timers:
  learn_throughput: 436.205
  learn_time_ms: 37826.235
  load_throughput: 4767567.925
  load_time_ms: 3.461
  training_iteration_time_ms: 50258.388
  update_time_ms: 2.989
timesteps_total: 924000
training_iteration: 56

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.391304347826087
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8561643835616438
  reward for individual goal_min: 0.0
episode_len_mean: 174.78
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 4349
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.06535768508911
time_total_s: 3072.6799240112305
timers:
  learn_throughput: 426.335
  learn_time_ms: 38701.928
  load_throughput: 4818286.733
  load_time_ms: 3.424
  training_iteration_time_ms: 51464.925
  update_time_ms: 2.722
timesteps_total: 907500
training_iteration: 55

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.923728813559322
  reward for individual goal_min: 0.5
episode_len_mean: 67.11693548387096
episode_reward_max: 2.0
episode_reward_mean: 1.9274193548387097
episode_reward_min: 1.0
episodes_this_iter: 248
episodes_total: 10238
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9274193548387096
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 44.76625990867615
time_total_s: 3089.209303855896
timers:
  learn_throughput: 517.996
  learn_time_ms: 31853.497
  load_throughput: 5456381.598
  load_time_ms: 3.024
  training_iteration_time_ms: 42789.055
  update_time_ms: 2.415
timesteps_total: 1138500
training_iteration: 69

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2635135135135135
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7530864197530864
  reward for individual goal_min: 0.0
episode_len_mean: 202.76
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 4722
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.39
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.484472036361694
time_total_s: 3111.9152131080627
timers:
  learn_throughput: 472.435
  learn_time_ms: 34925.463
  load_throughput: 4887465.024
  load_time_ms: 3.376
  training_iteration_time_ms: 46313.221
  update_time_ms: 2.61
timesteps_total: 1039500
training_iteration: 63

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8584905660377359
  reward for individual goal_min: 0.5
episode_len_mean: 81.15577889447236
episode_reward_max: 2.0
episode_reward_mean: 1.849246231155779
episode_reward_min: 1.0
episodes_this_iter: 199
episodes_total: 7129
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9949748743718593
  agent_1: 0.8542713567839196
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.23197627067566
time_total_s: 3096.884605884552
timers:
  learn_throughput: 420.847
  learn_time_ms: 39206.659
  load_throughput: 4338690.356
  load_time_ms: 3.803
  training_iteration_time_ms: 51736.694
  update_time_ms: 2.59
timesteps_total: 907500
training_iteration: 55

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2986111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8092105263157895
  reward for individual goal_min: 0.0
episode_len_mean: 199.66
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 4779
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.98184871673584
time_total_s: 3103.3747153282166
timers:
  learn_throughput: 479.343
  learn_time_ms: 34422.142
  load_throughput: 5291950.817
  load_time_ms: 3.118
  training_iteration_time_ms: 45790.319
  update_time_ms: 2.551
timesteps_total: 1023000
training_iteration: 62

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27631578947368424
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9696969696969697
  reward for individual goal_min: 0.0
episode_len_mean: 178.83
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 5265
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.42300081253052
time_total_s: 3102.8404581546783
timers:
  learn_throughput: 496.842
  learn_time_ms: 33209.775
  load_throughput: 5238633.532
  load_time_ms: 3.15
  training_iteration_time_ms: 44367.349
  update_time_ms: 2.524
timesteps_total: 1056000
training_iteration: 64

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8061224489795918
  reward for individual goal_min: 0.5
episode_len_mean: 86.04166666666667
episode_reward_max: 2.0
episode_reward_mean: 1.8020833333333333
episode_reward_min: 1.0
episodes_this_iter: 192
episodes_total: 8487
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8020833333333334
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.23673892021179
time_total_s: 3116.5736453533173
timers:
  learn_throughput: 456.083
  learn_time_ms: 36177.608
  load_throughput: 5019366.106
  load_time_ms: 3.287
  training_iteration_time_ms: 47566.27
  update_time_ms: 2.616
timesteps_total: 973500
training_iteration: 59

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8766233766233766
  reward for individual goal_min: 0.0
episode_len_mean: 206.06
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 3819
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.69469928741455
time_total_s: 3100.6860222816467
timers:
  learn_throughput: 402.4
  learn_time_ms: 41003.979
  load_throughput: 4510915.597
  load_time_ms: 3.658
  training_iteration_time_ms: 54124.119
  update_time_ms: 2.598
timesteps_total: 874500
training_iteration: 53

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3082191780821918
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9722222222222222
  reward for individual goal_min: 0.0
episode_len_mean: 171.39
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 4463
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.425058364868164
time_total_s: 3117.9562730789185
timers:
  learn_throughput: 421.512
  learn_time_ms: 39144.818
  load_throughput: 4944381.685
  load_time_ms: 3.337
  training_iteration_time_ms: 51746.375
  update_time_ms: 2.666
timesteps_total: 907500
training_iteration: 55

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6736111111111112
  reward for individual goal_min: 0.5
episode_len_mean: 114.47945205479452
episode_reward_max: 2.0
episode_reward_mean: 1.678082191780822
episode_reward_min: 1.0
episodes_this_iter: 146
episodes_total: 5453
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8493150684931506
  agent_1: 0.8287671232876712
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.1834762096405
time_total_s: 3131.960779905319
timers:
  learn_throughput: 378.325
  learn_time_ms: 43613.244
  load_throughput: 4346319.829
  load_time_ms: 3.796
  training_iteration_time_ms: 57406.713
  update_time_ms: 2.873
timesteps_total: 841500
training_iteration: 51

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24324324324324326
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9930555555555556
  reward for individual goal_min: 0.5
episode_len_mean: 180.75
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 4562
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.155550479888916
time_total_s: 3126.5833156108856
timers:
  learn_throughput: 436.908
  learn_time_ms: 37765.409
  load_throughput: 4900477.684
  load_time_ms: 3.367
  training_iteration_time_ms: 49917.524
  update_time_ms: 2.598
timesteps_total: 940500
training_iteration: 57

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2602739726027397
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9647887323943662
  reward for individual goal_min: 0.0
episode_len_mean: 185.25
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 3893
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.9912211894989
time_total_s: 3125.003050327301
timers:
  learn_throughput: 388.127
  learn_time_ms: 42511.857
  load_throughput: 4178627.814
  load_time_ms: 3.949
  training_iteration_time_ms: 55981.46
  update_time_ms: 2.745
timesteps_total: 825000
training_iteration: 50

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8851351351351351
  reward for individual goal_min: 0.0
episode_len_mean: 180.63
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 3783
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.80486869812012
time_total_s: 3121.76314330101
timers:
  learn_throughput: 380.585
  learn_time_ms: 43354.277
  load_throughput: 4345064.574
  load_time_ms: 3.797
  training_iteration_time_ms: 56935.716
  update_time_ms: 2.767
timesteps_total: 808500
training_iteration: 49

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9612403100775194
  reward for individual goal_min: 0.5
episode_len_mean: 62.6484375
episode_reward_max: 2.0
episode_reward_mean: 1.9609375
episode_reward_min: 1.0
episodes_this_iter: 256
episodes_total: 10494
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9609375
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 44.610387086868286
time_total_s: 3133.8196909427643
timers:
  learn_throughput: 515.544
  learn_time_ms: 32005.007
  load_throughput: 5266820.091
  load_time_ms: 3.133
  training_iteration_time_ms: 42971.614
  update_time_ms: 2.394
timesteps_total: 1155000
training_iteration: 70

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.323943661971831
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6666666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 215.9
episode_reward_max: 2.0
episode_reward_mean: 1.01
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 4365
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.64674639701843
time_total_s: 3136.639389038086
timers:
  learn_throughput: 434.909
  learn_time_ms: 37939.006
  load_throughput: 4744850.434
  load_time_ms: 3.477
  training_iteration_time_ms: 50359.057
  update_time_ms: 2.983
timesteps_total: 940500
training_iteration: 57

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8148148148148148
  reward for individual goal_min: 0.0
episode_len_mean: 194.93
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 4806
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.42476749420166
time_total_s: 3155.3399806022644
timers:
  learn_throughput: 479.173
  learn_time_ms: 34434.328
  load_throughput: 4937749.524
  load_time_ms: 3.342
  training_iteration_time_ms: 45725.788
  update_time_ms: 2.652
timesteps_total: 1056000
training_iteration: 64

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30985915492957744
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8875
  reward for individual goal_min: 0.0
episode_len_mean: 187.5
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 4437
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.82983875274658
time_total_s: 3120.509762763977
timers:
  learn_throughput: 429.864
  learn_time_ms: 38384.2
  load_throughput: 4783153.704
  load_time_ms: 3.45
  training_iteration_time_ms: 51023.712
  update_time_ms: 2.614
timesteps_total: 924000
training_iteration: 56

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17105263157894737
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9938271604938271
  reward for individual goal_min: 0.5
episode_len_mean: 189.99
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 5352
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.37522912025452
time_total_s: 3146.215687274933
timers:
  learn_throughput: 498.343
  learn_time_ms: 33109.722
  load_throughput: 5196505.128
  load_time_ms: 3.175
  training_iteration_time_ms: 44312.495
  update_time_ms: 2.509
timesteps_total: 1072500
training_iteration: 65

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8350515463917526
  reward for individual goal_min: 0.5
episode_len_mean: 81.71634615384616
episode_reward_max: 2.0
episode_reward_mean: 1.8461538461538463
episode_reward_min: 1.0
episodes_this_iter: 208
episodes_total: 7337
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8461538461538461
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 51.9048810005188
time_total_s: 3148.789486885071
timers:
  learn_throughput: 420.318
  learn_time_ms: 39255.976
  load_throughput: 4316500.196
  load_time_ms: 3.823
  training_iteration_time_ms: 51725.456
  update_time_ms: 2.595
timesteps_total: 924000
training_iteration: 56

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31645569620253167
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7025316455696202
  reward for individual goal_min: 0.0
episode_len_mean: 210.7
episode_reward_max: 2.0
episode_reward_mean: 1.03
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 4859
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.91526961326599
time_total_s: 3149.2899849414825
timers:
  learn_throughput: 482.048
  learn_time_ms: 34228.967
  load_throughput: 5268223.347
  load_time_ms: 3.132
  training_iteration_time_ms: 45592.715
  update_time_ms: 2.523
timesteps_total: 1039500
training_iteration: 63

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.85
  reward for individual goal_min: 0.5
episode_len_mean: 80.13333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.85
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.85
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23376623376623376
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.82
  reward for individual goal_min: 0.0
episode_len_mean: 209.47
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 3898
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.49
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.53683090209961
time_total_s: 3156.2228531837463
timers:
  learn_throughput: 403.098
  learn_time_ms: 40932.937
  load_throughput: 4533996.515
  load_time_ms: 3.639
  training_iteration_time_ms: 54030.466
  update_time_ms: 2.598
timesteps_total: 891000
training_iteration: 54

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8316831683168316
  reward for individual goal_min: 0.5
episode_len_mean: 81.39903846153847
episode_reward_max: 2.0
episode_reward_mean: 1.8365384615384615
episode_reward_min: 1.0
episodes_this_iter: 208
episodes_total: 8695
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8365384615384616
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 57.96575164794922
time_total_s: 3174.5393970012665
timers:
  learn_throughput: 454.296
  learn_time_ms: 36319.897
  load_throughput: 5045016.001
  load_time_ms: 3.271
  training_iteration_time_ms: 47716.184
  update_time_ms: 2.619
timesteps_total: 990000
training_iteration: 60

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-18g5xov65h/checkpoint_000060/checkpoint-60
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9805194805194806
  reward for individual goal_min: 0.0
episode_len_mean: 173.36
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 4558
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.590847969055176
time_total_s: 3172.5471210479736
timers:
  learn_throughput: 419.421
  learn_time_ms: 39339.978
  load_throughput: 4936270.302
  load_time_ms: 3.343
  training_iteration_time_ms: 51907.616
  update_time_ms: 2.684
timesteps_total: 924000
training_iteration: 56

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23684210526315788
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9807692307692307
  reward for individual goal_min: 0.0
episode_len_mean: 184.35
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 4650
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.987048625946045
time_total_s: 3175.5703642368317
timers:
  learn_throughput: 437.235
  learn_time_ms: 37737.133
  load_throughput: 4938313.27
  load_time_ms: 3.341
  training_iteration_time_ms: 49876.574
  update_time_ms: 2.597
timesteps_total: 957000
training_iteration: 58

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6714285714285714
  reward for individual goal_min: 0.5
episode_len_mean: 115.13380281690141
episode_reward_max: 2.0
episode_reward_mean: 1.676056338028169
episode_reward_min: 1.0
episodes_this_iter: 142
episodes_total: 5595
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8309859154929577
  agent_1: 0.8450704225352113
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.73978543281555
time_total_s: 3189.7005653381348
timers:
  learn_throughput: 375.97
  learn_time_ms: 43886.529
  load_throughput: 4334451.257
  load_time_ms: 3.807
  training_iteration_time_ms: 57778.204
  update_time_ms: 2.962
timesteps_total: 858000
training_iteration: 52

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9364406779661016
  reward for individual goal_min: 0.5
episode_len_mean: 67.90204081632653
episode_reward_max: 2.0
episode_reward_mean: 1.9387755102040816
episode_reward_min: 1.0
episodes_this_iter: 245
episodes_total: 10739
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9387755102040817
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 43.8150589466095
time_total_s: 3177.634749889374
timers:
  learn_throughput: 513.314
  learn_time_ms: 32144.083
  load_throughput: 5291424.814
  load_time_ms: 3.118
  training_iteration_time_ms: 43150.694
  update_time_ms: 2.391
timesteps_total: 1171500
training_iteration: 71

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2463768115942029
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8311688311688312
  reward for individual goal_min: 0.0
episode_len_mean: 195.46
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 4896
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.31309366226196
time_total_s: 3201.6530742645264
timers:
  learn_throughput: 476.949
  learn_time_ms: 34594.921
  load_throughput: 4966273.609
  load_time_ms: 3.322
  training_iteration_time_ms: 45908.726
  update_time_ms: 2.669
timesteps_total: 1072500
training_iteration: 65

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2974683544303797
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8961038961038961
  reward for individual goal_min: 0.0
episode_len_mean: 188.02
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 3867
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.74145174026489
time_total_s: 3175.504595041275
timers:
  learn_throughput: 383.063
  learn_time_ms: 43073.833
  load_throughput: 4346101.471
  load_time_ms: 3.797
  training_iteration_time_ms: 56597.577
  update_time_ms: 2.744
timesteps_total: 825000
training_iteration: 50

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2987012987012987
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6621621621621622
  reward for individual goal_min: 0.0
episode_len_mean: 220.94
episode_reward_max: 2.0
episode_reward_mean: 0.98
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 4438
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.44
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.356884241104126
time_total_s: 3187.99627327919
timers:
  learn_throughput: 433.687
  learn_time_ms: 38045.846
  load_throughput: 4753454.266
  load_time_ms: 3.471
  training_iteration_time_ms: 50477.93
  update_time_ms: 2.967
timesteps_total: 957000
training_iteration: 58

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22972972972972974
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9050632911392406
  reward for individual goal_min: 0.0
episode_len_mean: 181.44
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 3986
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.917052030563354
time_total_s: 3179.9201023578644
timers:
  learn_throughput: 390.722
  learn_time_ms: 42229.463
  load_throughput: 4239551.578
  load_time_ms: 3.892
  training_iteration_time_ms: 55684.905
  update_time_ms: 2.731
timesteps_total: 841500
training_iteration: 51

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3698630136986301
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9154929577464789
  reward for individual goal_min: 0.0
episode_len_mean: 167.8
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 4534
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.58862781524658
time_total_s: 3172.0983905792236
timers:
  learn_throughput: 431.442
  learn_time_ms: 38243.865
  load_throughput: 4796912.499
  load_time_ms: 3.44
  training_iteration_time_ms: 50823.777
  update_time_ms: 2.586
timesteps_total: 940500
training_iteration: 57

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19285714285714287
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.0
episode_len_mean: 174.33
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 5446
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.31588387489319
time_total_s: 3190.531571149826
timers:
  learn_throughput: 497.154
  learn_time_ms: 33188.917
  load_throughput: 5194359.956
  load_time_ms: 3.177
  training_iteration_time_ms: 44401.925
  update_time_ms: 2.518
timesteps_total: 1089000
training_iteration: 66

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2898550724637681
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7402597402597403
  reward for individual goal_min: 0.0
episode_len_mean: 205.38
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 4940
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.365060329437256
time_total_s: 3193.65504527092
timers:
  learn_throughput: 486.732
  learn_time_ms: 33899.537
  load_throughput: 5308512.518
  load_time_ms: 3.108
  training_iteration_time_ms: 45224.139
  update_time_ms: 2.5
timesteps_total: 1056000
training_iteration: 64

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8564356435643564
  reward for individual goal_min: 0.5
episode_len_mean: 82.1015228426396
episode_reward_max: 2.0
episode_reward_mean: 1.8527918781725887
episode_reward_min: 1.0
episodes_this_iter: 197
episodes_total: 7534
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8527918781725888
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 51.49637150764465
time_total_s: 3200.2858583927155
timers:
  learn_throughput: 421.003
  learn_time_ms: 39192.151
  load_throughput: 4350855.699
  load_time_ms: 3.792
  training_iteration_time_ms: 51656.806
  update_time_ms: 2.603
timesteps_total: 940500
training_iteration: 57

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24358974358974358
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 188.01
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 4740
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.99946594238281
time_total_s: 3223.5698301792145
timers:
  learn_throughput: 436.474
  learn_time_ms: 37802.949
  load_throughput: 4966273.609
  load_time_ms: 3.322
  training_iteration_time_ms: 49934.089
  update_time_ms: 2.604
timesteps_total: 973500
training_iteration: 59

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3356164383561644
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9797297297297297
  reward for individual goal_min: 0.0
episode_len_mean: 169.75
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 4654
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.421666622161865
time_total_s: 3222.9687876701355
timers:
  learn_throughput: 420.206
  learn_time_ms: 39266.449
  load_throughput: 4915374.552
  load_time_ms: 3.357
  training_iteration_time_ms: 51801.615
  update_time_ms: 2.684
timesteps_total: 940500
training_iteration: 57

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8605769230769231
  reward for individual goal_min: 0.5
episode_len_mean: 74.64383561643835
episode_reward_max: 2.0
episode_reward_mean: 1.8675799086757991
episode_reward_min: 1.0
episodes_this_iter: 219
episodes_total: 8914
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.867579908675799
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 50.707700967788696
time_total_s: 3225.247097969055
timers:
  learn_throughput: 451.911
  learn_time_ms: 36511.582
  load_throughput: 5033201.405
  load_time_ms: 3.278
  training_iteration_time_ms: 48038.103
  update_time_ms: 2.608
timesteps_total: 1006500
training_iteration: 61

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.926829268292683
  reward for individual goal_min: 0.5
episode_len_mean: 68.56198347107438
episode_reward_max: 2.0
episode_reward_mean: 1.9256198347107438
episode_reward_min: 1.0
episodes_this_iter: 242
episodes_total: 10981
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9297520661157025
  agent_1: 0.9958677685950413
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.753554344177246
time_total_s: 3221.388304233551
timers:
  learn_throughput: 513.007
  learn_time_ms: 32163.273
  load_throughput: 5267702.051
  load_time_ms: 3.132
  training_iteration_time_ms: 43178.37
  update_time_ms: 2.392
timesteps_total: 1188000
training_iteration: 72

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26282051282051283
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.84
  reward for individual goal_min: 0.0
episode_len_mean: 207.78
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 3977
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.0303680896759
time_total_s: 3209.2532212734222
timers:
  learn_throughput: 403.407
  learn_time_ms: 40901.57
  load_throughput: 4584181.709
  load_time_ms: 3.599
  training_iteration_time_ms: 53984.963
  update_time_ms: 2.595
timesteps_total: 907500
training_iteration: 55

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22784810126582278
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8209876543209876
  reward for individual goal_min: 0.0
episode_len_mean: 204.3
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 4973
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.25923132896423
time_total_s: 3245.9123055934906
timers:
  learn_throughput: 478.788
  learn_time_ms: 34462.018
  load_throughput: 4893928.097
  load_time_ms: 3.372
  training_iteration_time_ms: 45723.324
  update_time_ms: 2.674
timesteps_total: 1089000
training_iteration: 66

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6690140845070423
  reward for individual goal_min: 0.5
episode_len_mean: 110.18
episode_reward_max: 2.0
episode_reward_mean: 1.6866666666666668
episode_reward_min: 1.0
episodes_this_iter: 150
episodes_total: 5745
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.84
  agent_1: 0.8466666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.05216860771179
time_total_s: 3246.7527339458466
timers:
  learn_throughput: 376.879
  learn_time_ms: 43780.588
  load_throughput: 4390436.785
  load_time_ms: 3.758
  training_iteration_time_ms: 57690.456
  update_time_ms: 2.993
timesteps_total: 874500
training_iteration: 53

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2565789473684211
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9551282051282052
  reward for individual goal_min: 0.0
episode_len_mean: 179.18
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 5540
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.90550756454468
time_total_s: 3235.4370787143707
timers:
  learn_throughput: 495.497
  learn_time_ms: 33299.903
  load_throughput: 5164628.06
  load_time_ms: 3.195
  training_iteration_time_ms: 44544.693
  update_time_ms: 2.52
timesteps_total: 1105500
training_iteration: 67

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33098591549295775
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6265060240963856
  reward for individual goal_min: 0.0
episode_len_mean: 205.84
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 4519
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.43
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.652690172195435
time_total_s: 3238.6489634513855
timers:
  learn_throughput: 434.327
  learn_time_ms: 37989.797
  load_throughput: 4741696.996
  load_time_ms: 3.48
  training_iteration_time_ms: 50380.723
  update_time_ms: 2.904
timesteps_total: 973500
training_iteration: 59

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36153846153846153
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.75
  reward for individual goal_min: 0.0
episode_len_mean: 192.26
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 5024
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.87893271446228
time_total_s: 3239.533977985382
timers:
  learn_throughput: 488.844
  learn_time_ms: 33753.065
  load_throughput: 5352028.954
  load_time_ms: 3.083
  training_iteration_time_ms: 45093.555
  update_time_ms: 2.461
timesteps_total: 1072500
training_iteration: 65

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27848101265822783
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9466666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 179.62
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 4076
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.30831742286682
time_total_s: 3231.228419780731
timers:
  learn_throughput: 393.657
  learn_time_ms: 41914.654
  load_throughput: 4300112.837
  load_time_ms: 3.837
  training_iteration_time_ms: 55271.103
  update_time_ms: 2.706
timesteps_total: 858000
training_iteration: 52

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2777777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9383561643835616
  reward for individual goal_min: 0.0
episode_len_mean: 186.91
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 4624
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.51595878601074
time_total_s: 3223.6143493652344
timers:
  learn_throughput: 429.846
  learn_time_ms: 38385.854
  load_throughput: 4822584.458
  load_time_ms: 3.421
  training_iteration_time_ms: 50980.021
  update_time_ms: 2.578
timesteps_total: 957000
training_iteration: 58

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2826086956521739
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.92
  reward for individual goal_min: 0.0
episode_len_mean: 189.12
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 3956
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.93254470825195
time_total_s: 3230.437139749527
timers:
  learn_throughput: 384.107
  learn_time_ms: 42956.831
  load_throughput: 4373207.962
  load_time_ms: 3.773
  training_iteration_time_ms: 56455.499
  update_time_ms: 2.726
timesteps_total: 841500
training_iteration: 51

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8350515463917526
  reward for individual goal_min: 0.5
episode_len_mean: 83.15656565656566
episode_reward_max: 2.0
episode_reward_mean: 1.8383838383838385
episode_reward_min: 1.0
episodes_this_iter: 198
episodes_total: 7732
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8383838383838383
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 50.48572635650635
time_total_s: 3250.771584749222
timers:
  learn_throughput: 421.614
  learn_time_ms: 39135.351
  load_throughput: 4365401.273
  load_time_ms: 3.78
  training_iteration_time_ms: 51634.625
  update_time_ms: 2.607
timesteps_total: 957000
training_iteration: 58

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9435483870967742
  reward for individual goal_min: 0.5
episode_len_mean: 63.85658914728682
episode_reward_max: 2.0
episode_reward_mean: 1.945736434108527
episode_reward_min: 1.0
episodes_this_iter: 258
episodes_total: 11239
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9496124031007752
  agent_1: 0.9961240310077519
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.64431953430176
time_total_s: 3266.032623767853
timers:
  learn_throughput: 510.283
  learn_time_ms: 32335.025
  load_throughput: 5254344.023
  load_time_ms: 3.14
  training_iteration_time_ms: 43406.364
  update_time_ms: 2.397
timesteps_total: 1204500
training_iteration: 73

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23972602739726026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 173.1
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 4748
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.9868745803833
time_total_s: 3272.955662250519
timers:
  learn_throughput: 421.859
  learn_time_ms: 39112.625
  load_throughput: 4918448.691
  load_time_ms: 3.355
  training_iteration_time_ms: 51662.27
  update_time_ms: 2.666
timesteps_total: 957000
training_iteration: 58

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1736111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8255813953488372
  reward for individual goal_min: 0.0
episode_len_mean: 206.39
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 5052
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.827399253845215
time_total_s: 3289.739704847336
timers:
  learn_throughput: 482.751
  learn_time_ms: 34179.095
  load_throughput: 4900998.244
  load_time_ms: 3.367
  training_iteration_time_ms: 45383.054
  update_time_ms: 2.64
timesteps_total: 1105500
training_iteration: 67

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8241758241758241
  reward for individual goal_min: 0.5
episode_len_mean: 79.25242718446601
episode_reward_max: 2.0
episode_reward_mean: 1.8446601941747574
episode_reward_min: 1.0
episodes_this_iter: 206
episodes_total: 9120
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8446601941747572
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 50.00878405570984
time_total_s: 3275.255882024765
timers:
  learn_throughput: 451.601
  learn_time_ms: 36536.703
  load_throughput: 5019074.888
  load_time_ms: 3.287
  training_iteration_time_ms: 48090.45
  update_time_ms: 2.592
timesteps_total: 1023000
training_iteration: 62

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.273972602739726
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8698630136986302
  reward for individual goal_min: 0.0
episode_len_mean: 200.7
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 4061
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.56261324882507
time_total_s: 3264.8158345222473
timers:
  learn_throughput: 403.16
  learn_time_ms: 40926.68
  load_throughput: 4602994.08
  load_time_ms: 3.585
  training_iteration_time_ms: 54100.635
  update_time_ms: 2.603
timesteps_total: 924000
training_iteration: 56

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2676056338028169
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.958904109589041
  reward for individual goal_min: 0.0
episode_len_mean: 173.99
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 5631
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.20276498794556
time_total_s: 3279.6398437023163
timers:
  learn_throughput: 496.366
  learn_time_ms: 33241.588
  load_throughput: 5182340.837
  load_time_ms: 3.184
  training_iteration_time_ms: 44488.809
  update_time_ms: 2.543
timesteps_total: 1122000
training_iteration: 68

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 121.93333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.6
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8166666666666667
  agent_1: 0.7833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2468354430379747
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9930555555555556
  reward for individual goal_min: 0.5
episode_len_mean: 189.24
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 4827
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.94351673126221
time_total_s: 3284.5133469104767
timers:
  learn_throughput: 439.278
  learn_time_ms: 37561.612
  load_throughput: 4983869.797
  load_time_ms: 3.311
  training_iteration_time_ms: 49585.976
  update_time_ms: 2.569
timesteps_total: 990000
training_iteration: 60

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-18pddilqzj/checkpoint_000060/checkpoint-60
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2916666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7635135135135135
  reward for individual goal_min: 0.0
episode_len_mean: 194.38
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 5110
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.261749029159546
time_total_s: 3285.7957270145416
timers:
  learn_throughput: 488.282
  learn_time_ms: 33791.953
  load_throughput: 5372428.794
  load_time_ms: 3.071
  training_iteration_time_ms: 45122.906
  update_time_ms: 2.453
timesteps_total: 1089000
training_iteration: 66

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2972972972972973
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9375
  reward for individual goal_min: 0.0
episode_len_mean: 180.85
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 4166
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.4803569316864
time_total_s: 3285.7087767124176
timers:
  learn_throughput: 394.784
  learn_time_ms: 41795.055
  load_throughput: 4333392.777
  load_time_ms: 3.808
  training_iteration_time_ms: 55110.714
  update_time_ms: 2.678
timesteps_total: 874500
training_iteration: 53

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.678082191780822
  reward for individual goal_min: 0.5
episode_len_mean: 110.13245033112582
episode_reward_max: 2.0
episode_reward_mean: 1.6887417218543046
episode_reward_min: 1.0
episodes_this_iter: 151
episodes_total: 5896
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8543046357615894
  agent_1: 0.8344370860927153
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.55703687667847
time_total_s: 3304.309770822525
timers:
  learn_throughput: 376.915
  learn_time_ms: 43776.42
  load_throughput: 4345337.394
  load_time_ms: 3.797
  training_iteration_time_ms: 57711.365
  update_time_ms: 3.032
timesteps_total: 891000
training_iteration: 54

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9294871794871795
  reward for individual goal_min: 0.0
episode_len_mean: 187.68
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 4710
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.18351197242737
time_total_s: 3275.7978613376617
timers:
  learn_throughput: 429.983
  learn_time_ms: 38373.576
  load_throughput: 4839615.382
  load_time_ms: 3.409
  training_iteration_time_ms: 51028.462
  update_time_ms: 2.572
timesteps_total: 973500
training_iteration: 59

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3055555555555556
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.94
  reward for individual goal_min: 0.0
episode_len_mean: 175.28
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 4051
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.985498905181885
time_total_s: 3283.422638654709
timers:
  learn_throughput: 389.033
  learn_time_ms: 42412.905
  load_throughput: 4349215.135
  load_time_ms: 3.794
  training_iteration_time_ms: 55776.684
  update_time_ms: 2.721
timesteps_total: 858000
training_iteration: 52

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 138.01666666666668
episode_reward_max: 2.0
episode_reward_mean: 1.5833333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8
  agent_1: 0.7833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27205882352941174
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7195121951219512
  reward for individual goal_min: 0.0
episode_len_mean: 206.95
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 4601
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.64690566062927
time_total_s: 3301.2958691120148
timers:
  learn_throughput: 434.557
  learn_time_ms: 37969.66
  load_throughput: 4732909.05
  load_time_ms: 3.486
  training_iteration_time_ms: 50393.222
  update_time_ms: 2.91
timesteps_total: 990000
training_iteration: 60

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19mcvnks1j/checkpoint_000060/checkpoint-60
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8571428571428571
  reward for individual goal_min: 0.5
episode_len_mean: 79.0093023255814
episode_reward_max: 2.0
episode_reward_mean: 1.8604651162790697
episode_reward_min: 1.0
episodes_this_iter: 215
episodes_total: 7947
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8604651162790697
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 51.92962431907654
time_total_s: 3302.7012090682983
timers:
  learn_throughput: 419.148
  learn_time_ms: 39365.571
  load_throughput: 4383984.391
  load_time_ms: 3.764
  training_iteration_time_ms: 51866.209
  update_time_ms: 2.607
timesteps_total: 973500
training_iteration: 59

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9465648854961832
  reward for individual goal_min: 0.5
episode_len_mean: 65.0859375
episode_reward_max: 2.0
episode_reward_mean: 1.9453125
episode_reward_min: 1.0
episodes_this_iter: 256
episodes_total: 11495
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9453125
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 44.238605260849
time_total_s: 3310.271229028702
timers:
  learn_throughput: 507.396
  learn_time_ms: 32518.972
  load_throughput: 5162663.166
  load_time_ms: 3.196
  training_iteration_time_ms: 43645.979
  update_time_ms: 2.391
timesteps_total: 1221000
training_iteration: 74

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19736842105263158
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 216.94
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 5130
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.36
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.39298939704895
time_total_s: 3332.1326942443848
timers:
  learn_throughput: 487.165
  learn_time_ms: 33869.433
  load_throughput: 4942721.975
  load_time_ms: 3.338
  training_iteration_time_ms: 44963.446
  update_time_ms: 2.625
timesteps_total: 1122000
training_iteration: 68

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19718309859154928
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.972972972972973
  reward for individual goal_min: 0.0
episode_len_mean: 181.82
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 5721
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.42755389213562
time_total_s: 3321.067397594452
timers:
  learn_throughput: 500.353
  learn_time_ms: 32976.707
  load_throughput: 5134320.244
  load_time_ms: 3.214
  training_iteration_time_ms: 44235.753
  update_time_ms: 2.522
timesteps_total: 1138500
training_iteration: 69

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8632075471698113
  reward for individual goal_min: 0.5
episode_len_mean: 72.58874458874459
episode_reward_max: 2.0
episode_reward_mean: 1.8744588744588744
episode_reward_min: 1.0
episodes_this_iter: 231
episodes_total: 9351
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8744588744588745
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 50.44841003417969
time_total_s: 3325.7042920589447
timers:
  learn_throughput: 448.444
  learn_time_ms: 36793.919
  load_throughput: 4989690.983
  load_time_ms: 3.307
  training_iteration_time_ms: 48380.823
  update_time_ms: 2.553
timesteps_total: 1039500
training_iteration: 63

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 167.08
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 4846
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.635905027389526
time_total_s: 3328.5915672779083
timers:
  learn_throughput: 417.921
  learn_time_ms: 39481.126
  load_throughput: 4901588.345
  load_time_ms: 3.366
  training_iteration_time_ms: 52189.993
  update_time_ms: 2.663
timesteps_total: 973500
training_iteration: 59

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34810126582278483
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7152777777777778
  reward for individual goal_min: 0.0
episode_len_mean: 197.03
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 5189
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.30474591255188
time_total_s: 3331.1004729270935
timers:
  learn_throughput: 488.344
  learn_time_ms: 33787.648
  load_throughput: 5340959.437
  load_time_ms: 3.089
  training_iteration_time_ms: 45092.154
  update_time_ms: 2.464
timesteps_total: 1105500
training_iteration: 67

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23717948717948717
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 190.44
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 4914
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.194175720214844
time_total_s: 3332.7075226306915
timers:
  learn_throughput: 441.147
  learn_time_ms: 37402.496
  load_throughput: 4991886.437
  load_time_ms: 3.305
  training_iteration_time_ms: 49393.491
  update_time_ms: 2.581
timesteps_total: 1006500
training_iteration: 61

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28378378378378377
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8881578947368421
  reward for individual goal_min: 0.0
episode_len_mean: 189.19
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 4146
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.150113344192505
time_total_s: 3318.96594786644
timers:
  learn_throughput: 404.159
  learn_time_ms: 40825.517
  load_throughput: 4626936.592
  load_time_ms: 3.566
  training_iteration_time_ms: 54047.18
  update_time_ms: 2.615
timesteps_total: 940500
training_iteration: 57

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29605263157894735
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9722222222222222
  reward for individual goal_min: 0.0
episode_len_mean: 179.4
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 4259
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.9260139465332
time_total_s: 3339.634790658951
timers:
  learn_throughput: 398.059
  learn_time_ms: 41451.11
  load_throughput: 4305623.95
  load_time_ms: 3.832
  training_iteration_time_ms: 54699.139
  update_time_ms: 2.676
timesteps_total: 891000
training_iteration: 54

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3674698795180723
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9318181818181818
  reward for individual goal_min: 0.0
episode_len_mean: 187.43
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 4137
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.97009015083313
time_total_s: 3337.392728805542
timers:
  learn_throughput: 392.037
  learn_time_ms: 42087.91
  load_throughput: 4365483.883
  load_time_ms: 3.78
  training_iteration_time_ms: 55359.064
  update_time_ms: 2.716
timesteps_total: 874500
training_iteration: 53

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6307692307692307
  reward for individual goal_min: 0.5
episode_len_mean: 114.64084507042253
episode_reward_max: 2.0
episode_reward_mean: 1.6619718309859155
episode_reward_min: 1.0
episodes_this_iter: 142
episodes_total: 6038
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8098591549295775
  agent_1: 0.852112676056338
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.03203201293945
time_total_s: 3361.3418028354645
timers:
  learn_throughput: 378.337
  learn_time_ms: 43611.887
  load_throughput: 4208920.433
  load_time_ms: 3.92
  training_iteration_time_ms: 57539.839
  update_time_ms: 2.986
timesteps_total: 907500
training_iteration: 55

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34415584415584416
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7142857142857143
  reward for individual goal_min: 0.0
episode_len_mean: 211.67
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 4678
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.46078944206238
time_total_s: 3351.756658554077
timers:
  learn_throughput: 434.115
  learn_time_ms: 38008.343
  load_throughput: 4759371.157
  load_time_ms: 3.467
  training_iteration_time_ms: 50452.888
  update_time_ms: 2.928
timesteps_total: 1006500
training_iteration: 61

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8
  reward for individual goal_min: 0.5
episode_len_mean: 120.53333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.7333333333333334
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9333333333333333
  agent_1: 0.8
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2971014492753623
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8896103896103896
  reward for individual goal_min: 0.0
episode_len_mean: 178.69
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 4804
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.624265909194946
time_total_s: 3339.4221272468567
timers:
  learn_throughput: 431.745
  learn_time_ms: 38217.015
  load_throughput: 4848566.644
  load_time_ms: 3.403
  training_iteration_time_ms: 50761.791
  update_time_ms: 2.558
timesteps_total: 990000
training_iteration: 60

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-28met_xp4k/checkpoint_000060/checkpoint-60
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 82.15
episode_reward_max: 2.0
episode_reward_mean: 1.8333333333333333
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8333333333333334
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.825
  reward for individual goal_min: 0.5
episode_len_mean: 85.34196891191709
episode_reward_max: 2.0
episode_reward_mean: 1.8186528497409327
episode_reward_min: 1.0
episodes_this_iter: 193
episodes_total: 8140
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8186528497409327
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 56.10341715812683
time_total_s: 3358.804626226425
timers:
  learn_throughput: 421.634
  learn_time_ms: 39133.424
  load_throughput: 4343019.517
  load_time_ms: 3.799
  training_iteration_time_ms: 51618.927
  update_time_ms: 2.607
timesteps_total: 990000
training_iteration: 60

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-184s1jdk42/checkpoint_000060/checkpoint-60
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.952
  reward for individual goal_min: 0.5
episode_len_mean: 60.733333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.9555555555555555
episode_reward_min: 1.0
episodes_this_iter: 270
episodes_total: 11765
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9592592592592593
  agent_1: 0.9962962962962963
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.295167684555054
time_total_s: 3355.566396713257
timers:
  learn_throughput: 504.471
  learn_time_ms: 32707.511
  load_throughput: 5133177.769
  load_time_ms: 3.214
  training_iteration_time_ms: 43883.589
  update_time_ms: 2.381
timesteps_total: 1237500
training_iteration: 75

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23076923076923078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8141025641025641
  reward for individual goal_min: 0.0
episode_len_mean: 207.69
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 5209
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.43
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.39325284957886
time_total_s: 3376.5259470939636
timers:
  learn_throughput: 489.22
  learn_time_ms: 33727.129
  load_throughput: 4945689.049
  load_time_ms: 3.336
  training_iteration_time_ms: 44783.557
  update_time_ms: 2.663
timesteps_total: 1138500
training_iteration: 69

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.98
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 5819
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.856908082962036
time_total_s: 3363.924305677414
timers:
  learn_throughput: 501.83
  learn_time_ms: 32879.634
  load_throughput: 5160738.25
  load_time_ms: 3.197
  training_iteration_time_ms: 44042.49
  update_time_ms: 2.513
timesteps_total: 1155000
training_iteration: 70

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7951807228915663
  reward for individual goal_min: 0.5
episode_len_mean: 81.55102040816327
episode_reward_max: 2.0
episode_reward_mean: 1.8265306122448979
episode_reward_min: 1.0
episodes_this_iter: 196
episodes_total: 9547
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.826530612244898
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.77121376991272
time_total_s: 3374.4755058288574
timers:
  learn_throughput: 446.649
  learn_time_ms: 36941.744
  load_throughput: 4913873.812
  load_time_ms: 3.358
  training_iteration_time_ms: 48591.355
  update_time_ms: 2.577
timesteps_total: 1056000
training_iteration: 64

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7205882352941176
  reward for individual goal_min: 0.0
episode_len_mean: 210.62
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 5266
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.47
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.0416522026062
time_total_s: 3378.1421251296997
timers:
  learn_throughput: 484.759
  learn_time_ms: 34037.547
  load_throughput: 5357622.413
  load_time_ms: 3.08
  training_iteration_time_ms: 45438.56
  update_time_ms: 2.463
timesteps_total: 1122000
training_iteration: 68

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24675324675324675
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9642857142857143
  reward for individual goal_min: 0.0
episode_len_mean: 193.43
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 5000
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.06376838684082
time_total_s: 3382.7712910175323
timers:
  learn_throughput: 441.64
  learn_time_ms: 37360.777
  load_throughput: 4977775.732
  load_time_ms: 3.315
  training_iteration_time_ms: 49394.598
  update_time_ms: 2.581
timesteps_total: 1023000
training_iteration: 62

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1794871794871795
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9444444444444444
  reward for individual goal_min: 0.0
episode_len_mean: 209.88
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 4224
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.967602014541626
time_total_s: 3369.9335498809814
timers:
  learn_throughput: 407.216
  learn_time_ms: 40519.008
  load_throughput: 4593553.389
  load_time_ms: 3.592
  training_iteration_time_ms: 53602.789
  update_time_ms: 2.65
timesteps_total: 957000
training_iteration: 58

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9333333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 107.88333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.7333333333333334
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8833333333333333
  agent_1: 0.85
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9791666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 174.54
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 4940
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 64.69067406654358
time_total_s: 3393.282241344452
timers:
  learn_throughput: 416.886
  learn_time_ms: 39579.202
  load_throughput: 4884739.762
  load_time_ms: 3.378
  training_iteration_time_ms: 52281.374
  update_time_ms: 2.664
timesteps_total: 990000
training_iteration: 60

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3835616438356164
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7236842105263158
  reward for individual goal_min: 0.0
episode_len_mean: 199.16
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 4762
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.280595779418945
time_total_s: 3400.037254333496
timers:
  learn_throughput: 435.857
  learn_time_ms: 37856.45
  load_throughput: 4717133.977
  load_time_ms: 3.498
  training_iteration_time_ms: 50238.955
  update_time_ms: 2.927
timesteps_total: 1023000
training_iteration: 62

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3787878787878788
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9407894736842105
  reward for individual goal_min: 0.0
episode_len_mean: 158.04807692307693
episode_reward_max: 2.0
episode_reward_mean: 1.4615384615384615
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 4363
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.7115384615384616
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.70033288002014
time_total_s: 3393.335123538971
timers:
  learn_throughput: 401.041
  learn_time_ms: 41142.888
  load_throughput: 4325943.78
  load_time_ms: 3.814
  training_iteration_time_ms: 54323.085
  update_time_ms: 2.64
timesteps_total: 907500
training_iteration: 55

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1956521739130435
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8395061728395061
  reward for individual goal_min: 0.0
episode_len_mean: 200.49
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 5293
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.86240267753601
time_total_s: 3418.3883497714996
timers:
  learn_throughput: 492.245
  learn_time_ms: 33519.919
  load_throughput: 4906105.586
  load_time_ms: 3.363
  training_iteration_time_ms: 44554.255
  update_time_ms: 2.616
timesteps_total: 1155000
training_iteration: 70

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27702702702702703
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9657534246575342
  reward for individual goal_min: 0.0
episode_len_mean: 188.37
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 4227
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.06432580947876
time_total_s: 3390.4570546150208
timers:
  learn_throughput: 394.773
  learn_time_ms: 41796.221
  load_throughput: 4395009.431
  load_time_ms: 3.754
  training_iteration_time_ms: 54984.538
  update_time_ms: 2.698
timesteps_total: 891000
training_iteration: 54

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9462809917355371
  reward for individual goal_min: 0.5
episode_len_mean: 65.304
episode_reward_max: 2.0
episode_reward_mean: 1.948
episode_reward_min: 1.0
episodes_this_iter: 250
episodes_total: 12015
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.956
  agent_1: 0.992
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.04257869720459
time_total_s: 3399.6089754104614
timers:
  learn_throughput: 503.671
  learn_time_ms: 32759.482
  load_throughput: 5091672.749
  load_time_ms: 3.241
  training_iteration_time_ms: 43962.966
  update_time_ms: 2.391
timesteps_total: 1254000
training_iteration: 76

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9875
  reward for individual goal_min: 0.0
episode_len_mean: 176.15
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 5914
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.3772976398468
time_total_s: 3406.3016033172607
timers:
  learn_throughput: 506.603
  learn_time_ms: 32569.852
  load_throughput: 5213455.573
  load_time_ms: 3.165
  training_iteration_time_ms: 43671.973
  update_time_ms: 2.49
timesteps_total: 1171500
training_iteration: 71

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33098591549295775
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.94375
  reward for individual goal_min: 0.0
episode_len_mean: 169.07
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 4901
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.510287284851074
time_total_s: 3389.9324145317078
timers:
  learn_throughput: 434.036
  learn_time_ms: 38015.267
  load_throughput: 4875414.128
  load_time_ms: 3.384
  training_iteration_time_ms: 50530.502
  update_time_ms: 2.563
timesteps_total: 1006500
training_iteration: 61

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8349056603773585
  reward for individual goal_min: 0.5
episode_len_mean: 84.11855670103093
episode_reward_max: 2.0
episode_reward_mean: 1.8195876288659794
episode_reward_min: 1.0
episodes_this_iter: 194
episodes_total: 8334
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9948453608247423
  agent_1: 0.8247422680412371
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.803176164627075
time_total_s: 3408.6078023910522
timers:
  learn_throughput: 422.305
  learn_time_ms: 39071.249
  load_throughput: 4345010.014
  load_time_ms: 3.797
  training_iteration_time_ms: 51478.287
  update_time_ms: 2.641
timesteps_total: 1006500
training_iteration: 61

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7
  reward for individual goal_min: 0.5
episode_len_mean: 107.15483870967742
episode_reward_max: 2.0
episode_reward_mean: 1.7096774193548387
episode_reward_min: 1.0
episodes_this_iter: 155
episodes_total: 6193
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8516129032258064
  agent_1: 0.8580645161290322
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.22949242591858
time_total_s: 3419.571295261383
timers:
  learn_throughput: 377.974
  learn_time_ms: 43653.762
  load_throughput: 4185349.888
  load_time_ms: 3.942
  training_iteration_time_ms: 57593.115
  update_time_ms: 2.858
timesteps_total: 924000
training_iteration: 56

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8348214285714286
  reward for individual goal_min: 0.5
episode_len_mean: 85.95897435897436
episode_reward_max: 2.0
episode_reward_mean: 1.8102564102564103
episode_reward_min: 1.0
episodes_this_iter: 195
episodes_total: 9742
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8102564102564103
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 49.17777895927429
time_total_s: 3423.6532847881317
timers:
  learn_throughput: 444.509
  learn_time_ms: 37119.615
  load_throughput: 4874795.974
  load_time_ms: 3.385
  training_iteration_time_ms: 48807.843
  update_time_ms: 2.594
timesteps_total: 1072500
training_iteration: 65

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3141025641025641
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7071428571428572
  reward for individual goal_min: 0.0
episode_len_mean: 207.15
episode_reward_max: 2.0
episode_reward_mean: 1.01
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 5344
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.47
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.224005460739136
time_total_s: 3425.366130590439
timers:
  learn_throughput: 482.337
  learn_time_ms: 34208.432
  load_throughput: 5283870.022
  load_time_ms: 3.123
  training_iteration_time_ms: 45693.923
  update_time_ms: 2.486
timesteps_total: 1138500
training_iteration: 69

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26582278481012656
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 182.71
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 5091
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.971291303634644
time_total_s: 3431.742582321167
timers:
  learn_throughput: 441.946
  learn_time_ms: 37334.837
  load_throughput: 4999531.588
  load_time_ms: 3.3
  training_iteration_time_ms: 49378.579
  update_time_ms: 2.588
timesteps_total: 1039500
training_iteration: 63

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2721518987341772
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.875
  reward for individual goal_min: 0.0
episode_len_mean: 206.61
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 4305
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.55176305770874
time_total_s: 3421.48531293869
timers:
  learn_throughput: 406.929
  learn_time_ms: 40547.581
  load_throughput: 4641365.999
  load_time_ms: 3.555
  training_iteration_time_ms: 53670.359
  update_time_ms: 2.675
timesteps_total: 973500
training_iteration: 59

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28846153846153844
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9615384615384616
  reward for individual goal_min: 0.0
episode_len_mean: 172.83
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 5035
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.75436234474182
time_total_s: 3446.0366036891937
timers:
  learn_throughput: 415.362
  learn_time_ms: 39724.371
  load_throughput: 4931486.514
  load_time_ms: 3.346
  training_iteration_time_ms: 52379.255
  update_time_ms: 2.642
timesteps_total: 1006500
training_iteration: 61

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25316455696202533
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7986111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 202.77
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 5376
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.920543909072876
time_total_s: 3463.3088936805725
timers:
  learn_throughput: 495.015
  learn_time_ms: 33332.322
  load_throughput: 4918378.782
  load_time_ms: 3.355
  training_iteration_time_ms: 44302.015
  update_time_ms: 2.616
timesteps_total: 1171500
training_iteration: 71

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9484126984126984
  reward for individual goal_min: 0.5
episode_len_mean: 66.3293172690763
episode_reward_max: 2.0
episode_reward_mean: 1.9477911646586346
episode_reward_min: 1.0
episodes_this_iter: 249
episodes_total: 12264
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9477911646586346
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 44.398112297058105
time_total_s: 3444.0070877075195
timers:
  learn_throughput: 500.677
  learn_time_ms: 32955.398
  load_throughput: 5067476.221
  load_time_ms: 3.256
  training_iteration_time_ms: 44245.189
  update_time_ms: 2.394
timesteps_total: 1270500
training_iteration: 77

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.38414634146341464
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7222222222222222
  reward for individual goal_min: 0.0
episode_len_mean: 198.87
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 4844
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.263858795166016
time_total_s: 3450.301113128662
timers:
  learn_throughput: 436.334
  learn_time_ms: 37815.03
  load_throughput: 4737509.738
  load_time_ms: 3.483
  training_iteration_time_ms: 50238.992
  update_time_ms: 2.947
timesteps_total: 1039500
training_iteration: 63

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22972972972972974
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9746835443037974
  reward for individual goal_min: 0.0
episode_len_mean: 172.72
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 6006
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.02017855644226
time_total_s: 3450.321781873703
timers:
  learn_throughput: 507.65
  learn_time_ms: 32502.687
  load_throughput: 5197129.511
  load_time_ms: 3.175
  training_iteration_time_ms: 43614.204
  update_time_ms: 2.483
timesteps_total: 1188000
training_iteration: 72

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2733333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.958904109589041
  reward for individual goal_min: 0.0
episode_len_mean: 184.39
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 4317
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.613752365112305
time_total_s: 3440.070806980133
timers:
  learn_throughput: 401.573
  learn_time_ms: 41088.38
  load_throughput: 4412973.524
  load_time_ms: 3.739
  training_iteration_time_ms: 54100.944
  update_time_ms: 2.685
timesteps_total: 907500
training_iteration: 55

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23943661971830985
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9556962025316456
  reward for individual goal_min: 0.0
episode_len_mean: 177.87
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 4454
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.11517405509949
time_total_s: 3446.4502975940704
timers:
  learn_throughput: 404.133
  learn_time_ms: 40828.17
  load_throughput: 4360835.04
  load_time_ms: 3.784
  training_iteration_time_ms: 53986.27
  update_time_ms: 2.661
timesteps_total: 924000
training_iteration: 56

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.273972602739726
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9276315789473685
  reward for individual goal_min: 0.0
episode_len_mean: 179.42
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 4995
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.745317697525024
time_total_s: 3441.677732229233
timers:
  learn_throughput: 431.952
  learn_time_ms: 38198.723
  load_throughput: 4923662.545
  load_time_ms: 3.351
  training_iteration_time_ms: 50743.079
  update_time_ms: 2.573
timesteps_total: 1023000
training_iteration: 62

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8557692307692307
  reward for individual goal_min: 0.5
episode_len_mean: 77.62735849056604
episode_reward_max: 2.0
episode_reward_mean: 1.8584905660377358
episode_reward_min: 1.0
episodes_this_iter: 212
episodes_total: 8546
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8584905660377359
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 51.84197497367859
time_total_s: 3460.449777364731
timers:
  learn_throughput: 421.904
  learn_time_ms: 39108.44
  load_throughput: 4373511.966
  load_time_ms: 3.773
  training_iteration_time_ms: 51503.739
  update_time_ms: 2.646
timesteps_total: 1023000
training_iteration: 62

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.678082191780822
  reward for individual goal_min: 0.5
episode_len_mean: 118.65217391304348
episode_reward_max: 2.0
episode_reward_mean: 1.6594202898550725
episode_reward_min: 1.0
episodes_this_iter: 138
episodes_total: 6331
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8333333333333334
  agent_1: 0.8260869565217391
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.557507038116455
time_total_s: 3476.1288022994995
timers:
  learn_throughput: 377.79
  learn_time_ms: 43675.012
  load_throughput: 4178400.753
  load_time_ms: 3.949
  training_iteration_time_ms: 57645.896
  update_time_ms: 2.865
timesteps_total: 940500
training_iteration: 57

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8221153846153846
  reward for individual goal_min: 0.5
episode_len_mean: 86.56020942408377
episode_reward_max: 2.0
episode_reward_mean: 1.806282722513089
episode_reward_min: 1.0
episodes_this_iter: 191
episodes_total: 9933
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.806282722513089
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.222355365753174
time_total_s: 3471.875640153885
timers:
  learn_throughput: 443.895
  learn_time_ms: 37170.948
  load_throughput: 4843815.643
  load_time_ms: 3.406
  training_iteration_time_ms: 48933.641
  update_time_ms: 2.606
timesteps_total: 1089000
training_iteration: 66

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3358208955223881
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6917808219178082
  reward for individual goal_min: 0.0
episode_len_mean: 197.83
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 5430
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.54172420501709
time_total_s: 3471.907854795456
timers:
  learn_throughput: 481.253
  learn_time_ms: 34285.496
  load_throughput: 5209021.361
  load_time_ms: 3.168
  training_iteration_time_ms: 45795.432
  update_time_ms: 2.461
timesteps_total: 1155000
training_iteration: 70

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2465753424657534
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9626865671641791
  reward for individual goal_min: 0.0
episode_len_mean: 186.36
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 5179
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.90224862098694
time_total_s: 3482.644830942154
timers:
  learn_throughput: 442.53
  learn_time_ms: 37285.594
  load_throughput: 5026109.969
  load_time_ms: 3.283
  training_iteration_time_ms: 49322.45
  update_time_ms: 2.602
timesteps_total: 1056000
training_iteration: 64

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2532467532467532
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8133333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 203.34
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 5456
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.818315744400024
time_total_s: 3508.1272094249725
timers:
  learn_throughput: 495.806
  learn_time_ms: 33279.173
  load_throughput: 4923662.545
  load_time_ms: 3.351
  training_iteration_time_ms: 44232.997
  update_time_ms: 2.605
timesteps_total: 1188000
training_iteration: 72

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.94921875
  reward for individual goal_min: 0.5
episode_len_mean: 63.7816091954023
episode_reward_max: 2.0
episode_reward_mean: 1.950191570881226
episode_reward_min: 1.0
episodes_this_iter: 261
episodes_total: 12525
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9540229885057471
  agent_1: 0.9961685823754789
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.84274363517761
time_total_s: 3488.849831342697
timers:
  learn_throughput: 499.712
  learn_time_ms: 33019.026
  load_throughput: 5059437.077
  load_time_ms: 3.261
  training_iteration_time_ms: 44362.6
  update_time_ms: 2.379
timesteps_total: 1287000
training_iteration: 78

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27710843373493976
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 193.45
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 6090
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.12260913848877
time_total_s: 3492.4443910121918
timers:
  learn_throughput: 508.073
  learn_time_ms: 32475.625
  load_throughput: 5178928.085
  load_time_ms: 3.186
  training_iteration_time_ms: 43553.796
  update_time_ms: 2.484
timesteps_total: 1204500
training_iteration: 73

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22972972972972974
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 181.23
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 5128
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.49194860458374
time_total_s: 3495.5285522937775
timers:
  learn_throughput: 418.527
  learn_time_ms: 39424.007
  load_throughput: 4958374.482
  load_time_ms: 3.328
  training_iteration_time_ms: 51997.531
  update_time_ms: 2.666
timesteps_total: 1023000
training_iteration: 62

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34415584415584416
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.71875
  reward for individual goal_min: 0.0
episode_len_mean: 196.33
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 4930
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.01604151725769
time_total_s: 3498.31715464592
timers:
  learn_throughput: 439.523
  learn_time_ms: 37540.698
  load_throughput: 4762285.974
  load_time_ms: 3.465
  training_iteration_time_ms: 49821.64
  update_time_ms: 2.929
timesteps_total: 1056000
training_iteration: 64

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34459459459459457
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9539473684210527
  reward for individual goal_min: 0.0
episode_len_mean: 176.5
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 4411
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.08293008804321
time_total_s: 3492.1537370681763
timers:
  learn_throughput: 406.634
  learn_time_ms: 40576.999
  load_throughput: 4504602.887
  load_time_ms: 3.663
  training_iteration_time_ms: 53441.25
  update_time_ms: 2.667
timesteps_total: 924000
training_iteration: 56

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9
  reward for individual goal_min: 0.5
episode_len_mean: 139.96666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.5
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.75
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17123287671232876
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8988095238095238
  reward for individual goal_min: 0.0
episode_len_mean: 195.85
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 4540
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.42468881607056
time_total_s: 3497.874986410141
timers:
  learn_throughput: 407.465
  learn_time_ms: 40494.292
  load_throughput: 4424625.889
  load_time_ms: 3.729
  training_iteration_time_ms: 53617.027
  update_time_ms: 2.642
timesteps_total: 940500
training_iteration: 57

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3161764705882353
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8933333333333333
  reward for individual goal_min: 0.0
episode_len_mean: 187.01
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 4392
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.47
  agent_1: 0.8
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 69.32767486572266
time_total_s: 3490.812987804413
timers:
  learn_throughput: 410.437
  learn_time_ms: 40201.026
  load_throughput: 4634310.74
  load_time_ms: 3.56
  training_iteration_time_ms: 53220.724
  update_time_ms: 2.675
timesteps_total: 990000
training_iteration: 60

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-28ftt3iad9/checkpoint_000060/checkpoint-60
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3092105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.5
episode_len_mean: 174.63
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 5090
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.78768181800842
time_total_s: 3491.465414047241
timers:
  learn_throughput: 430.609
  learn_time_ms: 38317.823
  load_throughput: 4912339.121
  load_time_ms: 3.359
  training_iteration_time_ms: 50795.708
  update_time_ms: 2.56
timesteps_total: 1039500
training_iteration: 63

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8476190476190476
  reward for individual goal_min: 0.5
episode_len_mean: 80.07246376811594
episode_reward_max: 2.0
episode_reward_mean: 1.8454106280193237
episode_reward_min: 1.0
episodes_this_iter: 207
episodes_total: 8753
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9951690821256038
  agent_1: 0.8502415458937198
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.55963635444641
time_total_s: 3512.0094137191772
timers:
  learn_throughput: 424.421
  learn_time_ms: 38876.48
  load_throughput: 4351731.172
  load_time_ms: 3.792
  training_iteration_time_ms: 51147.589
  update_time_ms: 2.609
timesteps_total: 1039500
training_iteration: 63

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7162162162162162
  reward for individual goal_min: 0.0
episode_len_mean: 191.73
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 5514
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.43307900428772
time_total_s: 3518.3409337997437
timers:
  learn_throughput: 479.657
  learn_time_ms: 34399.589
  load_throughput: 5202442.831
  load_time_ms: 3.172
  training_iteration_time_ms: 45958.828
  update_time_ms: 2.454
timesteps_total: 1171500
training_iteration: 71

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7916666666666666
  reward for individual goal_min: 0.5
episode_len_mean: 81.42079207920793
episode_reward_max: 2.0
episode_reward_mean: 1.8267326732673268
episode_reward_min: 1.0
episodes_this_iter: 202
episodes_total: 10135
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8267326732673267
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 50.59195256233215
time_total_s: 3522.467592716217
timers:
  learn_throughput: 440.754
  learn_time_ms: 37435.841
  load_throughput: 4799307.628
  load_time_ms: 3.438
  training_iteration_time_ms: 49307.768
  update_time_ms: 2.596
timesteps_total: 1105500
training_iteration: 67

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6901408450704225
  reward for individual goal_min: 0.5
episode_len_mean: 105.78709677419354
episode_reward_max: 2.0
episode_reward_mean: 1.7161290322580645
episode_reward_min: 1.0
episodes_this_iter: 155
episodes_total: 6486
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.896774193548387
  agent_1: 0.8193548387096774
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.13196349143982
time_total_s: 3533.2607657909393
timers:
  learn_throughput: 379.696
  learn_time_ms: 43455.808
  load_throughput: 4168057.866
  load_time_ms: 3.959
  training_iteration_time_ms: 57370.33
  update_time_ms: 2.849
timesteps_total: 957000
training_iteration: 58

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15492957746478872
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 184.87
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 5267
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.94884133338928
time_total_s: 3532.593672275543
timers:
  learn_throughput: 443.205
  learn_time_ms: 37228.838
  load_throughput: 4999351.008
  load_time_ms: 3.3
  training_iteration_time_ms: 49198.741
  update_time_ms: 2.584
timesteps_total: 1072500
training_iteration: 65

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3805970149253731
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8161764705882353
  reward for individual goal_min: 0.0
episode_len_mean: 189.78
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 5545
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.8120231628418
time_total_s: 3554.9392325878143
timers:
  learn_throughput: 495.005
  learn_time_ms: 33333.025
  load_throughput: 4885291.468
  load_time_ms: 3.377
  training_iteration_time_ms: 44265.658
  update_time_ms: 2.585
timesteps_total: 1204500
training_iteration: 73

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.275
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9923076923076923
  reward for individual goal_min: 0.5
episode_len_mean: 192.02
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 6177
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.745179176330566
time_total_s: 3537.1895701885223
timers:
  learn_throughput: 509.214
  learn_time_ms: 32402.889
  load_throughput: 5164435.357
  load_time_ms: 3.195
  training_iteration_time_ms: 43386.353
  update_time_ms: 2.473
timesteps_total: 1221000
training_iteration: 74

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9692307692307692
  reward for individual goal_min: 0.5
episode_len_mean: 62.761538461538464
episode_reward_max: 2.0
episode_reward_mean: 1.9692307692307693
episode_reward_min: 1.0
episodes_this_iter: 260
episodes_total: 12785
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9692307692307692
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 45.520204067230225
time_total_s: 3534.3700354099274
timers:
  learn_throughput: 497.756
  learn_time_ms: 33148.765
  load_throughput: 5023227.942
  load_time_ms: 3.285
  training_iteration_time_ms: 44437.492
  update_time_ms: 2.361
timesteps_total: 1303500
training_iteration: 79

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31976744186046513
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.704225352112676
  reward for individual goal_min: 0.0
episode_len_mean: 210.5
episode_reward_max: 2.0
episode_reward_mean: 1.06
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 5009
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.14553213119507
time_total_s: 3548.462686777115
timers:
  learn_throughput: 438.914
  learn_time_ms: 37592.753
  load_throughput: 4784145.669
  load_time_ms: 3.449
  training_iteration_time_ms: 49934.793
  update_time_ms: 2.587
timesteps_total: 1072500
training_iteration: 65

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23717948717948717
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9733333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 193.55
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 5214
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.062896490097046
time_total_s: 3549.5914487838745
timers:
  learn_throughput: 413.061
  learn_time_ms: 39945.677
  load_throughput: 4971767.985
  load_time_ms: 3.319
  training_iteration_time_ms: 52533.35
  update_time_ms: 2.679
timesteps_total: 1039500
training_iteration: 63

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3150684931506849
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9545454545454546
  reward for individual goal_min: 0.0
episode_len_mean: 175.63
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 4632
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.46481466293335
time_total_s: 3549.3398010730743
timers:
  learn_throughput: 410.711
  learn_time_ms: 40174.263
  load_throughput: 4459897.663
  load_time_ms: 3.7
  training_iteration_time_ms: 53174.305
  update_time_ms: 2.668
timesteps_total: 957000
training_iteration: 58

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2962962962962963
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9565217391304348
  reward for individual goal_min: 0.0
episode_len_mean: 190.12
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 4498
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.701778411865234
time_total_s: 3546.8555154800415
timers:
  learn_throughput: 406.905
  learn_time_ms: 40550.038
  load_throughput: 4524806.862
  load_time_ms: 3.647
  training_iteration_time_ms: 53418.186
  update_time_ms: 2.664
timesteps_total: 940500
training_iteration: 57

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9357142857142857
  reward for individual goal_min: 0.0
episode_len_mean: 196.09
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 4476
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.990439653396606
time_total_s: 3543.8034274578094
timers:
  learn_throughput: 412.049
  learn_time_ms: 40043.733
  load_throughput: 4617551.576
  load_time_ms: 3.573
  training_iteration_time_ms: 53060.997
  update_time_ms: 2.679
timesteps_total: 1006500
training_iteration: 61

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.5
episode_len_mean: 192.07
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 5174
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.17824959754944
time_total_s: 3542.6436636447906
timers:
  learn_throughput: 430.368
  learn_time_ms: 38339.324
  load_throughput: 4884360.536
  load_time_ms: 3.378
  training_iteration_time_ms: 50784.972
  update_time_ms: 2.537
timesteps_total: 1056000
training_iteration: 64

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7763157894736842
  reward for individual goal_min: 0.0
episode_len_mean: 198.81
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 5596
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.024937868118286
time_total_s: 3563.365871667862
timers:
  learn_throughput: 479.838
  learn_time_ms: 34386.603
  load_throughput: 5165167.704
  load_time_ms: 3.194
  training_iteration_time_ms: 45963.167
  update_time_ms: 2.477
timesteps_total: 1188000
training_iteration: 72

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8590909090909091
  reward for individual goal_min: 0.5
episode_len_mean: 76.08219178082192
episode_reward_max: 2.0
episode_reward_mean: 1.8584474885844748
episode_reward_min: 1.0
episodes_this_iter: 219
episodes_total: 8972
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8584474885844748
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 52.34234809875488
time_total_s: 3564.351761817932
timers:
  learn_throughput: 424.226
  learn_time_ms: 38894.36
  load_throughput: 4370998.295
  load_time_ms: 3.775
  training_iteration_time_ms: 51136.345
  update_time_ms: 2.621
timesteps_total: 1056000
training_iteration: 64

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8316326530612245
  reward for individual goal_min: 0.5
episode_len_mean: 79.2512077294686
episode_reward_max: 2.0
episode_reward_mean: 1.8405797101449275
episode_reward_min: 1.0
episodes_this_iter: 207
episodes_total: 10342
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8405797101449275
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 50.12243366241455
time_total_s: 3572.5900263786316
timers:
  learn_throughput: 438.177
  learn_time_ms: 37656.048
  load_throughput: 4854348.262
  load_time_ms: 3.399
  training_iteration_time_ms: 49597.89
  update_time_ms: 2.583
timesteps_total: 1122000
training_iteration: 68

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6470588235294118
  reward for individual goal_min: 0.5
episode_len_mean: 114.5374149659864
episode_reward_max: 2.0
episode_reward_mean: 1.6734693877551021
episode_reward_min: 1.0
episodes_this_iter: 147
episodes_total: 6633
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8435374149659864
  agent_1: 0.8299319727891157
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.62498617172241
time_total_s: 3590.8857519626617
timers:
  learn_throughput: 379.924
  learn_time_ms: 43429.706
  load_throughput: 4091761.965
  load_time_ms: 4.032
  training_iteration_time_ms: 57430.909
  update_time_ms: 2.821
timesteps_total: 973500
training_iteration: 59

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 166.78
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 6276
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.973018169403076
time_total_s: 3580.1625883579254
timers:
  learn_throughput: 508.908
  learn_time_ms: 32422.34
  load_throughput: 5179199.389
  load_time_ms: 3.186
  training_iteration_time_ms: 43346.15
  update_time_ms: 2.471
timesteps_total: 1237500
training_iteration: 75

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23846153846153847
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8376623376623377
  reward for individual goal_min: 0.0
episode_len_mean: 190.02
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 5631
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.846662282943726
time_total_s: 3599.785894870758
timers:
  learn_throughput: 493.328
  learn_time_ms: 33446.285
  load_throughput: 4867458.803
  load_time_ms: 3.39
  training_iteration_time_ms: 44408.138
  update_time_ms: 2.554
timesteps_total: 1221000
training_iteration: 74

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2536231884057971
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.971830985915493
  reward for individual goal_min: 0.0
episode_len_mean: 178.92
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 5359
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.14382290840149
time_total_s: 3584.7374951839447
timers:
  learn_throughput: 443.218
  learn_time_ms: 37227.725
  load_throughput: 5019256.895
  load_time_ms: 3.287
  training_iteration_time_ms: 49266.991
  update_time_ms: 2.585
timesteps_total: 1089000
training_iteration: 66

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 57.45
episode_reward_max: 2.0
episode_reward_mean: 1.9666666666666666
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9666666666666667
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9793103448275862
  reward for individual goal_min: 0.5
episode_len_mean: 55.13576158940398
episode_reward_max: 2.0
episode_reward_mean: 1.9801324503311257
episode_reward_min: 1.0
episodes_this_iter: 302
episodes_total: 13087
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9834437086092715
  agent_1: 0.9966887417218543
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.381338119506836
time_total_s: 3585.751373529434
timers:
  learn_throughput: 496.031
  learn_time_ms: 33264.017
  load_throughput: 5100867.219
  load_time_ms: 3.235
  training_iteration_time_ms: 44551.309
  update_time_ms: 2.375
timesteps_total: 1320000
training_iteration: 80

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-172kuwc_58/checkpoint_000080/checkpoint-80
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27611940298507465
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9805194805194806
  reward for individual goal_min: 0.0
episode_len_mean: 166.26732673267327
episode_reward_max: 2.0
episode_reward_mean: 1.396039603960396
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 5315
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7128712871287128
  agent_1: 0.6831683168316832
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.27758264541626
time_total_s: 3597.8690314292908
timers:
  learn_throughput: 415.873
  learn_time_ms: 39675.576
  load_throughput: 4966594.375
  load_time_ms: 3.322
  training_iteration_time_ms: 52209.281
  update_time_ms: 2.68
timesteps_total: 1056000
training_iteration: 64

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3466666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.636986301369863
  reward for individual goal_min: 0.0
episode_len_mean: 204.78
episode_reward_max: 2.0
episode_reward_mean: 1.02
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 5090
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.65100979804993
time_total_s: 3600.113696575165
timers:
  learn_throughput: 437.566
  learn_time_ms: 37708.612
  load_throughput: 4812289.463
  load_time_ms: 3.429
  training_iteration_time_ms: 50064.492
  update_time_ms: 2.607
timesteps_total: 1089000
training_iteration: 66

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22602739726027396
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7894736842105263
  reward for individual goal_min: 0.0
episode_len_mean: 210.23
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 5673
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.17411136627197
time_total_s: 3608.539983034134
timers:
  learn_throughput: 480.6
  learn_time_ms: 34332.05
  load_throughput: 5198847.338
  load_time_ms: 3.174
  training_iteration_time_ms: 45889.284
  update_time_ms: 2.492
timesteps_total: 1204500
training_iteration: 73

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32575757575757575
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9246575342465754
  reward for individual goal_min: 0.0
episode_len_mean: 183.79
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 4565
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.90572667121887
time_total_s: 3593.7091541290283
timers:
  learn_throughput: 414.999
  learn_time_ms: 39759.091
  load_throughput: 4609831.409
  load_time_ms: 3.579
  training_iteration_time_ms: 52714.196
  update_time_ms: 2.672
timesteps_total: 1023000
training_iteration: 62

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9411764705882353
  reward for individual goal_min: 0.0
episode_len_mean: 188.7
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 5258
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.77326059341431
time_total_s: 3591.416924238205
timers:
  learn_throughput: 433.422
  learn_time_ms: 38069.146
  load_throughput: 4878300.92
  load_time_ms: 3.382
  training_iteration_time_ms: 50555.65
  update_time_ms: 2.547
timesteps_total: 1072500
training_iteration: 65

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17567567567567569
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9662162162162162
  reward for individual goal_min: 0.0
episode_len_mean: 186.09
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 4724
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.3186457157135
time_total_s: 3602.658446788788
timers:
  learn_throughput: 409.733
  learn_time_ms: 40270.078
  load_throughput: 4488068.482
  load_time_ms: 3.676
  training_iteration_time_ms: 53226.436
  update_time_ms: 2.652
timesteps_total: 973500
training_iteration: 59

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3356164383561644
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9214285714285714
  reward for individual goal_min: 0.0
episode_len_mean: 175.67
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 4595
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.25514554977417
time_total_s: 3600.1106610298157
timers:
  learn_throughput: 407.906
  learn_time_ms: 40450.464
  load_throughput: 4467209.915
  load_time_ms: 3.694
  training_iteration_time_ms: 53276.279
  update_time_ms: 2.666
timesteps_total: 957000
training_iteration: 58

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8282828282828283
  reward for individual goal_min: 0.5
episode_len_mean: 83.7025641025641
episode_reward_max: 2.0
episode_reward_mean: 1.8256410256410256
episode_reward_min: 1.0
episodes_this_iter: 195
episodes_total: 9167
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9948717948717949
  agent_1: 0.8307692307692308
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.594242572784424
time_total_s: 3614.9460043907166
timers:
  learn_throughput: 425.677
  learn_time_ms: 38761.779
  load_throughput: 4377744.772
  load_time_ms: 3.769
  training_iteration_time_ms: 50972.499
  update_time_ms: 2.649
timesteps_total: 1072500
training_iteration: 65

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8551401869158879
  reward for individual goal_min: 0.5
episode_len_mean: 75.56502242152466
episode_reward_max: 2.0
episode_reward_mean: 1.8609865470852018
episode_reward_min: 1.0
episodes_this_iter: 223
episodes_total: 10565
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8609865470852018
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 51.34490966796875
time_total_s: 3623.9349360466003
timers:
  learn_throughput: 435.779
  learn_time_ms: 37863.204
  load_throughput: 4867664.217
  load_time_ms: 3.39
  training_iteration_time_ms: 49908.002
  update_time_ms: 2.589
timesteps_total: 1138500
training_iteration: 69

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2605633802816901
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.15
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 6369
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.86350655555725
time_total_s: 3623.0260949134827
timers:
  learn_throughput: 509.869
  learn_time_ms: 32361.234
  load_throughput: 5163741.746
  load_time_ms: 3.195
  training_iteration_time_ms: 43200.721
  update_time_ms: 2.49
timesteps_total: 1254000
training_iteration: 76

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2565789473684211
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8291139240506329
  reward for individual goal_min: 0.0
episode_len_mean: 205.48
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 5711
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.146817207336426
time_total_s: 3642.9327120780945
timers:
  learn_throughput: 497.441
  learn_time_ms: 33169.738
  load_throughput: 4813929.689
  load_time_ms: 3.428
  training_iteration_time_ms: 44091.527
  update_time_ms: 2.535
timesteps_total: 1237500
training_iteration: 75

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26811594202898553
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 175.03
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 5453
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.467326641082764
time_total_s: 3634.2048218250275
timers:
  learn_throughput: 442.587
  learn_time_ms: 37280.839
  load_throughput: 4985377.689
  load_time_ms: 3.31
  training_iteration_time_ms: 49298.128
  update_time_ms: 2.596
timesteps_total: 1105500
training_iteration: 67

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9837662337662337
  reward for individual goal_min: 0.5
episode_len_mean: 55.939597315436245
episode_reward_max: 2.0
episode_reward_mean: 1.983221476510067
episode_reward_min: 1.0
episodes_this_iter: 298
episodes_total: 13385
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9832214765100671
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 45.90873312950134
time_total_s: 3631.6601066589355
timers:
  learn_throughput: 493.225
  learn_time_ms: 33453.278
  load_throughput: 5046045.979
  load_time_ms: 3.27
  training_iteration_time_ms: 44759.076
  update_time_ms: 2.401
timesteps_total: 1336500
training_iteration: 81

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.65
  reward for individual goal_min: 0.5
episode_len_mean: 120.08333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.65
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8333333333333334
  agent_1: 0.8166666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2532467532467532
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 179.82
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 5401
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.11392259597778
time_total_s: 3647.9829540252686
timers:
  learn_throughput: 421.924
  learn_time_ms: 39106.613
  load_throughput: 4992606.679
  load_time_ms: 3.305
  training_iteration_time_ms: 51577.741
  update_time_ms: 2.702
timesteps_total: 1072500
training_iteration: 65

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6875
  reward for individual goal_min: 0.5
episode_len_mean: 124.9090909090909
episode_reward_max: 2.0
episode_reward_mean: 1.621212121212121
episode_reward_min: 1.0
episodes_this_iter: 132
episodes_total: 6765
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8106060606060606
  agent_1: 0.8106060606060606
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 67.90110373497009
time_total_s: 3658.786855697632
timers:
  learn_throughput: 383.445
  learn_time_ms: 43030.925
  load_throughput: 4199852.897
  load_time_ms: 3.929
  training_iteration_time_ms: 57021.11
  update_time_ms: 2.889
timesteps_total: 990000
training_iteration: 60

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-19wecnkw4d/checkpoint_000060/checkpoint-60
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.41333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6343283582089553
  reward for individual goal_min: 0.0
episode_len_mean: 201.31
episode_reward_max: 2.0
episode_reward_mean: 1.06
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 5173
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.85120987892151
time_total_s: 3651.9649064540863
timers:
  learn_throughput: 436.723
  learn_time_ms: 37781.382
  load_throughput: 4821979.627
  load_time_ms: 3.422
  training_iteration_time_ms: 50184.946
  update_time_ms: 2.59
timesteps_total: 1105500
training_iteration: 67

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2569444444444444
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7089552238805971
  reward for individual goal_min: 0.0
episode_len_mean: 211.3
episode_reward_max: 2.0
episode_reward_mean: 0.98
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 5753
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.72166872024536
time_total_s: 3655.2616517543793
timers:
  learn_throughput: 477.22
  learn_time_ms: 34575.279
  load_throughput: 5151058.48
  load_time_ms: 3.203
  training_iteration_time_ms: 46124.913
  update_time_ms: 2.488
timesteps_total: 1221000
training_iteration: 74

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.225
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9452054794520548
  reward for individual goal_min: 0.0
episode_len_mean: 194.6
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 5348
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.056084394454956
time_total_s: 3638.47300863266
timers:
  learn_throughput: 434.504
  learn_time_ms: 37974.298
  load_throughput: 4972375.252
  load_time_ms: 3.318
  training_iteration_time_ms: 50478.459
  update_time_ms: 2.574
timesteps_total: 1089000
training_iteration: 66

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2716049382716049
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9027777777777778
  reward for individual goal_min: 0.0
episode_len_mean: 206.37
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 4647
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.051724672317505
time_total_s: 3642.760878801346
timers:
  learn_throughput: 416.813
  learn_time_ms: 39586.14
  load_throughput: 4673524.355
  load_time_ms: 3.531
  training_iteration_time_ms: 52450.051
  update_time_ms: 2.689
timesteps_total: 1039500
training_iteration: 63

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9466666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 173.35
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 4691
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.32885456085205
time_total_s: 3652.4395155906677
timers:
  learn_throughput: 409.932
  learn_time_ms: 40250.546
  load_throughput: 4461220.154
  load_time_ms: 3.699
  training_iteration_time_ms: 53028.608
  update_time_ms: 2.675
timesteps_total: 973500
training_iteration: 59

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2676056338028169
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 167.48
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 6469
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.26154446601868
time_total_s: 3665.2876393795013
timers:
  learn_throughput: 512.751
  learn_time_ms: 32179.34
  load_throughput: 5196466.11
  load_time_ms: 3.175
  training_iteration_time_ms: 42936.42
  update_time_ms: 2.487
timesteps_total: 1270500
training_iteration: 77

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8785046728971962
  reward for individual goal_min: 0.5
episode_len_mean: 70.45021645021644
episode_reward_max: 2.0
episode_reward_mean: 1.8874458874458875
episode_reward_min: 1.0
episodes_this_iter: 231
episodes_total: 9398
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8874458874458875
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 52.571964263916016
time_total_s: 3667.5179686546326
timers:
  learn_throughput: 424.881
  learn_time_ms: 38834.382
  load_throughput: 4392387.36
  load_time_ms: 3.756
  training_iteration_time_ms: 51038.694
  update_time_ms: 2.642
timesteps_total: 1089000
training_iteration: 66

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2631578947368421
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8703703703703703
  reward for individual goal_min: 0.0
episode_len_mean: 191.42
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 5797
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.02715492248535
time_total_s: 3686.95986700058
timers:
  learn_throughput: 498.023
  learn_time_ms: 33131.026
  load_throughput: 4878404.083
  load_time_ms: 3.382
  training_iteration_time_ms: 44068.614
  update_time_ms: 2.561
timesteps_total: 1254000
training_iteration: 76

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8217821782178217
  reward for individual goal_min: 0.5
episode_len_mean: 82.7258883248731
episode_reward_max: 2.0
episode_reward_mean: 1.8172588832487309
episode_reward_min: 1.0
episodes_this_iter: 197
episodes_total: 10762
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.817258883248731
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 49.41702342033386
time_total_s: 3673.351959466934
timers:
  learn_throughput: 436.337
  learn_time_ms: 37814.85
  load_throughput: 4774705.643
  load_time_ms: 3.456
  training_iteration_time_ms: 49817.312
  update_time_ms: 2.592
timesteps_total: 1155000
training_iteration: 70

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9
  reward for individual goal_min: 0.5
episode_len_mean: 103.76666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.7666666666666666
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8333333333333334
  agent_1: 0.9333333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.96
  reward for individual goal_min: 0.0
episode_len_mean: 169.37
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 4818
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 64.9489278793335
time_total_s: 3667.6073746681213
timers:
  learn_throughput: 412.576
  learn_time_ms: 39992.61
  load_throughput: 4483242.165
  load_time_ms: 3.68
  training_iteration_time_ms: 52916.913
  update_time_ms: 2.651
timesteps_total: 990000
training_iteration: 60

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.96875
  reward for individual goal_min: 0.5
episode_len_mean: 57.656028368794324
episode_reward_max: 2.0
episode_reward_mean: 1.9680851063829787
episode_reward_min: 1.0
episodes_this_iter: 282
episodes_total: 13667
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9680851063829787
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 44.88229727745056
time_total_s: 3676.542403936386
timers:
  learn_throughput: 491.657
  learn_time_ms: 33559.973
  load_throughput: 4987317.748
  load_time_ms: 3.308
  training_iteration_time_ms: 44871.029
  update_time_ms: 2.386
timesteps_total: 1353000
training_iteration: 82

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2535211267605634
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9657534246575342
  reward for individual goal_min: 0.0
episode_len_mean: 176.86
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 5546
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.46813464164734
time_total_s: 3685.672956466675
timers:
  learn_throughput: 440.101
  learn_time_ms: 37491.363
  load_throughput: 4944593.643
  load_time_ms: 3.337
  training_iteration_time_ms: 49546.098
  update_time_ms: 2.594
timesteps_total: 1122000
training_iteration: 68

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.0
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 5496
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.89177966117859
time_total_s: 3696.874733686447
timers:
  learn_throughput: 427.59
  learn_time_ms: 38588.343
  load_throughput: 4983905.689
  load_time_ms: 3.311
  training_iteration_time_ms: 51008.112
  update_time_ms: 2.668
timesteps_total: 1089000
training_iteration: 66

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27631578947368424
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7960526315789473
  reward for individual goal_min: 0.0
episode_len_mean: 196.63
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 5835
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.40244483947754
time_total_s: 3700.664096593857
timers:
  learn_throughput: 477.88
  learn_time_ms: 34527.472
  load_throughput: 5139734.866
  load_time_ms: 3.21
  training_iteration_time_ms: 46077.561
  update_time_ms: 2.504
timesteps_total: 1237500
training_iteration: 75

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3716216216216216
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.717391304347826
  reward for individual goal_min: 0.0
episode_len_mean: 202.43
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 5256
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.49
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.972408056259155
time_total_s: 3701.9373145103455
timers:
  learn_throughput: 438.079
  learn_time_ms: 37664.43
  load_throughput: 4824904.382
  load_time_ms: 3.42
  training_iteration_time_ms: 50046.399
  update_time_ms: 2.586
timesteps_total: 1122000
training_iteration: 68

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6739130434782609
  reward for individual goal_min: 0.5
episode_len_mean: 111.82758620689656
episode_reward_max: 2.0
episode_reward_mean: 1.6896551724137931
episode_reward_min: 1.0
episodes_this_iter: 145
episodes_total: 6910
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8758620689655172
  agent_1: 0.8137931034482758
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.82169580459595
time_total_s: 3715.608551502228
timers:
  learn_throughput: 383.977
  learn_time_ms: 42971.322
  load_throughput: 4283133.595
  load_time_ms: 3.852
  training_iteration_time_ms: 56984.627
  update_time_ms: 2.843
timesteps_total: 1006500
training_iteration: 61

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.958904109589041
  reward for individual goal_min: 0.0
episode_len_mean: 171.96
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 5443
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.398648738861084
time_total_s: 3687.871657371521
timers:
  learn_throughput: 436.993
  learn_time_ms: 37758.042
  load_throughput: 4965026.581
  load_time_ms: 3.323
  training_iteration_time_ms: 50259.525
  update_time_ms: 2.581
timesteps_total: 1105500
training_iteration: 67

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21014492753623187
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9230769230769231
  reward for individual goal_min: 0.0
episode_len_mean: 193.73
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 4730
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.910505056381226
time_total_s: 3693.671383857727
timers:
  learn_throughput: 420.499
  learn_time_ms: 39239.066
  load_throughput: 4636235.597
  load_time_ms: 3.559
  training_iteration_time_ms: 51987.691
  update_time_ms: 2.667
timesteps_total: 1056000
training_iteration: 64

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3108108108108108
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.89
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 6560
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.80206823348999
time_total_s: 3707.0897076129913
timers:
  learn_throughput: 515.14
  learn_time_ms: 32030.153
  load_throughput: 5189257.671
  load_time_ms: 3.18
  training_iteration_time_ms: 42707.987
  update_time_ms: 2.465
timesteps_total: 1287000
training_iteration: 78

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2847222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8472222222222222
  reward for individual goal_min: 0.0
episode_len_mean: 191.12
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 5883
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.109155893325806
time_total_s: 3731.0690228939056
timers:
  learn_throughput: 497.598
  learn_time_ms: 33159.326
  load_throughput: 4859051.725
  load_time_ms: 3.396
  training_iteration_time_ms: 44096.65
  update_time_ms: 2.573
timesteps_total: 1270500
training_iteration: 77

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8551401869158879
  reward for individual goal_min: 0.5
episode_len_mean: 78.48571428571428
episode_reward_max: 2.0
episode_reward_mean: 1.8523809523809525
episode_reward_min: 1.0
episodes_this_iter: 210
episodes_total: 9608
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9952380952380953
  agent_1: 0.8571428571428571
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.95679044723511
time_total_s: 3718.4747591018677
timers:
  learn_throughput: 424.888
  learn_time_ms: 38833.763
  load_throughput: 4379739.517
  load_time_ms: 3.767
  training_iteration_time_ms: 50984.333
  update_time_ms: 2.671
timesteps_total: 1105500
training_iteration: 67

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8080808080808081
  reward for individual goal_min: 0.5
episode_len_mean: 88.57754010695187
episode_reward_max: 2.0
episode_reward_mean: 1.7967914438502675
episode_reward_min: 1.0
episodes_this_iter: 187
episodes_total: 10949
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7967914438502673
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.28066039085388
time_total_s: 3721.632619857788
timers:
  learn_throughput: 438.359
  learn_time_ms: 37640.37
  load_throughput: 4785502.019
  load_time_ms: 3.448
  training_iteration_time_ms: 49575.163
  update_time_ms: 2.603
timesteps_total: 1171500
training_iteration: 71

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30666666666666664
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9436619718309859
  reward for individual goal_min: 0.0
episode_len_mean: 181.27
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 4911
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.997358083724976
time_total_s: 3719.6047327518463
timers:
  learn_throughput: 414.75
  learn_time_ms: 39783.014
  load_throughput: 4497109.364
  load_time_ms: 3.669
  training_iteration_time_ms: 52625.249
  update_time_ms: 2.677
timesteps_total: 1006500
training_iteration: 61

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 105.7
episode_reward_max: 2.0
episode_reward_mean: 1.8
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9166666666666666
  agent_1: 0.8833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2962962962962963
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9657534246575342
  reward for individual goal_min: 0.0
episode_len_mean: 188.51
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 4776
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 64.29555130004883
time_total_s: 3716.7350668907166
timers:
  learn_throughput: 412.535
  learn_time_ms: 39996.601
  load_throughput: 4444374.117
  load_time_ms: 3.713
  training_iteration_time_ms: 52709.749
  update_time_ms: 2.657
timesteps_total: 990000
training_iteration: 60

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-2933yh83nn/checkpoint_000060/checkpoint-60
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9788732394366197
  reward for individual goal_min: 0.5
episode_len_mean: 54.23300970873787
episode_reward_max: 2.0
episode_reward_mean: 1.9805825242718447
episode_reward_min: 1.0
episodes_this_iter: 309
episodes_total: 13976
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9838187702265372
  agent_1: 0.9967637540453075
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.18194389343262
time_total_s: 3722.7243478298187
timers:
  learn_throughput: 489.535
  learn_time_ms: 33705.439
  load_throughput: 4973054.138
  load_time_ms: 3.318
  training_iteration_time_ms: 45029.34
  update_time_ms: 2.382
timesteps_total: 1369500
training_iteration: 83

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24074074074074073
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9675324675324676
  reward for individual goal_min: 0.0
episode_len_mean: 192.32
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 5630
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.06552791595459
time_total_s: 3733.7384843826294
timers:
  learn_throughput: 439.584
  learn_time_ms: 37535.475
  load_throughput: 4915269.819
  load_time_ms: 3.357
  training_iteration_time_ms: 49552.498
  update_time_ms: 2.596
timesteps_total: 1138500
training_iteration: 69

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22297297297297297
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.5
episode_len_mean: 182.83
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 5586
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.10466122627258
time_total_s: 3746.9793949127197
timers:
  learn_throughput: 427.614
  learn_time_ms: 38586.243
  load_throughput: 4956882.879
  load_time_ms: 3.329
  training_iteration_time_ms: 50976.125
  update_time_ms: 2.661
timesteps_total: 1105500
training_iteration: 67

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3161764705882353
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6623376623376623
  reward for individual goal_min: 0.0
episode_len_mean: 204.22
episode_reward_max: 2.0
episode_reward_mean: 1.03
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 5915
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.630969762802124
time_total_s: 3749.295066356659
timers:
  learn_throughput: 475.184
  learn_time_ms: 34723.398
  load_throughput: 5129791.416
  load_time_ms: 3.217
  training_iteration_time_ms: 46314.665
  update_time_ms: 2.524
timesteps_total: 1254000
training_iteration: 76

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21951219512195122
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 194.26
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 6645
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.698750495910645
time_total_s: 3747.788458108902
timers:
  learn_throughput: 515.598
  learn_time_ms: 32001.672
  load_throughput: 5233364.791
  load_time_ms: 3.153
  training_iteration_time_ms: 42635.347
  update_time_ms: 2.454
timesteps_total: 1303500
training_iteration: 79

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.390625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7236842105263158
  reward for individual goal_min: 0.0
episode_len_mean: 190.48
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 5343
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.81709551811218
time_total_s: 3752.7544100284576
timers:
  learn_throughput: 438.426
  learn_time_ms: 37634.639
  load_throughput: 4804438.582
  load_time_ms: 3.434
  training_iteration_time_ms: 50062.914
  update_time_ms: 2.593
timesteps_total: 1138500
training_iteration: 69

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2866666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.972972972972973
  reward for individual goal_min: 0.0
episode_len_mean: 178.63
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 5533
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.32701373100281
time_total_s: 3736.198671102524
timers:
  learn_throughput: 440.233
  learn_time_ms: 37480.153
  load_throughput: 4946926.382
  load_time_ms: 3.335
  training_iteration_time_ms: 49940.456
  update_time_ms: 2.594
timesteps_total: 1122000
training_iteration: 68

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3092105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8819444444444444
  reward for individual goal_min: 0.0
episode_len_mean: 194.34
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 5965
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.76
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.63651943206787
time_total_s: 3774.7055423259735
timers:
  learn_throughput: 496.55
  learn_time_ms: 33229.302
  load_throughput: 4852204.055
  load_time_ms: 3.401
  training_iteration_time_ms: 44220.697
  update_time_ms: 2.581
timesteps_total: 1287000
training_iteration: 78

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2905405405405405
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9294871794871795
  reward for individual goal_min: 0.0
episode_len_mean: 191.95
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 4817
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.410722970962524
time_total_s: 3744.0821068286896
timers:
  learn_throughput: 422.63
  learn_time_ms: 39041.284
  load_throughput: 4657859.859
  load_time_ms: 3.542
  training_iteration_time_ms: 51725.553
  update_time_ms: 2.632
timesteps_total: 1072500
training_iteration: 65

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6883116883116883
  reward for individual goal_min: 0.5
episode_len_mean: 116.1875
episode_reward_max: 2.0
episode_reward_mean: 1.6666666666666667
episode_reward_min: 1.0
episodes_this_iter: 144
episodes_total: 7054
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8402777777777778
  agent_1: 0.8263888888888888
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.88577342033386
time_total_s: 3771.4943249225616
timers:
  learn_throughput: 384.96
  learn_time_ms: 42861.646
  load_throughput: 4323025.355
  load_time_ms: 3.817
  training_iteration_time_ms: 56799.066
  update_time_ms: 2.758
timesteps_total: 1023000
training_iteration: 62

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8186274509803921
  reward for individual goal_min: 0.5
episode_len_mean: 86.22164948453609
episode_reward_max: 2.0
episode_reward_mean: 1.809278350515464
episode_reward_min: 1.0
episodes_this_iter: 194
episodes_total: 9802
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9948453608247423
  agent_1: 0.8144329896907216
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.762362241744995
time_total_s: 3768.2371213436127
timers:
  learn_throughput: 425.238
  learn_time_ms: 38801.776
  load_throughput: 4353099.804
  load_time_ms: 3.79
  training_iteration_time_ms: 50912.455
  update_time_ms: 2.671
timesteps_total: 1122000
training_iteration: 68

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 78.56666666666666
episode_reward_max: 2.0
episode_reward_mean: 1.838095238095238
episode_reward_min: 1.0
episodes_this_iter: 210
episodes_total: 11159
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8428571428571429
  agent_1: 0.9952380952380953
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.93913507461548
time_total_s: 3771.5717549324036
timers:
  learn_throughput: 439.031
  learn_time_ms: 37582.8
  load_throughput: 4802638.168
  load_time_ms: 3.436
  training_iteration_time_ms: 49568.014
  update_time_ms: 2.595
timesteps_total: 1188000
training_iteration: 72

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9711538461538461
  reward for individual goal_min: 0.5
episode_len_mean: 55.50847457627118
episode_reward_max: 2.0
episode_reward_mean: 1.9694915254237289
episode_reward_min: 1.0
episodes_this_iter: 295
episodes_total: 14271
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9694915254237289
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 45.079328536987305
time_total_s: 3767.803676366806
timers:
  learn_throughput: 488.385
  learn_time_ms: 33784.843
  load_throughput: 4973232.823
  load_time_ms: 3.318
  training_iteration_time_ms: 45112.362
  update_time_ms: 2.394
timesteps_total: 1386000
training_iteration: 84

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2708333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9452054794520548
  reward for individual goal_min: 0.0
episode_len_mean: 181.5
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 5000
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.489723205566406
time_total_s: 3772.0944559574127
timers:
  learn_throughput: 413.698
  learn_time_ms: 39884.16
  load_throughput: 4512092.007
  load_time_ms: 3.657
  training_iteration_time_ms: 52743.677
  update_time_ms: 2.686
timesteps_total: 1023000
training_iteration: 62

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35064935064935066
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.952054794520548
  reward for individual goal_min: 0.0
episode_len_mean: 184.39
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 4865
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.73431324958801
time_total_s: 3769.4693801403046
timers:
  learn_throughput: 414.054
  learn_time_ms: 39849.851
  load_throughput: 4471423.883
  load_time_ms: 3.69
  training_iteration_time_ms: 52489.913
  update_time_ms: 2.63
timesteps_total: 1006500
training_iteration: 61

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27564102564102566
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.946969696969697
  reward for individual goal_min: 0.0
episode_len_mean: 183.51
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 5721
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.701186656951904
time_total_s: 3785.4396710395813
timers:
  learn_throughput: 435.152
  learn_time_ms: 37917.782
  load_throughput: 4889640.515
  load_time_ms: 3.374
  training_iteration_time_ms: 50055.132
  update_time_ms: 2.6
timesteps_total: 1155000
training_iteration: 70

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2565789473684211
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.684931506849315
  reward for individual goal_min: 0.0
episode_len_mean: 210.23
episode_reward_max: 2.0
episode_reward_mean: 0.99
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 5995
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.49
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.98999643325806
time_total_s: 3796.285062789917
timers:
  learn_throughput: 473.936
  learn_time_ms: 34814.862
  load_throughput: 5094183.86
  load_time_ms: 3.239
  training_iteration_time_ms: 46483.326
  update_time_ms: 2.508
timesteps_total: 1270500
training_iteration: 77

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30128205128205127
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9657534246575342
  reward for individual goal_min: 0.0
episode_len_mean: 180.43
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 5677
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.89372992515564
time_total_s: 3799.8731248378754
timers:
  learn_throughput: 423.617
  learn_time_ms: 38950.324
  load_throughput: 4948659.688
  load_time_ms: 3.334
  training_iteration_time_ms: 51266.785
  update_time_ms: 2.687
timesteps_total: 1122000
training_iteration: 68

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2602739726027397
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8618421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 198.55
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 6046
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.78
  agent_1: 0.42
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.9858558177948
time_total_s: 3817.6913981437683
timers:
  learn_throughput: 498.21
  learn_time_ms: 33118.553
  load_throughput: 4850979.644
  load_time_ms: 3.401
  training_iteration_time_ms: 44080.044
  update_time_ms: 2.549
timesteps_total: 1303500
training_iteration: 79

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 83.75
episode_reward_max: 2.0
episode_reward_mean: 1.8166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9166666666666666
  agent_1: 0.9
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27710843373493976
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.948051948051948
  reward for individual goal_min: 0.0
episode_len_mean: 188.99
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 5622
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.849506855010986
time_total_s: 3784.048177957535
timers:
  learn_throughput: 443.972
  learn_time_ms: 37164.541
  load_throughput: 4943004.4
  load_time_ms: 3.338
  training_iteration_time_ms: 49507.262
  update_time_ms: 2.595
timesteps_total: 1138500
training_iteration: 69

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2847222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 167.59
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 6742
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.83004665374756
time_total_s: 3800.6185047626495
timers:
  learn_throughput: 515.595
  learn_time_ms: 32001.886
  load_throughput: 5257856.926
  load_time_ms: 3.138
  training_iteration_time_ms: 42678.51
  update_time_ms: 2.446
timesteps_total: 1320000
training_iteration: 80

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19ecg2ejpn/checkpoint_000080/checkpoint-80
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.363013698630137
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6438356164383562
  reward for individual goal_min: 0.0
episode_len_mean: 198.43
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 5423
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.143946170806885
time_total_s: 3805.8983561992645
timers:
  learn_throughput: 434.331
  learn_time_ms: 37989.451
  load_throughput: 4789211.095
  load_time_ms: 3.445
  training_iteration_time_ms: 50423.485
  update_time_ms: 2.59
timesteps_total: 1155000
training_iteration: 70

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3717948717948718
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8529411764705882
  reward for individual goal_min: 0.0
episode_len_mean: 192.27
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 4903
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.443012952804565
time_total_s: 3793.525119781494
timers:
  learn_throughput: 427.21
  learn_time_ms: 38622.662
  load_throughput: 4694734.214
  load_time_ms: 3.515
  training_iteration_time_ms: 51113.672
  update_time_ms: 2.613
timesteps_total: 1089000
training_iteration: 66

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7368421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 108.3489932885906
episode_reward_max: 2.0
episode_reward_mean: 1.7315436241610738
episode_reward_min: 1.0
episodes_this_iter: 149
episodes_total: 7203
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.87248322147651
  agent_1: 0.8590604026845637
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.71569085121155
time_total_s: 3829.210015773773
timers:
  learn_throughput: 384.282
  learn_time_ms: 42937.216
  load_throughput: 4314966.144
  load_time_ms: 3.824
  training_iteration_time_ms: 56865.553
  update_time_ms: 2.736
timesteps_total: 1039500
training_iteration: 63

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9908256880733946
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8316831683168316
  reward for individual goal_min: 0.5
episode_len_mean: 79.04761904761905
episode_reward_max: 2.0
episode_reward_mean: 1.8285714285714285
episode_reward_min: 0.0
episodes_this_iter: 210
episodes_total: 11369
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8333333333333334
  agent_1: 0.9952380952380953
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.870617151260376
time_total_s: 3820.442372083664
timers:
  learn_throughput: 440.471
  learn_time_ms: 37459.895
  load_throughput: 4825610.889
  load_time_ms: 3.419
  training_iteration_time_ms: 49410.486
  update_time_ms: 2.595
timesteps_total: 1204500
training_iteration: 73

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.875
  reward for individual goal_min: 0.5
episode_len_mean: 70.92307692307692
episode_reward_max: 2.0
episode_reward_mean: 1.8846153846153846
episode_reward_min: 1.0
episodes_this_iter: 234
episodes_total: 10036
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8846153846153846
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 52.53657269477844
time_total_s: 3820.773694038391
timers:
  learn_throughput: 425.082
  learn_time_ms: 38816.045
  load_throughput: 4355017.337
  load_time_ms: 3.789
  training_iteration_time_ms: 50972.812
  update_time_ms: 2.674
timesteps_total: 1138500
training_iteration: 69

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9794520547945206
  reward for individual goal_min: 0.5
episode_len_mean: 54.19869706840391
episode_reward_max: 2.0
episode_reward_mean: 1.980456026058632
episode_reward_min: 1.0
episodes_this_iter: 307
episodes_total: 14578
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9804560260586319
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.53469920158386
time_total_s: 3814.33837556839
timers:
  learn_throughput: 486.701
  learn_time_ms: 33901.72
  load_throughput: 4969732.936
  load_time_ms: 3.32
  training_iteration_time_ms: 45235.529
  update_time_ms: 2.389
timesteps_total: 1402500
training_iteration: 85

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3356164383561644
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9014084507042254
  reward for individual goal_min: 0.0
episode_len_mean: 176.34
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 5095
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.708118200302124
time_total_s: 3826.802574157715
timers:
  learn_throughput: 413.548
  learn_time_ms: 39898.671
  load_throughput: 4490339.861
  load_time_ms: 3.675
  training_iteration_time_ms: 52766.625
  update_time_ms: 2.701
timesteps_total: 1039500
training_iteration: 63

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24342105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.36
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 5811
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.193082094192505
time_total_s: 3834.632753133774
timers:
  learn_throughput: 434.117
  learn_time_ms: 38008.227
  load_throughput: 4825072.579
  load_time_ms: 3.42
  training_iteration_time_ms: 50155.068
  update_time_ms: 2.625
timesteps_total: 1171500
training_iteration: 71

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34705882352941175
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 175.22
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 4961
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.081921100616455
time_total_s: 3822.551301240921
timers:
  learn_throughput: 413.935
  learn_time_ms: 39861.364
  load_throughput: 4501204.293
  load_time_ms: 3.666
  training_iteration_time_ms: 52499.479
  update_time_ms: 2.631
timesteps_total: 1023000
training_iteration: 62

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6381578947368421
  reward for individual goal_min: 0.0
episode_len_mean: 201.81
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 6078
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.343708992004395
time_total_s: 3842.6287717819214
timers:
  learn_throughput: 474.084
  learn_time_ms: 34803.964
  load_throughput: 5032871.978
  load_time_ms: 3.278
  training_iteration_time_ms: 46413.64
  update_time_ms: 2.487
timesteps_total: 1287000
training_iteration: 78

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22602739726027396
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.97
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 6838
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.84837102890015
time_total_s: 3842.4668757915497
timers:
  learn_throughput: 516.817
  learn_time_ms: 31926.167
  load_throughput: 5213809.065
  load_time_ms: 3.165
  training_iteration_time_ms: 42625.714
  update_time_ms: 2.481
timesteps_total: 1336500
training_iteration: 81

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28205128205128205
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9662162162162162
  reward for individual goal_min: 0.0
episode_len_mean: 179.71
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 5713
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.6692590713501
time_total_s: 3830.717437028885
timers:
  learn_throughput: 448.313
  learn_time_ms: 36804.622
  load_throughput: 4940181.6
  load_time_ms: 3.34
  training_iteration_time_ms: 49091.563
  update_time_ms: 2.582
timesteps_total: 1155000
training_iteration: 70

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2808219178082192
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9714285714285714
  reward for individual goal_min: 0.0
episode_len_mean: 173.87
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 5774
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.52043843269348
time_total_s: 3851.393563270569
timers:
  learn_throughput: 427.652
  learn_time_ms: 38582.755
  load_throughput: 4983223.837
  load_time_ms: 3.311
  training_iteration_time_ms: 50855.509
  update_time_ms: 2.704
timesteps_total: 1138500
training_iteration: 69

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.4090909090909091
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8733333333333333
  reward for individual goal_min: 0.0
episode_len_mean: 180.1
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 4995
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.75
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.61372423171997
time_total_s: 3842.138844013214
timers:
  learn_throughput: 431.832
  learn_time_ms: 38209.287
  load_throughput: 4696454.621
  load_time_ms: 3.513
  training_iteration_time_ms: 50559.768
  update_time_ms: 2.604
timesteps_total: 1105500
training_iteration: 67

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3765432098765432
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6597222222222222
  reward for individual goal_min: 0.0
episode_len_mean: 211.0
episode_reward_max: 2.0
episode_reward_mean: 1.03
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 5501
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.30469059944153
time_total_s: 3858.203046798706
timers:
  learn_throughput: 433.053
  learn_time_ms: 38101.539
  load_throughput: 4751169.908
  load_time_ms: 3.473
  training_iteration_time_ms: 50607.965
  update_time_ms: 2.605
timesteps_total: 1171500
training_iteration: 71

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 133.56666666666666
episode_reward_max: 2.0
episode_reward_mean: 1.5166666666666666
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8333333333333334
  agent_1: 0.6833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21341463414634146
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8157894736842105
  reward for individual goal_min: 0.0
episode_len_mean: 213.23
episode_reward_max: 2.0
episode_reward_mean: 1.02
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 6123
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.6962456703186
time_total_s: 3875.387643814087
timers:
  learn_throughput: 496.599
  learn_time_ms: 33226.005
  load_throughput: 4888811.529
  load_time_ms: 3.375
  training_iteration_time_ms: 44264.79
  update_time_ms: 2.607
timesteps_total: 1320000
training_iteration: 80

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-091_srff7m/checkpoint_000080/checkpoint-80
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.845
  reward for individual goal_min: 0.5
episode_len_mean: 78.44174757281553
episode_reward_max: 2.0
episode_reward_mean: 1.8495145631067962
episode_reward_min: 1.0
episodes_this_iter: 206
episodes_total: 11575
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8495145631067961
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 49.49940013885498
time_total_s: 3869.941772222519
timers:
  learn_throughput: 439.887
  learn_time_ms: 37509.623
  load_throughput: 4848634.583
  load_time_ms: 3.403
  training_iteration_time_ms: 49471.852
  update_time_ms: 2.59
timesteps_total: 1221000
training_iteration: 74

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.5
episode_len_mean: 51.71383647798742
episode_reward_max: 2.0
episode_reward_mean: 1.9874213836477987
episode_reward_min: 1.0
episodes_this_iter: 318
episodes_total: 14896
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9874213836477987
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.105695486068726
time_total_s: 3861.4440710544586
timers:
  learn_throughput: 482.802
  learn_time_ms: 34175.478
  load_throughput: 4971410.838
  load_time_ms: 3.319
  training_iteration_time_ms: 45539.803
  update_time_ms: 2.391
timesteps_total: 1419000
training_iteration: 86

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8247422680412371
  reward for individual goal_min: 0.5
episode_len_mean: 82.17085427135679
episode_reward_max: 2.0
episode_reward_mean: 1.829145728643216
episode_reward_min: 1.0
episodes_this_iter: 199
episodes_total: 10235
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8291457286432161
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 50.22392797470093
time_total_s: 3870.997622013092
timers:
  learn_throughput: 423.567
  learn_time_ms: 38954.922
  load_throughput: 4320623.314
  load_time_ms: 3.819
  training_iteration_time_ms: 51156.731
  update_time_ms: 2.689
timesteps_total: 1155000
training_iteration: 70

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7368421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 100.30952380952381
episode_reward_max: 2.0
episode_reward_mean: 1.7619047619047619
episode_reward_min: 1.0
episodes_this_iter: 168
episodes_total: 7371
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9107142857142857
  agent_1: 0.8511904761904762
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.503801107406616
time_total_s: 3887.71381688118
timers:
  learn_throughput: 383.338
  learn_time_ms: 43042.917
  load_throughput: 4293257.071
  load_time_ms: 3.843
  training_iteration_time_ms: 56959.893
  update_time_ms: 2.735
timesteps_total: 1056000
training_iteration: 64

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.5
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 5909
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.00164175033569
time_total_s: 3885.6343948841095
timers:
  learn_throughput: 433.705
  learn_time_ms: 38044.315
  load_throughput: 4827832.687
  load_time_ms: 3.418
  training_iteration_time_ms: 50248.902
  update_time_ms: 2.621
timesteps_total: 1188000
training_iteration: 72

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7658227848101266
  reward for individual goal_min: 0.0
episode_len_mean: 203.21
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 6158
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.19235634803772
time_total_s: 3887.821128129959
timers:
  learn_throughput: 476.587
  learn_time_ms: 34621.166
  load_throughput: 5056775.344
  load_time_ms: 3.263
  training_iteration_time_ms: 46210.51
  update_time_ms: 2.453
timesteps_total: 1303500
training_iteration: 79

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26119402985074625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9506172839506173
  reward for individual goal_min: 0.0
episode_len_mean: 170.25
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 5056
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.0347204208374
time_total_s: 3875.5860216617584
timers:
  learn_throughput: 414.521
  learn_time_ms: 39804.965
  load_throughput: 4487486.448
  load_time_ms: 3.677
  training_iteration_time_ms: 52406.225
  update_time_ms: 2.63
timesteps_total: 1039500
training_iteration: 63

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3283582089552239
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9375
  reward for individual goal_min: 0.0
episode_len_mean: 168.55
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 5194
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.314942836761475
time_total_s: 3880.1175169944763
timers:
  learn_throughput: 413.007
  learn_time_ms: 39950.901
  load_throughput: 4518779.774
  load_time_ms: 3.651
  training_iteration_time_ms: 52705.042
  update_time_ms: 2.717
timesteps_total: 1056000
training_iteration: 64

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2647058823529412
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9788732394366197
  reward for individual goal_min: 0.0
episode_len_mean: 169.39
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 6936
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.64278984069824
time_total_s: 3886.109665632248
timers:
  learn_throughput: 517.979
  learn_time_ms: 31854.553
  load_throughput: 5277181.681
  load_time_ms: 3.127
  training_iteration_time_ms: 42588.308
  update_time_ms: 2.498
timesteps_total: 1353000
training_iteration: 82

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28289473684210525
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9320987654320988
  reward for individual goal_min: 0.0
episode_len_mean: 189.41
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 5798
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.556626081466675
time_total_s: 3877.2740631103516
timers:
  learn_throughput: 452.262
  learn_time_ms: 36483.309
  load_throughput: 4923522.431
  load_time_ms: 3.351
  training_iteration_time_ms: 48696.232
  update_time_ms: 2.573
timesteps_total: 1171500
training_iteration: 71

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2808219178082192
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8441558441558441
  reward for individual goal_min: 0.0
episode_len_mean: 191.21
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 6210
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.97343921661377
time_total_s: 3918.3610830307007
timers:
  learn_throughput: 498.898
  learn_time_ms: 33072.9
  load_throughput: 4839378.488
  load_time_ms: 3.41
  training_iteration_time_ms: 44070.242
  update_time_ms: 2.626
timesteps_total: 1336500
training_iteration: 81

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3072289156626506
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9621212121212122
  reward for individual goal_min: 0.0
episode_len_mean: 178.32
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 5866
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.268497943878174
time_total_s: 3904.662061214447
timers:
  learn_throughput: 425.48
  learn_time_ms: 38779.703
  load_throughput: 4955853.486
  load_time_ms: 3.329
  training_iteration_time_ms: 51097.869
  update_time_ms: 2.709
timesteps_total: 1155000
training_iteration: 70

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3433734939759036
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.78
  reward for individual goal_min: 0.0
episode_len_mean: 207.91
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 5579
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.964529514312744
time_total_s: 3906.167576313019
timers:
  learn_throughput: 433.597
  learn_time_ms: 38053.808
  load_throughput: 4749506.973
  load_time_ms: 3.474
  training_iteration_time_ms: 50576.293
  update_time_ms: 2.595
timesteps_total: 1188000
training_iteration: 72

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3026315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8961038961038961
  reward for individual goal_min: 0.0
episode_len_mean: 186.29
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 5080
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.99667048454285
time_total_s: 3892.135514497757
timers:
  learn_throughput: 433.242
  learn_time_ms: 38084.937
  load_throughput: 4736763.949
  load_time_ms: 3.483
  training_iteration_time_ms: 50462.733
  update_time_ms: 2.566
timesteps_total: 1122000
training_iteration: 68

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9819277108433735
  reward for individual goal_min: 0.5
episode_len_mean: 52.98717948717949
episode_reward_max: 2.0
episode_reward_mean: 1.9807692307692308
episode_reward_min: 1.0
episodes_this_iter: 312
episodes_total: 15208
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9807692307692307
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.38338303565979
time_total_s: 3907.8274540901184
timers:
  learn_throughput: 480.322
  learn_time_ms: 34351.984
  load_throughput: 4935777.424
  load_time_ms: 3.343
  training_iteration_time_ms: 45736.965
  update_time_ms: 2.381
timesteps_total: 1435500
training_iteration: 87

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8155339805825242
  reward for individual goal_min: 0.5
episode_len_mean: 88.08900523560209
episode_reward_max: 2.0
episode_reward_mean: 1.801047120418848
episode_reward_min: 1.0
episodes_this_iter: 191
episodes_total: 11766
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8010471204188482
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.20858120918274
time_total_s: 3918.1503534317017
timers:
  learn_throughput: 440.705
  learn_time_ms: 37440.053
  load_throughput: 4853156.802
  load_time_ms: 3.4
  training_iteration_time_ms: 49375.25
  update_time_ms: 2.567
timesteps_total: 1237500
training_iteration: 75

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8852459016393442
  reward for individual goal_min: 0.5
episode_len_mean: 72.68722466960352
episode_reward_max: 2.0
episode_reward_mean: 1.8766519823788546
episode_reward_min: 1.0
episodes_this_iter: 227
episodes_total: 10462
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8766519823788547
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 53.61403203010559
time_total_s: 3924.6116540431976
timers:
  learn_throughput: 420.816
  learn_time_ms: 39209.514
  load_throughput: 4312304.328
  load_time_ms: 3.826
  training_iteration_time_ms: 51537.276
  update_time_ms: 2.658
timesteps_total: 1171500
training_iteration: 71

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24050632911392406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.02
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 7025
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.320252656936646
time_total_s: 3927.4299182891846
timers:
  learn_throughput: 520.377
  learn_time_ms: 31707.803
  load_throughput: 5319038.967
  load_time_ms: 3.102
  training_iteration_time_ms: 42508.449
  update_time_ms: 2.492
timesteps_total: 1369500
training_iteration: 83

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7727272727272727
  reward for individual goal_min: 0.5
episode_len_mean: 99.67484662576688
episode_reward_max: 2.0
episode_reward_mean: 1.7852760736196318
episode_reward_min: 1.0
episodes_this_iter: 163
episodes_total: 7534
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9447852760736196
  agent_1: 0.8404907975460123
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.568257331848145
time_total_s: 3945.282074213028
timers:
  learn_throughput: 383.101
  learn_time_ms: 43069.568
  load_throughput: 4458633.406
  load_time_ms: 3.701
  training_iteration_time_ms: 57013.121
  update_time_ms: 2.784
timesteps_total: 1072500
training_iteration: 65

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9857142857142858
  reward for individual goal_min: 0.0
episode_len_mean: 193.76
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 5994
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.67500162124634
time_total_s: 3937.309396505356
timers:
  learn_throughput: 431.063
  learn_time_ms: 38277.465
  load_throughput: 4795283.846
  load_time_ms: 3.441
  training_iteration_time_ms: 50519.343
  update_time_ms: 2.62
timesteps_total: 1204500
training_iteration: 73

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29577464788732394
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9662162162162162
  reward for individual goal_min: 0.0
episode_len_mean: 173.14
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 5284
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.62976789474487
time_total_s: 3933.747284889221
timers:
  learn_throughput: 413.336
  learn_time_ms: 39919.061
  load_throughput: 4580480.114
  load_time_ms: 3.602
  training_iteration_time_ms: 52698.147
  update_time_ms: 2.755
timesteps_total: 1072500
training_iteration: 65

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2692307692307692
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.958904109589041
  reward for individual goal_min: 0.0
episode_len_mean: 182.92
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 5142
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.37591886520386
time_total_s: 3929.9619405269623
timers:
  learn_throughput: 413.289
  learn_time_ms: 39923.647
  load_throughput: 4496291.272
  load_time_ms: 3.67
  training_iteration_time_ms: 52537.368
  update_time_ms: 2.634
timesteps_total: 1056000
training_iteration: 64

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9797297297297297
  reward for individual goal_min: 0.0
episode_len_mean: 178.72
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 5892
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.53468465805054
time_total_s: 3924.808747768402
timers:
  learn_throughput: 456.254
  learn_time_ms: 36164.03
  load_throughput: 4939511.659
  load_time_ms: 3.34
  training_iteration_time_ms: 48275.537
  update_time_ms: 2.55
timesteps_total: 1188000
training_iteration: 72

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7166666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 134.95
episode_reward_max: 2.0
episode_reward_mean: 1.65
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.85
  agent_1: 0.8
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3356164383561644
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7051282051282052
  reward for individual goal_min: 0.0
episode_len_mean: 200.19
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 6241
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.318763732910156
time_total_s: 3945.1398918628693
timers:
  learn_throughput: 479.357
  learn_time_ms: 34421.1
  load_throughput: 5046671.528
  load_time_ms: 3.269
  training_iteration_time_ms: 46006.78
  update_time_ms: 2.466
timesteps_total: 1320000
training_iteration: 80

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2974683544303797
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8209876543209876
  reward for individual goal_min: 0.0
episode_len_mean: 196.24
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 6290
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.46167325973511
time_total_s: 3960.822756290436
timers:
  learn_throughput: 501.992
  learn_time_ms: 32869.036
  load_throughput: 4825846.437
  load_time_ms: 3.419
  training_iteration_time_ms: 43834.646
  update_time_ms: 2.612
timesteps_total: 1353000
training_iteration: 82

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.37349397590361444
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7132352941176471
  reward for individual goal_min: 0.0
episode_len_mean: 208.16
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 5660
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.66052055358887
time_total_s: 3954.8280968666077
timers:
  learn_throughput: 435.107
  learn_time_ms: 37921.683
  load_throughput: 4782360.429
  load_time_ms: 3.45
  training_iteration_time_ms: 50416.186
  update_time_ms: 2.573
timesteps_total: 1204500
training_iteration: 73

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 179.64
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 5960
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.300501585006714
time_total_s: 3955.9625627994537
timers:
  learn_throughput: 426.983
  learn_time_ms: 38643.2
  load_throughput: 4931099.996
  load_time_ms: 3.346
  training_iteration_time_ms: 50952.679
  update_time_ms: 2.737
timesteps_total: 1171500
training_iteration: 71

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9266666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 196.08
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 5166
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.71077752113342
time_total_s: 3940.8462920188904
timers:
  learn_throughput: 435.95
  learn_time_ms: 37848.348
  load_throughput: 4748855.159
  load_time_ms: 3.475
  training_iteration_time_ms: 50178.865
  update_time_ms: 2.589
timesteps_total: 1138500
training_iteration: 69

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9756944444444444
  reward for individual goal_min: 0.5
episode_len_mean: 55.66329966329966
episode_reward_max: 2.0
episode_reward_mean: 1.9764309764309764
episode_reward_min: 1.0
episodes_this_iter: 297
episodes_total: 15505
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9764309764309764
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.51820969581604
time_total_s: 3954.3456637859344
timers:
  learn_throughput: 478.073
  learn_time_ms: 34513.548
  load_throughput: 4903185.802
  load_time_ms: 3.365
  training_iteration_time_ms: 45903.42
  update_time_ms: 2.401
timesteps_total: 1452000
training_iteration: 88

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.806930693069307
  reward for individual goal_min: 0.5
episode_len_mean: 85.84375
episode_reward_max: 2.0
episode_reward_mean: 1.796875
episode_reward_min: 1.0
episodes_this_iter: 192
episodes_total: 11958
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.796875
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.54401135444641
time_total_s: 3964.694364786148
timers:
  learn_throughput: 441.879
  learn_time_ms: 37340.551
  load_throughput: 4873457.178
  load_time_ms: 3.386
  training_iteration_time_ms: 49207.364
  update_time_ms: 2.571
timesteps_total: 1254000
training_iteration: 76

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 162.16
episode_reward_max: 2.0
episode_reward_mean: 1.41
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 7123
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.44139766693115
time_total_s: 3968.8713159561157
timers:
  learn_throughput: 525.021
  learn_time_ms: 31427.343
  load_throughput: 5344176.435
  load_time_ms: 3.087
  training_iteration_time_ms: 42177.816
  update_time_ms: 2.494
timesteps_total: 1386000
training_iteration: 84

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8975409836065574
  reward for individual goal_min: 0.5
episode_len_mean: 69.8936170212766
episode_reward_max: 2.0
episode_reward_mean: 1.8936170212765957
episode_reward_min: 1.0
episodes_this_iter: 235
episodes_total: 10697
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8936170212765957
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 53.208587884902954
time_total_s: 3977.8202419281006
timers:
  learn_throughput: 420.23
  learn_time_ms: 39264.167
  load_throughput: 4272793.93
  load_time_ms: 3.862
  training_iteration_time_ms: 51673.283
  update_time_ms: 2.662
timesteps_total: 1188000
training_iteration: 72

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2222222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9823529411764705
  reward for individual goal_min: 0.0
episode_len_mean: 173.84
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 6091
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.194366693496704
time_total_s: 3985.5037631988525
timers:
  learn_throughput: 433.01
  learn_time_ms: 38105.347
  load_throughput: 4747226.407
  load_time_ms: 3.476
  training_iteration_time_ms: 50248.591
  update_time_ms: 2.629
timesteps_total: 1221000
training_iteration: 74

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7285714285714285
  reward for individual goal_min: 0.0
episode_len_mean: 203.39
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 6372
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.98152136802673
time_total_s: 4003.8042776584625
timers:
  learn_throughput: 507.462
  learn_time_ms: 32514.774
  load_throughput: 4862124.114
  load_time_ms: 3.394
  training_iteration_time_ms: 43451.935
  update_time_ms: 2.6
timesteps_total: 1369500
training_iteration: 83

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2911392405063291
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6944444444444444
  reward for individual goal_min: 0.0
episode_len_mean: 212.9
episode_reward_max: 2.0
episode_reward_mean: 1.0
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 6318
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.06843113899231
time_total_s: 3991.2083230018616
timers:
  learn_throughput: 480.113
  learn_time_ms: 34366.93
  load_throughput: 5056258.11
  load_time_ms: 3.263
  training_iteration_time_ms: 45970.074
  update_time_ms: 2.469
timesteps_total: 1336500
training_iteration: 81

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2597402597402597
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9423076923076923
  reward for individual goal_min: 0.0
episode_len_mean: 186.74
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 5981
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.14050650596619
time_total_s: 3972.9492542743683
timers:
  learn_throughput: 458.382
  learn_time_ms: 35996.189
  load_throughput: 4921911.698
  load_time_ms: 3.352
  training_iteration_time_ms: 48110.785
  update_time_ms: 2.565
timesteps_total: 1204500
training_iteration: 73

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8048780487804879
  reward for individual goal_min: 0.5
episode_len_mean: 96.63583815028902
episode_reward_max: 2.0
episode_reward_mean: 1.8150289017341041
episode_reward_min: 1.0
episodes_this_iter: 173
episodes_total: 7707
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9653179190751445
  agent_1: 0.8497109826589595
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.66753029823303
time_total_s: 4003.949604511261
timers:
  learn_throughput: 382.815
  learn_time_ms: 43101.753
  load_throughput: 4410751.611
  load_time_ms: 3.741
  training_iteration_time_ms: 57056.774
  update_time_ms: 2.81
timesteps_total: 1089000
training_iteration: 66

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2565789473684211
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9938271604938271
  reward for individual goal_min: 0.5
episode_len_mean: 176.51
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 5377
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.383615016937256
time_total_s: 3988.1308999061584
timers:
  learn_throughput: 411.761
  learn_time_ms: 40071.779
  load_throughput: 4583270.926
  load_time_ms: 3.6
  training_iteration_time_ms: 52824.772
  update_time_ms: 2.747
timesteps_total: 1089000
training_iteration: 66

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.345679012345679
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9859154929577465
  reward for individual goal_min: 0.0
episode_len_mean: 173.29
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 5238
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.184027910232544
time_total_s: 3986.145968437195
timers:
  learn_throughput: 407.927
  learn_time_ms: 40448.451
  load_throughput: 4486468.251
  load_time_ms: 3.678
  training_iteration_time_ms: 53194.255
  update_time_ms: 2.655
timesteps_total: 1072500
training_iteration: 65

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.47530864197530864
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7183098591549296
  reward for individual goal_min: 0.0
episode_len_mean: 187.7
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 5746
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.65928077697754
time_total_s: 4003.487377643585
timers:
  learn_throughput: 434.491
  learn_time_ms: 37975.449
  load_throughput: 4815571.034
  load_time_ms: 3.426
  training_iteration_time_ms: 50480.533
  update_time_ms: 2.596
timesteps_total: 1221000
training_iteration: 74

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 179.48
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 6050
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.08577013015747
time_total_s: 4004.048332929611
timers:
  learn_throughput: 429.118
  learn_time_ms: 38450.933
  load_throughput: 4940428.466
  load_time_ms: 3.34
  training_iteration_time_ms: 50812.198
  update_time_ms: 2.71
timesteps_total: 1188000
training_iteration: 72

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25316455696202533
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9507042253521126
  reward for individual goal_min: 0.0
episode_len_mean: 201.38
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 5249
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.92551326751709
time_total_s: 3989.7718052864075
timers:
  learn_throughput: 439.308
  learn_time_ms: 37559.044
  load_throughput: 4721929.546
  load_time_ms: 3.494
  training_iteration_time_ms: 49859.418
  update_time_ms: 2.585
timesteps_total: 1155000
training_iteration: 70

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9791666666666666
  reward for individual goal_min: 0.5
episode_len_mean: 50.28086419753087
episode_reward_max: 2.0
episode_reward_mean: 1.9814814814814814
episode_reward_min: 1.0
episodes_this_iter: 324
episodes_total: 15829
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9876543209876543
  agent_1: 0.9938271604938271
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.65974950790405
time_total_s: 4001.0054132938385
timers:
  learn_throughput: 477.287
  learn_time_ms: 34570.387
  load_throughput: 4892647.951
  load_time_ms: 3.372
  training_iteration_time_ms: 46016.517
  update_time_ms: 2.421
timesteps_total: 1468500
training_iteration: 89

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3013698630136986
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 166.92079207920793
episode_reward_max: 2.0
episode_reward_mean: 1.3762376237623761
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 7224
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6732673267326733
  agent_1: 0.7029702970297029
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.457213163375854
time_total_s: 4009.3285291194916
timers:
  learn_throughput: 528.687
  learn_time_ms: 31209.402
  load_throughput: 5394498.09
  load_time_ms: 3.059
  training_iteration_time_ms: 41926.135
  update_time_ms: 2.508
timesteps_total: 1402500
training_iteration: 85

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8796296296296297
  reward for individual goal_min: 0.5
episode_len_mean: 67.95918367346938
episode_reward_max: 2.0
episode_reward_mean: 1.8938775510204082
episode_reward_min: 1.0
episodes_this_iter: 245
episodes_total: 12203
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8938775510204081
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 51.69647789001465
time_total_s: 4016.3908426761627
timers:
  learn_throughput: 440.177
  learn_time_ms: 37484.935
  load_throughput: 4883395.499
  load_time_ms: 3.379
  training_iteration_time_ms: 49316.689
  update_time_ms: 2.582
timesteps_total: 1270500
training_iteration: 77

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3698630136986301
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.825
  reward for individual goal_min: 0.0
episode_len_mean: 181.52
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 6459
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.73273539543152
time_total_s: 4047.537013053894
timers:
  learn_throughput: 509.088
  learn_time_ms: 32410.882
  load_throughput: 4890296.996
  load_time_ms: 3.374
  training_iteration_time_ms: 43340.512
  update_time_ms: 2.585
timesteps_total: 1386000
training_iteration: 84

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8446601941747572
  reward for individual goal_min: 0.5
episode_len_mean: 80.66666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.8431372549019607
episode_reward_min: 1.0
episodes_this_iter: 204
episodes_total: 10901
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8431372549019608
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 51.798330545425415
time_total_s: 4029.618572473526
timers:
  learn_throughput: 420.375
  learn_time_ms: 39250.684
  load_throughput: 4250043.971
  load_time_ms: 3.882
  training_iteration_time_ms: 51696.879
  update_time_ms: 2.658
timesteps_total: 1204500
training_iteration: 73

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2222222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 188.35
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 6181
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.24122977256775
time_total_s: 4034.7449929714203
timers:
  learn_throughput: 433.834
  learn_time_ms: 38032.967
  load_throughput: 4771018.993
  load_time_ms: 3.458
  training_iteration_time_ms: 50177.816
  update_time_ms: 2.663
timesteps_total: 1237500
training_iteration: 75

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25675675675675674
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7746478873239436
  reward for individual goal_min: 0.0
episode_len_mean: 204.19
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 6401
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.28603529930115
time_total_s: 4036.4943583011627
timers:
  learn_throughput: 479.144
  learn_time_ms: 34436.39
  load_throughput: 5086508.401
  load_time_ms: 3.244
  training_iteration_time_ms: 45996.303
  update_time_ms: 2.446
timesteps_total: 1353000
training_iteration: 82

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.40588235294117647
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9411764705882353
  reward for individual goal_min: 0.0
episode_len_mean: 172.72
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 6076
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.676069021224976
time_total_s: 4021.6253232955933
timers:
  learn_throughput: 460.397
  learn_time_ms: 35838.62
  load_throughput: 4931873.094
  load_time_ms: 3.346
  training_iteration_time_ms: 47860.616
  update_time_ms: 2.573
timesteps_total: 1221000
training_iteration: 74

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.5
episode_len_mean: 189.7
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 5465
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.643083810806274
time_total_s: 4039.7739837169647
timers:
  learn_throughput: 411.428
  learn_time_ms: 40104.262
  load_throughput: 4559146.223
  load_time_ms: 3.619
  training_iteration_time_ms: 52846.089
  update_time_ms: 2.782
timesteps_total: 1105500
training_iteration: 67

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.5
episode_len_mean: 170.75
episode_reward_max: 2.0
episode_reward_mean: 1.41
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 5332
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.1186089515686
time_total_s: 4040.2645773887634
timers:
  learn_throughput: 406.822
  learn_time_ms: 40558.316
  load_throughput: 4389072.413
  load_time_ms: 3.759
  training_iteration_time_ms: 53397.553
  update_time_ms: 2.687
timesteps_total: 1089000
training_iteration: 66

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7944444444444444
  reward for individual goal_min: 0.5
episode_len_mean: 103.93081761006289
episode_reward_max: 2.0
episode_reward_mean: 1.7672955974842768
episode_reward_min: 1.0
episodes_this_iter: 159
episodes_total: 7866
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9433962264150944
  agent_1: 0.8238993710691824
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.48618984222412
time_total_s: 4062.435794353485
timers:
  learn_throughput: 381.338
  learn_time_ms: 43268.706
  load_throughput: 4506832.337
  load_time_ms: 3.661
  training_iteration_time_ms: 57248.706
  update_time_ms: 2.828
timesteps_total: 1105500
training_iteration: 67

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.45454545454545453
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6575342465753424
  reward for individual goal_min: 0.0
episode_len_mean: 195.52
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 5833
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.31506681442261
time_total_s: 4053.802444458008
timers:
  learn_throughput: 433.682
  learn_time_ms: 38046.305
  load_throughput: 4840630.906
  load_time_ms: 3.409
  training_iteration_time_ms: 50497.354
  update_time_ms: 2.613
timesteps_total: 1237500
training_iteration: 75

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28169014084507044
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9791666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 171.14
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 6144
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.013039350509644
time_total_s: 4054.061372280121
timers:
  learn_throughput: 434.488
  learn_time_ms: 37975.756
  load_throughput: 4914187.845
  load_time_ms: 3.358
  training_iteration_time_ms: 50407.221
  update_time_ms: 2.699
timesteps_total: 1204500
training_iteration: 73

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28169014084507044
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.0
episode_len_mean: 167.4
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 7320
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.11426568031311
time_total_s: 4050.4427947998047
timers:
  learn_throughput: 531.655
  learn_time_ms: 31035.168
  load_throughput: 5435340.465
  load_time_ms: 3.036
  training_iteration_time_ms: 41751.261
  update_time_ms: 2.476
timesteps_total: 1419000
training_iteration: 86

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30357142857142855
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9682539682539683
  reward for individual goal_min: 0.0
episode_len_mean: 204.93
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 5331
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.5235493183136
time_total_s: 4039.295354604721
timers:
  learn_throughput: 442.627
  learn_time_ms: 37277.42
  load_throughput: 4780675.592
  load_time_ms: 3.451
  training_iteration_time_ms: 49512.799
  update_time_ms: 2.57
timesteps_total: 1171500
training_iteration: 71

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9693251533742331
  reward for individual goal_min: 0.5
episode_len_mean: 54.58169934640523
episode_reward_max: 2.0
episode_reward_mean: 1.9673202614379084
episode_reward_min: 1.0
episodes_this_iter: 306
episodes_total: 16135
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9705882352941176
  agent_1: 0.9967320261437909
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.96663188934326
time_total_s: 4045.9720451831818
timers:
  learn_throughput: 477.832
  learn_time_ms: 34530.943
  load_throughput: 4867766.93
  load_time_ms: 3.39
  training_iteration_time_ms: 45936.986
  update_time_ms: 2.41
timesteps_total: 1485000
training_iteration: 90

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8443396226415094
  reward for individual goal_min: 0.5
episode_len_mean: 79.4390243902439
episode_reward_max: 2.0
episode_reward_mean: 1.8390243902439025
episode_reward_min: 1.0
episodes_this_iter: 205
episodes_total: 12408
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8390243902439024
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 49.14906191825867
time_total_s: 4065.5399045944214
timers:
  learn_throughput: 440.361
  learn_time_ms: 37469.217
  load_throughput: 4837754.694
  load_time_ms: 3.411
  training_iteration_time_ms: 49219.251
  update_time_ms: 2.603
timesteps_total: 1287000
training_iteration: 78

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8266666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 201.01
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 6541
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.92694163322449
time_total_s: 4090.4639546871185
timers:
  learn_throughput: 509.271
  learn_time_ms: 32399.256
  load_throughput: 4898847.314
  load_time_ms: 3.368
  training_iteration_time_ms: 43318.388
  update_time_ms: 2.59
timesteps_total: 1402500
training_iteration: 85

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2647058823529412
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7676056338028169
  reward for individual goal_min: 0.0
episode_len_mean: 202.92
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 6485
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.688232421875
time_total_s: 4079.1825907230377
timers:
  learn_throughput: 481.83
  learn_time_ms: 34244.411
  load_throughput: 5065992.431
  load_time_ms: 3.257
  training_iteration_time_ms: 45747.791
  update_time_ms: 2.463
timesteps_total: 1369500
training_iteration: 83

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.864406779661017
  reward for individual goal_min: 0.5
episode_len_mean: 77.78971962616822
episode_reward_max: 2.0
episode_reward_mean: 1.8504672897196262
episode_reward_min: 1.0
episodes_this_iter: 214
episodes_total: 11115
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8504672897196262
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 51.934393644332886
time_total_s: 4081.552966117859
timers:
  learn_throughput: 421.235
  learn_time_ms: 39170.518
  load_throughput: 4246705.776
  load_time_ms: 3.885
  training_iteration_time_ms: 51655.975
  update_time_ms: 2.656
timesteps_total: 1221000
training_iteration: 74

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24050632911392406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9927536231884058
  reward for individual goal_min: 0.5
episode_len_mean: 190.2
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 6266
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.035980463027954
time_total_s: 4084.7809734344482
timers:
  learn_throughput: 436.21
  learn_time_ms: 37825.804
  load_throughput: 4777111.617
  load_time_ms: 3.454
  training_iteration_time_ms: 49966.8
  update_time_ms: 2.648
timesteps_total: 1254000
training_iteration: 76

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32098765432098764
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9577464788732394
  reward for individual goal_min: 0.0
episode_len_mean: 182.07
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 6163
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.283796310424805
time_total_s: 4068.909119606018
timers:
  learn_throughput: 461.369
  learn_time_ms: 35763.147
  load_throughput: 4967663.894
  load_time_ms: 3.321
  training_iteration_time_ms: 47712.037
  update_time_ms: 2.572
timesteps_total: 1237500
training_iteration: 75

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3355263157894737
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 167.2
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 7417
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.44702172279358
time_total_s: 4092.8898165225983
timers:
  learn_throughput: 532.09
  learn_time_ms: 31009.815
  load_throughput: 5394161.717
  load_time_ms: 3.059
  training_iteration_time_ms: 41769.716
  update_time_ms: 2.495
timesteps_total: 1435500
training_iteration: 87

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2112676056338028
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9642857142857143
  reward for individual goal_min: 0.0
episode_len_mean: 185.81
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 5421
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.574859380722046
time_total_s: 4084.870213985443
timers:
  learn_throughput: 446.355
  learn_time_ms: 36966.092
  load_throughput: 4780015.195
  load_time_ms: 3.452
  training_iteration_time_ms: 49079.665
  update_time_ms: 2.57
timesteps_total: 1188000
training_iteration: 72

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.974025974025974
  reward for individual goal_min: 0.0
episode_len_mean: 182.35
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 5555
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.88819646835327
time_total_s: 4095.662180185318
timers:
  learn_throughput: 408.335
  learn_time_ms: 40407.967
  load_throughput: 4548628.366
  load_time_ms: 3.627
  training_iteration_time_ms: 53288.15
  update_time_ms: 2.76
timesteps_total: 1122000
training_iteration: 68

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9825581395348837
  reward for individual goal_min: 0.5
episode_len_mean: 52.49523809523809
episode_reward_max: 2.0
episode_reward_mean: 1.980952380952381
episode_reward_min: 1.0
episodes_this_iter: 315
episodes_total: 16450
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9809523809523809
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 45.972094774246216
time_total_s: 4091.944139957428
timers:
  learn_throughput: 477.282
  learn_time_ms: 34570.753
  load_throughput: 4871398.927
  load_time_ms: 3.387
  training_iteration_time_ms: 45943.285
  update_time_ms: 2.395
timesteps_total: 1501500
training_iteration: 91

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.43243243243243246
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6111111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 195.49
episode_reward_max: 2.0
episode_reward_mean: 1.06
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 5918
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.89843034744263
time_total_s: 4106.70087480545
timers:
  learn_throughput: 432.441
  learn_time_ms: 38155.457
  load_throughput: 4837315.104
  load_time_ms: 3.411
  training_iteration_time_ms: 50622.03
  update_time_ms: 2.598
timesteps_total: 1254000
training_iteration: 76

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.975609756097561
  reward for individual goal_min: 0.0
episode_len_mean: 177.91
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 5427
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.17807102203369
time_total_s: 4095.442648410797
timers:
  learn_throughput: 406.071
  learn_time_ms: 40633.321
  load_throughput: 4405641.277
  load_time_ms: 3.745
  training_iteration_time_ms: 53445.137
  update_time_ms: 2.723
timesteps_total: 1105500
training_iteration: 67

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8176470588235294
  reward for individual goal_min: 0.5
episode_len_mean: 99.74545454545455
episode_reward_max: 2.0
episode_reward_mean: 1.812121212121212
episode_reward_min: 1.0
episodes_this_iter: 165
episodes_total: 8031
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9696969696969697
  agent_1: 0.8424242424242424
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.295092821121216
time_total_s: 4119.730887174606
timers:
  learn_throughput: 381.68
  learn_time_ms: 43229.932
  load_throughput: 4471683.908
  load_time_ms: 3.69
  training_iteration_time_ms: 57264.811
  update_time_ms: 2.831
timesteps_total: 1122000
training_iteration: 68

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20714285714285716
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 176.19
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 6239
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.41664934158325
time_total_s: 4110.478021621704
timers:
  learn_throughput: 426.074
  learn_time_ms: 38725.703
  load_throughput: 4886567.767
  load_time_ms: 3.377
  training_iteration_time_ms: 51220.724
  update_time_ms: 2.693
timesteps_total: 1221000
training_iteration: 74

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2635135135135135
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7875
  reward for individual goal_min: 0.0
episode_len_mean: 202.74
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 6626
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.4
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.50418758392334
time_total_s: 4131.968142271042
timers:
  learn_throughput: 512.287
  learn_time_ms: 32208.487
  load_throughput: 4854246.114
  load_time_ms: 3.399
  training_iteration_time_ms: 43065.779
  update_time_ms: 2.554
timesteps_total: 1419000
training_iteration: 86

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9924242424242424
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8640776699029126
  reward for individual goal_min: 0.5
episode_len_mean: 70.31489361702128
episode_reward_max: 2.0
episode_reward_mean: 1.872340425531915
episode_reward_min: 0.0
episodes_this_iter: 235
episodes_total: 12643
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8765957446808511
  agent_1: 0.9957446808510638
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.54120635986328
time_total_s: 4117.081110954285
timers:
  learn_throughput: 439.467
  learn_time_ms: 37545.5
  load_throughput: 4813561.378
  load_time_ms: 3.428
  training_iteration_time_ms: 49238.549
  update_time_ms: 2.612
timesteps_total: 1303500
training_iteration: 79

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.74
  reward for individual goal_min: 0.0
episode_len_mean: 204.77
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 6564
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.46896147727966
time_total_s: 4123.651552200317
timers:
  learn_throughput: 484.887
  learn_time_ms: 34028.566
  load_throughput: 5135310.8
  load_time_ms: 3.213
  training_iteration_time_ms: 45522.503
  update_time_ms: 2.5
timesteps_total: 1386000
training_iteration: 84

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9807692307692307
  reward for individual goal_min: 0.5
episode_len_mean: 184.4
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 6255
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.933079957962036
time_total_s: 4116.84219956398
timers:
  learn_throughput: 460.836
  learn_time_ms: 35804.459
  load_throughput: 4904749.539
  load_time_ms: 3.364
  training_iteration_time_ms: 47799.813
  update_time_ms: 2.564
timesteps_total: 1254000
training_iteration: 76

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2635135135135135
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9810126582278481
  reward for individual goal_min: 0.0
episode_len_mean: 174.12
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 6360
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.12237215042114
time_total_s: 4135.903345584869
timers:
  learn_throughput: 435.029
  learn_time_ms: 37928.471
  load_throughput: 4819125.53
  load_time_ms: 3.424
  training_iteration_time_ms: 50132.352
  update_time_ms: 2.646
timesteps_total: 1270500
training_iteration: 77

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.853448275862069
  reward for individual goal_min: 0.5
episode_len_mean: 79.92270531400966
episode_reward_max: 2.0
episode_reward_mean: 1.8357487922705313
episode_reward_min: 1.0
episodes_this_iter: 207
episodes_total: 11322
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9951690821256038
  agent_1: 0.8405797101449275
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.8572256565094
time_total_s: 4133.410191774368
timers:
  learn_throughput: 419.943
  learn_time_ms: 39291.006
  load_throughput: 4254956.47
  load_time_ms: 3.878
  training_iteration_time_ms: 51781.881
  update_time_ms: 2.634
timesteps_total: 1237500
training_iteration: 75

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3157894736842105
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.27
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 7514
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.833266735076904
time_total_s: 4136.723083257675
timers:
  learn_throughput: 529.375
  learn_time_ms: 31168.86
  load_throughput: 5401487.309
  load_time_ms: 3.055
  training_iteration_time_ms: 41972.939
  update_time_ms: 2.484
timesteps_total: 1452000
training_iteration: 88

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26973684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9652777777777778
  reward for individual goal_min: 0.0
episode_len_mean: 189.21
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 5507
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.36482238769531
time_total_s: 4133.235036373138
timers:
  learn_throughput: 446.831
  learn_time_ms: 36926.725
  load_throughput: 4763334.87
  load_time_ms: 3.464
  training_iteration_time_ms: 49010.942
  update_time_ms: 2.544
timesteps_total: 1204500
training_iteration: 73

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9748427672955975
  reward for individual goal_min: 0.5
episode_len_mean: 52.69453376205788
episode_reward_max: 2.0
episode_reward_mean: 1.9742765273311897
episode_reward_min: 1.0
episodes_this_iter: 311
episodes_total: 16761
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9742765273311897
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.28461265563965
time_total_s: 4138.228752613068
timers:
  learn_throughput: 475.432
  learn_time_ms: 34705.251
  load_throughput: 4871296.06
  load_time_ms: 3.387
  training_iteration_time_ms: 46082.423
  update_time_ms: 2.394
timesteps_total: 1518000
training_iteration: 92

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35542168674698793
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7318840579710145
  reward for individual goal_min: 0.0
episode_len_mean: 200.49
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 6001
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.49
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.270833015441895
time_total_s: 4157.971707820892
timers:
  learn_throughput: 432.835
  learn_time_ms: 38120.797
  load_throughput: 4851455.731
  load_time_ms: 3.401
  training_iteration_time_ms: 50563.743
  update_time_ms: 2.63
timesteps_total: 1270500
training_iteration: 77

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23846153846153847
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8987341772151899
  reward for individual goal_min: 0.0
episode_len_mean: 185.96
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 6715
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.02437901496887
time_total_s: 4172.992521286011
timers:
  learn_throughput: 515.879
  learn_time_ms: 31984.243
  load_throughput: 4900373.586
  load_time_ms: 3.367
  training_iteration_time_ms: 42757.246
  update_time_ms: 2.557
timesteps_total: 1435500
training_iteration: 87

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2654320987654321
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935897435897436
  reward for individual goal_min: 0.5
episode_len_mean: 187.85
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 6327
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.86415410041809
time_total_s: 4159.342175722122
timers:
  learn_throughput: 427.583
  learn_time_ms: 38589.039
  load_throughput: 4839344.647
  load_time_ms: 3.41
  training_iteration_time_ms: 51095.927
  update_time_ms: 2.69
timesteps_total: 1237500
training_iteration: 75

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.189873417721519
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9571428571428572
  reward for individual goal_min: 0.0
episode_len_mean: 198.61
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 5638
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.880945444107056
time_total_s: 4153.543125629425
timers:
  learn_throughput: 405.361
  learn_time_ms: 40704.488
  load_throughput: 4470759.511
  load_time_ms: 3.691
  training_iteration_time_ms: 53744.361
  update_time_ms: 2.775
timesteps_total: 1138500
training_iteration: 69

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9657534246575342
  reward for individual goal_min: 0.0
episode_len_mean: 187.05
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 5516
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.689918518066406
time_total_s: 4150.1325669288635
timers:
  learn_throughput: 405.021
  learn_time_ms: 40738.579
  load_throughput: 4379462.36
  load_time_ms: 3.768
  training_iteration_time_ms: 53588.963
  update_time_ms: 2.735
timesteps_total: 1122000
training_iteration: 68

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34057971014492755
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6893939393939394
  reward for individual goal_min: 0.0
episode_len_mean: 207.2
episode_reward_max: 2.0
episode_reward_mean: 1.04
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 6642
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.383535623550415
time_total_s: 4169.035087823868
timers:
  learn_throughput: 485.307
  learn_time_ms: 33999.077
  load_throughput: 5165090.605
  load_time_ms: 3.195
  training_iteration_time_ms: 45520.136
  update_time_ms: 2.497
timesteps_total: 1402500
training_iteration: 85

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8210526315789474
  reward for individual goal_min: 0.5
episode_len_mean: 89.9572192513369
episode_reward_max: 2.0
episode_reward_mean: 1.8181818181818181
episode_reward_min: 1.0
episodes_this_iter: 187
episodes_total: 8218
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9518716577540107
  agent_1: 0.8663101604278075
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.83885169029236
time_total_s: 4179.569738864899
timers:
  learn_throughput: 379.658
  learn_time_ms: 43460.171
  load_throughput: 4391579.054
  load_time_ms: 3.757
  training_iteration_time_ms: 57485.206
  update_time_ms: 2.893
timesteps_total: 1138500
training_iteration: 69

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 76.45
episode_reward_max: 2.0
episode_reward_mean: 1.8333333333333333
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8333333333333334
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7956989247311828
  reward for individual goal_min: 0.5
episode_len_mean: 85.13089005235602
episode_reward_max: 2.0
episode_reward_mean: 1.801047120418848
episode_reward_min: 1.0
episodes_this_iter: 191
episodes_total: 12834
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8010471204188482
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 55.752363204956055
time_total_s: 4172.833474159241
timers:
  learn_throughput: 440.367
  learn_time_ms: 37468.716
  load_throughput: 4818957.747
  load_time_ms: 3.424
  training_iteration_time_ms: 49192.189
  update_time_ms: 2.622
timesteps_total: 1320000
training_iteration: 80

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 180.0
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 7604
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.925445556640625
time_total_s: 4178.648528814316
timers:
  learn_throughput: 528.204
  learn_time_ms: 31237.942
  load_throughput: 5443848.75
  load_time_ms: 3.031
  training_iteration_time_ms: 42095.396
  update_time_ms: 2.494
timesteps_total: 1468500
training_iteration: 89

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9794520547945206
  reward for individual goal_min: 0.0
episode_len_mean: 192.28
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 6442
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.59096717834473
time_total_s: 4183.494312763214
timers:
  learn_throughput: 438.692
  learn_time_ms: 37611.789
  load_throughput: 4796978.998
  load_time_ms: 3.44
  training_iteration_time_ms: 49744.828
  update_time_ms: 2.64
timesteps_total: 1287000
training_iteration: 78

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3223684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9626865671641791
  reward for individual goal_min: 0.0
episode_len_mean: 168.14
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 6352
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.64175224304199
time_total_s: 4166.483951807022
timers:
  learn_throughput: 460.506
  learn_time_ms: 35830.152
  load_throughput: 4911223.583
  load_time_ms: 3.36
  training_iteration_time_ms: 47824.197
  update_time_ms: 2.546
timesteps_total: 1270500
training_iteration: 77

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8486238532110092
  reward for individual goal_min: 0.5
episode_len_mean: 77.6086956521739
episode_reward_max: 2.0
episode_reward_mean: 1.8405797101449275
episode_reward_min: 1.0
episodes_this_iter: 207
episodes_total: 11529
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8405797101449275
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 50.209299087524414
time_total_s: 4183.619490861893
timers:
  learn_throughput: 422.34
  learn_time_ms: 39068.074
  load_throughput: 4265367.609
  load_time_ms: 3.868
  training_iteration_time_ms: 51546.31
  update_time_ms: 2.66
timesteps_total: 1254000
training_iteration: 76

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27586206896551724
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9533333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 200.74
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 5588
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.206541299819946
time_total_s: 4182.441577672958
timers:
  learn_throughput: 447.962
  learn_time_ms: 36833.498
  load_throughput: 4826687.869
  load_time_ms: 3.418
  training_iteration_time_ms: 48840.212
  update_time_ms: 2.537
timesteps_total: 1221000
training_iteration: 74

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9836601307189542
  reward for individual goal_min: 0.5
episode_len_mean: 51.52336448598131
episode_reward_max: 2.0
episode_reward_mean: 1.9844236760124612
episode_reward_min: 1.0
episodes_this_iter: 321
episodes_total: 17082
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9906542056074766
  agent_1: 0.9937694704049844
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.762285232543945
time_total_s: 4184.991037845612
timers:
  learn_throughput: 474.467
  learn_time_ms: 34775.873
  load_throughput: 4838194.364
  load_time_ms: 3.41
  training_iteration_time_ms: 46139.73
  update_time_ms: 2.406
timesteps_total: 1534500
training_iteration: 93

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22916666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8589743589743589
  reward for individual goal_min: 0.0
episode_len_mean: 201.1
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 6793
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.26072692871094
time_total_s: 4215.253248214722
timers:
  learn_throughput: 517.617
  learn_time_ms: 31876.873
  load_throughput: 4904749.539
  load_time_ms: 3.364
  training_iteration_time_ms: 42620.082
  update_time_ms: 2.554
timesteps_total: 1452000
training_iteration: 88

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7876712328767124
  reward for individual goal_min: 0.0
episode_len_mean: 187.45
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 6091
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.62558960914612
time_total_s: 4206.597297430038
timers:
  learn_throughput: 434.132
  learn_time_ms: 38006.887
  load_throughput: 4853088.736
  load_time_ms: 3.4
  training_iteration_time_ms: 50429.037
  update_time_ms: 2.629
timesteps_total: 1287000
training_iteration: 78

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21951219512195122
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9794520547945206
  reward for individual goal_min: 0.0
episode_len_mean: 195.25
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 6412
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.60727882385254
time_total_s: 4207.949454545975
timers:
  learn_throughput: 428.917
  learn_time_ms: 38468.953
  load_throughput: 4870096.268
  load_time_ms: 3.388
  training_iteration_time_ms: 51067.421
  update_time_ms: 2.703
timesteps_total: 1254000
training_iteration: 76

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2077922077922078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 189.51
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 5724
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.669312953948975
time_total_s: 4206.212438583374
timers:
  learn_throughput: 405.339
  learn_time_ms: 40706.711
  load_throughput: 4424173.32
  load_time_ms: 3.73
  training_iteration_time_ms: 53821.578
  update_time_ms: 2.779
timesteps_total: 1155000
training_iteration: 70

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2948717948717949
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.5822784810126582
  reward for individual goal_min: 0.0
episode_len_mean: 214.06
episode_reward_max: 2.0
episode_reward_mean: 0.92
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 6719
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.44
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.62074136734009
time_total_s: 4215.655829191208
timers:
  learn_throughput: 487.584
  learn_time_ms: 33840.296
  load_throughput: 5139315.016
  load_time_ms: 3.211
  training_iteration_time_ms: 45318.936
  update_time_ms: 2.499
timesteps_total: 1419000
training_iteration: 86

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9857142857142858
  reward for individual goal_min: 0.0
episode_len_mean: 161.13725490196077
episode_reward_max: 2.0
episode_reward_mean: 1.4313725490196079
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 5618
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7156862745098039
  agent_1: 0.7156862745098039
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.407447814941406
time_total_s: 4205.540014743805
timers:
  learn_throughput: 402.607
  learn_time_ms: 40982.867
  load_throughput: 4391802.005
  load_time_ms: 3.757
  training_iteration_time_ms: 53896.356
  update_time_ms: 2.741
timesteps_total: 1138500
training_iteration: 69

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.831858407079646
  reward for individual goal_min: 0.5
episode_len_mean: 86.01546391752578
episode_reward_max: 2.0
episode_reward_mean: 1.8041237113402062
episode_reward_min: 1.0
episodes_this_iter: 194
episodes_total: 13028
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8041237113402062
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.069032192230225
time_total_s: 4220.902506351471
timers:
  learn_throughput: 439.789
  learn_time_ms: 37518.023
  load_throughput: 4808110.271
  load_time_ms: 3.432
  training_iteration_time_ms: 49170.841
  update_time_ms: 2.614
timesteps_total: 1336500
training_iteration: 81

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3433734939759036
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.55
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 7696
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.10690903663635
time_total_s: 4220.755437850952
timers:
  learn_throughput: 529.599
  learn_time_ms: 31155.664
  load_throughput: 5473384.108
  load_time_ms: 3.015
  training_iteration_time_ms: 41977.42
  update_time_ms: 2.488
timesteps_total: 1485000
training_iteration: 90

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 102.75316455696202
episode_reward_max: 2.0
episode_reward_mean: 1.7784810126582278
episode_reward_min: 1.0
episodes_this_iter: 158
episodes_total: 8376
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9683544303797469
  agent_1: 0.810126582278481
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.782705783843994
time_total_s: 4237.352444648743
timers:
  learn_throughput: 377.342
  learn_time_ms: 43726.877
  load_throughput: 4388710.58
  load_time_ms: 3.76
  training_iteration_time_ms: 57803.844
  update_time_ms: 2.864
timesteps_total: 1155000
training_iteration: 70

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21176470588235294
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9726027397260274
  reward for individual goal_min: 0.0
episode_len_mean: 200.75
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 6525
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.07467722892761
time_total_s: 4233.568989992142
timers:
  learn_throughput: 437.067
  learn_time_ms: 37751.61
  load_throughput: 4786759.811
  load_time_ms: 3.447
  training_iteration_time_ms: 49945.672
  update_time_ms: 2.626
timesteps_total: 1303500
training_iteration: 79

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25903614457831325
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9736842105263158
  reward for individual goal_min: 0.0
episode_len_mean: 193.61
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 6438
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.79395079612732
time_total_s: 4215.277902603149
timers:
  learn_throughput: 459.418
  learn_time_ms: 35914.988
  load_throughput: 4936129.469
  load_time_ms: 3.343
  training_iteration_time_ms: 47871.164
  update_time_ms: 2.54
timesteps_total: 1287000
training_iteration: 78

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8378378378378378
  reward for individual goal_min: 0.5
episode_len_mean: 87.85405405405406
episode_reward_max: 2.0
episode_reward_mean: 1.8054054054054054
episode_reward_min: 1.0
episodes_this_iter: 185
episodes_total: 11714
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8054054054054054
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 50.31655216217041
time_total_s: 4233.936043024063
timers:
  learn_throughput: 423.133
  learn_time_ms: 38994.822
  load_throughput: 4257574.132
  load_time_ms: 3.875
  training_iteration_time_ms: 51482.781
  update_time_ms: 2.606
timesteps_total: 1270500
training_iteration: 77

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24050632911392406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 206.15
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 6871
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.41
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.94112753868103
time_total_s: 4258.194375753403
timers:
  learn_throughput: 517.68
  learn_time_ms: 31872.993
  load_throughput: 4920546.901
  load_time_ms: 3.353
  training_iteration_time_ms: 42615.642
  update_time_ms: 2.541
timesteps_total: 1468500
training_iteration: 89

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9872611464968153
  reward for individual goal_min: 0.5
episode_len_mean: 50.561349693251536
episode_reward_max: 2.0
episode_reward_mean: 1.9877300613496933
episode_reward_min: 1.0
episodes_this_iter: 326
episodes_total: 17408
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.99079754601227
  agent_1: 0.9969325153374233
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.723183155059814
time_total_s: 4231.714221000671
timers:
  learn_throughput: 472.138
  learn_time_ms: 34947.383
  load_throughput: 4821912.433
  load_time_ms: 3.422
  training_iteration_time_ms: 46303.023
  update_time_ms: 2.411
timesteps_total: 1551000
training_iteration: 94

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2631578947368421
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9487179487179487
  reward for individual goal_min: 0.0
episode_len_mean: 188.75
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 5678
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.65748071670532
time_total_s: 4231.099058389664
timers:
  learn_throughput: 450.019
  learn_time_ms: 36665.079
  load_throughput: 4816073.71
  load_time_ms: 3.426
  training_iteration_time_ms: 48664.994
  update_time_ms: 2.569
timesteps_total: 1237500
training_iteration: 75

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3472222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6907894736842105
  reward for individual goal_min: 0.0
episode_len_mean: 199.77
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 6175
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.02964973449707
time_total_s: 4257.6269471645355
timers:
  learn_throughput: 434.467
  learn_time_ms: 37977.589
  load_throughput: 4870096.268
  load_time_ms: 3.388
  training_iteration_time_ms: 50450.242
  update_time_ms: 2.646
timesteps_total: 1303500
training_iteration: 79

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3402777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7115384615384616
  reward for individual goal_min: 0.0
episode_len_mean: 198.61
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 6803
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.29612684249878
time_total_s: 4260.951956033707
timers:
  learn_throughput: 489.151
  learn_time_ms: 33731.891
  load_throughput: 5185913.526
  load_time_ms: 3.182
  training_iteration_time_ms: 45149.393
  update_time_ms: 2.505
timesteps_total: 1435500
training_iteration: 87

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3223684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9817073170731707
  reward for individual goal_min: 0.0
episode_len_mean: 164.2621359223301
episode_reward_max: 2.0
episode_reward_mean: 1.4077669902912622
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 6515
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7087378640776699
  agent_1: 0.6990291262135923
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.8391535282135
time_total_s: 4260.788608074188
timers:
  learn_throughput: 425.987
  learn_time_ms: 38733.579
  load_throughput: 4910422.103
  load_time_ms: 3.36
  training_iteration_time_ms: 51341.314
  update_time_ms: 2.71
timesteps_total: 1270500
training_iteration: 77

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31097560975609756
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9802631578947368
  reward for individual goal_min: 0.0
episode_len_mean: 181.49
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 7785
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 39.47230839729309
time_total_s: 4260.227746248245
timers:
  learn_throughput: 533.34
  learn_time_ms: 30937.104
  load_throughput: 5559743.246
  load_time_ms: 2.968
  training_iteration_time_ms: 41739.794
  update_time_ms: 2.497
timesteps_total: 1501500
training_iteration: 91

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9683544303797469
  reward for individual goal_min: 0.0
episode_len_mean: 183.92
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 5813
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.62477350234985
time_total_s: 4259.837212085724
timers:
  learn_throughput: 403.73
  learn_time_ms: 40868.923
  load_throughput: 4408279.201
  load_time_ms: 3.743
  training_iteration_time_ms: 53983.897
  update_time_ms: 2.785
timesteps_total: 1171500
training_iteration: 71

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33783783783783783
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9802631578947368
  reward for individual goal_min: 0.5
episode_len_mean: 171.35
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 5714
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.03159236907959
time_total_s: 4259.5716071128845
timers:
  learn_throughput: 399.724
  learn_time_ms: 41278.454
  load_throughput: 4392638.274
  load_time_ms: 3.756
  training_iteration_time_ms: 54244.233
  update_time_ms: 2.76
timesteps_total: 1155000
training_iteration: 70

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8383838383838383
  reward for individual goal_min: 0.5
episode_len_mean: 76.8075117370892
episode_reward_max: 2.0
episode_reward_mean: 1.8497652582159625
episode_reward_min: 1.0
episodes_this_iter: 213
episodes_total: 13241
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8497652582159625
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 50.82197141647339
time_total_s: 4271.724477767944
timers:
  learn_throughput: 437.838
  learn_time_ms: 37685.13
  load_throughput: 4780543.498
  load_time_ms: 3.451
  training_iteration_time_ms: 49259.036
  update_time_ms: 2.628
timesteps_total: 1353000
training_iteration: 82

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2571428571428571
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9513888888888888
  reward for individual goal_min: 0.0
episode_len_mean: 182.0
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 6528
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.91295409202576
time_total_s: 4263.190856695175
timers:
  learn_throughput: 458.907
  learn_time_ms: 35954.965
  load_throughput: 4894793.439
  load_time_ms: 3.371
  training_iteration_time_ms: 47877.274
  update_time_ms: 2.547
timesteps_total: 1303500
training_iteration: 79

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7954545454545454
  reward for individual goal_min: 0.5
episode_len_mean: 101.4074074074074
episode_reward_max: 2.0
episode_reward_mean: 1.7777777777777777
episode_reward_min: 1.0
episodes_this_iter: 162
episodes_total: 8538
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9814814814814815
  agent_1: 0.7962962962962963
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.04037046432495
time_total_s: 4295.392815113068
timers:
  learn_throughput: 376.634
  learn_time_ms: 43809.075
  load_throughput: 4129483.621
  load_time_ms: 3.996
  training_iteration_time_ms: 57925.369
  update_time_ms: 2.914
timesteps_total: 1171500
training_iteration: 71

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22839506172839505
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7887323943661971
  reward for individual goal_min: 0.0
episode_len_mean: 214.17
episode_reward_max: 2.0
episode_reward_mean: 1.01
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 6950
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.4
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.119362115859985
time_total_s: 4301.313737869263
timers:
  learn_throughput: 518.999
  learn_time_ms: 31791.971
  load_throughput: 4919392.664
  load_time_ms: 3.354
  training_iteration_time_ms: 42556.71
  update_time_ms: 2.517
timesteps_total: 1485000
training_iteration: 90

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8425925925925926
  reward for individual goal_min: 0.5
episode_len_mean: 76.73991031390135
episode_reward_max: 2.0
episode_reward_mean: 1.8475336322869955
episode_reward_min: 1.0
episodes_this_iter: 223
episodes_total: 11937
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9955156950672646
  agent_1: 0.852017937219731
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.583054304122925
time_total_s: 4285.519097328186
timers:
  learn_throughput: 421.498
  learn_time_ms: 39146.074
  load_throughput: 4303562.318
  load_time_ms: 3.834
  training_iteration_time_ms: 51663.955
  update_time_ms: 2.62
timesteps_total: 1287000
training_iteration: 78

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9932432432432432
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9745222929936306
  reward for individual goal_min: 0.5
episode_len_mean: 54.059016393442626
episode_reward_max: 2.0
episode_reward_mean: 1.9672131147540983
episode_reward_min: 0.0
episodes_this_iter: 305
episodes_total: 17713
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9737704918032787
  agent_1: 0.9934426229508196
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.18203830718994
time_total_s: 4277.896259307861
timers:
  learn_throughput: 472.444
  learn_time_ms: 34924.805
  load_throughput: 4793855.532
  load_time_ms: 3.442
  training_iteration_time_ms: 46267.926
  update_time_ms: 2.403
timesteps_total: 1567500
training_iteration: 95

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9333333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 115.68333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.7666666666666666
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8833333333333333
  agent_1: 0.8833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24305555555555555
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9671052631578947
  reward for individual goal_min: 0.0
episode_len_mean: 177.65
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 6619
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.55646467208862
time_total_s: 4295.12545466423
timers:
  learn_throughput: 439.897
  learn_time_ms: 37508.801
  load_throughput: 4815705.071
  load_time_ms: 3.426
  training_iteration_time_ms: 49569.396
  update_time_ms: 2.628
timesteps_total: 1320000
training_iteration: 80

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-18pddilqzj/checkpoint_000080/checkpoint-80
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27631578947368424
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9625
  reward for individual goal_min: 0.0
episode_len_mean: 182.54
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 5771
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.79275894165039
time_total_s: 4279.891817331314
timers:
  learn_throughput: 450.912
  learn_time_ms: 36592.471
  load_throughput: 4809580.519
  load_time_ms: 3.431
  training_iteration_time_ms: 48599.737
  update_time_ms: 2.593
timesteps_total: 1254000
training_iteration: 76

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2926829268292683
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.75
  reward for individual goal_min: 0.0
episode_len_mean: 206.7
episode_reward_max: 2.0
episode_reward_mean: 1.04
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 6883
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.73300266265869
time_total_s: 4305.684958696365
timers:
  learn_throughput: 491.269
  learn_time_ms: 33586.511
  load_throughput: 5214084.036
  load_time_ms: 3.165
  training_iteration_time_ms: 44988.5
  update_time_ms: 2.516
timesteps_total: 1452000
training_iteration: 88

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3055555555555556
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 157.26666666666668
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 105
episodes_total: 7890
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6761904761904762
  agent_1: 0.7238095238095238
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.78522229194641
time_total_s: 4302.012968540192
timers:
  learn_throughput: 536.432
  learn_time_ms: 30758.797
  load_throughput: 5538785.415
  load_time_ms: 2.979
  training_iteration_time_ms: 41553.658
  update_time_ms: 2.481
timesteps_total: 1518000
training_iteration: 92

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.37681159420289856
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9878048780487805
  reward for individual goal_min: 0.0
episode_len_mean: 148.95454545454547
episode_reward_max: 2.0
episode_reward_mean: 1.5
episode_reward_min: 0.0
episodes_this_iter: 110
episodes_total: 6625
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7818181818181819
  agent_1: 0.7181818181818181
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.017122983932495
time_total_s: 4311.805731058121
timers:
  learn_throughput: 429.137
  learn_time_ms: 38449.243
  load_throughput: 4908471.768
  load_time_ms: 3.362
  training_iteration_time_ms: 51152.967
  update_time_ms: 2.732
timesteps_total: 1287000
training_iteration: 78

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7
  reward for individual goal_min: 0.5
episode_len_mean: 136.43333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.6666666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8333333333333334
  agent_1: 0.8333333333333334
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3402777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6363636363636364
  reward for individual goal_min: 0.0
episode_len_mean: 205.79
episode_reward_max: 2.0
episode_reward_mean: 1.03
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 6253
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.41860699653625
time_total_s: 4323.045554161072
timers:
  learn_throughput: 436.056
  learn_time_ms: 37839.212
  load_throughput: 4889329.612
  load_time_ms: 3.375
  training_iteration_time_ms: 50321.877
  update_time_ms: 2.648
timesteps_total: 1320000
training_iteration: 80

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19mcvnks1j/checkpoint_000080/checkpoint-80
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9930555555555556
  reward for individual goal_min: 0.5
episode_len_mean: 196.02
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 5897
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.05617094039917
time_total_s: 4315.893383026123
timers:
  learn_throughput: 401.308
  learn_time_ms: 41115.571
  load_throughput: 4352442.753
  load_time_ms: 3.791
  training_iteration_time_ms: 54340.433
  update_time_ms: 2.805
timesteps_total: 1188000
training_iteration: 72

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8672566371681416
  reward for individual goal_min: 0.5
episode_len_mean: 72.19111111111111
episode_reward_max: 2.0
episode_reward_mean: 1.8666666666666667
episode_reward_min: 1.0
episodes_this_iter: 225
episodes_total: 13466
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8666666666666667
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 51.578284740448
time_total_s: 4323.302762508392
timers:
  learn_throughput: 435.2
  learn_time_ms: 37913.611
  load_throughput: 4733588.87
  load_time_ms: 3.486
  training_iteration_time_ms: 49529.65
  update_time_ms: 2.646
timesteps_total: 1369500
training_iteration: 83

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2876712328767123
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9814814814814815
  reward for individual goal_min: 0.0
episode_len_mean: 175.38
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 5810
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.31990623474121
time_total_s: 4313.891513347626
timers:
  learn_throughput: 398.198
  learn_time_ms: 41436.635
  load_throughput: 4348941.829
  load_time_ms: 3.794
  training_iteration_time_ms: 54402.851
  update_time_ms: 2.801
timesteps_total: 1171500
training_iteration: 71

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2261904761904762
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8266666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 218.77
episode_reward_max: 2.0
episode_reward_mean: 1.06
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 7026
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.87140727043152
time_total_s: 4342.185145139694
timers:
  learn_throughput: 522.055
  learn_time_ms: 31605.877
  load_throughput: 4945335.639
  load_time_ms: 3.336
  training_iteration_time_ms: 42346.767
  update_time_ms: 2.481
timesteps_total: 1501500
training_iteration: 91

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.972972972972973
  reward for individual goal_min: 0.5
episode_len_mean: 53.95114006514658
episode_reward_max: 2.0
episode_reward_mean: 1.9739413680781759
episode_reward_min: 1.0
episodes_this_iter: 307
episodes_total: 18020
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9804560260586319
  agent_1: 0.993485342019544
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.98246932029724
time_total_s: 4323.878728628159
timers:
  learn_throughput: 473.661
  learn_time_ms: 34835.043
  load_throughput: 4757113.81
  load_time_ms: 3.468
  training_iteration_time_ms: 46155.991
  update_time_ms: 2.39
timesteps_total: 1584000
training_iteration: 96

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8761061946902655
  reward for individual goal_min: 0.5
episode_len_mean: 70.92672413793103
episode_reward_max: 2.0
episode_reward_mean: 1.8793103448275863
episode_reward_min: 1.0
episodes_this_iter: 232
episodes_total: 12169
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8793103448275862
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 52.41455006599426
time_total_s: 4337.93364739418
timers:
  learn_throughput: 421.8
  learn_time_ms: 39118.098
  load_throughput: 4274773.37
  load_time_ms: 3.86
  training_iteration_time_ms: 51651.843
  update_time_ms: 2.629
timesteps_total: 1303500
training_iteration: 79

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.85
  reward for individual goal_min: 0.5
episode_len_mean: 94.46666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.8166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8833333333333333
  agent_1: 0.9333333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29605263157894735
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.946969696969697
  reward for individual goal_min: 0.0
episode_len_mean: 182.45
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 6619
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.68505358695984
time_total_s: 4323.875910282135
timers:
  learn_throughput: 455.475
  learn_time_ms: 36225.932
  load_throughput: 4888051.871
  load_time_ms: 3.376
  training_iteration_time_ms: 48175.982
  update_time_ms: 2.569
timesteps_total: 1320000
training_iteration: 80

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-28met_xp4k/checkpoint_000080/checkpoint-80
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22151898734177214
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9941860465116279
  reward for individual goal_min: 0.5
episode_len_mean: 194.25
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 5856
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.75819659233093
time_total_s: 4327.650013923645
timers:
  learn_throughput: 451.31
  learn_time_ms: 36560.261
  load_throughput: 4795848.764
  load_time_ms: 3.44
  training_iteration_time_ms: 48514.36
  update_time_ms: 2.595
timesteps_total: 1270500
training_iteration: 77

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8
  reward for individual goal_min: 0.5
episode_len_mean: 93.0340909090909
episode_reward_max: 2.0
episode_reward_mean: 1.8068181818181819
episode_reward_min: 1.0
episodes_this_iter: 176
episodes_total: 8714
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9886363636363636
  agent_1: 0.8181818181818182
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.88054370880127
time_total_s: 4353.273358821869
timers:
  learn_throughput: 375.528
  learn_time_ms: 43938.105
  load_throughput: 4035053.874
  load_time_ms: 4.089
  training_iteration_time_ms: 58123.548
  update_time_ms: 2.88
timesteps_total: 1188000
training_iteration: 72

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2565789473684211
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.85
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 6714
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.668617725372314
time_total_s: 4343.794072389603
timers:
  learn_throughput: 440.623
  learn_time_ms: 37447.007
  load_throughput: 4826351.261
  load_time_ms: 3.419
  training_iteration_time_ms: 49516.598
  update_time_ms: 2.617
timesteps_total: 1336500
training_iteration: 81

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7823529411764706
  reward for individual goal_min: 0.0
episode_len_mean: 203.75
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 6963
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.97250986099243
time_total_s: 4347.657468557358
timers:
  learn_throughput: 494.707
  learn_time_ms: 33353.105
  load_throughput: 5252469.736
  load_time_ms: 3.141
  training_iteration_time_ms: 44666.352
  update_time_ms: 2.516
timesteps_total: 1468500
training_iteration: 89

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3219178082191781
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 164.01980198019803
episode_reward_max: 2.0
episode_reward_mean: 1.3663366336633664
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 7991
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6336633663366337
  agent_1: 0.7326732673267327
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.5704128742218
time_total_s: 4343.583381414413
timers:
  learn_throughput: 535.405
  learn_time_ms: 30817.767
  load_throughput: 5484401.404
  load_time_ms: 3.009
  training_iteration_time_ms: 41578.186
  update_time_ms: 2.493
timesteps_total: 1534500
training_iteration: 93

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 149.96330275229357
episode_reward_max: 2.0
episode_reward_mean: 1.4403669724770642
episode_reward_min: 0.0
episodes_this_iter: 109
episodes_total: 6734
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6788990825688074
  agent_1: 0.7614678899082569
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.69051718711853
time_total_s: 4365.496248245239
timers:
  learn_throughput: 427.261
  learn_time_ms: 38618.07
  load_throughput: 4899714.397
  load_time_ms: 3.368
  training_iteration_time_ms: 51369.42
  update_time_ms: 2.698
timesteps_total: 1303500
training_iteration: 79

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3157894736842105
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7972972972972973
  reward for individual goal_min: 0.0
episode_len_mean: 193.13
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 7111
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.58119082450867
time_total_s: 4385.766335964203
timers:
  learn_throughput: 521.067
  learn_time_ms: 31665.802
  load_throughput: 4942827.88
  load_time_ms: 3.338
  training_iteration_time_ms: 42458.78
  update_time_ms: 2.47
timesteps_total: 1518000
training_iteration: 92

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3561643835616438
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6266666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 210.92
episode_reward_max: 2.0
episode_reward_mean: 1.02
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 6334
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.07982277870178
time_total_s: 4374.125376939774
timers:
  learn_throughput: 437.486
  learn_time_ms: 37715.461
  load_throughput: 4863183.281
  load_time_ms: 3.393
  training_iteration_time_ms: 50199.3
  update_time_ms: 2.635
timesteps_total: 1336500
training_iteration: 81

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7789473684210526
  reward for individual goal_min: 0.5
episode_len_mean: 90.60326086956522
episode_reward_max: 2.0
episode_reward_mean: 1.7717391304347827
episode_reward_min: 1.0
episodes_this_iter: 184
episodes_total: 13650
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7717391304347826
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.4543354511261
time_total_s: 4371.757097959518
timers:
  learn_throughput: 435.747
  learn_time_ms: 37866.049
  load_throughput: 4766779.81
  load_time_ms: 3.461
  training_iteration_time_ms: 49437.079
  update_time_ms: 2.645
timesteps_total: 1386000
training_iteration: 84

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2808219178082192
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9802631578947368
  reward for individual goal_min: 0.0
episode_len_mean: 178.15
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 5900
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.76
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.39824414253235
time_total_s: 4369.289757490158
timers:
  learn_throughput: 396.34
  learn_time_ms: 41630.896
  load_throughput: 4301074.927
  load_time_ms: 3.836
  training_iteration_time_ms: 54634.723
  update_time_ms: 2.807
timesteps_total: 1188000
training_iteration: 72

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 185.99
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 5986
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.94744539260864
time_total_s: 4373.840828418732
timers:
  learn_throughput: 398.534
  learn_time_ms: 41401.702
  load_throughput: 4257495.555
  load_time_ms: 3.876
  training_iteration_time_ms: 54664.088
  update_time_ms: 2.85
timesteps_total: 1204500
training_iteration: 73

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9880239520958084
  reward for individual goal_min: 0.5
episode_len_mean: 50.26139817629179
episode_reward_max: 2.0
episode_reward_mean: 1.987841945288754
episode_reward_min: 1.0
episodes_this_iter: 329
episodes_total: 18349
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9878419452887538
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.55751347541809
time_total_s: 4370.436242103577
timers:
  learn_throughput: 473.228
  learn_time_ms: 34866.925
  load_throughput: 4758553.031
  load_time_ms: 3.467
  training_iteration_time_ms: 46173.805
  update_time_ms: 2.387
timesteps_total: 1600500
training_iteration: 97

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22972972972972974
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.89
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 8084
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.609432220458984
time_total_s: 4384.192813634872
timers:
  learn_throughput: 537.679
  learn_time_ms: 30687.434
  load_throughput: 5474726.367
  load_time_ms: 3.014
  training_iteration_time_ms: 41494.973
  update_time_ms: 2.491
timesteps_total: 1551000
training_iteration: 94

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2569444444444444
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7094594594594594
  reward for individual goal_min: 0.0
episode_len_mean: 208.34
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 7046
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.43
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.74522352218628
time_total_s: 4390.402692079544
timers:
  learn_throughput: 496.364
  learn_time_ms: 33241.759
  load_throughput: 5319815.821
  load_time_ms: 3.102
  training_iteration_time_ms: 44490.326
  update_time_ms: 2.519
timesteps_total: 1485000
training_iteration: 90

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19736842105263158
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 191.59
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 6798
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.47312688827515
time_total_s: 4391.267199277878
timers:
  learn_throughput: 443.528
  learn_time_ms: 37201.687
  load_throughput: 4831776.361
  load_time_ms: 3.415
  training_iteration_time_ms: 49163.868
  update_time_ms: 2.616
timesteps_total: 1353000
training_iteration: 82

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9533333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 180.6
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 6710
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.53790497779846
time_total_s: 4373.4138152599335
timers:
  learn_throughput: 452.633
  learn_time_ms: 36453.405
  load_throughput: 4906418.626
  load_time_ms: 3.363
  training_iteration_time_ms: 48474.244
  update_time_ms: 2.565
timesteps_total: 1336500
training_iteration: 81

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.972972972972973
  reward for individual goal_min: 0.0
episode_len_mean: 192.76
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 5938
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.702149391174316
time_total_s: 4376.352163314819
timers:
  learn_throughput: 453.173
  learn_time_ms: 36409.927
  load_throughput: 4757571.65
  load_time_ms: 3.468
  training_iteration_time_ms: 48384.912
  update_time_ms: 2.597
timesteps_total: 1287000
training_iteration: 78

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 77.1
episode_reward_max: 2.0
episode_reward_mean: 1.8333333333333333
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8333333333333334
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8413461538461539
  reward for individual goal_min: 0.5
episode_len_mean: 77.61722488038278
episode_reward_max: 2.0
episode_reward_mean: 1.8421052631578947
episode_reward_min: 1.0
episodes_this_iter: 209
episodes_total: 12378
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8421052631578947
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 58.4294695854187
time_total_s: 4396.363116979599
timers:
  learn_throughput: 421.177
  learn_time_ms: 39175.922
  load_throughput: 4314051.615
  load_time_ms: 3.825
  training_iteration_time_ms: 51758.421
  update_time_ms: 2.627
timesteps_total: 1320000
training_iteration: 80

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-184s1jdk42/checkpoint_000080/checkpoint-80
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8368421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 88.75
episode_reward_max: 2.0
episode_reward_mean: 1.8351063829787233
episode_reward_min: 1.0
episodes_this_iter: 188
episodes_total: 8902
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9840425531914894
  agent_1: 0.851063829787234
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.778624057769775
time_total_s: 4412.051982879639
timers:
  learn_throughput: 375.289
  learn_time_ms: 43966.083
  load_throughput: 4036183.455
  load_time_ms: 4.088
  training_iteration_time_ms: 58228.719
  update_time_ms: 2.9
timesteps_total: 1204500
training_iteration: 73

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.273972602739726
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8733333333333333
  reward for individual goal_min: 0.0
episode_len_mean: 182.01
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 7202
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.17169547080994
time_total_s: 4426.938031435013
timers:
  learn_throughput: 523.044
  learn_time_ms: 31546.097
  load_throughput: 4935531.023
  load_time_ms: 3.343
  training_iteration_time_ms: 42277.765
  update_time_ms: 2.477
timesteps_total: 1534500
training_iteration: 93

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.37037037037037035
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7378048780487805
  reward for individual goal_min: 0.0
episode_len_mean: 204.17
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 6414
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.032289266586304
time_total_s: 4422.15766620636
timers:
  learn_throughput: 437.297
  learn_time_ms: 37731.78
  load_throughput: 4872187.718
  load_time_ms: 3.387
  training_iteration_time_ms: 50205.9
  update_time_ms: 2.665
timesteps_total: 1353000
training_iteration: 82

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8317757009345794
  reward for individual goal_min: 0.5
episode_len_mean: 78.77619047619048
episode_reward_max: 2.0
episode_reward_mean: 1.8285714285714285
episode_reward_min: 1.0
episodes_this_iter: 210
episodes_total: 13860
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8285714285714286
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.72333645820618
time_total_s: 4419.480434417725
timers:
  learn_throughput: 436.145
  learn_time_ms: 37831.417
  load_throughput: 4768684.867
  load_time_ms: 3.46
  training_iteration_time_ms: 49388.009
  update_time_ms: 2.655
timesteps_total: 1402500
training_iteration: 85

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 73.73333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.9166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9666666666666667
  agent_1: 0.95
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3194444444444444
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.975
  reward for individual goal_min: 0.0
episode_len_mean: 163.72815533980582
episode_reward_max: 2.0
episode_reward_mean: 1.3980582524271845
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 6837
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6504854368932039
  agent_1: 0.7475728155339806
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.99657869338989
time_total_s: 4425.492826938629
timers:
  learn_throughput: 430.534
  learn_time_ms: 38324.538
  load_throughput: 4912443.728
  load_time_ms: 3.359
  training_iteration_time_ms: 51042.867
  update_time_ms: 2.684
timesteps_total: 1320000
training_iteration: 80

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19igysrqn6/checkpoint_000080/checkpoint-80
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16071428571428573
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.99375
  reward for individual goal_min: 0.5
episode_len_mean: 203.45
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 8165
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 39.89163875579834
time_total_s: 4424.084452390671
timers:
  learn_throughput: 538.826
  learn_time_ms: 30622.132
  load_throughput: 5451911.233
  load_time_ms: 3.026
  training_iteration_time_ms: 41438.554
  update_time_ms: 2.468
timesteps_total: 1567500
training_iteration: 95

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.775
  reward for individual goal_min: 0.0
episode_len_mean: 196.63
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 7125
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.309995889663696
time_total_s: 4432.712687969208
timers:
  learn_throughput: 501.305
  learn_time_ms: 32914.076
  load_throughput: 5295392.644
  load_time_ms: 3.116
  training_iteration_time_ms: 44114.566
  update_time_ms: 2.517
timesteps_total: 1501500
training_iteration: 91

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31756756756756754
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9662162162162162
  reward for individual goal_min: 0.0
episode_len_mean: 173.25
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 6083
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.37446880340576
time_total_s: 4425.215297222137
timers:
  learn_throughput: 400.633
  learn_time_ms: 41184.839
  load_throughput: 4280722.711
  load_time_ms: 3.854
  training_iteration_time_ms: 54470.403
  update_time_ms: 2.866
timesteps_total: 1221000
training_iteration: 74

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 162.32
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 6000
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.76
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.752076864242554
time_total_s: 4421.041834354401
timers:
  learn_throughput: 398.215
  learn_time_ms: 41434.91
  load_throughput: 4322512.336
  load_time_ms: 3.817
  training_iteration_time_ms: 54505.954
  update_time_ms: 2.817
timesteps_total: 1204500
training_iteration: 73

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9818181818181818
  reward for individual goal_min: 0.5
episode_len_mean: 52.06962025316456
episode_reward_max: 2.0
episode_reward_mean: 1.981012658227848
episode_reward_min: 1.0
episodes_this_iter: 316
episodes_total: 18665
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9841772151898734
  agent_1: 0.9968354430379747
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.372050523757935
time_total_s: 4416.808292627335
timers:
  learn_throughput: 472.817
  learn_time_ms: 34897.235
  load_throughput: 4701974.794
  load_time_ms: 3.509
  training_iteration_time_ms: 46158.411
  update_time_ms: 2.371
timesteps_total: 1617000
training_iteration: 98

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2532467532467532
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9539473684210527
  reward for individual goal_min: 0.0
episode_len_mean: 175.9
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 6806
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.71932649612427
time_total_s: 4421.133141756058
timers:
  learn_throughput: 452.586
  learn_time_ms: 36457.186
  load_throughput: 4857721.562
  load_time_ms: 3.397
  training_iteration_time_ms: 48492.368
  update_time_ms: 2.557
timesteps_total: 1353000
training_iteration: 82

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25675675675675674
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 181.4
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 6886
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.55103015899658
time_total_s: 4439.818229436874
timers:
  learn_throughput: 446.411
  learn_time_ms: 36961.459
  load_throughput: 4847344.068
  load_time_ms: 3.404
  training_iteration_time_ms: 48851.462
  update_time_ms: 2.606
timesteps_total: 1369500
training_iteration: 83

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.17
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 6033
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.815826654434204
time_total_s: 4424.1679899692535
timers:
  learn_throughput: 454.444
  learn_time_ms: 36308.075
  load_throughput: 4814699.977
  load_time_ms: 3.427
  training_iteration_time_ms: 48295.414
  update_time_ms: 2.585
timesteps_total: 1303500
training_iteration: 79

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8388888888888889
  reward for individual goal_min: 0.5
episode_len_mean: 77.49056603773585
episode_reward_max: 2.0
episode_reward_mean: 1.8632075471698113
episode_reward_min: 1.0
episodes_this_iter: 212
episodes_total: 12590
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8632075471698113
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 52.333921909332275
time_total_s: 4448.697038888931
timers:
  learn_throughput: 422.056
  learn_time_ms: 39094.335
  load_throughput: 4304258.233
  load_time_ms: 3.833
  training_iteration_time_ms: 51630.675
  update_time_ms: 2.638
timesteps_total: 1336500
training_iteration: 81

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8486842105263158
  reward for individual goal_min: 0.0
episode_len_mean: 189.39
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 7284
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.01574516296387
time_total_s: 4467.953776597977
timers:
  learn_throughput: 526.245
  learn_time_ms: 31354.213
  load_throughput: 4921561.678
  load_time_ms: 3.353
  training_iteration_time_ms: 42006.022
  update_time_ms: 2.469
timesteps_total: 1551000
training_iteration: 94

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8166666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 90.03804347826087
episode_reward_max: 2.0
episode_reward_mean: 1.8206521739130435
episode_reward_min: 1.0
episodes_this_iter: 184
episodes_total: 9086
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9891304347826086
  agent_1: 0.8315217391304348
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.72379279136658
time_total_s: 4471.775775671005
timers:
  learn_throughput: 374.573
  learn_time_ms: 44050.179
  load_throughput: 3980285.267
  load_time_ms: 4.145
  training_iteration_time_ms: 58349.941
  update_time_ms: 2.91
timesteps_total: 1221000
training_iteration: 74

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7692307692307693
  reward for individual goal_min: 0.5
episode_len_mean: 93.85227272727273
episode_reward_max: 2.0
episode_reward_mean: 1.7613636363636365
episode_reward_min: 1.0
episodes_this_iter: 176
episodes_total: 14036
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7613636363636364
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.14197063446045
time_total_s: 4467.622405052185
timers:
  learn_throughput: 435.195
  learn_time_ms: 37914.032
  load_throughput: 4751202.526
  load_time_ms: 3.473
  training_iteration_time_ms: 49548.316
  update_time_ms: 2.641
timesteps_total: 1419000
training_iteration: 86

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26973684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939024390243902
  reward for individual goal_min: 0.5
episode_len_mean: 176.0
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 8261
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.39383339881897
time_total_s: 4465.47828578949
timers:
  learn_throughput: 538.354
  learn_time_ms: 30648.95
  load_throughput: 5442521.587
  load_time_ms: 3.032
  training_iteration_time_ms: 41466.594
  update_time_ms: 2.463
timesteps_total: 1584000
training_iteration: 96

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.38414634146341464
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.676056338028169
  reward for individual goal_min: 0.0
episode_len_mean: 201.55
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 6493
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.24193716049194
time_total_s: 4474.399603366852
timers:
  learn_throughput: 434.092
  learn_time_ms: 38010.389
  load_throughput: 4812858.395
  load_time_ms: 3.428
  training_iteration_time_ms: 50563.945
  update_time_ms: 2.678
timesteps_total: 1369500
training_iteration: 83

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22388059701492538
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 168.99
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 6935
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.10196375846863
time_total_s: 4476.594790697098
timers:
  learn_throughput: 431.613
  learn_time_ms: 38228.72
  load_throughput: 4920162.095
  load_time_ms: 3.354
  training_iteration_time_ms: 51022.873
  update_time_ms: 2.681
timesteps_total: 1336500
training_iteration: 81

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2468354430379747
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6867469879518072
  reward for individual goal_min: 0.0
episode_len_mean: 205.57
episode_reward_max: 2.0
episode_reward_mean: 0.98
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 7205
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.452080965042114
time_total_s: 4479.16476893425
timers:
  learn_throughput: 500.797
  learn_time_ms: 32947.458
  load_throughput: 5311853.614
  load_time_ms: 3.106
  training_iteration_time_ms: 44231.144
  update_time_ms: 2.522
timesteps_total: 1518000
training_iteration: 92

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.99079754601227
  reward for individual goal_min: 0.5
episode_len_mean: 50.13109756097561
episode_reward_max: 2.0
episode_reward_mean: 1.9908536585365855
episode_reward_min: 1.0
episodes_this_iter: 328
episodes_total: 18993
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9939024390243902
  agent_1: 0.9969512195121951
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.510032415390015
time_total_s: 4463.318325042725
timers:
  learn_throughput: 472.77
  learn_time_ms: 34900.688
  load_throughput: 4689516.995
  load_time_ms: 3.518
  training_iteration_time_ms: 46142.676
  update_time_ms: 2.351
timesteps_total: 1633500
training_iteration: 99

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3918918918918919
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9807692307692307
  reward for individual goal_min: 0.0
episode_len_mean: 163.0
episode_reward_max: 2.0
episode_reward_mean: 1.45
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 6179
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.76
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.28350520133972
time_total_s: 4479.498802423477
timers:
  learn_throughput: 399.5
  learn_time_ms: 41301.616
  load_throughput: 4205442.05
  load_time_ms: 3.923
  training_iteration_time_ms: 54535.71
  update_time_ms: 2.848
timesteps_total: 1237500
training_iteration: 75

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.967948717948718
  reward for individual goal_min: 0.0
episode_len_mean: 169.67
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 6903
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.09199619293213
time_total_s: 4469.22513794899
timers:
  learn_throughput: 452.466
  learn_time_ms: 36466.852
  load_throughput: 4849382.038
  load_time_ms: 3.402
  training_iteration_time_ms: 48487.455
  update_time_ms: 2.539
timesteps_total: 1369500
training_iteration: 83

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24285714285714285
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.5
episode_len_mean: 179.5
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 6976
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.612208127975464
time_total_s: 4488.43043756485
timers:
  learn_throughput: 446.402
  learn_time_ms: 36962.214
  load_throughput: 4867253.406
  load_time_ms: 3.39
  training_iteration_time_ms: 48893.351
  update_time_ms: 2.598
timesteps_total: 1386000
training_iteration: 84

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3194444444444444
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9705882352941176
  reward for individual goal_min: 0.0
episode_len_mean: 170.75
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 6097
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.16474366188049
time_total_s: 4477.206578016281
timers:
  learn_throughput: 396.907
  learn_time_ms: 41571.483
  load_throughput: 4308411.629
  load_time_ms: 3.83
  training_iteration_time_ms: 54685.035
  update_time_ms: 2.834
timesteps_total: 1221000
training_iteration: 74

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2077922077922078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8402777777777778
  reward for individual goal_min: 0.0
episode_len_mean: 203.54
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 7367
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.64939546585083
time_total_s: 4510.6031720638275
timers:
  learn_throughput: 526.765
  learn_time_ms: 31323.242
  load_throughput: 4951775.615
  load_time_ms: 3.332
  training_iteration_time_ms: 41978.255
  update_time_ms: 2.453
timesteps_total: 1567500
training_iteration: 95

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9166666666666666
  reward for individual goal_min: 0.5
episode_len_mean: 109.75
episode_reward_max: 2.0
episode_reward_mean: 1.85
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9
  agent_1: 0.95
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.11428571428571428
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9880952380952381
  reward for individual goal_min: 0.5
episode_len_mean: 194.23
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 6118
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.804056882858276
time_total_s: 4485.972046852112
timers:
  learn_throughput: 454.663
  learn_time_ms: 36290.635
  load_throughput: 4827967.407
  load_time_ms: 3.418
  training_iteration_time_ms: 48219.267
  update_time_ms: 2.563
timesteps_total: 1320000
training_iteration: 80

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-28ftt3iad9/checkpoint_000080/checkpoint-80
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8620689655172413
  reward for individual goal_min: 0.5
episode_len_mean: 77.92626728110599
episode_reward_max: 2.0
episode_reward_mean: 1.8525345622119815
episode_reward_min: 1.0
episodes_this_iter: 217
episodes_total: 12807
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8525345622119815
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 51.11063551902771
time_total_s: 4499.807674407959
timers:
  learn_throughput: 423.649
  learn_time_ms: 38947.355
  load_throughput: 4295655.434
  load_time_ms: 3.841
  training_iteration_time_ms: 51421.381
  update_time_ms: 2.623
timesteps_total: 1353000
training_iteration: 82

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2662337662337662
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 179.09
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 8351
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.772024631500244
time_total_s: 4506.25031042099
timers:
  learn_throughput: 540.787
  learn_time_ms: 30511.11
  load_throughput: 5425157.057
  load_time_ms: 3.041
  training_iteration_time_ms: 41299.027
  update_time_ms: 2.455
timesteps_total: 1600500
training_iteration: 97

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8557692307692307
  reward for individual goal_min: 0.5
episode_len_mean: 69.5495867768595
episode_reward_max: 2.0
episode_reward_mean: 1.8760330578512396
episode_reward_min: 1.0
episodes_this_iter: 242
episodes_total: 14278
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8760330578512396
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 51.2122700214386
time_total_s: 4518.834675073624
timers:
  learn_throughput: 435.884
  learn_time_ms: 37854.104
  load_throughput: 4754172.66
  load_time_ms: 3.471
  training_iteration_time_ms: 49500.403
  update_time_ms: 2.647
timesteps_total: 1435500
training_iteration: 87

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8541666666666666
  reward for individual goal_min: 0.5
episode_len_mean: 83.77777777777777
episode_reward_max: 2.0
episode_reward_mean: 1.8585858585858586
episode_reward_min: 1.0
episodes_this_iter: 198
episodes_total: 9284
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9949494949494949
  agent_1: 0.8636363636363636
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.84693384170532
time_total_s: 4532.622709512711
timers:
  learn_throughput: 372.114
  learn_time_ms: 44341.287
  load_throughput: 3922754.305
  load_time_ms: 4.206
  training_iteration_time_ms: 58676.569
  update_time_ms: 2.9
timesteps_total: 1237500
training_iteration: 75

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36428571428571427
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7094594594594594
  reward for individual goal_min: 0.0
episode_len_mean: 192.07
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 7294
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.27657389640808
time_total_s: 4524.441342830658
timers:
  learn_throughput: 497.604
  learn_time_ms: 33158.911
  load_throughput: 5295797.859
  load_time_ms: 3.116
  training_iteration_time_ms: 44489.79
  update_time_ms: 2.486
timesteps_total: 1534500
training_iteration: 93

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3525641025641026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6666666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 207.0
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 6575
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.61825156211853
time_total_s: 4526.01785492897
timers:
  learn_throughput: 431.658
  learn_time_ms: 38224.684
  load_throughput: 4764745.053
  load_time_ms: 3.463
  training_iteration_time_ms: 50859.855
  update_time_ms: 2.671
timesteps_total: 1386000
training_iteration: 84

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2866666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9615384615384616
  reward for individual goal_min: 0.0
episode_len_mean: 172.05
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 7030
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.61611008644104
time_total_s: 4530.210900783539
timers:
  learn_throughput: 425.955
  learn_time_ms: 38736.454
  load_throughput: 4884084.772
  load_time_ms: 3.378
  training_iteration_time_ms: 51575.536
  update_time_ms: 2.698
timesteps_total: 1353000
training_iteration: 82

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 55.35
episode_reward_max: 2.0
episode_reward_mean: 1.9666666666666666
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9666666666666667
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9907407407407407
  reward for individual goal_min: 0.5
episode_len_mean: 48.72997032640949
episode_reward_max: 2.0
episode_reward_mean: 1.9910979228486647
episode_reward_min: 1.0
episodes_this_iter: 337
episodes_total: 19330
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9910979228486647
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 52.530139446258545
time_total_s: 4515.848464488983
timers:
  learn_throughput: 470.4
  learn_time_ms: 35076.547
  load_throughput: 4668889.549
  load_time_ms: 3.534
  training_iteration_time_ms: 46367.683
  update_time_ms: 2.346
timesteps_total: 1650000
training_iteration: 100

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-172kuwc_58/checkpoint_000100/checkpoint-100
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27205882352941174
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9382716049382716
  reward for individual goal_min: 0.0
episode_len_mean: 171.41
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 6999
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.72288799285889
time_total_s: 4516.948025941849
timers:
  learn_throughput: 453.566
  learn_time_ms: 36378.421
  load_throughput: 4845816.715
  load_time_ms: 3.405
  training_iteration_time_ms: 48391.935
  update_time_ms: 2.572
timesteps_total: 1386000
training_iteration: 84

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1987179487179487
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9722222222222222
  reward for individual goal_min: 0.0
episode_len_mean: 193.07
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 7061
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.10794973373413
time_total_s: 4538.538387298584
timers:
  learn_throughput: 445.166
  learn_time_ms: 37064.823
  load_throughput: 4898916.669
  load_time_ms: 3.368
  training_iteration_time_ms: 48980.245
  update_time_ms: 2.565
timesteps_total: 1402500
training_iteration: 85

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3026315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9507042253521126
  reward for individual goal_min: 0.0
episode_len_mean: 181.07
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 6270
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.024683237075806
time_total_s: 4533.523485660553
timers:
  learn_throughput: 399.971
  learn_time_ms: 41253.039
  load_throughput: 4181404.999
  load_time_ms: 3.946
  training_iteration_time_ms: 54499.991
  update_time_ms: 2.844
timesteps_total: 1254000
training_iteration: 76

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.98125
  reward for individual goal_min: 0.0
episode_len_mean: 173.56
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 6193
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.38112831115723
time_total_s: 4529.587706327438
timers:
  learn_throughput: 400.068
  learn_time_ms: 41243.01
  load_throughput: 4340214.107
  load_time_ms: 3.802
  training_iteration_time_ms: 54304.726
  update_time_ms: 2.801
timesteps_total: 1237500
training_iteration: 75

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2642857142857143
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8552631578947368
  reward for individual goal_min: 0.0
episode_len_mean: 176.81
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 7463
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.07289218902588
time_total_s: 4556.676064252853
timers:
  learn_throughput: 520.758
  learn_time_ms: 31684.576
  load_throughput: 4960329.131
  load_time_ms: 3.326
  training_iteration_time_ms: 42435.211
  update_time_ms: 2.48
timesteps_total: 1584000
training_iteration: 96

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 173.07
episode_reward_max: 2.0
episode_reward_mean: 1.41
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 6211
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.42445945739746
time_total_s: 4536.396506309509
timers:
  learn_throughput: 453.297
  learn_time_ms: 36399.989
  load_throughput: 4789144.811
  load_time_ms: 3.445
  training_iteration_time_ms: 48309.343
  update_time_ms: 2.573
timesteps_total: 1336500
training_iteration: 81

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3493150684931507
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 160.26470588235293
episode_reward_max: 2.0
episode_reward_mean: 1.4215686274509804
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 8453
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6372549019607843
  agent_1: 0.7843137254901961
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.33223533630371
time_total_s: 4547.582545757294
timers:
  learn_throughput: 543.807
  learn_time_ms: 30341.673
  load_throughput: 5425582.376
  load_time_ms: 3.041
  training_iteration_time_ms: 41048.814
  update_time_ms: 2.477
timesteps_total: 1617000
training_iteration: 98

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.993006993006993
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8857142857142857
  reward for individual goal_min: 0.5
episode_len_mean: 66.58064516129032
episode_reward_max: 2.0
episode_reward_mean: 1.8951612903225807
episode_reward_min: 0.0
episodes_this_iter: 248
episodes_total: 13055
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9959677419354839
  agent_1: 0.8991935483870968
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.194921016693115
time_total_s: 4555.002595424652
timers:
  learn_throughput: 420.357
  learn_time_ms: 39252.368
  load_throughput: 4305650.738
  load_time_ms: 3.832
  training_iteration_time_ms: 51760.317
  update_time_ms: 2.662
timesteps_total: 1369500
training_iteration: 83

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2236842105263158
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.740506329113924
  reward for individual goal_min: 0.0
episode_len_mean: 201.33
episode_reward_max: 2.0
episode_reward_mean: 1.04
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 7377
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.83546686172485
time_total_s: 4570.276809692383
timers:
  learn_throughput: 495.517
  learn_time_ms: 33298.583
  load_throughput: 5267301.123
  load_time_ms: 3.133
  training_iteration_time_ms: 44626.486
  update_time_ms: 2.47
timesteps_total: 1551000
training_iteration: 94

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8366336633663366
  reward for individual goal_min: 0.5
episode_len_mean: 76.0
episode_reward_max: 2.0
episode_reward_mean: 1.8457943925233644
episode_reward_min: 1.0
episodes_this_iter: 214
episodes_total: 14492
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8457943925233645
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 50.016515016555786
time_total_s: 4568.851190090179
timers:
  learn_throughput: 435.365
  learn_time_ms: 37899.253
  load_throughput: 4749311.41
  load_time_ms: 3.474
  training_iteration_time_ms: 49587.279
  update_time_ms: 2.678
timesteps_total: 1452000
training_iteration: 88

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2777777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7039473684210527
  reward for individual goal_min: 0.0
episode_len_mean: 199.9
episode_reward_max: 2.0
episode_reward_mean: 1.06
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 6661
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.42397475242615
time_total_s: 4576.4418296813965
timers:
  learn_throughput: 431.771
  learn_time_ms: 38214.742
  load_throughput: 4713599.869
  load_time_ms: 3.501
  training_iteration_time_ms: 50870.857
  update_time_ms: 2.667
timesteps_total: 1402500
training_iteration: 85

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2605633802816901
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.99375
  reward for individual goal_min: 0.5
episode_len_mean: 169.19
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 7126
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.46799659729004
time_total_s: 4577.678897380829
timers:
  learn_throughput: 428.157
  learn_time_ms: 38537.309
  load_throughput: 4926536.633
  load_time_ms: 3.349
  training_iteration_time_ms: 51320.818
  update_time_ms: 2.687
timesteps_total: 1369500
training_iteration: 83

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9908536585365854
  reward for individual goal_min: 0.5
episode_len_mean: 47.71551724137931
episode_reward_max: 2.0
episode_reward_mean: 1.9913793103448276
episode_reward_min: 1.0
episodes_this_iter: 348
episodes_total: 19678
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9913793103448276
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.49156165122986
time_total_s: 4563.340026140213
timers:
  learn_throughput: 469.038
  learn_time_ms: 35178.384
  load_throughput: 4648411.55
  load_time_ms: 3.55
  training_iteration_time_ms: 46518.568
  update_time_ms: 2.328
timesteps_total: 1666500
training_iteration: 101

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8535353535353535
  reward for individual goal_min: 0.5
episode_len_mean: 83.78894472361809
episode_reward_max: 2.0
episode_reward_mean: 1.8542713567839195
episode_reward_min: 1.0
episodes_this_iter: 199
episodes_total: 9483
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9949748743718593
  agent_1: 0.8592964824120602
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.16291403770447
time_total_s: 4592.785623550415
timers:
  learn_throughput: 370.952
  learn_time_ms: 44480.089
  load_throughput: 4008944.963
  load_time_ms: 4.116
  training_iteration_time_ms: 58825.234
  update_time_ms: 2.941
timesteps_total: 1254000
training_iteration: 76

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.958904109589041
  reward for individual goal_min: 0.0
episode_len_mean: 198.63
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 7079
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.4316201210022
time_total_s: 4566.379646062851
timers:
  learn_throughput: 451.099
  learn_time_ms: 36577.317
  load_throughput: 4824399.861
  load_time_ms: 3.42
  training_iteration_time_ms: 48606.548
  update_time_ms: 2.57
timesteps_total: 1402500
training_iteration: 85

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9807692307692307
  reward for individual goal_min: 0.0
episode_len_mean: 185.95
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 7151
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.12162399291992
time_total_s: 4586.660011291504
timers:
  learn_throughput: 446.428
  learn_time_ms: 36960.027
  load_throughput: 4903915.422
  load_time_ms: 3.365
  training_iteration_time_ms: 48788.701
  update_time_ms: 2.585
timesteps_total: 1419000
training_iteration: 86

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3815789473684211
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8571428571428571
  reward for individual goal_min: 0.0
episode_len_mean: 181.33
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 7553
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.85093140602112
time_total_s: 4602.5269956588745
timers:
  learn_throughput: 515.083
  learn_time_ms: 32033.656
  load_throughput: 4920197.075
  load_time_ms: 3.354
  training_iteration_time_ms: 42917.894
  update_time_ms: 2.509
timesteps_total: 1600500
training_iteration: 97

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23648648648648649
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.0
episode_len_mean: 179.71
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 8544
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.57760190963745
time_total_s: 4589.160147666931
timers:
  learn_throughput: 543.424
  learn_time_ms: 30363.015
  load_throughput: 5449292.598
  load_time_ms: 3.028
  training_iteration_time_ms: 41014.346
  update_time_ms: 2.502
timesteps_total: 1633500
training_iteration: 99

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 152.47663551401868
episode_reward_max: 2.0
episode_reward_mean: 1.4672897196261683
episode_reward_min: 0.0
episodes_this_iter: 107
episodes_total: 6300
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6822429906542056
  agent_1: 0.7850467289719626
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.62745118141174
time_total_s: 4583.21515750885
timers:
  learn_throughput: 400.067
  learn_time_ms: 41243.107
  load_throughput: 4374507.184
  load_time_ms: 3.772
  training_iteration_time_ms: 54255.663
  update_time_ms: 2.783
timesteps_total: 1254000
training_iteration: 76

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2565789473684211
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9675324675324676
  reward for individual goal_min: 0.0
episode_len_mean: 178.1
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 6366
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.480082511901855
time_total_s: 4588.003568172455
timers:
  learn_throughput: 396.811
  learn_time_ms: 41581.502
  load_throughput: 4124487.672
  load_time_ms: 4.0
  training_iteration_time_ms: 54783.965
  update_time_ms: 2.827
timesteps_total: 1270500
training_iteration: 77

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2785714285714286
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9722222222222222
  reward for individual goal_min: 0.0
episode_len_mean: 186.26
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 6302
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.75
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.17077565193176
time_total_s: 4587.567281961441
timers:
  learn_throughput: 447.885
  learn_time_ms: 36839.785
  load_throughput: 4762417.061
  load_time_ms: 3.465
  training_iteration_time_ms: 48868.896
  update_time_ms: 2.606
timesteps_total: 1353000
training_iteration: 82

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.889344262295082
  reward for individual goal_min: 0.5
episode_len_mean: 69.38912133891213
episode_reward_max: 2.0
episode_reward_mean: 1.887029288702929
episode_reward_min: 1.0
episodes_this_iter: 239
episodes_total: 13294
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9916317991631799
  agent_1: 0.895397489539749
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.99773979187012
time_total_s: 4609.000335216522
timers:
  learn_throughput: 418.398
  learn_time_ms: 39436.126
  load_throughput: 4291366.917
  load_time_ms: 3.845
  training_iteration_time_ms: 51966.197
  update_time_ms: 2.656
timesteps_total: 1386000
training_iteration: 84

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28313253012048195
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7378048780487805
  reward for individual goal_min: 0.0
episode_len_mean: 207.32
episode_reward_max: 2.0
episode_reward_mean: 1.06
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 7457
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.76271152496338
time_total_s: 4616.039521217346
timers:
  learn_throughput: 494.001
  learn_time_ms: 33400.725
  load_throughput: 5195100.815
  load_time_ms: 3.176
  training_iteration_time_ms: 44664.543
  update_time_ms: 2.469
timesteps_total: 1567500
training_iteration: 95

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8583333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 73.45814977973568
episode_reward_max: 2.0
episode_reward_mean: 1.8502202643171806
episode_reward_min: 1.0
episodes_this_iter: 227
episodes_total: 14719
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8502202643171806
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 49.13006901741028
time_total_s: 4617.98125910759
timers:
  learn_throughput: 437.265
  learn_time_ms: 37734.572
  load_throughput: 4733783.14
  load_time_ms: 3.486
  training_iteration_time_ms: 49346.539
  update_time_ms: 2.658
timesteps_total: 1468500
training_iteration: 89

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.4342105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.743421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 192.79
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 6746
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.20347595214844
time_total_s: 4626.645305633545
timers:
  learn_throughput: 433.47
  learn_time_ms: 38064.906
  load_throughput: 4683962.613
  load_time_ms: 3.523
  training_iteration_time_ms: 50601.61
  update_time_ms: 2.658
timesteps_total: 1419000
training_iteration: 86

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2986111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9873417721518988
  reward for individual goal_min: 0.0
episode_len_mean: 163.0891089108911
episode_reward_max: 2.0
episode_reward_mean: 1.3762376237623761
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 7227
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6633663366336634
  agent_1: 0.7128712871287128
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.58575224876404
time_total_s: 4627.264649629593
timers:
  learn_throughput: 435.046
  learn_time_ms: 37927.059
  load_throughput: 4959227.23
  load_time_ms: 3.327
  training_iteration_time_ms: 50638.106
  update_time_ms: 2.715
timesteps_total: 1386000
training_iteration: 84

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 48.15988372093023
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 344
episodes_total: 20022
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.76040196418762
time_total_s: 4611.100428104401
timers:
  learn_throughput: 467.429
  learn_time_ms: 35299.465
  load_throughput: 4604188.383
  load_time_ms: 3.584
  training_iteration_time_ms: 46665.385
  update_time_ms: 2.338
timesteps_total: 1683000
training_iteration: 102

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2867647058823529
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8661971830985915
  reward for individual goal_min: 0.0
episode_len_mean: 178.82
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 7645
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.73856592178345
time_total_s: 4648.265561580658
timers:
  learn_throughput: 510.541
  learn_time_ms: 32318.679
  load_throughput: 4851557.761
  load_time_ms: 3.401
  training_iteration_time_ms: 43265.281
  update_time_ms: 2.524
timesteps_total: 1617000
training_iteration: 98

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9805194805194806
  reward for individual goal_min: 0.0
episode_len_mean: 179.28
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 7173
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.68908095359802
time_total_s: 4616.068727016449
timers:
  learn_throughput: 448.854
  learn_time_ms: 36760.279
  load_throughput: 4849450.0
  load_time_ms: 3.402
  training_iteration_time_ms: 48781.917
  update_time_ms: 2.568
timesteps_total: 1419000
training_iteration: 86

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23376623376623376
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.6
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 7242
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.814032554626465
time_total_s: 4634.47404384613
timers:
  learn_throughput: 449.091
  learn_time_ms: 36740.924
  load_throughput: 4937115.463
  load_time_ms: 3.342
  training_iteration_time_ms: 48457.942
  update_time_ms: 2.563
timesteps_total: 1435500
training_iteration: 87

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8368421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 85.859375
episode_reward_max: 2.0
episode_reward_mean: 1.8385416666666667
episode_reward_min: 1.0
episodes_this_iter: 192
episodes_total: 9675
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9895833333333334
  agent_1: 0.8489583333333334
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.92529368400574
time_total_s: 4651.710917234421
timers:
  learn_throughput: 370.758
  learn_time_ms: 44503.482
  load_throughput: 3879848.183
  load_time_ms: 4.253
  training_iteration_time_ms: 58868.405
  update_time_ms: 2.972
timesteps_total: 1270500
training_iteration: 77

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3484848484848485
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 154.33018867924528
episode_reward_max: 2.0
episode_reward_mean: 1.4811320754716981
episode_reward_min: 0.0
episodes_this_iter: 106
episodes_total: 6406
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7075471698113207
  agent_1: 0.7735849056603774
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.984135150909424
time_total_s: 4636.1992926597595
timers:
  learn_throughput: 402.114
  learn_time_ms: 41033.19
  load_throughput: 4321081.925
  load_time_ms: 3.818
  training_iteration_time_ms: 54036.207
  update_time_ms: 2.741
timesteps_total: 1270500
training_iteration: 77

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2676056338028169
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9177215189873418
  reward for individual goal_min: 0.0
episode_len_mean: 176.4
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 6459
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.84304881095886
time_total_s: 4640.846616983414
timers:
  learn_throughput: 399.366
  learn_time_ms: 41315.49
  load_throughput: 4033924.924
  load_time_ms: 4.09
  training_iteration_time_ms: 54479.713
  update_time_ms: 2.817
timesteps_total: 1287000
training_iteration: 78

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 90.53333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.8166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9166666666666666
  agent_1: 0.9
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23943661971830985
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9878048780487805
  reward for individual goal_min: 0.5
episode_len_mean: 172.95
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 8639
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.64531493186951
time_total_s: 4643.805462598801
timers:
  learn_throughput: 541.009
  learn_time_ms: 30498.548
  load_throughput: 5393447.064
  load_time_ms: 3.059
  training_iteration_time_ms: 41162.672
  update_time_ms: 2.509
timesteps_total: 1650000
training_iteration: 100

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19ecg2ejpn/checkpoint_000100/checkpoint-100
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30303030303030304
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9615384615384616
  reward for individual goal_min: 0.0
episode_len_mean: 166.83
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 6400
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.8864483833313
time_total_s: 4638.453730344772
timers:
  learn_throughput: 445.16
  learn_time_ms: 37065.31
  load_throughput: 4731258.87
  load_time_ms: 3.487
  training_iteration_time_ms: 49120.767
  update_time_ms: 2.615
timesteps_total: 1369500
training_iteration: 83

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2974683544303797
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.756578947368421
  reward for individual goal_min: 0.0
episode_len_mean: 201.72
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 7539
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.571887493133545
time_total_s: 4663.61140871048
timers:
  learn_throughput: 492.916
  learn_time_ms: 33474.243
  load_throughput: 5194320.969
  load_time_ms: 3.177
  training_iteration_time_ms: 44759.825
  update_time_ms: 2.448
timesteps_total: 1584000
training_iteration: 96

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8739130434782608
  reward for individual goal_min: 0.5
episode_len_mean: 74.8348623853211
episode_reward_max: 2.0
episode_reward_mean: 1.8669724770642202
episode_reward_min: 1.0
episodes_this_iter: 218
episodes_total: 13512
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9954128440366973
  agent_1: 0.8715596330275229
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.12426519393921
time_total_s: 4661.124600410461
timers:
  learn_throughput: 418.487
  learn_time_ms: 39427.753
  load_throughput: 4305999.004
  load_time_ms: 3.832
  training_iteration_time_ms: 51992.67
  update_time_ms: 2.666
timesteps_total: 1402500
training_iteration: 85

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8366336633663366
  reward for individual goal_min: 0.5
episode_len_mean: 77.57487922705315
episode_reward_max: 2.0
episode_reward_mean: 1.8405797101449275
episode_reward_min: 1.0
episodes_this_iter: 207
episodes_total: 14926
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8405797101449275
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.147199869155884
time_total_s: 4665.128458976746
timers:
  learn_throughput: 438.942
  learn_time_ms: 37590.385
  load_throughput: 4773816.376
  load_time_ms: 3.456
  training_iteration_time_ms: 49165.575
  update_time_ms: 2.649
timesteps_total: 1485000
training_iteration: 90

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.4084507042253521
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7142857142857143
  reward for individual goal_min: 0.0
episode_len_mean: 198.56
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 6829
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.189368724823
time_total_s: 4677.834674358368
timers:
  learn_throughput: 433.865
  learn_time_ms: 38030.237
  load_throughput: 4668385.634
  load_time_ms: 3.534
  training_iteration_time_ms: 50593.236
  update_time_ms: 2.665
timesteps_total: 1435500
training_iteration: 87

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2631578947368421
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8
  reward for individual goal_min: 0.0
episode_len_mean: 197.33
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 7727
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.756566524505615
time_total_s: 4693.022128105164
timers:
  learn_throughput: 507.85
  learn_time_ms: 32489.911
  load_throughput: 4827765.33
  load_time_ms: 3.418
  training_iteration_time_ms: 43446.575
  update_time_ms: 2.536
timesteps_total: 1633500
training_iteration: 99

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.990909090909091
  reward for individual goal_min: 0.5
episode_len_mean: 49.42857142857143
episode_reward_max: 2.0
episode_reward_mean: 1.9908814589665653
episode_reward_min: 1.0
episodes_this_iter: 329
episodes_total: 20351
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9908814589665653
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.87871193885803
time_total_s: 4657.979140043259
timers:
  learn_throughput: 467.3
  learn_time_ms: 35309.234
  load_throughput: 4558815.865
  load_time_ms: 3.619
  training_iteration_time_ms: 46676.645
  update_time_ms: 2.365
timesteps_total: 1699500
training_iteration: 103

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28289473684210525
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9722222222222222
  reward for individual goal_min: 0.0
episode_len_mean: 177.63
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 7319
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.000442028045654
time_total_s: 4679.265091657639
timers:
  learn_throughput: 432.707
  learn_time_ms: 38132.076
  load_throughput: 5004556.933
  load_time_ms: 3.297
  training_iteration_time_ms: 50951.908
  update_time_ms: 2.7
timesteps_total: 1402500
training_iteration: 85

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2647058823529412
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9878048780487805
  reward for individual goal_min: 0.0
episode_len_mean: 163.3
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 7341
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.84189248085022
time_total_s: 4683.315936326981
timers:
  learn_throughput: 447.334
  learn_time_ms: 36885.176
  load_throughput: 4947916.693
  load_time_ms: 3.335
  training_iteration_time_ms: 48582.867
  update_time_ms: 2.591
timesteps_total: 1452000
training_iteration: 88

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3269230769230769
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9923076923076923
  reward for individual goal_min: 0.5
episode_len_mean: 173.51
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 7268
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.29746866226196
time_total_s: 4666.366195678711
timers:
  learn_throughput: 448.063
  learn_time_ms: 36825.174
  load_throughput: 4830865.711
  load_time_ms: 3.416
  training_iteration_time_ms: 48847.442
  update_time_ms: 2.581
timesteps_total: 1435500
training_iteration: 87

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2671232876712329
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9875
  reward for individual goal_min: 0.0
episode_len_mean: 167.15
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 8737
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.76
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.47752022743225
time_total_s: 4686.282982826233
timers:
  learn_throughput: 535.561
  learn_time_ms: 30808.84
  load_throughput: 5311405.174
  load_time_ms: 3.107
  training_iteration_time_ms: 41463.081
  update_time_ms: 2.476
timesteps_total: 1666500
training_iteration: 101

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.972972972972973
  reward for individual goal_min: 0.0
episode_len_mean: 173.35
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 6554
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.07670044898987
time_total_s: 4690.923317432404
timers:
  learn_throughput: 404.276
  learn_time_ms: 40813.739
  load_throughput: 4094352.178
  load_time_ms: 4.03
  training_iteration_time_ms: 53699.079
  update_time_ms: 2.792
timesteps_total: 1303500
training_iteration: 79

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.58
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 6501
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.459078311920166
time_total_s: 4686.65837097168
timers:
  learn_throughput: 404.315
  learn_time_ms: 40809.741
  load_throughput: 4424060.193
  load_time_ms: 3.73
  training_iteration_time_ms: 53612.92
  update_time_ms: 2.718
timesteps_total: 1287000
training_iteration: 78

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8033707865168539
  reward for individual goal_min: 0.5
episode_len_mean: 91.19101123595506
episode_reward_max: 2.0
episode_reward_mean: 1.803370786516854
episode_reward_min: 1.0
episodes_this_iter: 178
episodes_total: 9853
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9887640449438202
  agent_1: 0.8146067415730337
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.479666233062744
time_total_s: 4709.1905834674835
timers:
  learn_throughput: 370.404
  learn_time_ms: 44545.993
  load_throughput: 3874136.008
  load_time_ms: 4.259
  training_iteration_time_ms: 58885.829
  update_time_ms: 2.99
timesteps_total: 1287000
training_iteration: 78

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2808219178082192
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9785714285714285
  reward for individual goal_min: 0.0
episode_len_mean: 176.2
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 6490
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.19148635864258
time_total_s: 4690.645216703415
timers:
  learn_throughput: 442.425
  learn_time_ms: 37294.497
  load_throughput: 4717037.522
  load_time_ms: 3.498
  training_iteration_time_ms: 49419.562
  update_time_ms: 2.671
timesteps_total: 1386000
training_iteration: 84

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2986111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8525641025641025
  reward for individual goal_min: 0.0
episode_len_mean: 197.64
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 7623
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.425368785858154
time_total_s: 4708.036777496338
timers:
  learn_throughput: 493.336
  learn_time_ms: 33445.792
  load_throughput: 5176061.748
  load_time_ms: 3.188
  training_iteration_time_ms: 44672.769
  update_time_ms: 2.448
timesteps_total: 1600500
training_iteration: 97

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8578431372549019
  reward for individual goal_min: 0.5
episode_len_mean: 74.8177570093458
episode_reward_max: 2.0
episode_reward_mean: 1.8644859813084111
episode_reward_min: 1.0
episodes_this_iter: 214
episodes_total: 13726
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8644859813084113
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 52.04131078720093
time_total_s: 4713.165911197662
timers:
  learn_throughput: 416.941
  learn_time_ms: 39573.915
  load_throughput: 4268761.55
  load_time_ms: 3.865
  training_iteration_time_ms: 52175.482
  update_time_ms: 2.659
timesteps_total: 1419000
training_iteration: 86

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8577586206896551
  reward for individual goal_min: 0.5
episode_len_mean: 74.01304347826087
episode_reward_max: 2.0
episode_reward_mean: 1.8565217391304347
episode_reward_min: 1.0
episodes_this_iter: 230
episodes_total: 15156
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8565217391304348
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.917232036590576
time_total_s: 4714.045691013336
timers:
  learn_throughput: 438.081
  learn_time_ms: 37664.226
  load_throughput: 4767403.713
  load_time_ms: 3.461
  training_iteration_time_ms: 49249.824
  update_time_ms: 2.634
timesteps_total: 1501500
training_iteration: 91

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9943502824858758
  reward for individual goal_min: 0.5
episode_len_mean: 48.75218658892128
episode_reward_max: 2.0
episode_reward_mean: 1.9941690962099126
episode_reward_min: 1.0
episodes_this_iter: 343
episodes_total: 20694
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9941690962099126
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.01801347732544
time_total_s: 4705.997153520584
timers:
  learn_throughput: 465.795
  learn_time_ms: 35423.303
  load_throughput: 4513651.696
  load_time_ms: 3.656
  training_iteration_time_ms: 46805.489
  update_time_ms: 2.352
timesteps_total: 1716000
training_iteration: 104

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3958333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7013888888888888
  reward for individual goal_min: 0.0
episode_len_mean: 201.84
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 6910
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.04825139045715
time_total_s: 4727.882925748825
timers:
  learn_throughput: 432.63
  learn_time_ms: 38138.819
  load_throughput: 4602443.073
  load_time_ms: 3.585
  training_iteration_time_ms: 50735.707
  update_time_ms: 2.685
timesteps_total: 1452000
training_iteration: 88

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2676056338028169
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9807692307692307
  reward for individual goal_min: 0.0
episode_len_mean: 164.26732673267327
episode_reward_max: 2.0
episode_reward_mean: 1.3465346534653466
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 7420
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7128712871287128
  agent_1: 0.6336633663366337
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.55402421951294
time_total_s: 4730.8191158771515
timers:
  learn_throughput: 429.55
  learn_time_ms: 38412.321
  load_throughput: 4974698.525
  load_time_ms: 3.317
  training_iteration_time_ms: 51245.938
  update_time_ms: 2.68
timesteps_total: 1419000
training_iteration: 86

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3684210526315789
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9527027027027027
  reward for individual goal_min: 0.0
episode_len_mean: 166.31
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 7368
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.33050274848938
time_total_s: 4714.6966984272
timers:
  learn_throughput: 448.86
  learn_time_ms: 36759.797
  load_throughput: 4814733.473
  load_time_ms: 3.427
  training_iteration_time_ms: 48801.052
  update_time_ms: 2.57
timesteps_total: 1452000
training_iteration: 88

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8166666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 112.45
episode_reward_max: 2.0
episode_reward_mean: 1.7833333333333334
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9666666666666667
  agent_1: 0.8166666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26582278481012656
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8424657534246576
  reward for individual goal_min: 0.0
episode_len_mean: 201.44
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 7812
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.40293097496033
time_total_s: 4748.425059080124
timers:
  learn_throughput: 505.154
  learn_time_ms: 32663.287
  load_throughput: 4809747.649
  load_time_ms: 3.431
  training_iteration_time_ms: 43585.207
  update_time_ms: 2.535
timesteps_total: 1650000
training_iteration: 100

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-091_srff7m/checkpoint_000100/checkpoint-100
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18243243243243243
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 184.75
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 7431
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.79723882675171
time_total_s: 4734.113175153732
timers:
  learn_throughput: 446.567
  learn_time_ms: 36948.556
  load_throughput: 4923627.516
  load_time_ms: 3.351
  training_iteration_time_ms: 48655.426
  update_time_ms: 2.598
timesteps_total: 1468500
training_iteration: 89

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2682926829268293
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 188.08
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 8826
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.30301809310913
time_total_s: 4729.586000919342
timers:
  learn_throughput: 531.583
  learn_time_ms: 31039.347
  load_throughput: 5285484.206
  load_time_ms: 3.122
  training_iteration_time_ms: 41615.267
  update_time_ms: 2.478
timesteps_total: 1683000
training_iteration: 102

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 175.21
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 6595
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.88683009147644
time_total_s: 4734.545201063156
timers:
  learn_throughput: 411.16
  learn_time_ms: 40130.33
  load_throughput: 4429610.267
  load_time_ms: 3.725
  training_iteration_time_ms: 52861.488
  update_time_ms: 2.712
timesteps_total: 1303500
training_iteration: 79

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95
  reward for individual goal_min: 0.0
episode_len_mean: 188.25
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 6578
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.298051834106445
time_total_s: 4739.943268537521
timers:
  learn_throughput: 441.522
  learn_time_ms: 37370.744
  load_throughput: 4687896.929
  load_time_ms: 3.52
  training_iteration_time_ms: 49483.616
  update_time_ms: 2.655
timesteps_total: 1402500
training_iteration: 85

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27631578947368424
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.726027397260274
  reward for individual goal_min: 0.0
episode_len_mean: 205.01
episode_reward_max: 2.0
episode_reward_mean: 1.03
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 7704
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.47
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.19460844993591
time_total_s: 4756.231385946274
timers:
  learn_throughput: 489.655
  learn_time_ms: 33697.219
  load_throughput: 5174049.463
  load_time_ms: 3.189
  training_iteration_time_ms: 45018.923
  update_time_ms: 2.469
timesteps_total: 1617000
training_iteration: 98

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8383838383838383
  reward for individual goal_min: 0.5
episode_len_mean: 88.3010752688172
episode_reward_max: 2.0
episode_reward_mean: 1.8279569892473118
episode_reward_min: 1.0
episodes_this_iter: 186
episodes_total: 10039
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9946236559139785
  agent_1: 0.8333333333333334
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.590436935424805
time_total_s: 4767.781020402908
timers:
  learn_throughput: 370.144
  learn_time_ms: 44577.186
  load_throughput: 3965529.025
  load_time_ms: 4.161
  training_iteration_time_ms: 58760.655
  update_time_ms: 2.956
timesteps_total: 1303500
training_iteration: 79

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9333333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 97.68333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.8333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9333333333333333
  agent_1: 0.9
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24675324675324675
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.961038961038961
  reward for individual goal_min: 0.0
episode_len_mean: 185.58
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 6639
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.59246563911438
time_total_s: 4752.515783071518
timers:
  learn_throughput: 408.442
  learn_time_ms: 40397.409
  load_throughput: 4126996.44
  load_time_ms: 3.998
  training_iteration_time_ms: 53246.188
  update_time_ms: 2.778
timesteps_total: 1320000
training_iteration: 80

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-29ws591pkw/checkpoint_000080/checkpoint-80
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8092783505154639
  reward for individual goal_min: 0.5
episode_len_mean: 87.04864864864865
episode_reward_max: 2.0
episode_reward_mean: 1.8
episode_reward_min: 1.0
episodes_this_iter: 185
episodes_total: 15341
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.769115924835205
time_total_s: 4761.814806938171
timers:
  learn_throughput: 441.927
  learn_time_ms: 37336.445
  load_throughput: 4787355.838
  load_time_ms: 3.447
  training_iteration_time_ms: 48945.489
  update_time_ms: 2.632
timesteps_total: 1518000
training_iteration: 92

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8571428571428571
  reward for individual goal_min: 0.5
episode_len_mean: 73.29777777777778
episode_reward_max: 2.0
episode_reward_mean: 1.8666666666666667
episode_reward_min: 1.0
episodes_this_iter: 225
episodes_total: 13951
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9955555555555555
  agent_1: 0.8711111111111111
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.577139139175415
time_total_s: 4766.743050336838
timers:
  learn_throughput: 414.615
  learn_time_ms: 39795.997
  load_throughput: 4256474.322
  load_time_ms: 3.876
  training_iteration_time_ms: 52500.541
  update_time_ms: 2.671
timesteps_total: 1435500
training_iteration: 87

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9967105263157895
  reward for individual goal_min: 0.5
episode_len_mean: 51.664556962025316
episode_reward_max: 2.0
episode_reward_mean: 1.9968354430379747
episode_reward_min: 1.0
episodes_this_iter: 316
episodes_total: 21010
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9968354430379747
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.19715237617493
time_total_s: 4752.194305896759
timers:
  learn_throughput: 465.327
  learn_time_ms: 35458.899
  load_throughput: 4517128.087
  load_time_ms: 3.653
  training_iteration_time_ms: 46806.321
  update_time_ms: 2.359
timesteps_total: 1732500
training_iteration: 105

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3466666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.831081081081081
  reward for individual goal_min: 0.0
episode_len_mean: 192.19
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 7897
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.73894166946411
time_total_s: 4792.164000749588
timers:
  learn_throughput: 501.281
  learn_time_ms: 32915.648
  load_throughput: 4799640.474
  load_time_ms: 3.438
  training_iteration_time_ms: 43871.866
  update_time_ms: 2.538
timesteps_total: 1666500
training_iteration: 101

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36363636363636365
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 160.19607843137254
episode_reward_max: 2.0
episode_reward_mean: 1.4215686274509804
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 8928
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.696078431372549
  agent_1: 0.7254901960784313
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.63560461997986
time_total_s: 4772.221605539322
timers:
  learn_throughput: 529.327
  learn_time_ms: 31171.653
  load_throughput: 5291465.272
  load_time_ms: 3.118
  training_iteration_time_ms: 41721.842
  update_time_ms: 2.464
timesteps_total: 1699500
training_iteration: 103

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35526315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6493506493506493
  reward for individual goal_min: 0.0
episode_len_mean: 202.24
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 6989
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.6021511554718
time_total_s: 4778.485076904297
timers:
  learn_throughput: 433.313
  learn_time_ms: 38078.722
  load_throughput: 4619955.941
  load_time_ms: 3.571
  training_iteration_time_ms: 50692.929
  update_time_ms: 2.652
timesteps_total: 1468500
training_iteration: 89

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9855072463768116
  reward for individual goal_min: 0.0
episode_len_mean: 173.6
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 7467
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.88960647583008
time_total_s: 4762.58630490303
timers:
  learn_throughput: 449.973
  learn_time_ms: 36668.842
  load_throughput: 4849450.0
  load_time_ms: 3.402
  training_iteration_time_ms: 48798.832
  update_time_ms: 2.578
timesteps_total: 1468500
training_iteration: 89

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21428571428571427
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9814814814814815
  reward for individual goal_min: 0.0
episode_len_mean: 173.39
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 7509
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.27131009101868
time_total_s: 4781.09042596817
timers:
  learn_throughput: 433.682
  learn_time_ms: 38046.337
  load_throughput: 4994264.023
  load_time_ms: 3.304
  training_iteration_time_ms: 50989.377
  update_time_ms: 2.657
timesteps_total: 1435500
training_iteration: 87

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2318840579710145
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.5
episode_len_mean: 170.99
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 7527
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.61735224723816
time_total_s: 4784.7305274009705
timers:
  learn_throughput: 444.086
  learn_time_ms: 37154.976
  load_throughput: 4858676.477
  load_time_ms: 3.396
  training_iteration_time_ms: 48923.264
  update_time_ms: 2.604
timesteps_total: 1485000
training_iteration: 90

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23943661971830985
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 202.44
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 7790
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.495776653289795
time_total_s: 4802.727162599564
timers:
  learn_throughput: 484.194
  learn_time_ms: 34077.22
  load_throughput: 5100303.338
  load_time_ms: 3.235
  training_iteration_time_ms: 45471.184
  update_time_ms: 2.498
timesteps_total: 1633500
training_iteration: 99

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19480519480519481
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9423076923076923
  reward for individual goal_min: 0.0
episode_len_mean: 192.5
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 6663
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.64381647109985
time_total_s: 4788.587085008621
timers:
  learn_throughput: 440.583
  learn_time_ms: 37450.404
  load_throughput: 4675450.345
  load_time_ms: 3.529
  training_iteration_time_ms: 49468.872
  update_time_ms: 2.648
timesteps_total: 1419000
training_iteration: 86

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 73.73333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.9666666666666666
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9666666666666667
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27848101265822783
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.99375
  reward for individual goal_min: 0.5
episode_len_mean: 176.45
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 6688
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.069586753845215
time_total_s: 4795.614787817001
timers:
  learn_throughput: 412.282
  learn_time_ms: 40021.12
  load_throughput: 4446601.473
  load_time_ms: 3.711
  training_iteration_time_ms: 52637.267
  update_time_ms: 2.715
timesteps_total: 1320000
training_iteration: 80

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-2933yh83nn/checkpoint_000080/checkpoint-80
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9753086419753086
  reward for individual goal_min: 0.0
episode_len_mean: 179.98
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 6728
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.42304253578186
time_total_s: 4803.9388256073
timers:
  learn_throughput: 410.903
  learn_time_ms: 40155.51
  load_throughput: 4139710.006
  load_time_ms: 3.986
  training_iteration_time_ms: 53026.316
  update_time_ms: 2.756
timesteps_total: 1336500
training_iteration: 81

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8168316831683168
  reward for individual goal_min: 0.5
episode_len_mean: 80.68780487804878
episode_reward_max: 2.0
episode_reward_mean: 1.8195121951219513
episode_reward_min: 1.0
episodes_this_iter: 205
episodes_total: 15546
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8195121951219512
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.14600133895874
time_total_s: 4808.96080827713
timers:
  learn_throughput: 446.012
  learn_time_ms: 36994.502
  load_throughput: 4822315.625
  load_time_ms: 3.422
  training_iteration_time_ms: 48502.987
  update_time_ms: 2.628
timesteps_total: 1534500
training_iteration: 93

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2971014492753623
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935897435897436
  reward for individual goal_min: 0.5
episode_len_mean: 162.39805825242718
episode_reward_max: 2.0
episode_reward_mean: 1.4174757281553398
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 9031
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6699029126213593
  agent_1: 0.7475728155339806
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.370450019836426
time_total_s: 4815.592055559158
timers:
  learn_throughput: 524.77
  learn_time_ms: 31442.372
  load_throughput: 5282297.142
  load_time_ms: 3.124
  training_iteration_time_ms: 41998.352
  update_time_ms: 2.486
timesteps_total: 1716000
training_iteration: 104

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973544973544973
  reward for individual goal_min: 0.5
episode_len_mean: 48.55882352941177
episode_reward_max: 2.0
episode_reward_mean: 1.9970588235294118
episode_reward_min: 1.0
episodes_this_iter: 340
episodes_total: 21350
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9970588235294118
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.243173360824585
time_total_s: 4799.437479257584
timers:
  learn_throughput: 463.713
  learn_time_ms: 35582.366
  load_throughput: 4484433.242
  load_time_ms: 3.679
  training_iteration_time_ms: 46931.328
  update_time_ms: 2.378
timesteps_total: 1749000
training_iteration: 106

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 79.66350710900474
episode_reward_max: 2.0
episode_reward_mean: 1.8341232227488151
episode_reward_min: 1.0
episodes_this_iter: 211
episodes_total: 14162
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8341232227488151
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 51.87376284599304
time_total_s: 4818.616813182831
timers:
  learn_throughput: 414.42
  learn_time_ms: 39814.687
  load_throughput: 4225831.262
  load_time_ms: 3.905
  training_iteration_time_ms: 52530.075
  update_time_ms: 2.677
timesteps_total: 1452000
training_iteration: 88

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3712121212121212
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8040540540540541
  reward for individual goal_min: 0.0
episode_len_mean: 166.69
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 7992
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.98988127708435
time_total_s: 4839.153882026672
timers:
  learn_throughput: 496.676
  learn_time_ms: 33220.848
  load_throughput: 4803438.185
  load_time_ms: 3.435
  training_iteration_time_ms: 44212.75
  update_time_ms: 2.56
timesteps_total: 1683000
training_iteration: 102

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.4383561643835616
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6875
  reward for individual goal_min: 0.0
episode_len_mean: 196.4
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 7072
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.31257081031799
time_total_s: 4825.797647714615
timers:
  learn_throughput: 437.93
  learn_time_ms: 37677.216
  load_throughput: 4598711.941
  load_time_ms: 3.588
  training_iteration_time_ms: 50237.88
  update_time_ms: 2.674
timesteps_total: 1485000
training_iteration: 90

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 83.43333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.8333333333333333
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8333333333333334
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27848101265822783
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9791666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 181.08
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 7553
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.72911500930786
time_total_s: 4811.315419912338
timers:
  learn_throughput: 451.292
  learn_time_ms: 36561.699
  load_throughput: 4820166.044
  load_time_ms: 3.423
  training_iteration_time_ms: 48705.942
  update_time_ms: 2.58
timesteps_total: 1485000
training_iteration: 90

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8613861386138614
  reward for individual goal_min: 0.5
episode_len_mean: 79.19617224880383
episode_reward_max: 2.0
episode_reward_mean: 1.8660287081339713
episode_reward_min: 1.0
episodes_this_iter: 209
episodes_total: 10248
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9904306220095693
  agent_1: 0.8755980861244019
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 71.56780123710632
time_total_s: 4839.348821640015
timers:
  learn_throughput: 367.557
  learn_time_ms: 44890.937
  load_throughput: 3830817.465
  load_time_ms: 4.307
  training_iteration_time_ms: 59150.417
  update_time_ms: 2.953
timesteps_total: 1320000
training_iteration: 80

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-19wecnkw4d/checkpoint_000080/checkpoint-80
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2945205479452055
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9875
  reward for individual goal_min: 0.0
episode_len_mean: 166.5728155339806
episode_reward_max: 2.0
episode_reward_mean: 1.4077669902912622
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 7612
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7184466019417476
  agent_1: 0.6893203883495146
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.34039640426636
time_total_s: 4833.4308223724365
timers:
  learn_throughput: 431.768
  learn_time_ms: 38214.966
  load_throughput: 4992678.714
  load_time_ms: 3.305
  training_iteration_time_ms: 51121.908
  update_time_ms: 2.669
timesteps_total: 1452000
training_iteration: 88

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22151898734177214
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.5
episode_len_mean: 193.12
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 7610
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.96815776824951
time_total_s: 4833.69868516922
timers:
  learn_throughput: 443.599
  learn_time_ms: 37195.788
  load_throughput: 4888707.925
  load_time_ms: 3.375
  training_iteration_time_ms: 48953.583
  update_time_ms: 2.631
timesteps_total: 1501500
training_iteration: 91

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.22
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 6756
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.543912172317505
time_total_s: 4839.130997180939
timers:
  learn_throughput: 438.602
  learn_time_ms: 37619.486
  load_throughput: 4715269.878
  load_time_ms: 3.499
  training_iteration_time_ms: 49747.458
  update_time_ms: 2.646
timesteps_total: 1435500
training_iteration: 87

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26973684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.57
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 6776
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.64860391616821
time_total_s: 4846.26339173317
timers:
  learn_throughput: 415.87
  learn_time_ms: 39675.834
  load_throughput: 4442234.532
  load_time_ms: 3.714
  training_iteration_time_ms: 52270.186
  update_time_ms: 2.694
timesteps_total: 1336500
training_iteration: 81

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7
  reward for individual goal_min: 0.5
episode_len_mean: 134.21666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.6666666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8333333333333334
  agent_1: 0.8333333333333334
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2866666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7777777777777778
  reward for individual goal_min: 0.0
episode_len_mean: 202.54
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 7871
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.40120244026184
time_total_s: 4861.128365039825
timers:
  learn_throughput: 479.44
  learn_time_ms: 34415.161
  load_throughput: 5067402.011
  load_time_ms: 3.256
  training_iteration_time_ms: 45864.525
  update_time_ms: 2.494
timesteps_total: 1650000
training_iteration: 100

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8252427184466019
  reward for individual goal_min: 0.5
episode_len_mean: 78.69194312796209
episode_reward_max: 2.0
episode_reward_mean: 1.8293838862559242
episode_reward_min: 1.0
episodes_this_iter: 211
episodes_total: 15757
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8293838862559242
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.80178380012512
time_total_s: 4856.762592077255
timers:
  learn_throughput: 446.989
  learn_time_ms: 36913.635
  load_throughput: 4809647.37
  load_time_ms: 3.431
  training_iteration_time_ms: 48436.562
  update_time_ms: 2.626
timesteps_total: 1551000
training_iteration: 94

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19istq_qwi/checkpoint_000100/checkpoint-100
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2708333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 166.93
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 9128
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.152884006500244
time_total_s: 4857.744939565659
timers:
  learn_throughput: 521.763
  learn_time_ms: 31623.536
  load_throughput: 5223332.075
  load_time_ms: 3.159
  training_iteration_time_ms: 42224.247
  update_time_ms: 2.491
timesteps_total: 1732500
training_iteration: 105

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24050632911392406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.972972972972973
  reward for individual goal_min: 0.0
episode_len_mean: 181.96
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 6822
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.59772348403931
time_total_s: 4857.536549091339
timers:
  learn_throughput: 412.693
  learn_time_ms: 39981.275
  load_throughput: 4203449.688
  load_time_ms: 3.925
  training_iteration_time_ms: 52780.753
  update_time_ms: 2.733
timesteps_total: 1353000
training_iteration: 82

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2945205479452055
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8421052631578947
  reward for individual goal_min: 0.0
episode_len_mean: 189.08
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 8078
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.499985218048096
time_total_s: 4882.6538672447205
timers:
  learn_throughput: 493.63
  learn_time_ms: 33425.878
  load_throughput: 4734366.047
  load_time_ms: 3.485
  training_iteration_time_ms: 44445.644
  update_time_ms: 2.573
timesteps_total: 1699500
training_iteration: 103

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971751412429378
  reward for individual goal_min: 0.5
episode_len_mean: 49.3946587537092
episode_reward_max: 2.0
episode_reward_mean: 1.997032640949555
episode_reward_min: 1.0
episodes_this_iter: 337
episodes_total: 21687
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9970326409495549
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.23088788986206
time_total_s: 4846.668367147446
timers:
  learn_throughput: 462.592
  learn_time_ms: 35668.58
  load_throughput: 4440296.422
  load_time_ms: 3.716
  training_iteration_time_ms: 46997.924
  update_time_ms: 2.388
timesteps_total: 1765500
training_iteration: 107

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.44805194805194803
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7708333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 189.8
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 7161
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.8217077255249
time_total_s: 4875.61935544014
timers:
  learn_throughput: 438.114
  learn_time_ms: 37661.468
  load_throughput: 4665207.186
  load_time_ms: 3.537
  training_iteration_time_ms: 50111.898
  update_time_ms: 2.668
timesteps_total: 1501500
training_iteration: 91

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8571428571428571
  reward for individual goal_min: 0.5
episode_len_mean: 77.4931506849315
episode_reward_max: 2.0
episode_reward_mean: 1.8538812785388128
episode_reward_min: 1.0
episodes_this_iter: 219
episodes_total: 14381
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9863013698630136
  agent_1: 0.867579908675799
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.15302634239197
time_total_s: 4870.769839525223
timers:
  learn_throughput: 413.928
  learn_time_ms: 39861.996
  load_throughput: 4247513.763
  load_time_ms: 3.885
  training_iteration_time_ms: 52504.21
  update_time_ms: 2.665
timesteps_total: 1468500
training_iteration: 89

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19736842105263158
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9807692307692307
  reward for individual goal_min: 0.0
episode_len_mean: 190.91
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 7643
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.90237212181091
time_total_s: 4858.217792034149
timers:
  learn_throughput: 454.114
  learn_time_ms: 36334.453
  load_throughput: 4834848.121
  load_time_ms: 3.413
  training_iteration_time_ms: 48442.482
  update_time_ms: 2.58
timesteps_total: 1501500
training_iteration: 91

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19402985074626866
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.04
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 7705
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.82523012161255
time_total_s: 4882.5239152908325
timers:
  learn_throughput: 442.483
  learn_time_ms: 37289.582
  load_throughput: 4872222.019
  load_time_ms: 3.387
  training_iteration_time_ms: 49088.702
  update_time_ms: 2.63
timesteps_total: 1518000
training_iteration: 92

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2948717948717949
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9848484848484849
  reward for individual goal_min: 0.0
episode_len_mean: 177.73
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 7705
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.89572262763977
time_total_s: 4886.326545000076
timers:
  learn_throughput: 432.656
  learn_time_ms: 38136.513
  load_throughput: 4984049.26
  load_time_ms: 3.311
  training_iteration_time_ms: 51042.789
  update_time_ms: 2.685
timesteps_total: 1468500
training_iteration: 89

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8367346938775511
  reward for individual goal_min: 0.5
episode_len_mean: 85.35051546391753
episode_reward_max: 2.0
episode_reward_mean: 1.8350515463917525
episode_reward_min: 1.0
episodes_this_iter: 194
episodes_total: 10442
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9896907216494846
  agent_1: 0.845360824742268
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.57904005050659
time_total_s: 4898.927861690521
timers:
  learn_throughput: 366.498
  learn_time_ms: 45020.68
  load_throughput: 3999492.363
  load_time_ms: 4.126
  training_iteration_time_ms: 59303.646
  update_time_ms: 2.966
timesteps_total: 1336500
training_iteration: 81

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34558823529411764
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9857142857142858
  reward for individual goal_min: 0.5
episode_len_mean: 165.84
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 6852
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.77
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.55789375305176
time_total_s: 4889.6888909339905
timers:
  learn_throughput: 436.52
  learn_time_ms: 37798.941
  load_throughput: 4730838.42
  load_time_ms: 3.488
  training_iteration_time_ms: 49933.058
  update_time_ms: 2.658
timesteps_total: 1452000
training_iteration: 88

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2323943661971831
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9805194805194806
  reward for individual goal_min: 0.5
episode_len_mean: 177.57
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 9224
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.22319483757019
time_total_s: 4900.968134403229
timers:
  learn_throughput: 519.422
  learn_time_ms: 31766.085
  load_throughput: 5208198.135
  load_time_ms: 3.168
  training_iteration_time_ms: 42407.051
  update_time_ms: 2.51
timesteps_total: 1749000
training_iteration: 106

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3310810810810811
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7066666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 202.4
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 7952
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.55426907539368
time_total_s: 4907.682634115219
timers:
  learn_throughput: 474.037
  learn_time_ms: 34807.391
  load_throughput: 5076993.097
  load_time_ms: 3.25
  training_iteration_time_ms: 46289.022
  update_time_ms: 2.488
timesteps_total: 1666500
training_iteration: 101

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2361111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.49
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 6875
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.411139726638794
time_total_s: 4896.674531459808
timers:
  learn_throughput: 420.45
  learn_time_ms: 39243.635
  load_throughput: 4519694.621
  load_time_ms: 3.651
  training_iteration_time_ms: 51771.279
  update_time_ms: 2.683
timesteps_total: 1353000
training_iteration: 82

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8648648648648649
  reward for individual goal_min: 0.5
episode_len_mean: 70.03404255319148
episode_reward_max: 2.0
episode_reward_mean: 1.872340425531915
episode_reward_min: 1.0
episodes_this_iter: 235
episodes_total: 15992
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8723404255319149
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.67797088623047
time_total_s: 4905.440562963486
timers:
  learn_throughput: 445.784
  learn_time_ms: 37013.454
  load_throughput: 4791299.977
  load_time_ms: 3.444
  training_iteration_time_ms: 48531.341
  update_time_ms: 2.611
timesteps_total: 1567500
training_iteration: 95

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2753623188405797
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8125
  reward for individual goal_min: 0.0
episode_len_mean: 194.38
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 8164
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.77
  agent_1: 0.41
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.61920094490051
time_total_s: 4928.273068189621
timers:
  learn_throughput: 488.432
  learn_time_ms: 33781.581
  load_throughput: 4726638.027
  load_time_ms: 3.491
  training_iteration_time_ms: 44906.112
  update_time_ms: 2.571
timesteps_total: 1716000
training_iteration: 104

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21621621621621623
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.975
  reward for individual goal_min: 0.0
episode_len_mean: 176.56
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 6914
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.7230498790741
time_total_s: 4907.259598970413
timers:
  learn_throughput: 419.696
  learn_time_ms: 39314.149
  load_throughput: 4294216.094
  load_time_ms: 3.842
  training_iteration_time_ms: 51958.422
  update_time_ms: 2.695
timesteps_total: 1369500
training_iteration: 83

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9940119760479041
  reward for individual goal_min: 0.5
episode_len_mean: 48.56508875739645
episode_reward_max: 2.0
episode_reward_mean: 1.9940828402366864
episode_reward_min: 1.0
episodes_this_iter: 338
episodes_total: 22025
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9940828402366864
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.96803259849548
time_total_s: 4893.636399745941
timers:
  learn_throughput: 461.691
  learn_time_ms: 35738.225
  load_throughput: 4467786.701
  load_time_ms: 3.693
  training_iteration_time_ms: 47056.79
  update_time_ms: 2.395
timesteps_total: 1782000
training_iteration: 108

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.39375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7945205479452054
  reward for individual goal_min: 0.0
episode_len_mean: 189.0
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 7246
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.84246826171875
time_total_s: 4924.4618237018585
timers:
  learn_throughput: 437.24
  learn_time_ms: 37736.705
  load_throughput: 4663352.47
  load_time_ms: 3.538
  training_iteration_time_ms: 50192.963
  update_time_ms: 2.66
timesteps_total: 1518000
training_iteration: 92

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22297297297297297
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9788732394366197
  reward for individual goal_min: 0.0
episode_len_mean: 178.57
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 7733
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.95514726638794
time_total_s: 4907.172939300537
timers:
  learn_throughput: 452.851
  learn_time_ms: 36435.801
  load_throughput: 4882637.524
  load_time_ms: 3.379
  training_iteration_time_ms: 48566.234
  update_time_ms: 2.593
timesteps_total: 1518000
training_iteration: 92

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8269230769230769
  reward for individual goal_min: 0.5
episode_len_mean: 81.83333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.8181818181818181
episode_reward_min: 1.0
episodes_this_iter: 198
episodes_total: 14579
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.98989898989899
  agent_1: 0.8282828282828283
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.57392072677612
time_total_s: 4921.343760251999
timers:
  learn_throughput: 413.838
  learn_time_ms: 39870.687
  load_throughput: 4225289.456
  load_time_ms: 3.905
  training_iteration_time_ms: 52432.66
  update_time_ms: 2.679
timesteps_total: 1485000
training_iteration: 90

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22435897435897437
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 186.81
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 7795
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.4471321105957
time_total_s: 4929.971047401428
timers:
  learn_throughput: 443.847
  learn_time_ms: 37175.01
  load_throughput: 4831304.129
  load_time_ms: 3.415
  training_iteration_time_ms: 48978.343
  update_time_ms: 2.634
timesteps_total: 1534500
training_iteration: 93

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9675324675324676
  reward for individual goal_min: 0.0
episode_len_mean: 185.78
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 7794
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.92967915534973
time_total_s: 4941.256224155426
timers:
  learn_throughput: 426.8
  learn_time_ms: 38659.754
  load_throughput: 4988719.841
  load_time_ms: 3.307
  training_iteration_time_ms: 51535.437
  update_time_ms: 2.67
timesteps_total: 1485000
training_iteration: 90

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3287671232876712
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9944444444444445
  reward for individual goal_min: 0.5
episode_len_mean: 149.5321100917431
episode_reward_max: 2.0
episode_reward_mean: 1.4678899082568808
episode_reward_min: 0.0
episodes_this_iter: 109
episodes_total: 9333
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7522935779816514
  agent_1: 0.7155963302752294
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.138118267059326
time_total_s: 4943.106252670288
timers:
  learn_throughput: 517.231
  learn_time_ms: 31900.665
  load_throughput: 5238633.532
  load_time_ms: 3.15
  training_iteration_time_ms: 42543.442
  update_time_ms: 2.489
timesteps_total: 1765500
training_iteration: 107

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8489583333333334
  reward for individual goal_min: 0.5
episode_len_mean: 76.32863849765258
episode_reward_max: 2.0
episode_reward_mean: 1.863849765258216
episode_reward_min: 1.0
episodes_this_iter: 213
episodes_total: 10655
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.863849765258216
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 59.513498067855835
time_total_s: 4958.441359758377
timers:
  learn_throughput: 364.986
  learn_time_ms: 45207.249
  load_throughput: 4042076.933
  load_time_ms: 4.082
  training_iteration_time_ms: 59466.22
  update_time_ms: 2.989
timesteps_total: 1353000
training_iteration: 82

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27611940298507465
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9882352941176471
  reward for individual goal_min: 0.5
episode_len_mean: 161.07843137254903
episode_reward_max: 2.0
episode_reward_mean: 1.4215686274509804
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 6954
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7549019607843137
  agent_1: 0.6666666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.17507719993591
time_total_s: 4937.863968133926
timers:
  learn_throughput: 435.565
  learn_time_ms: 37881.82
  load_throughput: 4660808.567
  load_time_ms: 3.54
  training_iteration_time_ms: 49968.793
  update_time_ms: 2.626
timesteps_total: 1468500
training_iteration: 89

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3395061728395062
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.726027397260274
  reward for individual goal_min: 0.0
episode_len_mean: 198.24
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 8032
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.4530713558197
time_total_s: 4954.135705471039
timers:
  learn_throughput: 474.096
  learn_time_ms: 34803.097
  load_throughput: 5007055.283
  load_time_ms: 3.295
  training_iteration_time_ms: 46289.236
  update_time_ms: 2.481
timesteps_total: 1683000
training_iteration: 102

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.88
  reward for individual goal_min: 0.0
episode_len_mean: 192.83
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 8248
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.94605374336243
time_total_s: 4970.219121932983
timers:
  learn_throughput: 489.005
  learn_time_ms: 33742.014
  load_throughput: 4691169.979
  load_time_ms: 3.517
  training_iteration_time_ms: 44835.809
  update_time_ms: 2.613
timesteps_total: 1732500
training_iteration: 105

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8020833333333334
  reward for individual goal_min: 0.5
episode_len_mean: 78.71904761904761
episode_reward_max: 2.0
episode_reward_mean: 1.819047619047619
episode_reward_min: 1.0
episodes_this_iter: 210
episodes_total: 16202
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.819047619047619
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.026065826416016
time_total_s: 4953.466628789902
timers:
  learn_throughput: 445.478
  learn_time_ms: 37038.877
  load_throughput: 4794685.844
  load_time_ms: 3.441
  training_iteration_time_ms: 48518.885
  update_time_ms: 2.606
timesteps_total: 1584000
training_iteration: 96

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1891891891891892
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9805194805194806
  reward for individual goal_min: 0.5
episode_len_mean: 186.05
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 6960
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.334662675857544
time_total_s: 4950.009194135666
timers:
  learn_throughput: 418.34
  learn_time_ms: 39441.575
  load_throughput: 4495210.678
  load_time_ms: 3.671
  training_iteration_time_ms: 51929.737
  update_time_ms: 2.694
timesteps_total: 1369500
training_iteration: 83

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972375690607734
  reward for individual goal_min: 0.5
episode_len_mean: 48.42058823529412
episode_reward_max: 2.0
episode_reward_mean: 1.9970588235294118
episode_reward_min: 1.0
episodes_this_iter: 340
episodes_total: 22365
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9970588235294118
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.723586320877075
time_total_s: 4940.359986066818
timers:
  learn_throughput: 461.386
  learn_time_ms: 35761.791
  load_throughput: 4432191.823
  load_time_ms: 3.723
  training_iteration_time_ms: 47077.924
  update_time_ms: 2.415
timesteps_total: 1798500
training_iteration: 109

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 182.82
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 7006
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.798744678497314
time_total_s: 4958.0583436489105
timers:
  learn_throughput: 420.305
  learn_time_ms: 39257.215
  load_throughput: 4286794.846
  load_time_ms: 3.849
  training_iteration_time_ms: 51900.959
  update_time_ms: 2.656
timesteps_total: 1386000
training_iteration: 84

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 189.57
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 7818
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.75895047187805
time_total_s: 4952.931889772415
timers:
  learn_throughput: 455.064
  learn_time_ms: 36258.617
  load_throughput: 4908332.518
  load_time_ms: 3.362
  training_iteration_time_ms: 48333.184
  update_time_ms: 2.589
timesteps_total: 1534500
training_iteration: 93

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.39634146341463417
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7426470588235294
  reward for individual goal_min: 0.0
episode_len_mean: 194.27
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 7333
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.64245820045471
time_total_s: 4975.104281902313
timers:
  learn_throughput: 438.373
  learn_time_ms: 37639.178
  load_throughput: 4687484.151
  load_time_ms: 3.52
  training_iteration_time_ms: 50033.012
  update_time_ms: 2.646
timesteps_total: 1534500
training_iteration: 93

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29577464788732394
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.5
episode_len_mean: 172.68
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 7890
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.73772168159485
time_total_s: 4980.708769083023
timers:
  learn_throughput: 441.784
  learn_time_ms: 37348.542
  load_throughput: 4856971.534
  load_time_ms: 3.397
  training_iteration_time_ms: 49190.745
  update_time_ms: 2.632
timesteps_total: 1551000
training_iteration: 94

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8962264150943396
  reward for individual goal_min: 0.5
episode_len_mean: 64.93050193050193
episode_reward_max: 2.0
episode_reward_mean: 1.915057915057915
episode_reward_min: 1.0
episodes_this_iter: 259
episodes_total: 14838
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9961389961389961
  agent_1: 0.918918918918919
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.778088092803955
time_total_s: 4976.121848344803
timers:
  learn_throughput: 411.819
  learn_time_ms: 40066.16
  load_throughput: 4229265.932
  load_time_ms: 3.901
  training_iteration_time_ms: 52675.883
  update_time_ms: 2.675
timesteps_total: 1501500
training_iteration: 91

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27848101265822783
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.52
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 7887
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.26257920265198
time_total_s: 4989.518803358078
timers:
  learn_throughput: 429.076
  learn_time_ms: 38454.687
  load_throughput: 4973518.746
  load_time_ms: 3.318
  training_iteration_time_ms: 51251.699
  update_time_ms: 2.641
timesteps_total: 1501500
training_iteration: 91

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2662337662337662
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.73
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 9423
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.133824586868286
time_total_s: 4986.240077257156
timers:
  learn_throughput: 514.108
  learn_time_ms: 32094.422
  load_throughput: 5233918.896
  load_time_ms: 3.153
  training_iteration_time_ms: 42723.519
  update_time_ms: 2.464
timesteps_total: 1782000
training_iteration: 108

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2564102564102564
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8857142857142857
  reward for individual goal_min: 0.0
episode_len_mean: 198.69
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 8331
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.78132367134094
time_total_s: 5014.000445604324
timers:
  learn_throughput: 491.766
  learn_time_ms: 33552.564
  load_throughput: 4681617.859
  load_time_ms: 3.524
  training_iteration_time_ms: 44606.67
  update_time_ms: 2.635
timesteps_total: 1749000
training_iteration: 106

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20512820512820512
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 190.24
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 7039
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.6845977306366
time_total_s: 4984.548565864563
timers:
  learn_throughput: 437.074
  learn_time_ms: 37751.038
  load_throughput: 4701655.355
  load_time_ms: 3.509
  training_iteration_time_ms: 49820.815
  update_time_ms: 2.639
timesteps_total: 1485000
training_iteration: 90

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8175675675675675
  reward for individual goal_min: 0.0
episode_len_mean: 192.54
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 8121
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.90180969238281
time_total_s: 5000.037515163422
timers:
  learn_throughput: 473.726
  learn_time_ms: 34830.23
  load_throughput: 5014093.013
  load_time_ms: 3.291
  training_iteration_time_ms: 46351.66
  update_time_ms: 2.5
timesteps_total: 1699500
training_iteration: 103

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.806930693069307
  reward for individual goal_min: 0.5
episode_len_mean: 78.66666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.8169014084507042
episode_reward_min: 1.0
episodes_this_iter: 213
episodes_total: 16415
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8169014084507042
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.806761503219604
time_total_s: 5000.273390293121
timers:
  learn_throughput: 450.004
  learn_time_ms: 36666.367
  load_throughput: 4788382.678
  load_time_ms: 3.446
  training_iteration_time_ms: 48078.559
  update_time_ms: 2.582
timesteps_total: 1600500
training_iteration: 97

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8064516129032258
  reward for individual goal_min: 0.5
episode_len_mean: 88.62694300518135
episode_reward_max: 2.0
episode_reward_mean: 1.8134715025906736
episode_reward_min: 1.0
episodes_this_iter: 193
episodes_total: 10848
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9948186528497409
  agent_1: 0.8186528497409327
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.71766114234924
time_total_s: 5018.159020900726
timers:
  learn_throughput: 364.181
  learn_time_ms: 45307.129
  load_throughput: 4009339.791
  load_time_ms: 4.115
  training_iteration_time_ms: 59559.993
  update_time_ms: 2.978
timesteps_total: 1369500
training_iteration: 83

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32926829268292684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.63
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 7055
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.10657000541687
time_total_s: 5001.115764141083
timers:
  learn_throughput: 423.149
  learn_time_ms: 38993.369
  load_throughput: 4461996.763
  load_time_ms: 3.698
  training_iteration_time_ms: 51423.815
  update_time_ms: 2.673
timesteps_total: 1386000
training_iteration: 84

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939759036144579
  reward for individual goal_min: 0.5
episode_len_mean: 49.83783783783784
episode_reward_max: 2.0
episode_reward_mean: 1.993993993993994
episode_reward_min: 1.0
episodes_this_iter: 333
episodes_total: 22698
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.993993993993994
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.10957479476929
time_total_s: 4988.4695608615875
timers:
  learn_throughput: 460.353
  learn_time_ms: 35842.076
  load_throughput: 4467036.908
  load_time_ms: 3.694
  training_iteration_time_ms: 47156.343
  update_time_ms: 2.421
timesteps_total: 1815000
training_iteration: 110

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2972972972972973
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9788732394366197
  reward for individual goal_min: 0.0
episode_len_mean: 173.54
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 7100
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.4394371509552
time_total_s: 5010.497780799866
timers:
  learn_throughput: 423.097
  learn_time_ms: 38998.151
  load_throughput: 4344600.859
  load_time_ms: 3.798
  training_iteration_time_ms: 51716.581
  update_time_ms: 2.654
timesteps_total: 1402500
training_iteration: 85

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2894736842105263
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 172.02
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 7914
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.79054927825928
time_total_s: 5000.722439050674
timers:
  learn_throughput: 454.308
  learn_time_ms: 36318.947
  load_throughput: 4890642.583
  load_time_ms: 3.374
  training_iteration_time_ms: 48340.218
  update_time_ms: 2.579
timesteps_total: 1551000
training_iteration: 94

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2974683544303797
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7533333333333333
  reward for individual goal_min: 0.0
episode_len_mean: 203.16
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 7413
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.582223892211914
time_total_s: 5023.686505794525
timers:
  learn_throughput: 441.507
  learn_time_ms: 37371.973
  load_throughput: 4654382.675
  load_time_ms: 3.545
  training_iteration_time_ms: 49729.455
  update_time_ms: 2.63
timesteps_total: 1551000
training_iteration: 94

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3223684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9710144927536232
  reward for individual goal_min: 0.0
episode_len_mean: 179.54
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 7984
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.10702705383301
time_total_s: 5029.815796136856
timers:
  learn_throughput: 442.968
  learn_time_ms: 37248.779
  load_throughput: 4820602.523
  load_time_ms: 3.423
  training_iteration_time_ms: 49090.554
  update_time_ms: 2.623
timesteps_total: 1567500
training_iteration: 95

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8803418803418803
  reward for individual goal_min: 0.5
episode_len_mean: 73.12272727272727
episode_reward_max: 2.0
episode_reward_mean: 1.8727272727272728
episode_reward_min: 1.0
episodes_this_iter: 220
episodes_total: 15058
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9863636363636363
  agent_1: 0.8863636363636364
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.80905723571777
time_total_s: 5027.930905580521
timers:
  learn_throughput: 411.134
  learn_time_ms: 40132.912
  load_throughput: 4210379.996
  load_time_ms: 3.919
  training_iteration_time_ms: 52745.441
  update_time_ms: 2.657
timesteps_total: 1518000
training_iteration: 92

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2792207792207792
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.5
episode_len_mean: 177.05
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 9518
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.45511317253113
time_total_s: 5029.6951904296875
timers:
  learn_throughput: 512.272
  learn_time_ms: 32209.474
  load_throughput: 5212630.945
  load_time_ms: 3.165
  training_iteration_time_ms: 42910.949
  update_time_ms: 2.434
timesteps_total: 1798500
training_iteration: 109

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2564102564102564
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9785714285714285
  reward for individual goal_min: 0.0
episode_len_mean: 187.77
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 7977
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.85732436180115
time_total_s: 5043.376127719879
timers:
  learn_throughput: 428.043
  learn_time_ms: 38547.521
  load_throughput: 5007562.499
  load_time_ms: 3.295
  training_iteration_time_ms: 51275.952
  update_time_ms: 2.643
timesteps_total: 1518000
training_iteration: 92

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8642857142857143
  reward for individual goal_min: 0.0
episode_len_mean: 202.8
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 8413
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.67133712768555
time_total_s: 5058.67178273201
timers:
  learn_throughput: 492.445
  learn_time_ms: 33506.254
  load_throughput: 4708276.594
  load_time_ms: 3.504
  training_iteration_time_ms: 44488.81
  update_time_ms: 2.615
timesteps_total: 1765500
training_iteration: 107

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33766233766233766
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.678082191780822
  reward for individual goal_min: 0.0
episode_len_mean: 204.59
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 73
episodes_total: 8194
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.98130202293396
time_total_s: 5047.018817186356
timers:
  learn_throughput: 471.885
  learn_time_ms: 34966.174
  load_throughput: 4977704.126
  load_time_ms: 3.315
  training_iteration_time_ms: 46466.302
  update_time_ms: 2.521
timesteps_total: 1716000
training_iteration: 104

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18666666666666668
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.975609756097561
  reward for individual goal_min: 0.0
episode_len_mean: 194.64
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 7124
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.23089003562927
time_total_s: 5031.779455900192
timers:
  learn_throughput: 439.953
  learn_time_ms: 37504.042
  load_throughput: 4728995.791
  load_time_ms: 3.489
  training_iteration_time_ms: 49501.399
  update_time_ms: 2.633
timesteps_total: 1501500
training_iteration: 91

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8520408163265306
  reward for individual goal_min: 0.5
episode_len_mean: 69.2094017094017
episode_reward_max: 2.0
episode_reward_mean: 1.876068376068376
episode_reward_min: 1.0
episodes_this_iter: 234
episodes_total: 16649
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8760683760683761
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.6683623790741
time_total_s: 5048.941752672195
timers:
  learn_throughput: 451.445
  learn_time_ms: 36549.269
  load_throughput: 4866466.212
  load_time_ms: 3.391
  training_iteration_time_ms: 47942.971
  update_time_ms: 2.531
timesteps_total: 1617000
training_iteration: 98

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971098265895953
  reward for individual goal_min: 0.5
episode_len_mean: 50.332317073170735
episode_reward_max: 2.0
episode_reward_mean: 1.9969512195121952
episode_reward_min: 1.0
episodes_this_iter: 328
episodes_total: 23026
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9969512195121951
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 45.73193430900574
time_total_s: 5034.201495170593
timers:
  learn_throughput: 461.978
  learn_time_ms: 35715.965
  load_throughput: 4457944.113
  load_time_ms: 3.701
  training_iteration_time_ms: 46980.457
  update_time_ms: 2.448
timesteps_total: 1831500
training_iteration: 111

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.310126582278481
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.19
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 7152
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.34786868095398
time_total_s: 5050.463632822037
timers:
  learn_throughput: 425.112
  learn_time_ms: 38813.317
  load_throughput: 4447916.088
  load_time_ms: 3.71
  training_iteration_time_ms: 51120.673
  update_time_ms: 2.692
timesteps_total: 1402500
training_iteration: 85

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2777777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.5
episode_len_mean: 176.51
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 8008
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.2112135887146
time_total_s: 5046.933652639389
timers:
  learn_throughput: 457.613
  learn_time_ms: 36056.663
  load_throughput: 4921876.694
  load_time_ms: 3.352
  training_iteration_time_ms: 48018.382
  update_time_ms: 2.581
timesteps_total: 1567500
training_iteration: 95

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22142857142857142
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.99375
  reward for individual goal_min: 0.5
episode_len_mean: 172.24
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 7194
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.85972309112549
time_total_s: 5059.357503890991
timers:
  learn_throughput: 427.228
  learn_time_ms: 38621.078
  load_throughput: 4364712.976
  load_time_ms: 3.78
  training_iteration_time_ms: 51200.104
  update_time_ms: 2.639
timesteps_total: 1419000
training_iteration: 86

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8406593406593407
  reward for individual goal_min: 0.5
episode_len_mean: 79.42156862745098
episode_reward_max: 2.0
episode_reward_mean: 1.857843137254902
episode_reward_min: 1.0
episodes_this_iter: 204
episodes_total: 11052
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9852941176470589
  agent_1: 0.8725490196078431
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.39776349067688
time_total_s: 5078.556784391403
timers:
  learn_throughput: 363.522
  learn_time_ms: 45389.247
  load_throughput: 4138101.064
  load_time_ms: 3.987
  training_iteration_time_ms: 59626.929
  update_time_ms: 3.026
timesteps_total: 1386000
training_iteration: 84

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.38405797101449274
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6626506024096386
  reward for individual goal_min: 0.0
episode_len_mean: 195.6
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 7498
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.927454471588135
time_total_s: 5074.613960266113
timers:
  learn_throughput: 440.781
  learn_time_ms: 37433.597
  load_throughput: 4661781.831
  load_time_ms: 3.539
  training_iteration_time_ms: 49779.966
  update_time_ms: 2.616
timesteps_total: 1567500
training_iteration: 95

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9875
  reward for individual goal_min: 0.0
episode_len_mean: 188.06
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 8071
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.917837381362915
time_total_s: 5075.733633518219
timers:
  learn_throughput: 445.209
  learn_time_ms: 37061.251
  load_throughput: 4798741.896
  load_time_ms: 3.438
  training_iteration_time_ms: 48870.498
  update_time_ms: 2.61
timesteps_total: 1584000
training_iteration: 96

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 164.74
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 9616
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.87298536300659
time_total_s: 5074.568175792694
timers:
  learn_throughput: 510.737
  learn_time_ms: 32306.242
  load_throughput: 5201269.841
  load_time_ms: 3.172
  training_iteration_time_ms: 43039.311
  update_time_ms: 2.439
timesteps_total: 1815000
training_iteration: 110

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8791666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 75.68493150684931
episode_reward_max: 2.0
episode_reward_mean: 1.8675799086757991
episode_reward_min: 1.0
episodes_this_iter: 219
episodes_total: 15277
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.867579908675799
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 51.130213499069214
time_total_s: 5079.06111907959
timers:
  learn_throughput: 414.25
  learn_time_ms: 39831.057
  load_throughput: 4175854.316
  load_time_ms: 3.951
  training_iteration_time_ms: 52339.689
  update_time_ms: 2.632
timesteps_total: 1534500
training_iteration: 93

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27976190476190477
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8214285714285714
  reward for individual goal_min: 0.0
episode_len_mean: 206.92
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 8494
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.50674343109131
time_total_s: 5104.178526163101
timers:
  learn_throughput: 492.956
  learn_time_ms: 33471.55
  load_throughput: 4747389.231
  load_time_ms: 3.476
  training_iteration_time_ms: 44465.972
  update_time_ms: 2.606
timesteps_total: 1782000
training_iteration: 108

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3717948717948718
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 173.57
episode_reward_max: 2.0
episode_reward_mean: 1.41
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 7220
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.55323338508606
time_total_s: 5078.332689285278
timers:
  learn_throughput: 444.369
  learn_time_ms: 37131.277
  load_throughput: 4791499.013
  load_time_ms: 3.444
  training_iteration_time_ms: 49039.656
  update_time_ms: 2.594
timesteps_total: 1518000
training_iteration: 92

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22988505747126436
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9733333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 189.46
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 8066
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.99256944656372
time_total_s: 5094.368697166443
timers:
  learn_throughput: 424.62
  learn_time_ms: 38858.266
  load_throughput: 4991490.393
  load_time_ms: 3.306
  training_iteration_time_ms: 51628.398
  update_time_ms: 2.689
timesteps_total: 1534500
training_iteration: 93

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22602739726027396
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7253521126760564
  reward for individual goal_min: 0.0
episode_len_mean: 205.25
episode_reward_max: 2.0
episode_reward_mean: 1.01
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 8277
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.49
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.47083568572998
time_total_s: 5096.489652872086
timers:
  learn_throughput: 468.934
  learn_time_ms: 35186.184
  load_throughput: 4966095.424
  load_time_ms: 3.323
  training_iteration_time_ms: 46837.204
  update_time_ms: 2.522
timesteps_total: 1732500
training_iteration: 105

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8238095238095238
  reward for individual goal_min: 0.5
episode_len_mean: 77.98113207547169
episode_reward_max: 2.0
episode_reward_mean: 1.8254716981132075
episode_reward_min: 1.0
episodes_this_iter: 212
episodes_total: 16861
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8301886792452831
  agent_1: 0.9952830188679245
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.90227770805359
time_total_s: 5096.844030380249
timers:
  learn_throughput: 452.832
  learn_time_ms: 36437.386
  load_throughput: 4866911.116
  load_time_ms: 3.39
  training_iteration_time_ms: 47820.396
  update_time_ms: 2.52
timesteps_total: 1633500
training_iteration: 99

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 48.486646884273
episode_reward_max: 2.0
episode_reward_mean: 1.9940652818991098
episode_reward_min: 1.0
episodes_this_iter: 337
episodes_total: 23363
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9970326409495549
  agent_1: 0.9970326409495549
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.748541831970215
time_total_s: 5081.9500370025635
timers:
  learn_throughput: 462.133
  learn_time_ms: 35703.979
  load_throughput: 4486293.749
  load_time_ms: 3.678
  training_iteration_time_ms: 46979.522
  update_time_ms: 2.436
timesteps_total: 1848000
training_iteration: 112

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2635135135135135
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 175.12
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 8102
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.147459268569946
time_total_s: 5092.081111907959
timers:
  learn_throughput: 462.244
  learn_time_ms: 35695.418
  load_throughput: 4914118.056
  load_time_ms: 3.358
  training_iteration_time_ms: 47564.546
  update_time_ms: 2.571
timesteps_total: 1584000
training_iteration: 96

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2222222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9743589743589743
  reward for individual goal_min: 0.0
episode_len_mean: 177.85
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 7246
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.39883017539978
time_total_s: 5098.8624629974365
timers:
  learn_throughput: 430.289
  learn_time_ms: 38346.358
  load_throughput: 4521348.185
  load_time_ms: 3.649
  training_iteration_time_ms: 50598.372
  update_time_ms: 2.703
timesteps_total: 1419000
training_iteration: 86

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.67
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 7292
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.32014083862305
time_total_s: 5109.677644729614
timers:
  learn_throughput: 432.229
  learn_time_ms: 38174.17
  load_throughput: 4468652.16
  load_time_ms: 3.692
  training_iteration_time_ms: 50784.222
  update_time_ms: 2.625
timesteps_total: 1435500
training_iteration: 87

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15492957746478872
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9942528735632183
  reward for individual goal_min: 0.5
episode_len_mean: 177.8
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 9709
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.14986801147461
time_total_s: 5117.718043804169
timers:
  learn_throughput: 510.208
  learn_time_ms: 32339.771
  load_throughput: 5245383.478
  load_time_ms: 3.146
  training_iteration_time_ms: 43106.635
  update_time_ms: 2.448
timesteps_total: 1831500
training_iteration: 111

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.95
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 8162
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.77906250953674
time_total_s: 5124.512696027756
timers:
  learn_throughput: 443.914
  learn_time_ms: 37169.346
  load_throughput: 4772170.459
  load_time_ms: 3.458
  training_iteration_time_ms: 48967.156
  update_time_ms: 2.622
timesteps_total: 1600500
training_iteration: 97

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8097826086956522
  reward for individual goal_min: 0.5
episode_len_mean: 95.77058823529411
episode_reward_max: 2.0
episode_reward_mean: 1.7941176470588236
episode_reward_min: 1.0
episodes_this_iter: 170
episodes_total: 11222
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9882352941176471
  agent_1: 0.8058823529411765
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.38296556472778
time_total_s: 5135.939749956131
timers:
  learn_throughput: 365.385
  learn_time_ms: 45157.877
  load_throughput: 4137235.228
  load_time_ms: 3.988
  training_iteration_time_ms: 59281.376
  update_time_ms: 2.986
timesteps_total: 1402500
training_iteration: 85

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.38235294117647056
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6538461538461539
  reward for individual goal_min: 0.0
episode_len_mean: 198.42
episode_reward_max: 2.0
episode_reward_mean: 1.02
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 7580
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.46
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.845715284347534
time_total_s: 5127.459675550461
timers:
  learn_throughput: 439.371
  learn_time_ms: 37553.668
  load_throughput: 4684216.24
  load_time_ms: 3.522
  training_iteration_time_ms: 50043.946
  update_time_ms: 2.636
timesteps_total: 1584000
training_iteration: 96

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.37142857142857144
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.831081081081081
  reward for individual goal_min: 0.0
episode_len_mean: 178.55
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 8587
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.76
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.8731472492218
time_total_s: 5148.051673412323
timers:
  learn_throughput: 494.036
  learn_time_ms: 33398.359
  load_throughput: 4737120.601
  load_time_ms: 3.483
  training_iteration_time_ms: 44377.922
  update_time_ms: 2.58
timesteps_total: 1798500
training_iteration: 109

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.992
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8991228070175439
  reward for individual goal_min: 0.5
episode_len_mean: 69.08786610878661
episode_reward_max: 2.0
episode_reward_mean: 1.895397489539749
episode_reward_min: 0.0
episodes_this_iter: 239
episodes_total: 15516
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.99581589958159
  agent_1: 0.899581589958159
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.22429537773132
time_total_s: 5132.285414457321
timers:
  learn_throughput: 414.943
  learn_time_ms: 39764.53
  load_throughput: 4165599.234
  load_time_ms: 3.961
  training_iteration_time_ms: 52262.509
  update_time_ms: 2.653
timesteps_total: 1551000
training_iteration: 94

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20422535211267606
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9821428571428571
  reward for individual goal_min: 0.5
episode_len_mean: 179.55
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 7313
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.523319721221924
time_total_s: 5123.8560090065
timers:
  learn_throughput: 449.93
  learn_time_ms: 36672.373
  load_throughput: 4828775.886
  load_time_ms: 3.417
  training_iteration_time_ms: 48503.577
  update_time_ms: 2.592
timesteps_total: 1534500
training_iteration: 93

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2887323943661972
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8082191780821918
  reward for individual goal_min: 0.0
episode_len_mean: 198.71
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 8362
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.83539581298828
time_total_s: 5142.325048685074
timers:
  learn_throughput: 470.098
  learn_time_ms: 35099.087
  load_throughput: 4969161.994
  load_time_ms: 3.32
  training_iteration_time_ms: 46663.516
  update_time_ms: 2.535
timesteps_total: 1749000
training_iteration: 106

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24342105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 176.65
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 8158
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.47920560836792
time_total_s: 5145.847902774811
timers:
  learn_throughput: 423.613
  learn_time_ms: 38950.631
  load_throughput: 4973268.562
  load_time_ms: 3.318
  training_iteration_time_ms: 51817.611
  update_time_ms: 2.674
timesteps_total: 1551000
training_iteration: 94

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3051948051948052
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.37
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 8192
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.5669310092926
time_total_s: 5136.648042917252
timers:
  learn_throughput: 468.383
  learn_time_ms: 35227.601
  load_throughput: 4956314.885
  load_time_ms: 3.329
  training_iteration_time_ms: 46991.212
  update_time_ms: 2.567
timesteps_total: 1600500
training_iteration: 97

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 77.28333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.8333333333333333
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8333333333333334
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.85
  reward for individual goal_min: 0.5
episode_len_mean: 75.3409090909091
episode_reward_max: 2.0
episode_reward_mean: 1.8363636363636364
episode_reward_min: 1.0
episodes_this_iter: 220
episodes_total: 17081
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8363636363636363
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 55.260180950164795
time_total_s: 5152.104211330414
timers:
  learn_throughput: 450.728
  learn_time_ms: 36607.408
  load_throughput: 4839378.488
  load_time_ms: 3.41
  training_iteration_time_ms: 47972.631
  update_time_ms: 2.523
timesteps_total: 1650000
training_iteration: 100

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971428571428571
  reward for individual goal_min: 0.5
episode_len_mean: 46.257617728531855
episode_reward_max: 2.0
episode_reward_mean: 1.997229916897507
episode_reward_min: 1.0
episodes_this_iter: 361
episodes_total: 23724
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.997229916897507
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.65993571281433
time_total_s: 5130.609972715378
timers:
  learn_throughput: 460.057
  learn_time_ms: 35865.093
  load_throughput: 4509857.352
  load_time_ms: 3.659
  training_iteration_time_ms: 47146.528
  update_time_ms: 2.429
timesteps_total: 1864500
training_iteration: 113

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26973684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 178.57
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 7335
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.90878129005432
time_total_s: 5146.771244287491
timers:
  learn_throughput: 434.019
  learn_time_ms: 38016.763
  load_throughput: 4572279.07
  load_time_ms: 3.609
  training_iteration_time_ms: 50091.033
  update_time_ms: 2.695
timesteps_total: 1435500
training_iteration: 87

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.0
episode_len_mean: 180.23
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 7384
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.78742980957031
time_total_s: 5158.465074539185
timers:
  learn_throughput: 435.409
  learn_time_ms: 37895.432
  load_throughput: 4581389.788
  load_time_ms: 3.602
  training_iteration_time_ms: 50378.736
  update_time_ms: 2.628
timesteps_total: 1452000
training_iteration: 88

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24647887323943662
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 166.55
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 9806
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.34688377380371
time_total_s: 5160.064927577972
timers:
  learn_throughput: 511.721
  learn_time_ms: 32244.125
  load_throughput: 5254344.023
  load_time_ms: 3.14
  training_iteration_time_ms: 43010.881
  update_time_ms: 2.463
timesteps_total: 1848000
training_iteration: 112

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2631578947368421
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 176.04
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 8256
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.22149634361267
time_total_s: 5171.734192371368
timers:
  learn_throughput: 445.163
  learn_time_ms: 37065.038
  load_throughput: 4756786.836
  load_time_ms: 3.469
  training_iteration_time_ms: 48805.209
  update_time_ms: 2.603
timesteps_total: 1617000
training_iteration: 98

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26811594202898553
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8181818181818182
  reward for individual goal_min: 0.0
episode_len_mean: 199.75
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 8666
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.225799322128296
time_total_s: 5192.277472734451
timers:
  learn_throughput: 494.332
  learn_time_ms: 33378.391
  load_throughput: 4740722.555
  load_time_ms: 3.48
  training_iteration_time_ms: 44349.808
  update_time_ms: 2.599
timesteps_total: 1815000
training_iteration: 110

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3618421052631579
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6351351351351351
  reward for individual goal_min: 0.0
episode_len_mean: 195.17
episode_reward_max: 2.0
episode_reward_mean: 1.04
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 7665
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.071486473083496
time_total_s: 5180.531162023544
timers:
  learn_throughput: 437.504
  learn_time_ms: 37713.915
  load_throughput: 4664578.304
  load_time_ms: 3.537
  training_iteration_time_ms: 50232.799
  update_time_ms: 2.622
timesteps_total: 1600500
training_iteration: 97

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2605633802816901
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9883720930232558
  reward for individual goal_min: 0.5
episode_len_mean: 172.71
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 7408
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.65987753868103
time_total_s: 5168.515886545181
timers:
  learn_throughput: 456.957
  learn_time_ms: 36108.44
  load_throughput: 4895105.038
  load_time_ms: 3.371
  training_iteration_time_ms: 47750.184
  update_time_ms: 2.547
timesteps_total: 1551000
training_iteration: 94

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8232323232323232
  reward for individual goal_min: 0.5
episode_len_mean: 84.14
episode_reward_max: 2.0
episode_reward_mean: 1.825
episode_reward_min: 1.0
episodes_this_iter: 200
episodes_total: 11422
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.99
  agent_1: 0.835
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.44135785102844
time_total_s: 5194.381107807159
timers:
  learn_throughput: 366.289
  learn_time_ms: 45046.409
  load_throughput: 4027703.536
  load_time_ms: 4.097
  training_iteration_time_ms: 59109.184
  update_time_ms: 2.929
timesteps_total: 1419000
training_iteration: 86

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3597560975609756
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.75
  reward for individual goal_min: 0.0
episode_len_mean: 193.94
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 8446
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.420055866241455
time_total_s: 5189.745104551315
timers:
  learn_throughput: 467.605
  learn_time_ms: 35286.203
  load_throughput: 4991310.394
  load_time_ms: 3.306
  training_iteration_time_ms: 46962.87
  update_time_ms: 2.554
timesteps_total: 1765500
training_iteration: 107

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8949579831932774
  reward for individual goal_min: 0.5
episode_len_mean: 68.85654008438819
episode_reward_max: 2.0
episode_reward_mean: 1.8945147679324894
episode_reward_min: 1.0
episodes_this_iter: 237
episodes_total: 15753
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9957805907172996
  agent_1: 0.8987341772151899
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.898499727249146
time_total_s: 5184.18391418457
timers:
  learn_throughput: 414.757
  learn_time_ms: 39782.287
  load_throughput: 4131554.452
  load_time_ms: 3.994
  training_iteration_time_ms: 52239.466
  update_time_ms: 2.649
timesteps_total: 1567500
training_iteration: 95

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2692307692307692
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 180.13
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 8249
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.404316663742065
time_total_s: 5196.252219438553
timers:
  learn_throughput: 424.735
  learn_time_ms: 38847.75
  load_throughput: 4970875.215
  load_time_ms: 3.319
  training_iteration_time_ms: 51657.944
  update_time_ms: 2.697
timesteps_total: 1567500
training_iteration: 95

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1796875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9878048780487805
  reward for individual goal_min: 0.5
episode_len_mean: 173.81
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 8287
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.25861358642578
time_total_s: 5180.906656503677
timers:
  learn_throughput: 472.357
  learn_time_ms: 34931.177
  load_throughput: 4976594.493
  load_time_ms: 3.316
  training_iteration_time_ms: 46583.948
  update_time_ms: 2.56
timesteps_total: 1617000
training_iteration: 98

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30120481927710846
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 183.8
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 7426
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.99385690689087
time_total_s: 5193.765101194382
timers:
  learn_throughput: 438.624
  learn_time_ms: 37617.642
  load_throughput: 4545282.446
  load_time_ms: 3.63
  training_iteration_time_ms: 49744.498
  update_time_ms: 2.732
timesteps_total: 1452000
training_iteration: 88

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 48.44117647058823
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 340
episodes_total: 24064
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 46.921436071395874
time_total_s: 5177.531408786774
timers:
  learn_throughput: 461.601
  learn_time_ms: 35745.194
  load_throughput: 4534709.529
  load_time_ms: 3.639
  training_iteration_time_ms: 47037.019
  update_time_ms: 2.433
timesteps_total: 1881000
training_iteration: 114

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8620689655172413
  reward for individual goal_min: 0.5
episode_len_mean: 72.47161572052401
episode_reward_max: 2.0
episode_reward_mean: 1.8602620087336244
episode_reward_min: 1.0
episodes_this_iter: 229
episodes_total: 17310
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8646288209606987
  agent_1: 0.9956331877729258
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.875853300094604
time_total_s: 5200.980064630508
timers:
  learn_throughput: 451.421
  learn_time_ms: 36551.26
  load_throughput: 4844900.765
  load_time_ms: 3.406
  training_iteration_time_ms: 47968.311
  update_time_ms: 2.548
timesteps_total: 1666500
training_iteration: 101

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24050632911392406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 189.16
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 9896
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.518622398376465
time_total_s: 5202.583549976349
timers:
  learn_throughput: 511.79
  learn_time_ms: 32239.781
  load_throughput: 5303468.105
  load_time_ms: 3.111
  training_iteration_time_ms: 42999.445
  update_time_ms: 2.472
timesteps_total: 1864500
training_iteration: 113

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26282051282051283
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.975
  reward for individual goal_min: 0.0
episode_len_mean: 177.28
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 7478
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.58119249343872
time_total_s: 5206.046267032623
timers:
  learn_throughput: 438.875
  learn_time_ms: 37596.163
  load_throughput: 4597948.111
  load_time_ms: 3.589
  training_iteration_time_ms: 50129.303
  update_time_ms: 2.645
timesteps_total: 1468500
training_iteration: 89

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3271604938271605
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9779411764705882
  reward for individual goal_min: 0.0
episode_len_mean: 184.38
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 8347
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.21197199821472
time_total_s: 5219.946164369583
timers:
  learn_throughput: 447.925
  learn_time_ms: 36836.553
  load_throughput: 4767699.303
  load_time_ms: 3.461
  training_iteration_time_ms: 48546.613
  update_time_ms: 2.618
timesteps_total: 1633500
training_iteration: 99

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3223684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7464788732394366
  reward for individual goal_min: 0.0
episode_len_mean: 210.56
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 8746
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.41
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.755216121673584
time_total_s: 5235.032688856125
timers:
  learn_throughput: 495.939
  learn_time_ms: 33270.203
  load_throughput: 4757538.944
  load_time_ms: 3.468
  training_iteration_time_ms: 44251.48
  update_time_ms: 2.609
timesteps_total: 1831500
training_iteration: 111

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27564102564102566
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9571428571428572
  reward for individual goal_min: 0.0
episode_len_mean: 190.32
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 7497
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.72928833961487
time_total_s: 5215.245174884796
timers:
  learn_throughput: 459.707
  learn_time_ms: 35892.449
  load_throughput: 4930502.768
  load_time_ms: 3.347
  training_iteration_time_ms: 47493.358
  update_time_ms: 2.531
timesteps_total: 1567500
training_iteration: 95

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3076923076923077
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6875
  reward for individual goal_min: 0.0
episode_len_mean: 206.83
episode_reward_max: 2.0
episode_reward_mean: 0.99
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 7741
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.35564470291138
time_total_s: 5231.886806726456
timers:
  learn_throughput: 436.409
  learn_time_ms: 37808.582
  load_throughput: 4672009.937
  load_time_ms: 3.532
  training_iteration_time_ms: 50363.222
  update_time_ms: 2.595
timesteps_total: 1617000
training_iteration: 98

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3287671232876712
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7066666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 199.91
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 8527
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.93267822265625
time_total_s: 5236.677782773972
timers:
  learn_throughput: 468.224
  learn_time_ms: 35239.57
  load_throughput: 4988360.255
  load_time_ms: 3.308
  training_iteration_time_ms: 46836.731
  update_time_ms: 2.543
timesteps_total: 1782000
training_iteration: 108

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8010752688172043
  reward for individual goal_min: 0.5
episode_len_mean: 96.43452380952381
episode_reward_max: 2.0
episode_reward_mean: 1.7797619047619047
episode_reward_min: 1.0
episodes_this_iter: 168
episodes_total: 11590
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9940476190476191
  agent_1: 0.7857142857142857
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.0299768447876
time_total_s: 5251.411084651947
timers:
  learn_throughput: 367.247
  learn_time_ms: 44928.868
  load_throughput: 4062267.981
  load_time_ms: 4.062
  training_iteration_time_ms: 58920.559
  update_time_ms: 2.905
timesteps_total: 1435500
training_iteration: 87

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.904
  reward for individual goal_min: 0.5
episode_len_mean: 66.65725806451613
episode_reward_max: 2.0
episode_reward_mean: 1.903225806451613
episode_reward_min: 1.0
episodes_this_iter: 248
episodes_total: 16001
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9959677419354839
  agent_1: 0.907258064516129
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.80334734916687
time_total_s: 5236.987261533737
timers:
  learn_throughput: 413.858
  learn_time_ms: 39868.777
  load_throughput: 4131011.891
  load_time_ms: 3.994
  training_iteration_time_ms: 52315.081
  update_time_ms: 2.627
timesteps_total: 1584000
training_iteration: 96

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24285714285714285
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 175.08
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 8380
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.46297097206116
time_total_s: 5226.3696274757385
timers:
  learn_throughput: 474.069
  learn_time_ms: 34805.08
  load_throughput: 4970982.33
  load_time_ms: 3.319
  training_iteration_time_ms: 46341.348
  update_time_ms: 2.565
timesteps_total: 1633500
training_iteration: 99

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2564102564102564
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9929577464788732
  reward for individual goal_min: 0.5
episode_len_mean: 188.13
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 8338
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.84056615829468
time_total_s: 5248.0927855968475
timers:
  learn_throughput: 424.36
  learn_time_ms: 38882.09
  load_throughput: 4971946.578
  load_time_ms: 3.319
  training_iteration_time_ms: 51687.434
  update_time_ms: 2.698
timesteps_total: 1584000
training_iteration: 96

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2463768115942029
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 166.42
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 9994
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.13513493537903
time_total_s: 5245.718684911728
timers:
  learn_throughput: 512.567
  learn_time_ms: 32190.935
  load_throughput: 5294177.37
  load_time_ms: 3.117
  training_iteration_time_ms: 42975.654
  update_time_ms: 2.47
timesteps_total: 1881000
training_iteration: 114

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.291044776119403
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 163.99
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 7525
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.86912536621094
time_total_s: 5240.634226560593
timers:
  learn_throughput: 439.053
  learn_time_ms: 37580.874
  load_throughput: 4581693.093
  load_time_ms: 3.601
  training_iteration_time_ms: 49642.228
  update_time_ms: 2.729
timesteps_total: 1468500
training_iteration: 89

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 48.31578947368421
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 342
episodes_total: 24406
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.889678716659546
time_total_s: 5225.421087503433
timers:
  learn_throughput: 460.026
  learn_time_ms: 35867.509
  load_throughput: 4509357.798
  load_time_ms: 3.659
  training_iteration_time_ms: 47205.715
  update_time_ms: 2.424
timesteps_total: 1897500
training_iteration: 115

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8282828282828283
  reward for individual goal_min: 0.5
episode_len_mean: 73.75675675675676
episode_reward_max: 2.0
episode_reward_mean: 1.8468468468468469
episode_reward_min: 1.0
episodes_this_iter: 222
episodes_total: 17532
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8513513513513513
  agent_1: 0.9954954954954955
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.14899277687073
time_total_s: 5249.129057407379
timers:
  learn_throughput: 450.153
  learn_time_ms: 36654.196
  load_throughput: 4839547.695
  load_time_ms: 3.409
  training_iteration_time_ms: 48005.122
  update_time_ms: 2.546
timesteps_total: 1683000
training_iteration: 102

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3170731707317073
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 187.07
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 7565
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.61288022994995
time_total_s: 5251.659147262573
timers:
  learn_throughput: 439.479
  learn_time_ms: 37544.496
  load_throughput: 4625761.38
  load_time_ms: 3.567
  training_iteration_time_ms: 49876.66
  update_time_ms: 2.65
timesteps_total: 1485000
training_iteration: 90

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.4383561643835616
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7857142857142857
  reward for individual goal_min: 0.0
episode_len_mean: 181.04
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 8836
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.687315225601196
time_total_s: 5278.720004081726
timers:
  learn_throughput: 499.792
  learn_time_ms: 33013.725
  load_throughput: 4778661.953
  load_time_ms: 3.453
  training_iteration_time_ms: 43921.317
  update_time_ms: 2.616
timesteps_total: 1848000
training_iteration: 112

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95
  reward for individual goal_min: 0.5
episode_len_mean: 90.26666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.8833333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9166666666666666
  agent_1: 0.9666666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2865853658536585
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 190.65
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 7581
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.53005027770996
time_total_s: 5261.775225162506
timers:
  learn_throughput: 463.165
  learn_time_ms: 35624.443
  load_throughput: 4945052.947
  load_time_ms: 3.337
  training_iteration_time_ms: 47282.043
  update_time_ms: 2.543
timesteps_total: 1584000
training_iteration: 96

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24342105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 189.66
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 8433
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.433587312698364
time_total_s: 5277.3797516822815
timers:
  learn_throughput: 452.008
  learn_time_ms: 36503.785
  load_throughput: 4831304.129
  load_time_ms: 3.415
  training_iteration_time_ms: 48200.111
  update_time_ms: 2.618
timesteps_total: 1650000
training_iteration: 100

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-18pddilqzj/checkpoint_000100/checkpoint-100
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3581081081081081
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6351351351351351
  reward for individual goal_min: 0.0
episode_len_mean: 204.94
episode_reward_max: 2.0
episode_reward_mean: 1.04
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 7824
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.08783316612244
time_total_s: 5281.974639892578
timers:
  learn_throughput: 436.239
  learn_time_ms: 37823.328
  load_throughput: 4633100.761
  load_time_ms: 3.561
  training_iteration_time_ms: 50311.739
  update_time_ms: 2.614
timesteps_total: 1633500
training_iteration: 99

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7115384615384616
  reward for individual goal_min: 0.0
episode_len_mean: 213.2
episode_reward_max: 2.0
episode_reward_mean: 1.01
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 8605
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.87605118751526
time_total_s: 5283.553833961487
timers:
  learn_throughput: 468.17
  learn_time_ms: 35243.6
  load_throughput: 5036901.265
  load_time_ms: 3.276
  training_iteration_time_ms: 46874.595
  update_time_ms: 2.527
timesteps_total: 1798500
training_iteration: 109

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26973684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.53
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 10082
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.82986330986023
time_total_s: 5289.548548221588
timers:
  learn_throughput: 510.547
  learn_time_ms: 32318.253
  load_throughput: 5328499.295
  load_time_ms: 3.097
  training_iteration_time_ms: 43143.541
  update_time_ms: 2.475
timesteps_total: 1897500
training_iteration: 115

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.91015625
  reward for individual goal_min: 0.5
episode_len_mean: 67.68951612903226
episode_reward_max: 2.0
episode_reward_mean: 1.907258064516129
episode_reward_min: 1.0
episodes_this_iter: 248
episodes_total: 16249
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9919354838709677
  agent_1: 0.9153225806451613
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.46172595024109
time_total_s: 5289.448987483978
timers:
  learn_throughput: 414.27
  learn_time_ms: 39829.079
  load_throughput: 4146133.468
  load_time_ms: 3.98
  training_iteration_time_ms: 52203.091
  update_time_ms: 2.622
timesteps_total: 1600500
training_iteration: 97

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.971830985915493
  reward for individual goal_min: 0.0
episode_len_mean: 185.95
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 8426
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.12955093383789
time_total_s: 5299.222336530685
timers:
  learn_throughput: 422.8
  learn_time_ms: 39025.576
  load_throughput: 4918308.874
  load_time_ms: 3.355
  training_iteration_time_ms: 51773.028
  update_time_ms: 2.711
timesteps_total: 1600500
training_iteration: 97

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95
  reward for individual goal_min: 0.5
episode_len_mean: 79.88333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.9166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.95
  agent_1: 0.9666666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3181818181818182
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.07
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 8476
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.15078806877136
time_total_s: 5281.52041554451
timers:
  learn_throughput: 476.6
  learn_time_ms: 34620.21
  load_throughput: 5023774.908
  load_time_ms: 3.284
  training_iteration_time_ms: 46090.803
  update_time_ms: 2.575
timesteps_total: 1650000
training_iteration: 100

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-28met_xp4k/checkpoint_000100/checkpoint-100
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2702702702702703
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.79
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 7623
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.555514335632324
time_total_s: 5289.189740896225
timers:
  learn_throughput: 442.445
  learn_time_ms: 37292.802
  load_throughput: 4624185.058
  load_time_ms: 3.568
  training_iteration_time_ms: 49318.656
  update_time_ms: 2.705
timesteps_total: 1485000
training_iteration: 90

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8366336633663366
  reward for individual goal_min: 0.5
episode_len_mean: 85.37244897959184
episode_reward_max: 2.0
episode_reward_mean: 1.8316326530612246
episode_reward_min: 1.0
episodes_this_iter: 196
episodes_total: 11786
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9897959183673469
  agent_1: 0.8418367346938775
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.9959499835968
time_total_s: 5310.407034635544
timers:
  learn_throughput: 366.139
  learn_time_ms: 45064.806
  load_throughput: 4061958.022
  load_time_ms: 4.062
  training_iteration_time_ms: 59072.232
  update_time_ms: 2.917
timesteps_total: 1452000
training_iteration: 88

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 188.33
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 7652
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.114824295043945
time_total_s: 5294.773971557617
timers:
  learn_throughput: 446.984
  learn_time_ms: 36914.037
  load_throughput: 4647974.479
  load_time_ms: 3.55
  training_iteration_time_ms: 49045.815
  update_time_ms: 2.645
timesteps_total: 1501500
training_iteration: 91

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972222222222222
  reward for individual goal_min: 0.5
episode_len_mean: 47.52023121387283
episode_reward_max: 2.0
episode_reward_mean: 1.9971098265895955
episode_reward_min: 1.0
episodes_this_iter: 346
episodes_total: 24752
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9971098265895953
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 46.71391534805298
time_total_s: 5272.135002851486
timers:
  learn_throughput: 460.782
  learn_time_ms: 35808.684
  load_throughput: 4536582.258
  load_time_ms: 3.637
  training_iteration_time_ms: 47142.59
  update_time_ms: 2.422
timesteps_total: 1914000
training_iteration: 116

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8457943925233645
  reward for individual goal_min: 0.5
episode_len_mean: 71.73913043478261
episode_reward_max: 2.0
episode_reward_mean: 1.8565217391304347
episode_reward_min: 1.0
episodes_this_iter: 230
episodes_total: 17762
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8565217391304348
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 49.363885164260864
time_total_s: 5298.49294257164
timers:
  learn_throughput: 448.236
  learn_time_ms: 36810.986
  load_throughput: 4864960.985
  load_time_ms: 3.392
  training_iteration_time_ms: 48226.249
  update_time_ms: 2.522
timesteps_total: 1699500
training_iteration: 103

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33088235294117646
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8076923076923077
  reward for individual goal_min: 0.0
episode_len_mean: 187.59
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 8925
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.77
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.64601182937622
time_total_s: 5322.366015911102
timers:
  learn_throughput: 499.6
  learn_time_ms: 33026.444
  load_throughput: 4858130.765
  load_time_ms: 3.396
  training_iteration_time_ms: 43935.788
  update_time_ms: 2.608
timesteps_total: 1864500
training_iteration: 113

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2571428571428571
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9821428571428571
  reward for individual goal_min: 0.0
episode_len_mean: 176.25
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 7678
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.56909513473511
time_total_s: 5307.344320297241
timers:
  learn_throughput: 467.641
  learn_time_ms: 35283.458
  load_throughput: 4965489.69
  load_time_ms: 3.323
  training_iteration_time_ms: 46784.452
  update_time_ms: 2.542
timesteps_total: 1600500
training_iteration: 97

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21621621621621623
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 184.89
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 8523
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.61783838272095
time_total_s: 5326.997590065002
timers:
  learn_throughput: 451.303
  learn_time_ms: 36560.837
  load_throughput: 4856085.437
  load_time_ms: 3.398
  training_iteration_time_ms: 48264.993
  update_time_ms: 2.578
timesteps_total: 1666500
training_iteration: 101

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7397260273972602
  reward for individual goal_min: 0.0
episode_len_mean: 196.51
episode_reward_max: 2.0
episode_reward_mean: 1.04
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 8691
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.889408349990845
time_total_s: 5332.443242311478
timers:
  learn_throughput: 466.256
  learn_time_ms: 35388.293
  load_throughput: 5059696.006
  load_time_ms: 3.261
  training_iteration_time_ms: 47095.803
  update_time_ms: 2.533
timesteps_total: 1815000
training_iteration: 110

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29577464788732394
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 151.7570093457944
episode_reward_max: 2.0
episode_reward_mean: 1.4485981308411215
episode_reward_min: 0.0
episodes_this_iter: 107
episodes_total: 10189
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7757009345794392
  agent_1: 0.6728971962616822
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.480029344558716
time_total_s: 5332.028577566147
timers:
  learn_throughput: 510.84
  learn_time_ms: 32299.729
  load_throughput: 5295109.03
  load_time_ms: 3.116
  training_iteration_time_ms: 43069.164
  update_time_ms: 2.469
timesteps_total: 1914000
training_iteration: 116

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7166666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 133.66666666666666
episode_reward_max: 2.0
episode_reward_mean: 1.6833333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.85
  agent_1: 0.8333333333333334
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3873239436619718
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6907894736842105
  reward for individual goal_min: 0.0
episode_len_mean: 200.14
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 7907
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.623931884765625
time_total_s: 5345.598571777344
timers:
  learn_throughput: 433.415
  learn_time_ms: 38069.778
  load_throughput: 4640494.585
  load_time_ms: 3.556
  training_iteration_time_ms: 50569.608
  update_time_ms: 2.592
timesteps_total: 1650000
training_iteration: 100

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19mcvnks1j/checkpoint_000100/checkpoint-100
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.21
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 7717
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.79
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.13058161735535
time_total_s: 5335.32032251358
timers:
  learn_throughput: 446.083
  learn_time_ms: 36988.598
  load_throughput: 4693683.475
  load_time_ms: 3.515
  training_iteration_time_ms: 48866.892
  update_time_ms: 2.684
timesteps_total: 1501500
training_iteration: 91

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27631578947368424
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9925373134328358
  reward for individual goal_min: 0.5
episode_len_mean: 179.64
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 8569
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.82380175590515
time_total_s: 5329.344217300415
timers:
  learn_throughput: 474.83
  learn_time_ms: 34749.252
  load_throughput: 4984300.53
  load_time_ms: 3.31
  training_iteration_time_ms: 46182.828
  update_time_ms: 2.58
timesteps_total: 1666500
training_iteration: 101

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23943661971830985
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 171.51
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 7747
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.340580463409424
time_total_s: 5342.114552021027
timers:
  learn_throughput: 453.291
  learn_time_ms: 36400.468
  load_throughput: 4704244.056
  load_time_ms: 3.507
  training_iteration_time_ms: 48419.996
  update_time_ms: 2.643
timesteps_total: 1518000
training_iteration: 92

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9921875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9338842975206612
  reward for individual goal_min: 0.5
episode_len_mean: 65.5421686746988
episode_reward_max: 2.0
episode_reward_mean: 1.927710843373494
episode_reward_min: 0.0
episodes_this_iter: 249
episodes_total: 16498
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9879518072289156
  agent_1: 0.9397590361445783
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.992913246154785
time_total_s: 5342.441900730133
timers:
  learn_throughput: 412.919
  learn_time_ms: 39959.426
  load_throughput: 4129212.594
  load_time_ms: 3.996
  training_iteration_time_ms: 52314.048
  update_time_ms: 2.591
timesteps_total: 1617000
training_iteration: 98

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2792207792207792
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9788732394366197
  reward for individual goal_min: 0.0
episode_len_mean: 183.1
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 8519
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.505918979644775
time_total_s: 5349.72825551033
timers:
  learn_throughput: 424.346
  learn_time_ms: 38883.33
  load_throughput: 4874555.623
  load_time_ms: 3.385
  training_iteration_time_ms: 51589.862
  update_time_ms: 2.665
timesteps_total: 1617000
training_iteration: 98

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8661971830985915
  reward for individual goal_min: 0.0
episode_len_mean: 200.95
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 9005
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.46341395378113
time_total_s: 5365.829429864883
timers:
  learn_throughput: 502.142
  learn_time_ms: 32859.238
  load_throughput: 4812088.696
  load_time_ms: 3.429
  training_iteration_time_ms: 43719.965
  update_time_ms: 2.623
timesteps_total: 1881000
training_iteration: 114

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 45.26170798898072
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 363
episodes_total: 25115
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.604480266571045
time_total_s: 5320.739483118057
timers:
  learn_throughput: 460.214
  learn_time_ms: 35852.915
  load_throughput: 4571916.603
  load_time_ms: 3.609
  training_iteration_time_ms: 47279.564
  update_time_ms: 2.439
timesteps_total: 1930500
training_iteration: 117

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7908163265306123
  reward for individual goal_min: 0.5
episode_len_mean: 86.85340314136126
episode_reward_max: 2.0
episode_reward_mean: 1.7853403141361257
episode_reward_min: 1.0
episodes_this_iter: 191
episodes_total: 17953
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7853403141361257
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.13389992713928
time_total_s: 5344.626842498779
timers:
  learn_throughput: 450.471
  learn_time_ms: 36628.316
  load_throughput: 4891506.764
  load_time_ms: 3.373
  training_iteration_time_ms: 48060.346
  update_time_ms: 2.504
timesteps_total: 1716000
training_iteration: 104

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7988505747126436
  reward for individual goal_min: 0.5
episode_len_mean: 85.98941798941799
episode_reward_max: 2.0
episode_reward_mean: 1.8148148148148149
episode_reward_min: 1.0
episodes_this_iter: 189
episodes_total: 11975
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9947089947089947
  agent_1: 0.8201058201058201
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.15109205245972
time_total_s: 5370.5581266880035
timers:
  learn_throughput: 366.288
  learn_time_ms: 45046.482
  load_throughput: 4112478.147
  load_time_ms: 4.012
  training_iteration_time_ms: 59228.301
  update_time_ms: 2.905
timesteps_total: 1468500
training_iteration: 89

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2597402597402597
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 179.62
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 7769
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.07359600067139
time_total_s: 5354.417916297913
timers:
  learn_throughput: 471.89
  learn_time_ms: 34965.812
  load_throughput: 5037377.88
  load_time_ms: 3.276
  training_iteration_time_ms: 46435.961
  update_time_ms: 2.512
timesteps_total: 1617000
training_iteration: 98

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2361111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9817073170731707
  reward for individual goal_min: 0.0
episode_len_mean: 174.18
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 8618
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.45041251182556
time_total_s: 5374.448002576828
timers:
  learn_throughput: 452.826
  learn_time_ms: 36437.819
  load_throughput: 4884877.677
  load_time_ms: 3.378
  training_iteration_time_ms: 48127.533
  update_time_ms: 2.586
timesteps_total: 1683000
training_iteration: 102

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3223684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6907894736842105
  reward for individual goal_min: 0.0
episode_len_mean: 195.21
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 8772
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.38756608963013
time_total_s: 5378.830808401108
timers:
  learn_throughput: 466.822
  learn_time_ms: 35345.37
  load_throughput: 4977453.52
  load_time_ms: 3.315
  training_iteration_time_ms: 47079.091
  update_time_ms: 2.546
timesteps_total: 1831500
training_iteration: 111

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2662337662337662
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 179.78
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 10283
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.228644371032715
time_total_s: 5375.25722193718
timers:
  learn_throughput: 509.795
  learn_time_ms: 32365.929
  load_throughput: 5281087.871
  load_time_ms: 3.124
  training_iteration_time_ms: 43178.421
  update_time_ms: 2.481
timesteps_total: 1930500
training_iteration: 117

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27439024390243905
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9927536231884058
  reward for individual goal_min: 0.5
episode_len_mean: 186.81
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 8657
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.42142701148987
time_total_s: 5374.765644311905
timers:
  learn_throughput: 478.967
  learn_time_ms: 34449.138
  load_throughput: 4951208.791
  load_time_ms: 3.333
  training_iteration_time_ms: 45829.684
  update_time_ms: 2.58
timesteps_total: 1683000
training_iteration: 102

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.358974358974359
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 157.07692307692307
episode_reward_max: 2.0
episode_reward_mean: 1.4326923076923077
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 7821
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7403846153846154
  agent_1: 0.6923076923076923
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.92424011230469
time_total_s: 5382.244562625885
timers:
  learn_throughput: 448.808
  learn_time_ms: 36764.057
  load_throughput: 4706515.509
  load_time_ms: 3.506
  training_iteration_time_ms: 48518.196
  update_time_ms: 2.676
timesteps_total: 1518000
training_iteration: 92

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3805970149253731
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7142857142857143
  reward for individual goal_min: 0.0
episode_len_mean: 187.08
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 7995
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.70262336730957
time_total_s: 5395.301195144653
timers:
  learn_throughput: 434.397
  learn_time_ms: 37983.706
  load_throughput: 4617397.535
  load_time_ms: 3.573
  training_iteration_time_ms: 50557.865
  update_time_ms: 2.591
timesteps_total: 1666500
training_iteration: 101

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2602739726027397
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.31
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 7844
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.681824684143066
time_total_s: 5388.79637670517
timers:
  learn_throughput: 455.986
  learn_time_ms: 36185.312
  load_throughput: 4717166.129
  load_time_ms: 3.498
  training_iteration_time_ms: 48115.891
  update_time_ms: 2.623
timesteps_total: 1534500
training_iteration: 93

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27564102564102566
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7878787878787878
  reward for individual goal_min: 0.0
episode_len_mean: 198.35
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 9091
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.03077697753906
time_total_s: 5410.8602068424225
timers:
  learn_throughput: 499.012
  learn_time_ms: 33065.356
  load_throughput: 4811787.577
  load_time_ms: 3.429
  training_iteration_time_ms: 44028.36
  update_time_ms: 2.634
timesteps_total: 1897500
training_iteration: 115

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8440366972477065
  reward for individual goal_min: 0.5
episode_len_mean: 74.5
episode_reward_max: 2.0
episode_reward_mean: 1.8440366972477065
episode_reward_min: 1.0
episodes_this_iter: 218
episodes_total: 18171
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8440366972477065
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.424877643585205
time_total_s: 5391.0517201423645
timers:
  learn_throughput: 453.624
  learn_time_ms: 36373.743
  load_throughput: 4886015.772
  load_time_ms: 3.377
  training_iteration_time_ms: 47835.235
  update_time_ms: 2.512
timesteps_total: 1732500
training_iteration: 105

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9970760233918129
  reward for individual goal_min: 0.5
episode_len_mean: 46.67605633802817
episode_reward_max: 2.0
episode_reward_mean: 1.9971830985915493
episode_reward_min: 1.0
episodes_this_iter: 355
episodes_total: 25470
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9971830985915493
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.1002151966095
time_total_s: 5368.839698314667
timers:
  learn_throughput: 459.331
  learn_time_ms: 35921.8
  load_throughput: 4532927.414
  load_time_ms: 3.64
  training_iteration_time_ms: 47392.768
  update_time_ms: 2.43
timesteps_total: 1947000
training_iteration: 118

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2847222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9779411764705882
  reward for individual goal_min: 0.0
episode_len_mean: 176.25
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 8611
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.91599440574646
time_total_s: 5402.644249916077
timers:
  learn_throughput: 423.676
  learn_time_ms: 38944.81
  load_throughput: 4836334.768
  load_time_ms: 3.412
  training_iteration_time_ms: 51591.816
  update_time_ms: 2.672
timesteps_total: 1633500
training_iteration: 99

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9571428571428572
  reward for individual goal_min: 0.5
episode_len_mean: 61.23616236162361
episode_reward_max: 2.0
episode_reward_mean: 1.955719557195572
episode_reward_min: 1.0
episodes_this_iter: 271
episodes_total: 16769
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.996309963099631
  agent_1: 0.959409594095941
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.7322313785553
time_total_s: 5396.174132108688
timers:
  learn_throughput: 411.779
  learn_time_ms: 40070.072
  load_throughput: 4101534.741
  load_time_ms: 4.023
  training_iteration_time_ms: 52470.476
  update_time_ms: 2.59
timesteps_total: 1633500
training_iteration: 99

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.275
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9852941176470589
  reward for individual goal_min: 0.0
episode_len_mean: 191.81
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 7855
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.690696716308594
time_total_s: 5401.108613014221
timers:
  learn_throughput: 472.945
  learn_time_ms: 34887.768
  load_throughput: 5112699.818
  load_time_ms: 3.227
  training_iteration_time_ms: 46287.595
  update_time_ms: 2.521
timesteps_total: 1633500
training_iteration: 99

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8138297872340425
  reward for individual goal_min: 0.5
episode_len_mean: 92.50555555555556
episode_reward_max: 2.0
episode_reward_mean: 1.8055555555555556
episode_reward_min: 1.0
episodes_this_iter: 180
episodes_total: 12155
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9777777777777777
  agent_1: 0.8277777777777777
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.17486238479614
time_total_s: 5428.7329890728
timers:
  learn_throughput: 368.235
  learn_time_ms: 44808.3
  load_throughput: 4283345.671
  load_time_ms: 3.852
  training_iteration_time_ms: 58877.31
  update_time_ms: 2.904
timesteps_total: 1485000
training_iteration: 90

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2785714285714286
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 172.5
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 8714
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.80497193336487
time_total_s: 5422.252974510193
timers:
  learn_throughput: 452.19
  learn_time_ms: 36489.126
  load_throughput: 4950571.269
  load_time_ms: 3.333
  training_iteration_time_ms: 48163.197
  update_time_ms: 2.593
timesteps_total: 1699500
training_iteration: 103

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24025974025974026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7831325301204819
  reward for individual goal_min: 0.0
episode_len_mean: 206.56
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 8852
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.227765798568726
time_total_s: 5423.0585741996765
timers:
  learn_throughput: 468.875
  learn_time_ms: 35190.63
  load_throughput: 4976737.644
  load_time_ms: 3.315
  training_iteration_time_ms: 46856.265
  update_time_ms: 2.558
timesteps_total: 1848000
training_iteration: 112

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24358974358974358
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9857142857142858
  reward for individual goal_min: 0.0
episode_len_mean: 189.58
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 10370
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.83842492103577
time_total_s: 5419.095646858215
timers:
  learn_throughput: 510.433
  learn_time_ms: 32325.49
  load_throughput: 5288594.289
  load_time_ms: 3.12
  training_iteration_time_ms: 43249.0
  update_time_ms: 2.511
timesteps_total: 1947000
training_iteration: 118

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9657534246575342
  reward for individual goal_min: 0.0
episode_len_mean: 179.58
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 8750
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.14099907875061
time_total_s: 5419.9066433906555
timers:
  learn_throughput: 479.617
  learn_time_ms: 34402.422
  load_throughput: 4975163.44
  load_time_ms: 3.316
  training_iteration_time_ms: 45767.785
  update_time_ms: 2.592
timesteps_total: 1699500
training_iteration: 103

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3819444444444444
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8955223880597015
  reward for individual goal_min: 0.0
episode_len_mean: 184.37
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 9183
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.76
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.752631425857544
time_total_s: 5454.61283826828
timers:
  learn_throughput: 498.469
  learn_time_ms: 33101.347
  load_throughput: 4827260.21
  load_time_ms: 3.418
  training_iteration_time_ms: 44025.334
  update_time_ms: 2.602
timesteps_total: 1914000
training_iteration: 116

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2318840579710145
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 162.98039215686273
episode_reward_max: 2.0
episode_reward_mean: 1.411764705882353
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 7923
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7156862745098039
  agent_1: 0.696078431372549
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.824520111083984
time_total_s: 5430.069082736969
timers:
  learn_throughput: 454.011
  learn_time_ms: 36342.71
  load_throughput: 4731582.344
  load_time_ms: 3.487
  training_iteration_time_ms: 47967.122
  update_time_ms: 2.658
timesteps_total: 1534500
training_iteration: 93

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8487394957983193
  reward for individual goal_min: 0.5
episode_len_mean: 79.5625
episode_reward_max: 2.0
episode_reward_mean: 1.8269230769230769
episode_reward_min: 1.0
episodes_this_iter: 208
episodes_total: 18379
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8269230769230769
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.17330312728882
time_total_s: 5437.225023269653
timers:
  learn_throughput: 456.048
  learn_time_ms: 36180.412
  load_throughput: 4842324.394
  load_time_ms: 3.407
  training_iteration_time_ms: 47650.0
  update_time_ms: 2.521
timesteps_total: 1749000
training_iteration: 106

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24358974358974358
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 179.87
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 7934
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.38826036453247
time_total_s: 5438.184637069702
timers:
  learn_throughput: 457.581
  learn_time_ms: 36059.204
  load_throughput: 4781104.947
  load_time_ms: 3.451
  training_iteration_time_ms: 47974.792
  update_time_ms: 2.628
timesteps_total: 1551000
training_iteration: 94

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26582278481012656
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6973684210526315
  reward for individual goal_min: 0.0
episode_len_mean: 204.24
episode_reward_max: 2.0
episode_reward_mean: 1.0
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 8076
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.46
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.49591302871704
time_total_s: 5446.79710817337
timers:
  learn_throughput: 432.676
  learn_time_ms: 38134.802
  load_throughput: 4605689.795
  load_time_ms: 3.583
  training_iteration_time_ms: 50823.206
  update_time_ms: 2.584
timesteps_total: 1683000
training_iteration: 102

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 47.60230547550432
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 347
episodes_total: 25817
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.550822257995605
time_total_s: 5416.390520572662
timers:
  learn_throughput: 458.238
  learn_time_ms: 36007.452
  load_throughput: 4539051.867
  load_time_ms: 3.635
  training_iteration_time_ms: 47475.46
  update_time_ms: 2.401
timesteps_total: 1963500
training_iteration: 119

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95
  reward for individual goal_min: 0.5
episode_len_mean: 96.1
episode_reward_max: 2.0
episode_reward_mean: 1.7833333333333334
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9166666666666666
  agent_1: 0.8666666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2535211267605634
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.87
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 8708
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.77
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.69272971153259
time_total_s: 5462.336979627609
timers:
  learn_throughput: 430.855
  learn_time_ms: 38295.92
  load_throughput: 4870918.919
  load_time_ms: 3.387
  training_iteration_time_ms: 50923.979
  update_time_ms: 2.682
timesteps_total: 1650000
training_iteration: 100

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95
  reward for individual goal_min: 0.5
episode_len_mean: 61.916666666666664
episode_reward_max: 2.0
episode_reward_mean: 1.95
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.95
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9402985074626866
  reward for individual goal_min: 0.5
episode_len_mean: 59.40860215053763
episode_reward_max: 2.0
episode_reward_mean: 1.9426523297491038
episode_reward_min: 1.0
episodes_this_iter: 279
episodes_total: 17048
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.992831541218638
  agent_1: 0.9498207885304659
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.03527212142944
time_total_s: 5456.209404230118
timers:
  learn_throughput: 409.364
  learn_time_ms: 40306.467
  load_throughput: 4093262.437
  load_time_ms: 4.031
  training_iteration_time_ms: 52748.826
  update_time_ms: 2.554
timesteps_total: 1650000
training_iteration: 100

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-184s1jdk42/checkpoint_000100/checkpoint-100
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26282051282051283
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.4
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 10460
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.366267681121826
time_total_s: 5460.461914539337
timers:
  learn_throughput: 513.446
  learn_time_ms: 32135.796
  load_throughput: 5295676.288
  load_time_ms: 3.116
  training_iteration_time_ms: 43039.902
  update_time_ms: 2.52
timesteps_total: 1963500
training_iteration: 119

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24305555555555555
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7171052631578947
  reward for individual goal_min: 0.0
episode_len_mean: 209.61
episode_reward_max: 2.0
episode_reward_mean: 1.02
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 8932
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.45
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.41739892959595
time_total_s: 5471.4759731292725
timers:
  learn_throughput: 466.066
  learn_time_ms: 35402.699
  load_throughput: 4989762.935
  load_time_ms: 3.307
  training_iteration_time_ms: 47107.87
  update_time_ms: 2.546
timesteps_total: 1864500
training_iteration: 113

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34210526315789475
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.971830985915493
  reward for individual goal_min: 0.0
episode_len_mean: 170.32
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 8808
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.406193256378174
time_total_s: 5472.659167766571
timers:
  learn_throughput: 453.29
  learn_time_ms: 36400.529
  load_throughput: 4953335.051
  load_time_ms: 3.331
  training_iteration_time_ms: 48130.062
  update_time_ms: 2.598
timesteps_total: 1716000
training_iteration: 104

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 99.21666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.7833333333333334
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9
  agent_1: 0.8833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31097560975609756
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9925373134328358
  reward for individual goal_min: 0.5
episode_len_mean: 190.02
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 7944
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.91335964202881
time_total_s: 5462.02197265625
timers:
  learn_throughput: 470.393
  learn_time_ms: 35077.078
  load_throughput: 5078893.308
  load_time_ms: 3.249
  training_iteration_time_ms: 46487.326
  update_time_ms: 2.534
timesteps_total: 1650000
training_iteration: 100

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8350515463917526
  reward for individual goal_min: 0.5
episode_len_mean: 82.08040201005025
episode_reward_max: 2.0
episode_reward_mean: 1.8391959798994975
episode_reward_min: 1.0
episodes_this_iter: 199
episodes_total: 12354
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9798994974874372
  agent_1: 0.8592964824120602
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.88961482048035
time_total_s: 5487.62260389328
timers:
  learn_throughput: 368.187
  learn_time_ms: 44814.186
  load_throughput: 4329678.618
  load_time_ms: 3.811
  training_iteration_time_ms: 58808.412
  update_time_ms: 2.843
timesteps_total: 1501500
training_iteration: 91

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30303030303030304
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8589743589743589
  reward for individual goal_min: 0.0
episode_len_mean: 179.07
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 9274
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.01436233520508
time_total_s: 5497.627200603485
timers:
  learn_throughput: 500.492
  learn_time_ms: 32967.55
  load_throughput: 4819192.646
  load_time_ms: 3.424
  training_iteration_time_ms: 43859.579
  update_time_ms: 2.606
timesteps_total: 1930500
training_iteration: 117

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2916666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.5
episode_len_mean: 170.7
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 8848
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.621466398239136
time_total_s: 5469.528109788895
timers:
  learn_throughput: 478.878
  learn_time_ms: 34455.566
  load_throughput: 4933138.686
  load_time_ms: 3.345
  training_iteration_time_ms: 45950.741
  update_time_ms: 2.573
timesteps_total: 1716000
training_iteration: 104

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.325
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.57
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 8019
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.546847343444824
time_total_s: 5479.615930080414
timers:
  learn_throughput: 455.399
  learn_time_ms: 36231.927
  load_throughput: 4813393.982
  load_time_ms: 3.428
  training_iteration_time_ms: 47811.229
  update_time_ms: 2.664
timesteps_total: 1551000
training_iteration: 94

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8491379310344828
  reward for individual goal_min: 0.5
episode_len_mean: 74.5
episode_reward_max: 2.0
episode_reward_mean: 1.845132743362832
episode_reward_min: 1.0
episodes_this_iter: 226
episodes_total: 18605
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8451327433628318
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.87327861785889
time_total_s: 5484.098301887512
timers:
  learn_throughput: 455.42
  learn_time_ms: 36230.279
  load_throughput: 4874418.29
  load_time_ms: 3.385
  training_iteration_time_ms: 47656.636
  update_time_ms: 2.529
timesteps_total: 1765500
training_iteration: 107

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3223684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.5
episode_len_mean: 171.62
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 8033
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.140742778778076
time_total_s: 5486.32537984848
timers:
  learn_throughput: 461.154
  learn_time_ms: 35779.794
  load_throughput: 4822584.458
  load_time_ms: 3.421
  training_iteration_time_ms: 47544.956
  update_time_ms: 2.63
timesteps_total: 1567500
training_iteration: 95

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3194444444444444
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7123287671232876
  reward for individual goal_min: 0.0
episode_len_mean: 199.62
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 8161
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.45410776138306
time_total_s: 5496.251215934753
timers:
  learn_throughput: 433.851
  learn_time_ms: 38031.464
  load_throughput: 4589136.628
  load_time_ms: 3.595
  training_iteration_time_ms: 50704.53
  update_time_ms: 2.619
timesteps_total: 1699500
training_iteration: 103

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 47.483333333333334
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 47.18803418803419
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 351
episodes_total: 26168
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 51.955300092697144
time_total_s: 5468.3458206653595
timers:
  learn_throughput: 458.935
  learn_time_ms: 35952.796
  load_throughput: 4531769.79
  load_time_ms: 3.641
  training_iteration_time_ms: 47408.859
  update_time_ms: 2.4
timesteps_total: 1980000
training_iteration: 120

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-172kuwc_58/checkpoint_000120/checkpoint-120
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2465753424657534
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9691358024691358
  reward for individual goal_min: 0.0
episode_len_mean: 173.18
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 8804
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.909563302993774
time_total_s: 5512.246542930603
timers:
  learn_throughput: 428.967
  learn_time_ms: 38464.528
  load_throughput: 4877647.656
  load_time_ms: 3.383
  training_iteration_time_ms: 51088.676
  update_time_ms: 2.701
timesteps_total: 1666500
training_iteration: 101

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9583333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 64.60700389105058
episode_reward_max: 2.0
episode_reward_mean: 1.953307392996109
episode_reward_min: 1.0
episodes_this_iter: 257
episodes_total: 17305
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.953307392996109
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 51.43273401260376
time_total_s: 5507.642138242722
timers:
  learn_throughput: 411.935
  learn_time_ms: 40054.831
  load_throughput: 4059670.562
  load_time_ms: 4.064
  training_iteration_time_ms: 52414.318
  update_time_ms: 2.544
timesteps_total: 1666500
training_iteration: 101

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 83.03333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.8666666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9333333333333333
  agent_1: 0.9333333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 180.37
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 10550
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.70802593231201
time_total_s: 5513.169940471649
timers:
  learn_throughput: 516.938
  learn_time_ms: 31918.693
  load_throughput: 5293286.524
  load_time_ms: 3.117
  training_iteration_time_ms: 42765.474
  update_time_ms: 2.538
timesteps_total: 1980000
training_iteration: 120

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19ecg2ejpn/checkpoint_000120/checkpoint-120
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2112676056338028
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.689873417721519
  reward for individual goal_min: 0.0
episode_len_mean: 207.47
episode_reward_max: 2.0
episode_reward_mean: 0.97
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 9012
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.47
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.29773187637329
time_total_s: 5519.773705005646
timers:
  learn_throughput: 464.844
  learn_time_ms: 35495.749
  load_throughput: 4999856.664
  load_time_ms: 3.3
  training_iteration_time_ms: 47239.603
  update_time_ms: 2.51
timesteps_total: 1881000
training_iteration: 114

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3493150684931507
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 170.33
episode_reward_max: 2.0
episode_reward_mean: 1.41
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 8906
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.05931568145752
time_total_s: 5519.718483448029
timers:
  learn_throughput: 455.759
  learn_time_ms: 36203.338
  load_throughput: 4953335.051
  load_time_ms: 3.331
  training_iteration_time_ms: 47925.143
  update_time_ms: 2.622
timesteps_total: 1732500
training_iteration: 105

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3402777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8625
  reward for individual goal_min: 0.0
episode_len_mean: 178.26
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 9364
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.76
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.81778693199158
time_total_s: 5540.444987535477
timers:
  learn_throughput: 504.168
  learn_time_ms: 32727.159
  load_throughput: 4809948.221
  load_time_ms: 3.43
  training_iteration_time_ms: 43590.28
  update_time_ms: 2.608
timesteps_total: 1947000
training_iteration: 118

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22916666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9879518072289156
  reward for individual goal_min: 0.0
episode_len_mean: 177.71
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 8037
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.61553478240967
time_total_s: 5511.63750743866
timers:
  learn_throughput: 468.042
  learn_time_ms: 35253.23
  load_throughput: 5046119.565
  load_time_ms: 3.27
  training_iteration_time_ms: 46725.521
  update_time_ms: 2.539
timesteps_total: 1666500
training_iteration: 101

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8021978021978022
  reward for individual goal_min: 0.5
episode_len_mean: 87.41052631578947
episode_reward_max: 2.0
episode_reward_mean: 1.8105263157894738
episode_reward_min: 1.0
episodes_this_iter: 190
episodes_total: 12544
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9894736842105263
  agent_1: 0.8210526315789474
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.08115339279175
time_total_s: 5545.703757286072
timers:
  learn_throughput: 368.94
  learn_time_ms: 44722.662
  load_throughput: 4310424.216
  load_time_ms: 3.828
  training_iteration_time_ms: 58666.025
  update_time_ms: 2.858
timesteps_total: 1518000
training_iteration: 92

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.98
  reward for individual goal_min: 0.5
episode_len_mean: 176.33
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 8941
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.99458026885986
time_total_s: 5518.5226900577545
timers:
  learn_throughput: 475.422
  learn_time_ms: 34706.042
  load_throughput: 4909829.874
  load_time_ms: 3.361
  training_iteration_time_ms: 46228.644
  update_time_ms: 2.561
timesteps_total: 1732500
training_iteration: 105

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7989130434782609
  reward for individual goal_min: 0.5
episode_len_mean: 81.35643564356435
episode_reward_max: 2.0
episode_reward_mean: 1.816831683168317
episode_reward_min: 1.0
episodes_this_iter: 202
episodes_total: 18807
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8217821782178217
  agent_1: 0.995049504950495
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.33254146575928
time_total_s: 5531.4308433532715
timers:
  learn_throughput: 457.285
  learn_time_ms: 36082.508
  load_throughput: 4840529.334
  load_time_ms: 3.409
  training_iteration_time_ms: 47523.932
  update_time_ms: 2.525
timesteps_total: 1782000
training_iteration: 108

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23170731707317074
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9814814814814815
  reward for individual goal_min: 0.0
episode_len_mean: 188.61
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 8102
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.24667024612427
time_total_s: 5527.862600326538
timers:
  learn_throughput: 457.058
  learn_time_ms: 36100.438
  load_throughput: 4853667.356
  load_time_ms: 3.399
  training_iteration_time_ms: 47701.142
  update_time_ms: 2.667
timesteps_total: 1567500
training_iteration: 95

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3402777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 158.26732673267327
episode_reward_max: 2.0
episode_reward_mean: 1.4158415841584158
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 8134
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7425742574257426
  agent_1: 0.6732673267326733
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.91111445426941
time_total_s: 5535.23649430275
timers:
  learn_throughput: 461.871
  learn_time_ms: 35724.261
  load_throughput: 4859665.892
  load_time_ms: 3.395
  training_iteration_time_ms: 47549.988
  update_time_ms: 2.632
timesteps_total: 1584000
training_iteration: 96

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7898550724637681
  reward for individual goal_min: 0.0
episode_len_mean: 195.25
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 8245
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.6859335899353
time_total_s: 5544.937149524689
timers:
  learn_throughput: 432.829
  learn_time_ms: 38121.294
  load_throughput: 4615180.49
  load_time_ms: 3.575
  training_iteration_time_ms: 50714.772
  update_time_ms: 2.609
timesteps_total: 1716000
training_iteration: 104

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.997093023255814
  reward for individual goal_min: 0.5
episode_len_mean: 46.51841359773371
episode_reward_max: 2.0
episode_reward_mean: 1.9971671388101984
episode_reward_min: 1.0
episodes_this_iter: 353
episodes_total: 26521
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9971671388101983
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.548080921173096
time_total_s: 5515.893901586533
timers:
  learn_throughput: 457.115
  learn_time_ms: 36095.974
  load_throughput: 4506538.862
  load_time_ms: 3.661
  training_iteration_time_ms: 47590.05
  update_time_ms: 2.4
timesteps_total: 1996500
training_iteration: 121

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24025974025974026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.5
episode_len_mean: 194.22
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 8889
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.42994141578674
time_total_s: 5560.67648434639
timers:
  learn_throughput: 434.852
  learn_time_ms: 37943.929
  load_throughput: 4872187.718
  load_time_ms: 3.387
  training_iteration_time_ms: 50546.115
  update_time_ms: 2.699
timesteps_total: 1683000
training_iteration: 102

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22077922077922077
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.98125
  reward for individual goal_min: 0.0
episode_len_mean: 184.41
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 10641
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.22972321510315
time_total_s: 5555.399663686752
timers:
  learn_throughput: 517.722
  learn_time_ms: 31870.394
  load_throughput: 5276900.014
  load_time_ms: 3.127
  training_iteration_time_ms: 42673.393
  update_time_ms: 2.54
timesteps_total: 1996500
training_iteration: 121

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2569444444444444
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.972972972972973
  reward for individual goal_min: 0.0
episode_len_mean: 171.99
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 8994
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.41642689704895
time_total_s: 5567.1349103450775
timers:
  learn_throughput: 454.147
  learn_time_ms: 36331.831
  load_throughput: 4999134.329
  load_time_ms: 3.301
  training_iteration_time_ms: 48074.849
  update_time_ms: 2.626
timesteps_total: 1749000
training_iteration: 106

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7361111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 199.34
episode_reward_max: 2.0
episode_reward_mean: 1.06
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 9095
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.366074562072754
time_total_s: 5568.1397795677185
timers:
  learn_throughput: 465.343
  learn_time_ms: 35457.731
  load_throughput: 5011369.897
  load_time_ms: 3.293
  training_iteration_time_ms: 47128.858
  update_time_ms: 2.509
timesteps_total: 1897500
training_iteration: 115

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3695652173913043
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7808219178082192
  reward for individual goal_min: 0.0
episode_len_mean: 174.34
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 9454
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.03377437591553
time_total_s: 5584.478761911392
timers:
  learn_throughput: 504.355
  learn_time_ms: 32715.035
  load_throughput: 4827630.621
  load_time_ms: 3.418
  training_iteration_time_ms: 43606.108
  update_time_ms: 2.611
timesteps_total: 1963500
training_iteration: 119

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9818840579710145
  reward for individual goal_min: 0.5
episode_len_mean: 56.11072664359862
episode_reward_max: 2.0
episode_reward_mean: 1.9826989619377162
episode_reward_min: 1.0
episodes_this_iter: 289
episodes_total: 17594
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9930795847750865
  agent_1: 0.9896193771626297
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.67419505119324
time_total_s: 5562.316333293915
timers:
  learn_throughput: 409.719
  learn_time_ms: 40271.541
  load_throughput: 4102507.291
  load_time_ms: 4.022
  training_iteration_time_ms: 52698.964
  update_time_ms: 2.554
timesteps_total: 1683000
training_iteration: 102

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2345679012345679
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.0
episode_len_mean: 189.5
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 8125
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.36174249649048
time_total_s: 5560.99924993515
timers:
  learn_throughput: 465.492
  learn_time_ms: 35446.362
  load_throughput: 5020677.007
  load_time_ms: 3.286
  training_iteration_time_ms: 47006.231
  update_time_ms: 2.544
timesteps_total: 1683000
training_iteration: 102

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1891891891891892
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9873417721518988
  reward for individual goal_min: 0.5
episode_len_mean: 189.0
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 9027
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.172149896621704
time_total_s: 5564.694839954376
timers:
  learn_throughput: 474.358
  learn_time_ms: 34783.857
  load_throughput: 4852170.036
  load_time_ms: 3.401
  training_iteration_time_ms: 46330.832
  update_time_ms: 2.576
timesteps_total: 1749000
training_iteration: 106

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8491379310344828
  reward for individual goal_min: 0.5
episode_len_mean: 76.70142180094787
episode_reward_max: 2.0
episode_reward_mean: 1.8341232227488151
episode_reward_min: 1.0
episodes_this_iter: 211
episodes_total: 19018
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8388625592417062
  agent_1: 0.995260663507109
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.500667095184326
time_total_s: 5577.931510448456
timers:
  learn_throughput: 458.549
  learn_time_ms: 35983.054
  load_throughput: 4916491.976
  load_time_ms: 3.356
  training_iteration_time_ms: 47383.851
  update_time_ms: 2.559
timesteps_total: 1798500
training_iteration: 109

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 179.62
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 8196
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.757630825042725
time_total_s: 5576.620231151581
timers:
  learn_throughput: 455.623
  learn_time_ms: 36214.157
  load_throughput: 4855744.717
  load_time_ms: 3.398
  training_iteration_time_ms: 47736.947
  update_time_ms: 2.665
timesteps_total: 1584000
training_iteration: 96

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2152777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.967948717948718
  reward for individual goal_min: 0.0
episode_len_mean: 173.41
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 8230
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.425159215927124
time_total_s: 5583.661653518677
timers:
  learn_throughput: 462.925
  learn_time_ms: 35642.899
  load_throughput: 4809079.197
  load_time_ms: 3.431
  training_iteration_time_ms: 47360.326
  update_time_ms: 2.62
timesteps_total: 1600500
training_iteration: 97

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3051948051948052
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7857142857142857
  reward for individual goal_min: 0.0
episode_len_mean: 202.99
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 8325
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.53690433502197
time_total_s: 5592.474053859711
timers:
  learn_throughput: 435.699
  learn_time_ms: 37870.162
  load_throughput: 4619154.208
  load_time_ms: 3.572
  training_iteration_time_ms: 50375.464
  update_time_ms: 2.598
timesteps_total: 1732500
training_iteration: 105

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.855
  reward for individual goal_min: 0.5
episode_len_mean: 78.22274881516587
episode_reward_max: 2.0
episode_reward_mean: 1.8625592417061612
episode_reward_min: 1.0
episodes_this_iter: 211
episodes_total: 12755
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.995260663507109
  agent_1: 0.8672985781990521
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.11619019508362
time_total_s: 5604.819947481155
timers:
  learn_throughput: 368.865
  learn_time_ms: 44731.789
  load_throughput: 4343455.634
  load_time_ms: 3.799
  training_iteration_time_ms: 58605.468
  update_time_ms: 2.869
timesteps_total: 1534500
training_iteration: 93

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972677595628415
  reward for individual goal_min: 0.5
episode_len_mean: 47.18155619596542
episode_reward_max: 2.0
episode_reward_mean: 1.9971181556195965
episode_reward_min: 1.0
episodes_this_iter: 347
episodes_total: 26868
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9971181556195965
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.02436113357544
time_total_s: 5562.918262720108
timers:
  learn_throughput: 457.854
  learn_time_ms: 36037.721
  load_throughput: 4490106.793
  load_time_ms: 3.675
  training_iteration_time_ms: 47517.116
  update_time_ms: 2.395
timesteps_total: 2013000
training_iteration: 122

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2808219178082192
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9726027397260274
  reward for individual goal_min: 0.0
episode_len_mean: 168.76
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 10739
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.01250743865967
time_total_s: 5600.412171125412
timers:
  learn_throughput: 514.482
  learn_time_ms: 32071.067
  load_throughput: 5284636.636
  load_time_ms: 3.122
  training_iteration_time_ms: 42939.944
  update_time_ms: 2.538
timesteps_total: 2013000
training_iteration: 122

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29285714285714287
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9675324675324676
  reward for individual goal_min: 0.0
episode_len_mean: 163.9903846153846
episode_reward_max: 2.0
episode_reward_mean: 1.375
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 8993
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6826923076923077
  agent_1: 0.6923076923076923
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.54365062713623
time_total_s: 5611.220134973526
timers:
  learn_throughput: 435.271
  learn_time_ms: 37907.445
  load_throughput: 4889709.61
  load_time_ms: 3.374
  training_iteration_time_ms: 50501.326
  update_time_ms: 2.652
timesteps_total: 1699500
training_iteration: 103

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2876712328767123
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7666666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 201.91
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 9178
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.63504242897034
time_total_s: 5613.774821996689
timers:
  learn_throughput: 466.315
  learn_time_ms: 35383.838
  load_throughput: 4998303.902
  load_time_ms: 3.301
  training_iteration_time_ms: 47108.377
  update_time_ms: 2.508
timesteps_total: 1914000
training_iteration: 116

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1962025316455696
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9710144927536232
  reward for individual goal_min: 0.0
episode_len_mean: 195.28
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 9082
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.58171248435974
time_total_s: 5616.716622829437
timers:
  learn_throughput: 453.899
  learn_time_ms: 36351.739
  load_throughput: 4964136.229
  load_time_ms: 3.324
  training_iteration_time_ms: 48154.903
  update_time_ms: 2.612
timesteps_total: 1765500
training_iteration: 107

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 89.93333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.8333333333333333
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8333333333333334
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27564102564102566
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8445945945945946
  reward for individual goal_min: 0.0
episode_len_mean: 188.55
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 9542
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.28448033332825
time_total_s: 5637.7632422447205
timers:
  learn_throughput: 502.712
  learn_time_ms: 32821.987
  load_throughput: 4818689.319
  load_time_ms: 3.424
  training_iteration_time_ms: 43740.026
  update_time_ms: 2.597
timesteps_total: 1980000
training_iteration: 120

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-091_srff7m/checkpoint_000120/checkpoint-120
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.2
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 8220
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.59581685066223
time_total_s: 5609.595066785812
timers:
  learn_throughput: 462.526
  learn_time_ms: 35673.705
  load_throughput: 4995525.784
  load_time_ms: 3.303
  training_iteration_time_ms: 47313.478
  update_time_ms: 2.529
timesteps_total: 1699500
training_iteration: 103

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.972027972027972
  reward for individual goal_min: 0.5
episode_len_mean: 58.11340206185567
episode_reward_max: 2.0
episode_reward_mean: 1.9725085910652922
episode_reward_min: 1.0
episodes_this_iter: 291
episodes_total: 17885
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9725085910652921
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 53.94386124610901
time_total_s: 5616.260194540024
timers:
  learn_throughput: 407.864
  learn_time_ms: 40454.701
  load_throughput: 4127956.481
  load_time_ms: 3.997
  training_iteration_time_ms: 52978.247
  update_time_ms: 2.545
timesteps_total: 1699500
training_iteration: 103

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8318965517241379
  reward for individual goal_min: 0.5
episode_len_mean: 79.71698113207547
episode_reward_max: 2.0
episode_reward_mean: 1.8160377358490567
episode_reward_min: 1.0
episodes_this_iter: 212
episodes_total: 19230
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8160377358490566
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 45.68408393859863
time_total_s: 5623.615594387054
timers:
  learn_throughput: 461.563
  learn_time_ms: 35748.131
  load_throughput: 4953193.244
  load_time_ms: 3.331
  training_iteration_time_ms: 47084.998
  update_time_ms: 2.562
timesteps_total: 1815000
training_iteration: 110

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28205128205128205
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.88
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 9122
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.24499464035034
time_total_s: 5613.939834594727
timers:
  learn_throughput: 468.921
  learn_time_ms: 35187.185
  load_throughput: 4809580.519
  load_time_ms: 3.431
  training_iteration_time_ms: 46798.951
  update_time_ms: 2.59
timesteps_total: 1765500
training_iteration: 107

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23684210526315788
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.0
episode_len_mean: 177.87
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 8291
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.477927684783936
time_total_s: 5625.098158836365
timers:
  learn_throughput: 455.841
  learn_time_ms: 36196.798
  load_throughput: 4893166.848
  load_time_ms: 3.372
  training_iteration_time_ms: 47793.714
  update_time_ms: 2.681
timesteps_total: 1600500
training_iteration: 97

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33766233766233766
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.971830985915493
  reward for individual goal_min: 0.0
episode_len_mean: 170.98
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 8327
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.56964063644409
time_total_s: 5634.231294155121
timers:
  learn_throughput: 460.949
  learn_time_ms: 35795.716
  load_throughput: 4786362.542
  load_time_ms: 3.447
  training_iteration_time_ms: 47538.289
  update_time_ms: 2.623
timesteps_total: 1617000
training_iteration: 98

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3698630136986301
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6602564102564102
  reward for individual goal_min: 0.0
episode_len_mean: 195.1
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 8412
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.66260385513306
time_total_s: 5643.136657714844
timers:
  learn_throughput: 437.438
  learn_time_ms: 37719.608
  load_throughput: 4653193.481
  load_time_ms: 3.546
  training_iteration_time_ms: 50157.209
  update_time_ms: 2.585
timesteps_total: 1749000
training_iteration: 106

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9947089947089947
  reward for individual goal_min: 0.5
episode_len_mean: 46.23480662983425
episode_reward_max: 2.0
episode_reward_mean: 1.9944751381215469
episode_reward_min: 1.0
episodes_this_iter: 362
episodes_total: 27230
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.994475138121547
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.08435368537903
time_total_s: 5611.002616405487
timers:
  learn_throughput: 458.761
  learn_time_ms: 35966.44
  load_throughput: 4500823.735
  load_time_ms: 3.666
  training_iteration_time_ms: 47469.807
  update_time_ms: 2.393
timesteps_total: 2029500
training_iteration: 123

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2357142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 170.89
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 10837
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.94215416908264
time_total_s: 5644.354325294495
timers:
  learn_throughput: 513.426
  learn_time_ms: 32137.071
  load_throughput: 5243356.669
  load_time_ms: 3.147
  training_iteration_time_ms: 43082.352
  update_time_ms: 2.534
timesteps_total: 2029500
training_iteration: 123

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8367346938775511
  reward for individual goal_min: 0.5
episode_len_mean: 82.595
episode_reward_max: 2.0
episode_reward_mean: 1.84
episode_reward_min: 1.0
episodes_this_iter: 200
episodes_total: 12955
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.99
  agent_1: 0.85
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.2967529296875
time_total_s: 5665.116700410843
timers:
  learn_throughput: 368.637
  learn_time_ms: 44759.523
  load_throughput: 4252943.063
  load_time_ms: 3.88
  training_iteration_time_ms: 58595.428
  update_time_ms: 2.81
timesteps_total: 1551000
training_iteration: 94

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3219178082191781
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7468354430379747
  reward for individual goal_min: 0.0
episode_len_mean: 194.0
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 9262
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.41107940673828
time_total_s: 5659.185901403427
timers:
  learn_throughput: 468.464
  learn_time_ms: 35221.466
  load_throughput: 4978348.656
  load_time_ms: 3.314
  training_iteration_time_ms: 46907.525
  update_time_ms: 2.495
timesteps_total: 1930500
training_iteration: 117

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9493670886075949
  reward for individual goal_min: 0.0
episode_len_mean: 177.92
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 9085
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.625571489334106
time_total_s: 5661.84570646286
timers:
  learn_throughput: 436.106
  learn_time_ms: 37834.881
  load_throughput: 4851285.689
  load_time_ms: 3.401
  training_iteration_time_ms: 50415.95
  update_time_ms: 2.662
timesteps_total: 1716000
training_iteration: 104

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28205128205128205
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 179.45
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 9174
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.67791175842285
time_total_s: 5663.39453458786
timers:
  learn_throughput: 455.089
  learn_time_ms: 36256.657
  load_throughput: 4955534.106
  load_time_ms: 3.33
  training_iteration_time_ms: 48100.414
  update_time_ms: 2.616
timesteps_total: 1782000
training_iteration: 108

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2602739726027397
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8088235294117647
  reward for individual goal_min: 0.0
episode_len_mean: 193.29
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 9630
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.9640531539917
time_total_s: 5684.727295398712
timers:
  learn_throughput: 497.54
  learn_time_ms: 33163.143
  load_throughput: 4784410.262
  load_time_ms: 3.449
  training_iteration_time_ms: 44160.629
  update_time_ms: 2.608
timesteps_total: 1996500
training_iteration: 121

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2054794520547945
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9817073170731707
  reward for individual goal_min: 0.0
episode_len_mean: 181.86
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 8310
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.69235610961914
time_total_s: 5657.2874228954315
timers:
  learn_throughput: 459.87
  learn_time_ms: 35879.697
  load_throughput: 4920162.095
  load_time_ms: 3.354
  training_iteration_time_ms: 47617.081
  update_time_ms: 2.516
timesteps_total: 1716000
training_iteration: 104

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9855072463768116
  reward for individual goal_min: 0.5
episode_len_mean: 54.29042904290429
episode_reward_max: 2.0
episode_reward_mean: 1.9867986798679869
episode_reward_min: 1.0
episodes_this_iter: 303
episodes_total: 18188
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9867986798679867
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 56.045931339263916
time_total_s: 5672.306125879288
timers:
  learn_throughput: 405.527
  learn_time_ms: 40687.805
  load_throughput: 4140502.561
  load_time_ms: 3.985
  training_iteration_time_ms: 53258.446
  update_time_ms: 2.518
timesteps_total: 1716000
training_iteration: 104

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.38311688311688313
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 167.27
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 9221
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.80875015258789
time_total_s: 5663.748584747314
timers:
  learn_throughput: 463.75
  learn_time_ms: 35579.495
  load_throughput: 4781831.725
  load_time_ms: 3.451
  training_iteration_time_ms: 47353.922
  update_time_ms: 2.606
timesteps_total: 1782000
training_iteration: 108

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8739495798319328
  reward for individual goal_min: 0.5
episode_len_mean: 67.51239669421487
episode_reward_max: 2.0
episode_reward_mean: 1.8760330578512396
episode_reward_min: 1.0
episodes_this_iter: 242
episodes_total: 19472
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8760330578512396
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 50.0436007976532
time_total_s: 5673.659195184708
timers:
  learn_throughput: 460.171
  learn_time_ms: 35856.217
  load_throughput: 4960720.245
  load_time_ms: 3.326
  training_iteration_time_ms: 47201.581
  update_time_ms: 2.539
timesteps_total: 1831500
training_iteration: 111

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2721518987341772
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 178.57
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 8380
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.70823287963867
time_total_s: 5673.806391716003
timers:
  learn_throughput: 454.676
  learn_time_ms: 36289.582
  load_throughput: 4898916.669
  load_time_ms: 3.368
  training_iteration_time_ms: 47965.304
  update_time_ms: 2.654
timesteps_total: 1617000
training_iteration: 98

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35185185185185186
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7
  reward for individual goal_min: 0.0
episode_len_mean: 206.13
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 8493
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.93172788619995
time_total_s: 5692.068385601044
timers:
  learn_throughput: 441.202
  learn_time_ms: 37397.824
  load_throughput: 4667535.526
  load_time_ms: 3.535
  training_iteration_time_ms: 49742.951
  update_time_ms: 2.562
timesteps_total: 1765500
training_iteration: 107

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23972602739726026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9695121951219512
  reward for individual goal_min: 0.0
episode_len_mean: 177.02
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 8419
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.302239418029785
time_total_s: 5684.533533573151
timers:
  learn_throughput: 458.163
  learn_time_ms: 36013.399
  load_throughput: 4749767.748
  load_time_ms: 3.474
  training_iteration_time_ms: 47810.244
  update_time_ms: 2.612
timesteps_total: 1633500
training_iteration: 99

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 184.9
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 10926
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.38452172279358
time_total_s: 5686.738847017288
timers:
  learn_throughput: 513.853
  learn_time_ms: 32110.331
  load_throughput: 5225737.996
  load_time_ms: 3.157
  training_iteration_time_ms: 43007.409
  update_time_ms: 2.513
timesteps_total: 2046000
training_iteration: 124

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 45.8268156424581
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 358
episodes_total: 27588
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.14504814147949
time_total_s: 5658.147664546967
timers:
  learn_throughput: 458.216
  learn_time_ms: 36009.256
  load_throughput: 4499623.937
  load_time_ms: 3.667
  training_iteration_time_ms: 47491.901
  update_time_ms: 2.396
timesteps_total: 2046000
training_iteration: 124

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.4097222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.777027027027027
  reward for individual goal_min: 0.0
episode_len_mean: 178.52
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 9353
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.641688108444214
time_total_s: 5705.827589511871
timers:
  learn_throughput: 469.255
  learn_time_ms: 35162.126
  load_throughput: 4994912.849
  load_time_ms: 3.303
  training_iteration_time_ms: 46878.478
  update_time_ms: 2.515
timesteps_total: 1947000
training_iteration: 118

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9634146341463414
  reward for individual goal_min: 0.0
episode_len_mean: 157.21153846153845
episode_reward_max: 2.0
episode_reward_mean: 1.4326923076923077
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 9189
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7403846153846154
  agent_1: 0.6923076923076923
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.964367628097534
time_total_s: 5710.810074090958
timers:
  learn_throughput: 436.8
  learn_time_ms: 37774.748
  load_throughput: 4840360.058
  load_time_ms: 3.409
  training_iteration_time_ms: 50271.944
  update_time_ms: 2.631
timesteps_total: 1732500
training_iteration: 105

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26282051282051283
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 179.69
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 9264
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.6748149394989
time_total_s: 5712.069349527359
timers:
  learn_throughput: 455.315
  learn_time_ms: 36238.608
  load_throughput: 5004484.554
  load_time_ms: 3.297
  training_iteration_time_ms: 48146.738
  update_time_ms: 2.592
timesteps_total: 1798500
training_iteration: 109

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7921348314606742
  reward for individual goal_min: 0.5
episode_len_mean: 92.88135593220339
episode_reward_max: 2.0
episode_reward_mean: 1.7909604519774012
episode_reward_min: 1.0
episodes_this_iter: 177
episodes_total: 13132
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9774011299435028
  agent_1: 0.8135593220338984
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.96418046951294
time_total_s: 5722.080880880356
timers:
  learn_throughput: 368.799
  learn_time_ms: 44739.796
  load_throughput: 4232809.742
  load_time_ms: 3.898
  training_iteration_time_ms: 58553.64
  update_time_ms: 2.868
timesteps_total: 1567500
training_iteration: 95

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27702702702702703
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7391304347826086
  reward for individual goal_min: 0.0
episode_len_mean: 200.73
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 9712
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.37
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.78274869918823
time_total_s: 5729.5100440979
timers:
  learn_throughput: 496.47
  learn_time_ms: 33234.631
  load_throughput: 4767140.997
  load_time_ms: 3.461
  training_iteration_time_ms: 44270.252
  update_time_ms: 2.608
timesteps_total: 2013000
training_iteration: 122

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.09876543209876543
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 207.1
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 8390
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.28405737876892
time_total_s: 5705.5714802742
timers:
  learn_throughput: 458.369
  learn_time_ms: 35997.16
  load_throughput: 4908262.895
  load_time_ms: 3.362
  training_iteration_time_ms: 47772.419
  update_time_ms: 2.538
timesteps_total: 1732500
training_iteration: 105

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8302752293577982
  reward for individual goal_min: 0.5
episode_len_mean: 81.56716417910448
episode_reward_max: 2.0
episode_reward_mean: 1.8159203980099503
episode_reward_min: 1.0
episodes_this_iter: 201
episodes_total: 19673
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8159203980099502
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.15362787246704
time_total_s: 5720.812823057175
timers:
  learn_throughput: 461.092
  learn_time_ms: 35784.653
  load_throughput: 4953902.362
  load_time_ms: 3.331
  training_iteration_time_ms: 47102.63
  update_time_ms: 2.546
timesteps_total: 1848000
training_iteration: 112

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2565789473684211
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9662162162162162
  reward for individual goal_min: 0.0
episode_len_mean: 176.73
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 9315
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.82045817375183
time_total_s: 5714.569042921066
timers:
  learn_throughput: 459.287
  learn_time_ms: 35925.273
  load_throughput: 4787355.838
  load_time_ms: 3.447
  training_iteration_time_ms: 47889.733
  update_time_ms: 2.597
timesteps_total: 1798500
training_iteration: 109

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3026315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 168.63
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 11023
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.743486404418945
time_total_s: 5728.482333421707
timers:
  learn_throughput: 516.442
  learn_time_ms: 31949.405
  load_throughput: 5249362.167
  load_time_ms: 3.143
  training_iteration_time_ms: 42798.603
  update_time_ms: 2.508
timesteps_total: 2062500
training_iteration: 125

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9834437086092715
  reward for individual goal_min: 0.5
episode_len_mean: 56.86805555555556
episode_reward_max: 2.0
episode_reward_mean: 1.9826388888888888
episode_reward_min: 1.0
episodes_this_iter: 288
episodes_total: 18476
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9826388888888888
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 52.813902139663696
time_total_s: 5725.120028018951
timers:
  learn_throughput: 405.06
  learn_time_ms: 40734.702
  load_throughput: 4148768.127
  load_time_ms: 3.977
  training_iteration_time_ms: 53348.392
  update_time_ms: 2.52
timesteps_total: 1732500
training_iteration: 105

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 160.69607843137254
episode_reward_max: 2.0
episode_reward_mean: 1.392156862745098
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 8482
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6666666666666666
  agent_1: 0.7254901960784313
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.37649440765381
time_total_s: 5726.182886123657
timers:
  learn_throughput: 449.482
  learn_time_ms: 36708.884
  load_throughput: 4837348.916
  load_time_ms: 3.411
  training_iteration_time_ms: 48516.165
  update_time_ms: 2.647
timesteps_total: 1633500
training_iteration: 99

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.39634146341463417
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7642857142857142
  reward for individual goal_min: 0.0
episode_len_mean: 182.79
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 8581
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.86935234069824
time_total_s: 5740.937737941742
timers:
  learn_throughput: 443.842
  learn_time_ms: 37175.419
  load_throughput: 4707700.094
  load_time_ms: 3.505
  training_iteration_time_ms: 49494.473
  update_time_ms: 2.565
timesteps_total: 1782000
training_iteration: 108

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 47.892753623188405
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 345
episodes_total: 27933
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.564783334732056
time_total_s: 5705.712447881699
timers:
  learn_throughput: 458.755
  learn_time_ms: 35966.905
  load_throughput: 4510033.692
  load_time_ms: 3.659
  training_iteration_time_ms: 47448.171
  update_time_ms: 2.419
timesteps_total: 2062500
training_iteration: 125

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 81.48333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.8666666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9166666666666666
  agent_1: 0.95
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23717948717948717
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9746835443037974
  reward for individual goal_min: 0.0
episode_len_mean: 183.03
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 8511
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.39791989326477
time_total_s: 5745.931453466415
timers:
  learn_throughput: 454.092
  learn_time_ms: 36336.261
  load_throughput: 4733362.242
  load_time_ms: 3.486
  training_iteration_time_ms: 48231.226
  update_time_ms: 2.609
timesteps_total: 1650000
training_iteration: 100

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-29ws591pkw/checkpoint_000100/checkpoint-100
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.4342105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6883116883116883
  reward for individual goal_min: 0.0
episode_len_mean: 188.9
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 9436
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.90565013885498
time_total_s: 5753.733239650726
timers:
  learn_throughput: 468.109
  learn_time_ms: 35248.212
  load_throughput: 4963673.373
  load_time_ms: 3.324
  training_iteration_time_ms: 46981.522
  update_time_ms: 2.511
timesteps_total: 1963500
training_iteration: 119

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19480519480519481
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 187.0
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 9277
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.33100461959839
time_total_s: 5759.141078710556
timers:
  learn_throughput: 438.861
  learn_time_ms: 37597.293
  load_throughput: 4863012.416
  load_time_ms: 3.393
  training_iteration_time_ms: 49920.974
  update_time_ms: 2.648
timesteps_total: 1749000
training_iteration: 106

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2721518987341772
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.852112676056338
  reward for individual goal_min: 0.0
episode_len_mean: 196.95
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 9794
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.92284154891968
time_total_s: 5773.43288564682
timers:
  learn_throughput: 496.292
  learn_time_ms: 33246.553
  load_throughput: 4689040.389
  load_time_ms: 3.519
  training_iteration_time_ms: 44297.886
  update_time_ms: 2.633
timesteps_total: 2029500
training_iteration: 123

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2468354430379747
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9746835443037974
  reward for individual goal_min: 0.0
episode_len_mean: 183.46
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 9359
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.99441385269165
time_total_s: 5759.063763380051
timers:
  learn_throughput: 455.165
  learn_time_ms: 36250.585
  load_throughput: 4942333.693
  load_time_ms: 3.339
  training_iteration_time_ms: 48130.606
  update_time_ms: 2.607
timesteps_total: 1815000
training_iteration: 110

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8679245283018868
  reward for individual goal_min: 0.5
episode_len_mean: 84.95897435897436
episode_reward_max: 2.0
episode_reward_mean: 1.8564102564102565
episode_reward_min: 1.0
episodes_this_iter: 195
episodes_total: 13327
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9897435897435898
  agent_1: 0.8666666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.288856744766235
time_total_s: 5779.369737625122
timers:
  learn_throughput: 369.508
  learn_time_ms: 44653.95
  load_throughput: 4400682.682
  load_time_ms: 3.749
  training_iteration_time_ms: 58438.679
  update_time_ms: 2.887
timesteps_total: 1584000
training_iteration: 96

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18055555555555555
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.42
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 8479
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.88970398902893
time_total_s: 5756.461184263229
timers:
  learn_throughput: 454.284
  learn_time_ms: 36320.874
  load_throughput: 4884119.241
  load_time_ms: 3.378
  training_iteration_time_ms: 48208.306
  update_time_ms: 2.518
timesteps_total: 1749000
training_iteration: 106

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2792207792207792
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 180.62
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 11115
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.10658931732178
time_total_s: 5770.588922739029
timers:
  learn_throughput: 516.909
  learn_time_ms: 31920.522
  load_throughput: 5274607.564
  load_time_ms: 3.128
  training_iteration_time_ms: 42761.576
  update_time_ms: 2.531
timesteps_total: 2079000
training_iteration: 126

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7978723404255319
  reward for individual goal_min: 0.5
episode_len_mean: 86.83597883597884
episode_reward_max: 2.0
episode_reward_mean: 1.7989417989417988
episode_reward_min: 1.0
episodes_this_iter: 189
episodes_total: 19862
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8095238095238095
  agent_1: 0.9894179894179894
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.43437600135803
time_total_s: 5768.247199058533
timers:
  learn_throughput: 462.718
  learn_time_ms: 35658.841
  load_throughput: 4897703.233
  load_time_ms: 3.369
  training_iteration_time_ms: 46910.496
  update_time_ms: 2.561
timesteps_total: 1864500
training_iteration: 113

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31756756756756754
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 155.40952380952382
episode_reward_max: 2.0
episode_reward_mean: 1.438095238095238
episode_reward_min: 0.0
episodes_this_iter: 105
episodes_total: 9420
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6857142857142857
  agent_1: 0.7523809523809524
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.45344114303589
time_total_s: 5764.022484064102
timers:
  learn_throughput: 455.174
  learn_time_ms: 36249.843
  load_throughput: 4742411.841
  load_time_ms: 3.479
  training_iteration_time_ms: 48212.489
  update_time_ms: 2.577
timesteps_total: 1815000
training_iteration: 110

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9943820224719101
  reward for individual goal_min: 0.5
episode_len_mean: 45.69529085872576
episode_reward_max: 2.0
episode_reward_mean: 1.994459833795014
episode_reward_min: 1.0
episodes_this_iter: 361
episodes_total: 28294
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.997229916897507
  agent_1: 0.997229916897507
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.1963324546814
time_total_s: 5752.90878033638
timers:
  learn_throughput: 458.315
  learn_time_ms: 36001.423
  load_throughput: 4516420.591
  load_time_ms: 3.653
  training_iteration_time_ms: 47506.293
  update_time_ms: 2.41
timesteps_total: 2079000
training_iteration: 126

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.5
episode_len_mean: 53.80921052631579
episode_reward_max: 2.0
episode_reward_mean: 1.986842105263158
episode_reward_min: 1.0
episodes_this_iter: 304
episodes_total: 18780
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9868421052631579
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 54.35109090805054
time_total_s: 5779.471118927002
timers:
  learn_throughput: 403.941
  learn_time_ms: 40847.527
  load_throughput: 4121073.768
  load_time_ms: 4.004
  training_iteration_time_ms: 53501.485
  update_time_ms: 2.516
timesteps_total: 1749000
training_iteration: 106

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.48026315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.75
  reward for individual goal_min: 0.0
episode_len_mean: 176.53
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 8673
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.993510484695435
time_total_s: 5790.931248426437
timers:
  learn_throughput: 443.86
  learn_time_ms: 37173.92
  load_throughput: 4751267.764
  load_time_ms: 3.473
  training_iteration_time_ms: 49484.991
  update_time_ms: 2.559
timesteps_total: 1798500
training_iteration: 109

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 70.15
episode_reward_max: 2.0
episode_reward_mean: 1.9833333333333334
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9833333333333333
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25675675675675674
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935897435897436
  reward for individual goal_min: 0.5
episode_len_mean: 172.78
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 8579
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.87911319732666
time_total_s: 5784.061999320984
timers:
  learn_throughput: 448.832
  learn_time_ms: 36762.049
  load_throughput: 4720287.012
  load_time_ms: 3.496
  training_iteration_time_ms: 48594.935
  update_time_ms: 2.653
timesteps_total: 1650000
training_iteration: 100

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-2933yh83nn/checkpoint_000100/checkpoint-100
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.86
  reward for individual goal_min: 0.0
episode_len_mean: 176.42
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 9888
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.16309118270874
time_total_s: 5817.595976829529
timers:
  learn_throughput: 495.459
  learn_time_ms: 33302.435
  load_throughput: 4713728.289
  load_time_ms: 3.5
  training_iteration_time_ms: 44367.831
  update_time_ms: 2.633
timesteps_total: 2046000
training_iteration: 124

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2597402597402597
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9637681159420289
  reward for individual goal_min: 0.0
episode_len_mean: 188.37
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 8597
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.362285137176514
time_total_s: 5798.293738603592
timers:
  learn_throughput: 445.293
  learn_time_ms: 37054.282
  load_throughput: 4723573.223
  load_time_ms: 3.493
  training_iteration_time_ms: 49156.018
  update_time_ms: 2.61
timesteps_total: 1666500
training_iteration: 101

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.363013698630137
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 161.44554455445544
episode_reward_max: 2.0
episode_reward_mean: 1.4554455445544554
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 9460
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7326732673267327
  agent_1: 0.7227722772277227
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.74659776687622
time_total_s: 5805.810361146927
timers:
  learn_throughput: 457.587
  learn_time_ms: 36058.698
  load_throughput: 4921946.703
  load_time_ms: 3.352
  training_iteration_time_ms: 47843.53
  update_time_ms: 2.614
timesteps_total: 1831500
training_iteration: 111

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 167.66
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 9374
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.62252116203308
time_total_s: 5808.763599872589
timers:
  learn_throughput: 439.73
  learn_time_ms: 37523.032
  load_throughput: 4875826.317
  load_time_ms: 3.384
  training_iteration_time_ms: 49769.866
  update_time_ms: 2.687
timesteps_total: 1765500
training_iteration: 107

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7166666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 136.55
episode_reward_max: 2.0
episode_reward_mean: 1.6166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8333333333333334
  agent_1: 0.7833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31690140845070425
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7152777777777778
  reward for individual goal_min: 0.0
episode_len_mean: 200.52
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 9522
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.122010946273804
time_total_s: 5814.855250597
timers:
  learn_throughput: 469.021
  learn_time_ms: 35179.665
  load_throughput: 4898535.239
  load_time_ms: 3.368
  training_iteration_time_ms: 46878.373
  update_time_ms: 2.513
timesteps_total: 1980000
training_iteration: 120

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3013698630136986
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 168.97
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 11211
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.76
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.20019459724426
time_total_s: 5812.789117336273
timers:
  learn_throughput: 517.716
  learn_time_ms: 31870.77
  load_throughput: 5263495.357
  load_time_ms: 3.135
  training_iteration_time_ms: 42658.867
  update_time_ms: 2.526
timesteps_total: 2095500
training_iteration: 127

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21232876712328766
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9797297297297297
  reward for individual goal_min: 0.0
episode_len_mean: 183.87
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 8567
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.1118643283844
time_total_s: 5807.573048591614
timers:
  learn_throughput: 450.042
  learn_time_ms: 36663.279
  load_throughput: 4796480.299
  load_time_ms: 3.44
  training_iteration_time_ms: 48762.69
  update_time_ms: 2.512
timesteps_total: 1765500
training_iteration: 107

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8198198198198198
  reward for individual goal_min: 0.5
episode_len_mean: 81.74757281553399
episode_reward_max: 2.0
episode_reward_mean: 1.8058252427184467
episode_reward_min: 1.0
episodes_this_iter: 206
episodes_total: 20068
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8058252427184466
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.672632455825806
time_total_s: 5815.9198315143585
timers:
  learn_throughput: 460.036
  learn_time_ms: 35866.78
  load_throughput: 4873903.361
  load_time_ms: 3.385
  training_iteration_time_ms: 47064.255
  update_time_ms: 2.601
timesteps_total: 1881000
training_iteration: 114

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8210526315789474
  reward for individual goal_min: 0.5
episode_len_mean: 93.48
episode_reward_max: 2.0
episode_reward_mean: 1.8057142857142856
episode_reward_min: 1.0
episodes_this_iter: 175
episodes_total: 13502
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9942857142857143
  agent_1: 0.8114285714285714
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.5890576839447
time_total_s: 5835.958795309067
timers:
  learn_throughput: 369.866
  learn_time_ms: 44610.791
  load_throughput: 4408054.574
  load_time_ms: 3.743
  training_iteration_time_ms: 58393.697
  update_time_ms: 2.891
timesteps_total: 1600500
training_iteration: 97

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24096385542168675
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 186.48
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 9511
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.352824687957764
time_total_s: 5811.37530875206
timers:
  learn_throughput: 456.255
  learn_time_ms: 36163.973
  load_throughput: 4724669.643
  load_time_ms: 3.492
  training_iteration_time_ms: 48165.27
  update_time_ms: 2.585
timesteps_total: 1831500
training_iteration: 111

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.29775280898876
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 356
episodes_total: 28650
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.20401620864868
time_total_s: 5801.112796545029
timers:
  learn_throughput: 458.161
  learn_time_ms: 36013.532
  load_throughput: 4504866.786
  load_time_ms: 3.663
  training_iteration_time_ms: 47466.301
  update_time_ms: 2.387
timesteps_total: 2095500
training_iteration: 127

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3115942028985507
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7435897435897436
  reward for individual goal_min: 0.0
episode_len_mean: 194.6
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 8758
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.98999857902527
time_total_s: 5840.921247005463
timers:
  learn_throughput: 444.117
  learn_time_ms: 37152.343
  load_throughput: 4704563.846
  load_time_ms: 3.507
  training_iteration_time_ms: 49495.171
  update_time_ms: 2.557
timesteps_total: 1815000
training_iteration: 110

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2746478873239437
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 158.72380952380954
episode_reward_max: 2.0
episode_reward_mean: 1.4666666666666666
episode_reward_min: 0.0
episodes_this_iter: 105
episodes_total: 8684
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6761904761904762
  agent_1: 0.7904761904761904
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.6112494468689
time_total_s: 5833.673248767853
timers:
  learn_throughput: 445.753
  learn_time_ms: 37016.016
  load_throughput: 4727800.466
  load_time_ms: 3.49
  training_iteration_time_ms: 48942.756
  update_time_ms: 2.67
timesteps_total: 1666500
training_iteration: 101

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9892086330935251
  reward for individual goal_min: 0.5
episode_len_mean: 55.245847176079735
episode_reward_max: 2.0
episode_reward_mean: 1.9900332225913622
episode_reward_min: 1.0
episodes_this_iter: 301
episodes_total: 19081
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9900332225913622
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 55.20963764190674
time_total_s: 5834.680756568909
timers:
  learn_throughput: 402.085
  learn_time_ms: 41036.128
  load_throughput: 4079460.992
  load_time_ms: 4.045
  training_iteration_time_ms: 53774.841
  update_time_ms: 2.501
timesteps_total: 1765500
training_iteration: 107

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3157894736842105
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8354430379746836
  reward for individual goal_min: 0.0
episode_len_mean: 186.02
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 9974
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.122848987579346
time_total_s: 5862.718825817108
timers:
  learn_throughput: 495.406
  learn_time_ms: 33306.03
  load_throughput: 4770427.026
  load_time_ms: 3.459
  training_iteration_time_ms: 44376.972
  update_time_ms: 2.591
timesteps_total: 2062500
training_iteration: 125

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 161.1809523809524
episode_reward_max: 2.0
episode_reward_mean: 1.457142857142857
episode_reward_min: 0.0
episodes_this_iter: 105
episodes_total: 9565
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7333333333333333
  agent_1: 0.7238095238095238
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.46687936782837
time_total_s: 5853.277240514755
timers:
  learn_throughput: 457.023
  learn_time_ms: 36103.181
  load_throughput: 4941945.472
  load_time_ms: 3.339
  training_iteration_time_ms: 47845.17
  update_time_ms: 2.593
timesteps_total: 1848000
training_iteration: 112

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28378378378378377
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 171.48
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 8693
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.25110363960266
time_total_s: 5847.544842243195
timers:
  learn_throughput: 442.945
  learn_time_ms: 37250.643
  load_throughput: 4646944.564
  load_time_ms: 3.551
  training_iteration_time_ms: 49346.728
  update_time_ms: 2.596
timesteps_total: 1683000
training_iteration: 102

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3355263157894737
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 161.44660194174756
episode_reward_max: 2.0
episode_reward_mean: 1.3883495145631068
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 9477
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6990291262135923
  agent_1: 0.6893203883495146
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.68819999694824
time_total_s: 5857.451799869537
timers:
  learn_throughput: 441.877
  learn_time_ms: 37340.731
  load_throughput: 4945936.466
  load_time_ms: 3.336
  training_iteration_time_ms: 49588.485
  update_time_ms: 3.019
timesteps_total: 1782000
training_iteration: 108

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.16
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 11302
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.826104164123535
time_total_s: 5854.615221500397
timers:
  learn_throughput: 520.023
  learn_time_ms: 31729.366
  load_throughput: 5215931.023
  load_time_ms: 3.163
  training_iteration_time_ms: 42457.895
  update_time_ms: 2.509
timesteps_total: 2112000
training_iteration: 128

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35526315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7112676056338029
  reward for individual goal_min: 0.0
episode_len_mean: 202.42
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 9603
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.40791368484497
time_total_s: 5861.263164281845
timers:
  learn_throughput: 468.138
  learn_time_ms: 35246.025
  load_throughput: 4923522.431
  load_time_ms: 3.351
  training_iteration_time_ms: 46880.332
  update_time_ms: 2.514
timesteps_total: 1996500
training_iteration: 121

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22857142857142856
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.28
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 8665
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.11913061141968
time_total_s: 5854.692179203033
timers:
  learn_throughput: 449.23
  learn_time_ms: 36729.509
  load_throughput: 4717294.744
  load_time_ms: 3.498
  training_iteration_time_ms: 48767.528
  update_time_ms: 2.532
timesteps_total: 1782000
training_iteration: 108

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8478260869565217
  reward for individual goal_min: 0.5
episode_len_mean: 72.85087719298245
episode_reward_max: 2.0
episode_reward_mean: 1.8464912280701755
episode_reward_min: 1.0
episodes_this_iter: 228
episodes_total: 20296
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8464912280701754
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 49.0398108959198
time_total_s: 5864.959642410278
timers:
  learn_throughput: 456.509
  learn_time_ms: 36143.844
  load_throughput: 4891368.475
  load_time_ms: 3.373
  training_iteration_time_ms: 47325.626
  update_time_ms: 2.616
timesteps_total: 1897500
training_iteration: 115

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20422535211267606
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9810126582278481
  reward for individual goal_min: 0.0
episode_len_mean: 173.99
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 9605
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.94129037857056
time_total_s: 5858.3165991306305
timers:
  learn_throughput: 454.557
  learn_time_ms: 36299.075
  load_throughput: 4694288.389
  load_time_ms: 3.515
  training_iteration_time_ms: 48317.034
  update_time_ms: 2.589
timesteps_total: 1848000
training_iteration: 112

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8011363636363636
  reward for individual goal_min: 0.5
episode_len_mean: 89.62295081967213
episode_reward_max: 2.0
episode_reward_mean: 1.8087431693989071
episode_reward_min: 1.0
episodes_this_iter: 183
episodes_total: 13685
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9726775956284153
  agent_1: 0.8360655737704918
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.25308322906494
time_total_s: 5893.211878538132
timers:
  learn_throughput: 371.323
  learn_time_ms: 44435.65
  load_throughput: 4317712.061
  load_time_ms: 3.821
  training_iteration_time_ms: 58219.56
  update_time_ms: 2.952
timesteps_total: 1617000
training_iteration: 98

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.997093023255814
  reward for individual goal_min: 0.5
episode_len_mean: 48.19117647058823
episode_reward_max: 2.0
episode_reward_mean: 1.9970588235294118
episode_reward_min: 1.0
episodes_this_iter: 340
episodes_total: 28990
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9970588235294118
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.71800756454468
time_total_s: 5847.830804109573
timers:
  learn_throughput: 459.873
  learn_time_ms: 35879.48
  load_throughput: 4513828.333
  load_time_ms: 3.655
  training_iteration_time_ms: 47318.656
  update_time_ms: 2.377
timesteps_total: 2112000
training_iteration: 128

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26282051282051283
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7642857142857142
  reward for individual goal_min: 0.0
episode_len_mean: 202.96
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 8840
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.11806392669678
time_total_s: 5891.039310932159
timers:
  learn_throughput: 443.493
  learn_time_ms: 37204.667
  load_throughput: 4691328.981
  load_time_ms: 3.517
  training_iteration_time_ms: 49536.576
  update_time_ms: 2.556
timesteps_total: 1831500
training_iteration: 111

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2865853658536585
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9090909090909091
  reward for individual goal_min: 0.0
episode_len_mean: 193.87
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 10062
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.40060639381409
time_total_s: 5907.119432210922
timers:
  learn_throughput: 496.042
  learn_time_ms: 33263.333
  load_throughput: 4778793.943
  load_time_ms: 3.453
  training_iteration_time_ms: 44441.924
  update_time_ms: 2.582
timesteps_total: 2079000
training_iteration: 126

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2986111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.54
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 8783
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.976540088653564
time_total_s: 5884.649788856506
timers:
  learn_throughput: 441.9
  learn_time_ms: 37338.8
  load_throughput: 4672419.995
  load_time_ms: 3.531
  training_iteration_time_ms: 49348.141
  update_time_ms: 2.66
timesteps_total: 1683000
training_iteration: 102

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2972972972972973
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.975
  reward for individual goal_min: 0.0
episode_len_mean: 170.13
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 9657
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.218929052352905
time_total_s: 5901.496169567108
timers:
  learn_throughput: 457.438
  learn_time_ms: 36070.429
  load_throughput: 4861441.025
  load_time_ms: 3.394
  training_iteration_time_ms: 47886.749
  update_time_ms: 2.618
timesteps_total: 1864500
training_iteration: 113

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9831081081081081
  reward for individual goal_min: 0.5
episode_len_mean: 53.97377049180328
episode_reward_max: 2.0
episode_reward_mean: 1.9836065573770492
episode_reward_min: 1.0
episodes_this_iter: 305
episodes_total: 19386
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9836065573770492
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 55.88388538360596
time_total_s: 5890.564641952515
timers:
  learn_throughput: 400.176
  learn_time_ms: 41231.871
  load_throughput: 4053844.433
  load_time_ms: 4.07
  training_iteration_time_ms: 54062.351
  update_time_ms: 2.518
timesteps_total: 1782000
training_iteration: 108

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1917808219178082
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.21
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 11395
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.14914393424988
time_total_s: 5896.764365434647
timers:
  learn_throughput: 518.572
  learn_time_ms: 31818.148
  load_throughput: 5240497.955
  load_time_ms: 3.149
  training_iteration_time_ms: 42536.461
  update_time_ms: 2.513
timesteps_total: 2128500
training_iteration: 129

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24342105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 180.38
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 8787
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.47493767738342
time_total_s: 5899.019779920578
timers:
  learn_throughput: 438.637
  learn_time_ms: 37616.496
  load_throughput: 4610937.098
  load_time_ms: 3.578
  training_iteration_time_ms: 49825.879
  update_time_ms: 2.605
timesteps_total: 1699500
training_iteration: 103

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2361111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 173.74
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 9567
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.32271862030029
time_total_s: 5907.774518489838
timers:
  learn_throughput: 444.683
  learn_time_ms: 37105.096
  load_throughput: 5014202.0
  load_time_ms: 3.291
  training_iteration_time_ms: 49329.364
  update_time_ms: 2.997
timesteps_total: 1798500
training_iteration: 109

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.37341772151898733
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7391304347826086
  reward for individual goal_min: 0.0
episode_len_mean: 204.08
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 9685
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.25599241256714
time_total_s: 5910.519156694412
timers:
  learn_throughput: 462.454
  learn_time_ms: 35679.244
  load_throughput: 4952236.256
  load_time_ms: 3.332
  training_iteration_time_ms: 47383.364
  update_time_ms: 2.499
timesteps_total: 2013000
training_iteration: 122

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24074074074074073
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 188.27
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 8749
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.00187540054321
time_total_s: 5902.694054603577
timers:
  learn_throughput: 448.496
  learn_time_ms: 36789.643
  load_throughput: 4679560.214
  load_time_ms: 3.526
  training_iteration_time_ms: 48898.424
  update_time_ms: 2.532
timesteps_total: 1798500
training_iteration: 109

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7989130434782609
  reward for individual goal_min: 0.5
episode_len_mean: 84.58201058201058
episode_reward_max: 2.0
episode_reward_mean: 1.8042328042328042
episode_reward_min: 1.0
episodes_this_iter: 189
episodes_total: 20485
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8042328042328042
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.98181390762329
time_total_s: 5913.941456317902
timers:
  learn_throughput: 453.164
  learn_time_ms: 36410.648
  load_throughput: 4890262.44
  load_time_ms: 3.374
  training_iteration_time_ms: 47606.863
  update_time_ms: 2.61
timesteps_total: 1914000
training_iteration: 116

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23376623376623376
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 175.75
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 9700
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.57843327522278
time_total_s: 5906.895032405853
timers:
  learn_throughput: 451.449
  learn_time_ms: 36548.942
  load_throughput: 4656825.559
  load_time_ms: 3.543
  training_iteration_time_ms: 48660.99
  update_time_ms: 2.568
timesteps_total: 1864500
training_iteration: 113

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972222222222222
  reward for individual goal_min: 0.5
episode_len_mean: 45.02964959568733
episode_reward_max: 2.0
episode_reward_mean: 1.9973045822102427
episode_reward_min: 1.0
episodes_this_iter: 371
episodes_total: 29361
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973045822102425
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 48.19077205657959
time_total_s: 5896.021576166153
timers:
  learn_throughput: 459.221
  learn_time_ms: 35930.392
  load_throughput: 4557825.079
  load_time_ms: 3.62
  training_iteration_time_ms: 47381.706
  update_time_ms: 2.398
timesteps_total: 2128500
training_iteration: 129

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26785714285714285
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.855072463768116
  reward for individual goal_min: 0.0
episode_len_mean: 199.22
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 10143
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.16510081291199
time_total_s: 5952.284533023834
timers:
  learn_throughput: 494.201
  learn_time_ms: 33387.258
  load_throughput: 4776221.454
  load_time_ms: 3.455
  training_iteration_time_ms: 44657.079
  update_time_ms: 2.579
timesteps_total: 2095500
training_iteration: 127

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30985915492957744
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7857142857142857
  reward for individual goal_min: 0.0
episode_len_mean: 193.2
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 8926
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.89052438735962
time_total_s: 5938.929835319519
timers:
  learn_throughput: 446.86
  learn_time_ms: 36924.348
  load_throughput: 4714723.783
  load_time_ms: 3.5
  training_iteration_time_ms: 49176.093
  update_time_ms: 2.557
timesteps_total: 1848000
training_iteration: 112

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8267326732673267
  reward for individual goal_min: 0.5
episode_len_mean: 91.61538461538461
episode_reward_max: 2.0
episode_reward_mean: 1.8076923076923077
episode_reward_min: 1.0
episodes_this_iter: 182
episodes_total: 13867
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.978021978021978
  agent_1: 0.8296703296703297
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.09937810897827
time_total_s: 5952.31125664711
timers:
  learn_throughput: 371.653
  learn_time_ms: 44396.257
  load_throughput: 4266393.115
  load_time_ms: 3.867
  training_iteration_time_ms: 58114.826
  update_time_ms: 2.967
timesteps_total: 1633500
training_iteration: 99

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14285714285714285
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.0
episode_len_mean: 181.07
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 11488
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.0345253944397
time_total_s: 5940.798890829086
timers:
  learn_throughput: 516.853
  learn_time_ms: 31923.98
  load_throughput: 5252230.562
  load_time_ms: 3.142
  training_iteration_time_ms: 42726.868
  update_time_ms: 2.491
timesteps_total: 2145000
training_iteration: 130

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3961038961038961
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 164.71
episode_reward_max: 2.0
episode_reward_mean: 1.41
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 8882
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.37300109863281
time_total_s: 5938.022789955139
timers:
  learn_throughput: 437.151
  learn_time_ms: 37744.401
  load_throughput: 4661028.3
  load_time_ms: 3.54
  training_iteration_time_ms: 49903.009
  update_time_ms: 2.674
timesteps_total: 1699500
training_iteration: 103

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3561643835616438
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9705882352941176
  reward for individual goal_min: 0.0
episode_len_mean: 161.01960784313727
episode_reward_max: 2.0
episode_reward_mean: 1.3823529411764706
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 9759
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7254901960784313
  agent_1: 0.6568627450980392
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.94869565963745
time_total_s: 5952.444865226746
timers:
  learn_throughput: 456.906
  learn_time_ms: 36112.497
  load_throughput: 4833699.973
  load_time_ms: 3.414
  training_iteration_time_ms: 47941.055
  update_time_ms: 2.604
timesteps_total: 1881000
training_iteration: 114

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9761904761904762
  reward for individual goal_min: 0.0
episode_len_mean: 182.3
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 8877
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.26919341087341
time_total_s: 5949.288973331451
timers:
  learn_throughput: 438.119
  learn_time_ms: 37660.999
  load_throughput: 4515477.607
  load_time_ms: 3.654
  training_iteration_time_ms: 49913.97
  update_time_ms: 2.619
timesteps_total: 1716000
training_iteration: 104

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7692307692307693
  reward for individual goal_min: 0.0
episode_len_mean: 198.15
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 9770
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.24189233779907
time_total_s: 5957.761049032211
timers:
  learn_throughput: 464.013
  learn_time_ms: 35559.359
  load_throughput: 4922646.902
  load_time_ms: 3.352
  training_iteration_time_ms: 47265.762
  update_time_ms: 2.523
timesteps_total: 2029500
training_iteration: 123

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9967741935483871
  reward for individual goal_min: 0.5
episode_len_mean: 51.661490683229815
episode_reward_max: 2.0
episode_reward_mean: 1.9968944099378882
episode_reward_min: 1.0
episodes_this_iter: 322
episodes_total: 19708
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9968944099378882
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 57.41870164871216
time_total_s: 5947.983343601227
timers:
  learn_throughput: 397.617
  learn_time_ms: 41497.189
  load_throughput: 4032091.728
  load_time_ms: 4.092
  training_iteration_time_ms: 54429.741
  update_time_ms: 2.52
timesteps_total: 1798500
training_iteration: 109

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30666666666666664
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 168.1881188118812
episode_reward_max: 2.0
episode_reward_mean: 1.3366336633663367
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 9668
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6534653465346535
  agent_1: 0.6831683168316832
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.82050395011902
time_total_s: 5960.595022439957
timers:
  learn_throughput: 439.783
  learn_time_ms: 37518.531
  load_throughput: 5002061.075
  load_time_ms: 3.299
  training_iteration_time_ms: 49785.956
  update_time_ms: 3.008
timesteps_total: 1815000
training_iteration: 110

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22602739726027396
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 181.49
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 8842
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.76871466636658
time_total_s: 5950.462769269943
timers:
  learn_throughput: 449.31
  learn_time_ms: 36722.996
  load_throughput: 4709430.018
  load_time_ms: 3.504
  training_iteration_time_ms: 48807.229
  update_time_ms: 2.517
timesteps_total: 1815000
training_iteration: 110

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25308641975308643
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 187.34
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 9785
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.723957777023315
time_total_s: 5952.618990182877
timers:
  learn_throughput: 454.965
  learn_time_ms: 36266.524
  load_throughput: 4652098.707
  load_time_ms: 3.547
  training_iteration_time_ms: 48271.399
  update_time_ms: 2.551
timesteps_total: 1881000
training_iteration: 114

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.839622641509434
  reward for individual goal_min: 0.5
episode_len_mean: 73.96916299559471
episode_reward_max: 2.0
episode_reward_mean: 1.8502202643171806
episode_reward_min: 1.0
episodes_this_iter: 227
episodes_total: 20712
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8546255506607929
  agent_1: 0.9955947136563876
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.47926068305969
time_total_s: 5964.420717000961
timers:
  learn_throughput: 449.417
  learn_time_ms: 36714.262
  load_throughput: 4868246.317
  load_time_ms: 3.389
  training_iteration_time_ms: 47967.127
  update_time_ms: 2.614
timesteps_total: 1930500
training_iteration: 117

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7948717948717948
  reward for individual goal_min: 0.0
episode_len_mean: 180.21
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 10234
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.87528443336487
time_total_s: 5997.159817457199
timers:
  learn_throughput: 491.509
  learn_time_ms: 33570.069
  load_throughput: 4765401.237
  load_time_ms: 3.462
  training_iteration_time_ms: 44862.872
  update_time_ms: 2.57
timesteps_total: 2112000
training_iteration: 128

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.702127659574465
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 376
episodes_total: 29737
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.59392857551575
time_total_s: 5943.615504741669
timers:
  learn_throughput: 459.009
  learn_time_ms: 35946.977
  load_throughput: 4554076.004
  load_time_ms: 3.623
  training_iteration_time_ms: 47405.715
  update_time_ms: 2.409
timesteps_total: 2145000
training_iteration: 130

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3732394366197183
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7302631578947368
  reward for individual goal_min: 0.0
episode_len_mean: 178.79
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 9017
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.5751268863678
time_total_s: 5988.504962205887
timers:
  learn_throughput: 446.443
  learn_time_ms: 36958.821
  load_throughput: 4707315.839
  load_time_ms: 3.505
  training_iteration_time_ms: 49187.904
  update_time_ms: 2.525
timesteps_total: 1864500
training_iteration: 113

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 189.07
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 11575
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.92533040046692
time_total_s: 5984.724221229553
timers:
  learn_throughput: 515.142
  learn_time_ms: 32030.014
  load_throughput: 5232850.37
  load_time_ms: 3.153
  training_iteration_time_ms: 42896.474
  update_time_ms: 2.488
timesteps_total: 2161500
training_iteration: 131

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3092105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.0
episode_len_mean: 172.95
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 9856
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.77
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.64875411987305
time_total_s: 6001.093619346619
timers:
  learn_throughput: 454.705
  learn_time_ms: 36287.288
  load_throughput: 4836503.763
  load_time_ms: 3.412
  training_iteration_time_ms: 48100.146
  update_time_ms: 2.61
timesteps_total: 1897500
training_iteration: 115

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.19
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 8972
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.03244876861572
time_total_s: 5991.055238723755
timers:
  learn_throughput: 433.334
  learn_time_ms: 38076.827
  load_throughput: 4575180.876
  load_time_ms: 3.606
  training_iteration_time_ms: 50251.337
  update_time_ms: 2.678
timesteps_total: 1716000
training_iteration: 104

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3082191780821918
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7635135135135135
  reward for individual goal_min: 0.0
episode_len_mean: 200.76
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 9848
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.49
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.97947549819946
time_total_s: 6003.740524530411
timers:
  learn_throughput: 466.845
  learn_time_ms: 35343.64
  load_throughput: 4886429.757
  load_time_ms: 3.377
  training_iteration_time_ms: 47033.861
  update_time_ms: 2.526
timesteps_total: 2046000
training_iteration: 124

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2642857142857143
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9873417721518988
  reward for individual goal_min: 0.0
episode_len_mean: 161.7596153846154
episode_reward_max: 2.0
episode_reward_mean: 1.3557692307692308
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 8981
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6923076923076923
  agent_1: 0.6634615384615384
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.44775128364563
time_total_s: 6003.736724615097
timers:
  learn_throughput: 432.207
  learn_time_ms: 38176.115
  load_throughput: 4437221.72
  load_time_ms: 3.719
  training_iteration_time_ms: 50544.394
  update_time_ms: 2.61
timesteps_total: 1732500
training_iteration: 105

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 82.4
episode_reward_max: 2.0
episode_reward_mean: 1.8333333333333333
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9833333333333333
  agent_1: 0.85
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3380281690140845
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 156.60194174757282
episode_reward_max: 2.0
episode_reward_mean: 1.3883495145631068
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 9771
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7184466019417476
  agent_1: 0.6699029126213593
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.716269731521606
time_total_s: 6012.311292171478
timers:
  learn_throughput: 438.279
  learn_time_ms: 37647.241
  load_throughput: 4977847.34
  load_time_ms: 3.315
  training_iteration_time_ms: 49966.578
  update_time_ms: 3.001
timesteps_total: 1831500
training_iteration: 111

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8045977011494253
  reward for individual goal_min: 0.5
episode_len_mean: 87.0979381443299
episode_reward_max: 2.0
episode_reward_mean: 1.824742268041237
episode_reward_min: 1.0
episodes_this_iter: 194
episodes_total: 14061
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9896907216494846
  agent_1: 0.8350515463917526
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 68.57466506958008
time_total_s: 6020.88592171669
timers:
  learn_throughput: 371.227
  learn_time_ms: 44447.217
  load_throughput: 4227431.692
  load_time_ms: 3.903
  training_iteration_time_ms: 58173.499
  update_time_ms: 2.964
timesteps_total: 1650000
training_iteration: 100

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-19wecnkw4d/checkpoint_000100/checkpoint-100
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9726027397260274
  reward for individual goal_min: 0.0
episode_len_mean: 174.28
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 8937
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.87715172767639
time_total_s: 5998.33992099762
timers:
  learn_throughput: 451.147
  learn_time_ms: 36573.415
  load_throughput: 4722606.215
  load_time_ms: 3.494
  training_iteration_time_ms: 48633.583
  update_time_ms: 2.535
timesteps_total: 1831500
training_iteration: 111

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9968354430379747
  reward for individual goal_min: 0.5
episode_len_mean: 50.157575757575756
episode_reward_max: 2.0
episode_reward_mean: 1.996969696969697
episode_reward_min: 1.0
episodes_this_iter: 330
episodes_total: 20038
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.996969696969697
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 56.60904288291931
time_total_s: 6004.592386484146
timers:
  learn_throughput: 395.166
  learn_time_ms: 41754.592
  load_throughput: 4005441.371
  load_time_ms: 4.119
  training_iteration_time_ms: 54750.662
  update_time_ms: 2.525
timesteps_total: 1815000
training_iteration: 110

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2922077922077922
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.28
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 9885
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.43071436882019
time_total_s: 6001.049704551697
timers:
  learn_throughput: 456.288
  learn_time_ms: 36161.334
  load_throughput: 4671915.319
  load_time_ms: 3.532
  training_iteration_time_ms: 48215.274
  update_time_ms: 2.555
timesteps_total: 1897500
training_iteration: 115

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8392857142857143
  reward for individual goal_min: 0.5
episode_len_mean: 77.90654205607477
episode_reward_max: 2.0
episode_reward_mean: 1.8317757009345794
episode_reward_min: 1.0
episodes_this_iter: 214
episodes_total: 20926
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8411214953271028
  agent_1: 0.9906542056074766
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.4432897567749
time_total_s: 6013.864006757736
timers:
  learn_throughput: 446.786
  learn_time_ms: 36930.395
  load_throughput: 4821341.359
  load_time_ms: 3.422
  training_iteration_time_ms: 48178.113
  update_time_ms: 2.617
timesteps_total: 1947000
training_iteration: 118

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21710526315789475
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7891566265060241
  reward for individual goal_min: 0.0
episode_len_mean: 198.06
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 10317
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.08166861534119
time_total_s: 6039.24148607254
timers:
  learn_throughput: 494.008
  learn_time_ms: 33400.259
  load_throughput: 4751496.111
  load_time_ms: 3.473
  training_iteration_time_ms: 44667.715
  update_time_ms: 2.569
timesteps_total: 2128500
training_iteration: 129

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9970059880239521
  reward for individual goal_min: 0.5
episode_len_mean: 48.75516224188791
episode_reward_max: 2.0
episode_reward_mean: 1.9970501474926254
episode_reward_min: 1.0
episodes_this_iter: 339
episodes_total: 30076
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9970501474926253
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.92579388618469
time_total_s: 5990.541298627853
timers:
  learn_throughput: 459.555
  learn_time_ms: 35904.267
  load_throughput: 4577662.42
  load_time_ms: 3.604
  training_iteration_time_ms: 47343.979
  update_time_ms: 2.384
timesteps_total: 2161500
training_iteration: 131

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27564102564102566
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.41
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 11671
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.83271288871765
time_total_s: 6026.556934118271
timers:
  learn_throughput: 518.985
  learn_time_ms: 31792.826
  load_throughput: 5235423.488
  load_time_ms: 3.152
  training_iteration_time_ms: 42578.482
  update_time_ms: 2.466
timesteps_total: 2178000
training_iteration: 132

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35714285714285715
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6756756756756757
  reward for individual goal_min: 0.0
episode_len_mean: 195.48
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 9097
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.77796292304993
time_total_s: 6038.282925128937
timers:
  learn_throughput: 446.041
  learn_time_ms: 36992.11
  load_throughput: 4726896.297
  load_time_ms: 3.491
  training_iteration_time_ms: 49297.121
  update_time_ms: 2.551
timesteps_total: 1881000
training_iteration: 114

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28205128205128205
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7134146341463414
  reward for individual goal_min: 0.0
episode_len_mean: 206.57
episode_reward_max: 2.0
episode_reward_mean: 1.06
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 9930
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.5848867893219
time_total_s: 6049.325411319733
timers:
  learn_throughput: 470.216
  learn_time_ms: 35090.256
  load_throughput: 4892232.912
  load_time_ms: 3.373
  training_iteration_time_ms: 46755.816
  update_time_ms: 2.536
timesteps_total: 2062500
training_iteration: 125

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.275
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.41
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 9947
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.40927529335022
time_total_s: 6049.502894639969
timers:
  learn_throughput: 453.884
  learn_time_ms: 36352.881
  load_throughput: 4788912.831
  load_time_ms: 3.445
  training_iteration_time_ms: 48199.185
  update_time_ms: 2.604
timesteps_total: 1914000
training_iteration: 116

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 163.34
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 9070
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.94339179992676
time_total_s: 6043.998630523682
timers:
  learn_throughput: 428.973
  learn_time_ms: 38463.942
  load_throughput: 4465048.292
  load_time_ms: 3.695
  training_iteration_time_ms: 50720.622
  update_time_ms: 2.689
timesteps_total: 1732500
training_iteration: 105

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18243243243243243
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9810126582278481
  reward for individual goal_min: 0.5
episode_len_mean: 196.14
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 9021
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.33716654777527
time_total_s: 6044.677087545395
timers:
  learn_throughput: 453.9
  learn_time_ms: 36351.657
  load_throughput: 4717037.522
  load_time_ms: 3.498
  training_iteration_time_ms: 48330.994
  update_time_ms: 2.541
timesteps_total: 1848000
training_iteration: 112

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23026315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9698795180722891
  reward for individual goal_min: 0.0
episode_len_mean: 179.8
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 9074
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.46067214012146
time_total_s: 6053.1973967552185
timers:
  learn_throughput: 431.487
  learn_time_ms: 38239.838
  load_throughput: 4378880.445
  load_time_ms: 3.768
  training_iteration_time_ms: 50599.465
  update_time_ms: 2.651
timesteps_total: 1749000
training_iteration: 106

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15671641791044777
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9938271604938271
  reward for individual goal_min: 0.5
episode_len_mean: 173.09
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 9866
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.07730150222778
time_total_s: 6062.388593673706
timers:
  learn_throughput: 436.883
  learn_time_ms: 37767.529
  load_throughput: 4947633.707
  load_time_ms: 3.335
  training_iteration_time_ms: 50131.339
  update_time_ms: 2.989
timesteps_total: 1848000
training_iteration: 112

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1962025316455696
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 186.15
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 9970
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.660359382629395
time_total_s: 6047.710063934326
timers:
  learn_throughput: 455.945
  learn_time_ms: 36188.538
  load_throughput: 4751333.004
  load_time_ms: 3.473
  training_iteration_time_ms: 48264.099
  update_time_ms: 2.529
timesteps_total: 1914000
training_iteration: 116

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22560975609756098
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8289473684210527
  reward for individual goal_min: 0.0
episode_len_mean: 206.06
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 10398
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.42
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.6451461315155
time_total_s: 6081.886632204056
timers:
  learn_throughput: 497.218
  learn_time_ms: 33184.639
  load_throughput: 4754989.282
  load_time_ms: 3.47
  training_iteration_time_ms: 44375.551
  update_time_ms: 2.547
timesteps_total: 2145000
training_iteration: 130

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8125
  reward for individual goal_min: 0.5
episode_len_mean: 81.89583333333333
episode_reward_max: 2.0
episode_reward_mean: 1.8125
episode_reward_min: 1.0
episodes_this_iter: 192
episodes_total: 21118
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8125
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.551095724105835
time_total_s: 6061.415102481842
timers:
  learn_throughput: 446.084
  learn_time_ms: 36988.585
  load_throughput: 4769177.802
  load_time_ms: 3.46
  training_iteration_time_ms: 48283.737
  update_time_ms: 2.62
timesteps_total: 1963500
training_iteration: 119

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8543689320388349
  reward for individual goal_min: 0.5
episode_len_mean: 77.89320388349515
episode_reward_max: 2.0
episode_reward_mean: 1.854368932038835
episode_reward_min: 1.0
episodes_this_iter: 206
episodes_total: 14267
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9951456310679612
  agent_1: 0.8592233009708737
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.09495782852173
time_total_s: 6079.980879545212
timers:
  learn_throughput: 371.427
  learn_time_ms: 44423.249
  load_throughput: 4136246.145
  load_time_ms: 3.989
  training_iteration_time_ms: 58193.706
  update_time_ms: 2.963
timesteps_total: 1666500
training_iteration: 101

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9875776397515528
  reward for individual goal_min: 0.5
episode_len_mean: 52.00626959247649
episode_reward_max: 2.0
episode_reward_mean: 1.9874608150470219
episode_reward_min: 1.0
episodes_this_iter: 319
episodes_total: 20357
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.987460815047022
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 55.403833866119385
time_total_s: 6059.9962203502655
timers:
  learn_throughput: 392.273
  learn_time_ms: 42062.571
  load_throughput: 3988842.356
  load_time_ms: 4.137
  training_iteration_time_ms: 55133.64
  update_time_ms: 2.551
timesteps_total: 1831500
training_iteration: 111

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28846153846153844
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.05
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 11759
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.314770221710205
time_total_s: 6066.871704339981
timers:
  learn_throughput: 523.092
  learn_time_ms: 31543.188
  load_throughput: 5287341.737
  load_time_ms: 3.121
  training_iteration_time_ms: 42215.459
  update_time_ms: 2.463
timesteps_total: 2194500
training_iteration: 133

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972067039106145
  reward for individual goal_min: 0.5
episode_len_mean: 48.43529411764706
episode_reward_max: 2.0
episode_reward_mean: 1.9970588235294118
episode_reward_min: 1.0
episodes_this_iter: 340
episodes_total: 30416
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9970588235294118
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 46.47985363006592
time_total_s: 6037.021152257919
timers:
  learn_throughput: 459.977
  learn_time_ms: 35871.383
  load_throughput: 4582542.56
  load_time_ms: 3.601
  training_iteration_time_ms: 47290.04
  update_time_ms: 2.388
timesteps_total: 2178000
training_iteration: 132

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30405405405405406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7638888888888888
  reward for individual goal_min: 0.0
episode_len_mean: 198.43
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 9184
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.373223304748535
time_total_s: 6088.656148433685
timers:
  learn_throughput: 443.495
  learn_time_ms: 37204.464
  load_throughput: 4719321.35
  load_time_ms: 3.496
  training_iteration_time_ms: 49581.0
  update_time_ms: 2.558
timesteps_total: 1897500
training_iteration: 115

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2905405405405405
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7628205128205128
  reward for individual goal_min: 0.0
episode_len_mean: 197.73
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 10016
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.39964175224304
time_total_s: 6094.725053071976
timers:
  learn_throughput: 469.635
  learn_time_ms: 35133.657
  load_throughput: 4897980.537
  load_time_ms: 3.369
  training_iteration_time_ms: 46732.589
  update_time_ms: 2.54
timesteps_total: 2079000
training_iteration: 126

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.325
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9779411764705882
  reward for individual goal_min: 0.0
episode_len_mean: 178.22
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 10038
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.989519119262695
time_total_s: 6097.492413759232
timers:
  learn_throughput: 455.6
  learn_time_ms: 36215.961
  load_throughput: 4811754.121
  load_time_ms: 3.429
  training_iteration_time_ms: 48040.053
  update_time_ms: 2.622
timesteps_total: 1930500
training_iteration: 117

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25949367088607594
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9929577464788732
  reward for individual goal_min: 0.5
episode_len_mean: 186.34
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 9111
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.13221788406372
time_total_s: 6091.809305429459
timers:
  learn_throughput: 455.275
  learn_time_ms: 36241.826
  load_throughput: 4737736.764
  load_time_ms: 3.483
  training_iteration_time_ms: 48172.426
  update_time_ms: 2.555
timesteps_total: 1864500
training_iteration: 113

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24324324324324326
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9726027397260274
  reward for individual goal_min: 0.0
episode_len_mean: 175.83
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 9163
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.42936038970947
time_total_s: 6098.427990913391
timers:
  learn_throughput: 424.748
  learn_time_ms: 38846.564
  load_throughput: 4393028.641
  load_time_ms: 3.756
  training_iteration_time_ms: 51287.552
  update_time_ms: 2.661
timesteps_total: 1749000
training_iteration: 106

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7857142857142857
  reward for individual goal_min: 0.0
episode_len_mean: 180.7
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 10493
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.66166663169861
time_total_s: 6125.548298835754
timers:
  learn_throughput: 500.582
  learn_time_ms: 32961.626
  load_throughput: 4767863.535
  load_time_ms: 3.461
  training_iteration_time_ms: 44045.214
  update_time_ms: 2.539
timesteps_total: 2161500
training_iteration: 131

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28205128205128205
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9797297297297297
  reward for individual goal_min: 0.0
episode_len_mean: 175.8
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 9959
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.694493770599365
time_total_s: 6112.083087444305
timers:
  learn_throughput: 437.456
  learn_time_ms: 37718.099
  load_throughput: 4911920.735
  load_time_ms: 3.359
  training_iteration_time_ms: 50046.036
  update_time_ms: 3.005
timesteps_total: 1864500
training_iteration: 113

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30666666666666664
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 162.47524752475246
episode_reward_max: 2.0
episode_reward_mean: 1.4059405940594059
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 9175
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7227722772277227
  agent_1: 0.6831683168316832
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.40905570983887
time_total_s: 6105.606452465057
timers:
  learn_throughput: 427.919
  learn_time_ms: 38558.705
  load_throughput: 4422957.5
  load_time_ms: 3.731
  training_iteration_time_ms: 50998.081
  update_time_ms: 2.674
timesteps_total: 1765500
training_iteration: 107

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27631578947368424
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 166.97058823529412
episode_reward_max: 2.0
episode_reward_mean: 1.3627450980392157
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 10072
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6078431372549019
  agent_1: 0.7549019607843137
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.80381631851196
time_total_s: 6094.513880252838
timers:
  learn_throughput: 458.401
  learn_time_ms: 35994.698
  load_throughput: 4771578.207
  load_time_ms: 3.458
  training_iteration_time_ms: 48019.872
  update_time_ms: 2.557
timesteps_total: 1930500
training_iteration: 117

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22916666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 179.62
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 11851
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.94452786445618
time_total_s: 6109.816232204437
timers:
  learn_throughput: 522.675
  learn_time_ms: 31568.376
  load_throughput: 5289968.737
  load_time_ms: 3.119
  training_iteration_time_ms: 42271.16
  update_time_ms: 2.476
timesteps_total: 2211000
training_iteration: 134

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 75.71666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.8333333333333333
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8333333333333334
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8
  reward for individual goal_min: 0.5
episode_len_mean: 83.24630541871922
episode_reward_max: 2.0
episode_reward_mean: 1.8029556650246306
episode_reward_min: 1.0
episodes_this_iter: 203
episodes_total: 21321
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8029556650246306
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 53.78700256347656
time_total_s: 6115.202105045319
timers:
  learn_throughput: 444.233
  learn_time_ms: 37142.668
  load_throughput: 4764417.029
  load_time_ms: 3.463
  training_iteration_time_ms: 48484.783
  update_time_ms: 2.622
timesteps_total: 1980000
training_iteration: 120

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.997093023255814
  reward for individual goal_min: 0.5
episode_len_mean: 46.39718309859155
episode_reward_max: 2.0
episode_reward_mean: 1.9971830985915493
episode_reward_min: 1.0
episodes_this_iter: 355
episodes_total: 30771
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9971830985915493
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.553245067596436
time_total_s: 6084.574397325516
timers:
  learn_throughput: 460.107
  learn_time_ms: 35861.223
  load_throughput: 4563565.602
  load_time_ms: 3.616
  training_iteration_time_ms: 47237.529
  update_time_ms: 2.363
timesteps_total: 2194500
training_iteration: 133

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9937106918238994
  reward for individual goal_min: 0.5
episode_len_mean: 51.77917981072555
episode_reward_max: 2.0
episode_reward_mean: 1.9936908517350158
episode_reward_min: 1.0
episodes_this_iter: 317
episodes_total: 20674
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9936908517350158
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 55.89561891555786
time_total_s: 6115.891839265823
timers:
  learn_throughput: 391.115
  learn_time_ms: 42187.052
  load_throughput: 3974342.223
  load_time_ms: 4.152
  training_iteration_time_ms: 55254.703
  update_time_ms: 2.59
timesteps_total: 1848000
training_iteration: 112

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 84.86224489795919
episode_reward_max: 2.0
episode_reward_mean: 1.8316326530612246
episode_reward_min: 1.0
episodes_this_iter: 196
episodes_total: 14463
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9846938775510204
  agent_1: 0.8469387755102041
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.66637444496155
time_total_s: 6138.647253990173
timers:
  learn_throughput: 371.605
  learn_time_ms: 44401.985
  load_throughput: 4062697.232
  load_time_ms: 4.061
  training_iteration_time_ms: 58238.087
  update_time_ms: 2.96
timesteps_total: 1683000
training_iteration: 102

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34459459459459457
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7215189873417721
  reward for individual goal_min: 0.0
episode_len_mean: 194.69
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 9265
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.563687801361084
time_total_s: 6141.219836235046
timers:
  learn_throughput: 441.696
  learn_time_ms: 37356.047
  load_throughput: 4724637.388
  load_time_ms: 3.492
  training_iteration_time_ms: 49771.176
  update_time_ms: 2.582
timesteps_total: 1914000
training_iteration: 116

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29411764705882354
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.76
  reward for individual goal_min: 0.0
episode_len_mean: 198.13
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 10098
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.089553356170654
time_total_s: 6140.814606428146
timers:
  learn_throughput: 468.67
  learn_time_ms: 35206.042
  load_throughput: 4899090.066
  load_time_ms: 3.368
  training_iteration_time_ms: 46800.649
  update_time_ms: 2.532
timesteps_total: 2095500
training_iteration: 127

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3618421052631579
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9938271604938271
  reward for individual goal_min: 0.5
episode_len_mean: 156.78301886792454
episode_reward_max: 2.0
episode_reward_mean: 1.4339622641509433
episode_reward_min: 0.0
episodes_this_iter: 106
episodes_total: 10144
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7169811320754716
  agent_1: 0.7169811320754716
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.630003690719604
time_total_s: 6145.122417449951
timers:
  learn_throughput: 455.332
  learn_time_ms: 36237.305
  load_throughput: 4843510.54
  load_time_ms: 3.407
  training_iteration_time_ms: 48135.049
  update_time_ms: 2.624
timesteps_total: 1947000
training_iteration: 118

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30405405405405406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8289473684210527
  reward for individual goal_min: 0.0
episode_len_mean: 198.35
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 10574
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.13734841346741
time_total_s: 6167.685647249222
timers:
  learn_throughput: 503.446
  learn_time_ms: 32774.102
  load_throughput: 4731258.87
  load_time_ms: 3.487
  training_iteration_time_ms: 43780.423
  update_time_ms: 2.543
timesteps_total: 2178000
training_iteration: 132

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23684210526315788
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 179.79
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 11942
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.34537744522095
time_total_s: 6150.161609649658
timers:
  learn_throughput: 524.407
  learn_time_ms: 31464.135
  load_throughput: 5286130.156
  load_time_ms: 3.121
  training_iteration_time_ms: 42131.673
  update_time_ms: 2.484
timesteps_total: 2227500
training_iteration: 135

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21428571428571427
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 177.16
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 9206
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.04776644706726
time_total_s: 6141.857071876526
timers:
  learn_throughput: 452.558
  learn_time_ms: 36459.431
  load_throughput: 4742476.838
  load_time_ms: 3.479
  training_iteration_time_ms: 48407.633
  update_time_ms: 2.556
timesteps_total: 1881000
training_iteration: 114

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25301204819277107
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 185.41
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 10161
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.92566776275635
time_total_s: 6140.4395480155945
timers:
  learn_throughput: 461.773
  learn_time_ms: 35731.815
  load_throughput: 4785866.049
  load_time_ms: 3.448
  training_iteration_time_ms: 47631.74
  update_time_ms: 2.546
timesteps_total: 1947000
training_iteration: 118

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.0
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 10053
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.537721395492554
time_total_s: 6160.620808839798
timers:
  learn_throughput: 438.715
  learn_time_ms: 37609.836
  load_throughput: 4918658.432
  load_time_ms: 3.355
  training_iteration_time_ms: 49837.328
  update_time_ms: 2.992
timesteps_total: 1881000
training_iteration: 114

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3048780487804878
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 180.72
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 9257
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.18436336517334
time_total_s: 6151.612354278564
timers:
  learn_throughput: 420.802
  learn_time_ms: 39210.806
  load_throughput: 4362814.401
  load_time_ms: 3.782
  training_iteration_time_ms: 51758.488
  update_time_ms: 2.665
timesteps_total: 1765500
training_iteration: 107

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3227848101265823
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9714285714285714
  reward for individual goal_min: 0.0
episode_len_mean: 175.27
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 9270
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.11856031417847
time_total_s: 6158.725012779236
timers:
  learn_throughput: 426.111
  learn_time_ms: 38722.271
  load_throughput: 4423579.464
  load_time_ms: 3.73
  training_iteration_time_ms: 51252.935
  update_time_ms: 2.678
timesteps_total: 1782000
training_iteration: 108

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8095238095238095
  reward for individual goal_min: 0.5
episode_len_mean: 86.53125
episode_reward_max: 2.0
episode_reward_mean: 1.7916666666666667
episode_reward_min: 1.0
episodes_this_iter: 192
episodes_total: 21513
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7916666666666666
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.07264947891235
time_total_s: 6163.274754524231
timers:
  learn_throughput: 445.982
  learn_time_ms: 36997.029
  load_throughput: 4762089.357
  load_time_ms: 3.465
  training_iteration_time_ms: 48288.531
  update_time_ms: 2.613
timesteps_total: 1996500
training_iteration: 121

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.337078651685395
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 356
episodes_total: 31127
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 46.94128131866455
time_total_s: 6131.51567864418
timers:
  learn_throughput: 460.323
  learn_time_ms: 35844.397
  load_throughput: 4567330.324
  load_time_ms: 3.613
  training_iteration_time_ms: 47216.893
  update_time_ms: 2.365
timesteps_total: 2211000
training_iteration: 134

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29605263157894735
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7533333333333333
  reward for individual goal_min: 0.0
episode_len_mean: 210.26
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 10177
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.06796073913574
time_total_s: 6185.882567167282
timers:
  learn_throughput: 469.596
  learn_time_ms: 35136.55
  load_throughput: 4847445.926
  load_time_ms: 3.404
  training_iteration_time_ms: 46643.207
  update_time_ms: 2.493
timesteps_total: 2112000
training_iteration: 128

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8469387755102041
  reward for individual goal_min: 0.5
episode_len_mean: 85.41450777202073
episode_reward_max: 2.0
episode_reward_mean: 1.8445595854922279
episode_reward_min: 1.0
episodes_this_iter: 193
episodes_total: 14656
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9844559585492227
  agent_1: 0.8601036269430051
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.44842457771301
time_total_s: 6195.095678567886
timers:
  learn_throughput: 373.619
  learn_time_ms: 44162.669
  load_throughput: 4046591.16
  load_time_ms: 4.078
  training_iteration_time_ms: 57972.031
  update_time_ms: 2.934
timesteps_total: 1699500
training_iteration: 103

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9967741935483871
  reward for individual goal_min: 0.5
episode_len_mean: 51.30218068535826
episode_reward_max: 2.0
episode_reward_mean: 1.9968847352024923
episode_reward_min: 1.0
episodes_this_iter: 321
episodes_total: 20995
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9968847352024922
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 56.991310596466064
time_total_s: 6172.883149862289
timers:
  learn_throughput: 388.489
  learn_time_ms: 42472.251
  load_throughput: 3928499.353
  load_time_ms: 4.2
  training_iteration_time_ms: 55558.883
  update_time_ms: 2.581
timesteps_total: 1864500
training_iteration: 113

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36923076923076925
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7266666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 192.29
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 9352
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.29479670524597
time_total_s: 6192.514632940292
timers:
  learn_throughput: 439.722
  learn_time_ms: 37523.67
  load_throughput: 4720061.656
  load_time_ms: 3.496
  training_iteration_time_ms: 50007.639
  update_time_ms: 2.594
timesteps_total: 1930500
training_iteration: 117

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9788732394366197
  reward for individual goal_min: 0.0
episode_len_mean: 174.88
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 10235
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.14374899864197
time_total_s: 6192.266166448593
timers:
  learn_throughput: 455.737
  learn_time_ms: 36205.114
  load_throughput: 4825240.788
  load_time_ms: 3.42
  training_iteration_time_ms: 47981.8
  update_time_ms: 2.636
timesteps_total: 1963500
training_iteration: 119

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2702702702702703
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.81875
  reward for individual goal_min: 0.0
episode_len_mean: 197.5
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 10658
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.24666714668274
time_total_s: 6210.9323143959045
timers:
  learn_throughput: 504.987
  learn_time_ms: 32674.133
  load_throughput: 4791532.188
  load_time_ms: 3.444
  training_iteration_time_ms: 43712.847
  update_time_ms: 2.515
timesteps_total: 2194500
training_iteration: 133

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.12
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 12035
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.5531542301178
time_total_s: 6192.714763879776
timers:
  learn_throughput: 524.149
  learn_time_ms: 31479.603
  load_throughput: 5284354.174
  load_time_ms: 3.122
  training_iteration_time_ms: 42176.178
  update_time_ms: 2.47
timesteps_total: 2244000
training_iteration: 136

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30985915492957744
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 158.3106796116505
episode_reward_max: 2.0
episode_reward_mean: 1.4077669902912622
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 10264
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6893203883495146
  agent_1: 0.7184466019417476
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.94013047218323
time_total_s: 6188.379678487778
timers:
  learn_throughput: 463.021
  learn_time_ms: 35635.564
  load_throughput: 4820602.523
  load_time_ms: 3.423
  training_iteration_time_ms: 47343.344
  update_time_ms: 2.547
timesteps_total: 1963500
training_iteration: 119

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31645569620253167
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 180.75
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 9296
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.25319147109985
time_total_s: 6193.110263347626
timers:
  learn_throughput: 449.251
  learn_time_ms: 36727.837
  load_throughput: 4737704.33
  load_time_ms: 3.483
  training_iteration_time_ms: 48704.658
  update_time_ms: 2.562
timesteps_total: 1897500
training_iteration: 115

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3353658536585366
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9852941176470589
  reward for individual goal_min: 0.0
episode_len_mean: 176.46
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 10149
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.35326075553894
time_total_s: 6210.974069595337
timers:
  learn_throughput: 436.934
  learn_time_ms: 37763.162
  load_throughput: 4914397.222
  load_time_ms: 3.357
  training_iteration_time_ms: 49976.405
  update_time_ms: 2.992
timesteps_total: 1897500
training_iteration: 115

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23376623376623376
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.0
episode_len_mean: 176.98
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 9349
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.69702219963074
time_total_s: 6203.309376478195
timers:
  learn_throughput: 417.177
  learn_time_ms: 39551.539
  load_throughput: 4350965.114
  load_time_ms: 3.792
  training_iteration_time_ms: 52056.865
  update_time_ms: 2.683
timesteps_total: 1782000
training_iteration: 108

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17721518987341772
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 196.99
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 9353
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.46662497520447
time_total_s: 6209.19163775444
timers:
  learn_throughput: 425.333
  learn_time_ms: 38793.152
  load_throughput: 4405220.624
  load_time_ms: 3.746
  training_iteration_time_ms: 51269.406
  update_time_ms: 2.698
timesteps_total: 1798500
training_iteration: 109

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8258928571428571
  reward for individual goal_min: 0.5
episode_len_mean: 80.79611650485437
episode_reward_max: 2.0
episode_reward_mean: 1.8106796116504855
episode_reward_min: 1.0
episodes_this_iter: 206
episodes_total: 21719
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8155339805825242
  agent_1: 0.9951456310679612
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.116188049316406
time_total_s: 6211.390942573547
timers:
  learn_throughput: 445.185
  learn_time_ms: 37063.275
  load_throughput: 4785303.481
  load_time_ms: 3.448
  training_iteration_time_ms: 48384.42
  update_time_ms: 2.612
timesteps_total: 2013000
training_iteration: 122

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939024390243902
  reward for individual goal_min: 0.5
episode_len_mean: 46.06983240223464
episode_reward_max: 2.0
episode_reward_mean: 1.994413407821229
episode_reward_min: 1.0
episodes_this_iter: 358
episodes_total: 31485
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.994413407821229
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.83805465698242
time_total_s: 6179.353733301163
timers:
  learn_throughput: 460.083
  learn_time_ms: 35863.095
  load_throughput: 4536463.308
  load_time_ms: 3.637
  training_iteration_time_ms: 47255.384
  update_time_ms: 2.378
timesteps_total: 2227500
training_iteration: 135

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2357142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.6917808219178082
  reward for individual goal_min: 0.0
episode_len_mean: 204.99
episode_reward_max: 2.0
episode_reward_mean: 1.02
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 10259
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.97374629974365
time_total_s: 6234.856313467026
timers:
  learn_throughput: 468.36
  learn_time_ms: 35229.344
  load_throughput: 4857278.336
  load_time_ms: 3.397
  training_iteration_time_ms: 46738.285
  update_time_ms: 2.498
timesteps_total: 2128500
training_iteration: 129

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9917127071823204
  reward for individual goal_min: 0.5
episode_len_mean: 51.47352024922118
episode_reward_max: 2.0
episode_reward_mean: 1.9906542056074767
episode_reward_min: 1.0
episodes_this_iter: 321
episodes_total: 21316
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9906542056074766
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 52.8772177696228
time_total_s: 6225.760367631912
timers:
  learn_throughput: 390.405
  learn_time_ms: 42263.832
  load_throughput: 3885380.897
  load_time_ms: 4.247
  training_iteration_time_ms: 55241.295
  update_time_ms: 2.603
timesteps_total: 1881000
training_iteration: 114

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29012345679012347
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 188.29
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 12121
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.36697745323181
time_total_s: 6234.081741333008
timers:
  learn_throughput: 525.565
  learn_time_ms: 31394.771
  load_throughput: 5253267.142
  load_time_ms: 3.141
  training_iteration_time_ms: 42092.51
  update_time_ms: 2.48
timesteps_total: 2260500
training_iteration: 137

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19863013698630136
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7922077922077922
  reward for individual goal_min: 0.0
episode_len_mean: 204.93
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 10740
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.39
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.1694495677948
time_total_s: 6255.101763963699
timers:
  learn_throughput: 505.065
  learn_time_ms: 32669.072
  load_throughput: 4775694.106
  load_time_ms: 3.455
  training_iteration_time_ms: 43713.554
  update_time_ms: 2.515
timesteps_total: 2211000
training_iteration: 134

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3611111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.726027397260274
  reward for individual goal_min: 0.0
episode_len_mean: 190.97
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 9435
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.354342460632324
time_total_s: 6242.868975400925
timers:
  learn_throughput: 438.148
  learn_time_ms: 37658.538
  load_throughput: 4687928.685
  load_time_ms: 3.52
  training_iteration_time_ms: 50156.104
  update_time_ms: 2.606
timesteps_total: 1947000
training_iteration: 118

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8523809523809524
  reward for individual goal_min: 0.5
episode_len_mean: 79.64114832535886
episode_reward_max: 2.0
episode_reward_mean: 1.8516746411483254
episode_reward_min: 1.0
episodes_this_iter: 209
episodes_total: 14865
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8516746411483254
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 59.956732749938965
time_total_s: 6255.052411317825
timers:
  learn_throughput: 374.015
  learn_time_ms: 44115.835
  load_throughput: 4042525.541
  load_time_ms: 4.082
  training_iteration_time_ms: 57937.836
  update_time_ms: 2.92
timesteps_total: 1716000
training_iteration: 104

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 77.95
episode_reward_max: 2.0
episode_reward_mean: 1.9166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.95
  agent_1: 0.9666666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18354430379746836
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 192.45
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 10324
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.45849609375
time_total_s: 6248.724662542343
timers:
  learn_throughput: 455.734
  learn_time_ms: 36205.359
  load_throughput: 4853395.047
  load_time_ms: 3.4
  training_iteration_time_ms: 47989.081
  update_time_ms: 2.61
timesteps_total: 1980000
training_iteration: 120

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-18pddilqzj/checkpoint_000120/checkpoint-120
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33766233766233766
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9807692307692307
  reward for individual goal_min: 0.5
episode_len_mean: 172.68
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 9390
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.018107652664185
time_total_s: 6241.12837100029
timers:
  learn_throughput: 451.118
  learn_time_ms: 36575.76
  load_throughput: 4708436.758
  load_time_ms: 3.504
  training_iteration_time_ms: 48417.514
  update_time_ms: 2.562
timesteps_total: 1914000
training_iteration: 116

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24074074074074073
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9805194805194806
  reward for individual goal_min: 0.0
episode_len_mean: 183.51
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 10237
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.67689633369446
time_total_s: 6261.650965929031
timers:
  learn_throughput: 434.994
  learn_time_ms: 37931.537
  load_throughput: 4869445.199
  load_time_ms: 3.388
  training_iteration_time_ms: 50210.987
  update_time_ms: 2.992
timesteps_total: 1914000
training_iteration: 116

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 68.56666666666666
episode_reward_max: 2.0
episode_reward_mean: 1.9333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9833333333333333
  agent_1: 0.95
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.79
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 10360
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.629847049713135
time_total_s: 6246.009525537491
timers:
  learn_throughput: 463.997
  learn_time_ms: 35560.576
  load_throughput: 4831540.234
  load_time_ms: 3.415
  training_iteration_time_ms: 47346.638
  update_time_ms: 2.577
timesteps_total: 1980000
training_iteration: 120

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-28met_xp4k/checkpoint_000120/checkpoint-120
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2987012987012987
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9873417721518988
  reward for individual goal_min: 0.0
episode_len_mean: 170.76
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 9447
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.46454858779907
time_total_s: 6260.656186342239
timers:
  learn_throughput: 423.788
  learn_time_ms: 38934.565
  load_throughput: 4414916.015
  load_time_ms: 3.737
  training_iteration_time_ms: 51433.307
  update_time_ms: 2.764
timesteps_total: 1815000
training_iteration: 110

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 170.61
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 9444
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.05861949920654
time_total_s: 6257.367995977402
timers:
  learn_throughput: 415.404
  learn_time_ms: 39720.416
  load_throughput: 4374949.648
  load_time_ms: 3.771
  training_iteration_time_ms: 52225.12
  update_time_ms: 2.717
timesteps_total: 1798500
training_iteration: 109

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9911764705882353
  reward for individual goal_min: 0.5
episode_len_mean: 45.443835616438356
episode_reward_max: 2.0
episode_reward_mean: 1.9917808219178081
episode_reward_min: 1.0
episodes_this_iter: 365
episodes_total: 31850
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9945205479452055
  agent_1: 0.9972602739726028
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.07586860656738
time_total_s: 6226.42960190773
timers:
  learn_throughput: 459.918
  learn_time_ms: 35875.952
  load_throughput: 4498249.345
  load_time_ms: 3.668
  training_iteration_time_ms: 47243.181
  update_time_ms: 2.371
timesteps_total: 2244000
training_iteration: 136

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8803418803418803
  reward for individual goal_min: 0.5
episode_len_mean: 66.04065040650407
episode_reward_max: 2.0
episode_reward_mean: 1.886178861788618
episode_reward_min: 1.0
episodes_this_iter: 246
episodes_total: 21965
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8861788617886179
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 49.56960320472717
time_total_s: 6260.9605457782745
timers:
  learn_throughput: 443.216
  learn_time_ms: 37227.895
  load_throughput: 4811319.244
  load_time_ms: 3.429
  training_iteration_time_ms: 48596.726
  update_time_ms: 2.675
timesteps_total: 2029500
training_iteration: 123

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29545454545454547
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8223684210526315
  reward for individual goal_min: 0.0
episode_len_mean: 191.12
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 10345
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.857507944107056
time_total_s: 6279.713821411133
timers:
  learn_throughput: 471.793
  learn_time_ms: 34972.946
  load_throughput: 4834814.344
  load_time_ms: 3.413
  training_iteration_time_ms: 46438.312
  update_time_ms: 2.489
timesteps_total: 2145000
training_iteration: 130

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21686746987951808
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9807692307692307
  reward for individual goal_min: 0.0
episode_len_mean: 198.81
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 12207
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.313647985458374
time_total_s: 6274.395389318466
timers:
  learn_throughput: 527.548
  learn_time_ms: 31276.749
  load_throughput: 5280765.492
  load_time_ms: 3.125
  training_iteration_time_ms: 41941.118
  update_time_ms: 2.464
timesteps_total: 2277000
training_iteration: 138

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2785714285714286
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7972972972972973
  reward for individual goal_min: 0.0
episode_len_mean: 198.12
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 10823
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.43
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.41701698303223
time_total_s: 6298.518780946732
timers:
  learn_throughput: 507.154
  learn_time_ms: 32534.476
  load_throughput: 4738223.321
  load_time_ms: 3.482
  training_iteration_time_ms: 43543.096
  update_time_ms: 2.541
timesteps_total: 2227500
training_iteration: 135

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3051948051948052
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7857142857142857
  reward for individual goal_min: 0.0
episode_len_mean: 188.54
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 9520
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.9657518863678
time_total_s: 6294.8347272872925
timers:
  learn_throughput: 436.018
  learn_time_ms: 37842.507
  load_throughput: 4641926.366
  load_time_ms: 3.555
  training_iteration_time_ms: 50353.445
  update_time_ms: 2.599
timesteps_total: 1963500
training_iteration: 119

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23780487804878048
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 189.22
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 10409
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.28607249259949
time_total_s: 6295.010735034943
timers:
  learn_throughput: 456.392
  learn_time_ms: 36153.141
  load_throughput: 4834780.568
  load_time_ms: 3.413
  training_iteration_time_ms: 47943.169
  update_time_ms: 2.598
timesteps_total: 1996500
training_iteration: 121

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.990909090909091
  reward for individual goal_min: 0.5
episode_len_mean: 51.861635220125784
episode_reward_max: 2.0
episode_reward_mean: 1.990566037735849
episode_reward_min: 1.0
episodes_this_iter: 318
episodes_total: 21634
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9905660377358491
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 57.77701926231384
time_total_s: 6283.537386894226
timers:
  learn_throughput: 386.616
  learn_time_ms: 42678.039
  load_throughput: 3881131.936
  load_time_ms: 4.251
  training_iteration_time_ms: 55737.219
  update_time_ms: 2.636
timesteps_total: 1897500
training_iteration: 115

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29878048780487804
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.9
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 9480
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.4054696559906
time_total_s: 6290.5338406562805
timers:
  learn_throughput: 452.587
  learn_time_ms: 36457.052
  load_throughput: 4748268.679
  load_time_ms: 3.475
  training_iteration_time_ms: 48246.848
  update_time_ms: 2.572
timesteps_total: 1930500
training_iteration: 117

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8415841584158416
  reward for individual goal_min: 0.5
episode_len_mean: 82.60913705583756
episode_reward_max: 2.0
episode_reward_mean: 1.8375634517766497
episode_reward_min: 1.0
episodes_this_iter: 197
episodes_total: 15062
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9949238578680203
  agent_1: 0.8426395939086294
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.036051750183105
time_total_s: 6314.088463068008
timers:
  learn_throughput: 372.58
  learn_time_ms: 44285.735
  load_throughput: 4036442.407
  load_time_ms: 4.088
  training_iteration_time_ms: 58144.395
  update_time_ms: 2.914
timesteps_total: 1732500
training_iteration: 105

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2962962962962963
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.33
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 10331
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.49820685386658
time_total_s: 6310.149172782898
timers:
  learn_throughput: 436.58
  learn_time_ms: 37793.751
  load_throughput: 4886705.785
  load_time_ms: 3.377
  training_iteration_time_ms: 50099.18
  update_time_ms: 2.956
timesteps_total: 1930500
training_iteration: 117

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33116883116883117
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.09
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 10460
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.0018413066864
time_total_s: 6295.011366844177
timers:
  learn_throughput: 461.748
  learn_time_ms: 35733.744
  load_throughput: 4864824.193
  load_time_ms: 3.392
  training_iteration_time_ms: 47511.48
  update_time_ms: 2.567
timesteps_total: 1996500
training_iteration: 121

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973958333333334
  reward for individual goal_min: 0.5
episode_len_mean: 45.402234636871505
episode_reward_max: 2.0
episode_reward_mean: 1.9972067039106145
episode_reward_min: 1.0
episodes_this_iter: 358
episodes_total: 32208
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9972067039106145
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 46.46341586112976
time_total_s: 6272.89301776886
timers:
  learn_throughput: 461.67
  learn_time_ms: 35739.824
  load_throughput: 4500940.823
  load_time_ms: 3.666
  training_iteration_time_ms: 47068.681
  update_time_ms: 2.363
timesteps_total: 2260500
training_iteration: 137

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.822429906542056
  reward for individual goal_min: 0.5
episode_len_mean: 80.32367149758454
episode_reward_max: 2.0
episode_reward_mean: 1.816425120772947
episode_reward_min: 1.0
episodes_this_iter: 207
episodes_total: 22172
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8164251207729468
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.64858818054199
time_total_s: 6308.6091339588165
timers:
  learn_throughput: 443.141
  learn_time_ms: 37234.16
  load_throughput: 4781303.137
  load_time_ms: 3.451
  training_iteration_time_ms: 48594.143
  update_time_ms: 2.644
timesteps_total: 2046000
training_iteration: 124

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31690140845070425
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9882352941176471
  reward for individual goal_min: 0.0
episode_len_mean: 152.26168224299064
episode_reward_max: 2.0
episode_reward_mean: 1.439252336448598
episode_reward_min: 0.0
episodes_this_iter: 107
episodes_total: 9554
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6728971962616822
  agent_1: 0.7663551401869159
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.37519884109497
time_total_s: 6316.031385183334
timers:
  learn_throughput: 420.834
  learn_time_ms: 39207.887
  load_throughput: 4385317.813
  load_time_ms: 3.763
  training_iteration_time_ms: 51734.102
  update_time_ms: 2.776
timesteps_total: 1831500
training_iteration: 111

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25903614457831325
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 187.74
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 12297
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.89329719543457
time_total_s: 6316.288686513901
timers:
  learn_throughput: 527.987
  learn_time_ms: 31250.767
  load_throughput: 5248764.979
  load_time_ms: 3.144
  training_iteration_time_ms: 41915.406
  update_time_ms: 2.458
timesteps_total: 2293500
training_iteration: 139

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32142857142857145
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9940476190476191
  reward for individual goal_min: 0.5
episode_len_mean: 156.52380952380952
episode_reward_max: 2.0
episode_reward_mean: 1.4666666666666666
episode_reward_min: 0.0
episodes_this_iter: 105
episodes_total: 9549
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7428571428571429
  agent_1: 0.7238095238095238
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.91877889633179
time_total_s: 6312.2867748737335
timers:
  learn_throughput: 411.079
  learn_time_ms: 40138.254
  load_throughput: 4430375.909
  load_time_ms: 3.724
  training_iteration_time_ms: 52782.321
  update_time_ms: 2.728
timesteps_total: 1815000
training_iteration: 110

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2222222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7407407407407407
  reward for individual goal_min: 0.0
episode_len_mean: 200.74
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 10429
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.02985739707947
time_total_s: 6325.743678808212
timers:
  learn_throughput: 473.195
  learn_time_ms: 34869.36
  load_throughput: 4860075.423
  load_time_ms: 3.395
  training_iteration_time_ms: 46400.572
  update_time_ms: 2.486
timesteps_total: 2161500
training_iteration: 131

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3013698630136986
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8098591549295775
  reward for individual goal_min: 0.0
episode_len_mean: 191.73
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 10908
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.58102893829346
time_total_s: 6343.099809885025
timers:
  learn_throughput: 506.223
  learn_time_ms: 32594.353
  load_throughput: 4715237.751
  load_time_ms: 3.499
  training_iteration_time_ms: 43561.127
  update_time_ms: 2.547
timesteps_total: 2244000
training_iteration: 136

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20714285714285716
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9487179487179487
  reward for individual goal_min: 0.0
episode_len_mean: 178.22
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 10505
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.90564560890198
time_total_s: 6343.916380643845
timers:
  learn_throughput: 455.745
  learn_time_ms: 36204.456
  load_throughput: 4812189.078
  load_time_ms: 3.429
  training_iteration_time_ms: 48086.988
  update_time_ms: 2.629
timesteps_total: 2013000
training_iteration: 122

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2945205479452055
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9928571428571429
  reward for individual goal_min: 0.5
episode_len_mean: 166.72
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 9579
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.84686875343323
time_total_s: 6341.380709409714
timers:
  learn_throughput: 449.454
  learn_time_ms: 36711.223
  load_throughput: 4776617.041
  load_time_ms: 3.454
  training_iteration_time_ms: 48619.476
  update_time_ms: 2.571
timesteps_total: 1947000
training_iteration: 118

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939393939393939
  reward for individual goal_min: 0.5
episode_len_mean: 49.58258258258258
episode_reward_max: 2.0
episode_reward_mean: 1.993993993993994
episode_reward_min: 1.0
episodes_this_iter: 333
episodes_total: 21967
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.993993993993994
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 57.999430656433105
time_total_s: 6341.536817550659
timers:
  learn_throughput: 383.714
  learn_time_ms: 43000.746
  load_throughput: 3878673.968
  load_time_ms: 4.254
  training_iteration_time_ms: 56101.177
  update_time_ms: 2.655
timesteps_total: 1914000
training_iteration: 116

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7166666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 123.55
episode_reward_max: 2.0
episode_reward_mean: 1.6833333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.85
  agent_1: 0.8333333333333334
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2916666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 171.77
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 10428
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.45594239234924
time_total_s: 6358.605115175247
timers:
  learn_throughput: 436.917
  learn_time_ms: 37764.604
  load_throughput: 4872976.764
  load_time_ms: 3.386
  training_iteration_time_ms: 50075.493
  update_time_ms: 2.636
timesteps_total: 1947000
training_iteration: 118

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7466666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 195.76
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 9606
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 64.77003979682922
time_total_s: 6359.604767084122
timers:
  learn_throughput: 434.297
  learn_time_ms: 37992.43
  load_throughput: 4665332.983
  load_time_ms: 3.537
  training_iteration_time_ms: 50523.448
  update_time_ms: 2.614
timesteps_total: 1980000
training_iteration: 120

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19mcvnks1j/checkpoint_000120/checkpoint-120
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23376623376623376
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.71
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 10547
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.8213050365448
time_total_s: 6341.832671880722
timers:
  learn_throughput: 461.42
  learn_time_ms: 35759.209
  load_throughput: 4885015.6
  load_time_ms: 3.378
  training_iteration_time_ms: 47499.477
  update_time_ms: 2.569
timesteps_total: 2013000
training_iteration: 122

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7530120481927711
  reward for individual goal_min: 0.5
episode_len_mean: 98.86309523809524
episode_reward_max: 2.0
episode_reward_mean: 1.755952380952381
episode_reward_min: 1.0
episodes_this_iter: 168
episodes_total: 15230
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9880952380952381
  agent_1: 0.7678571428571429
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.40870404243469
time_total_s: 6370.497167110443
timers:
  learn_throughput: 373.356
  learn_time_ms: 44193.697
  load_throughput: 3842046.545
  load_time_ms: 4.295
  training_iteration_time_ms: 58057.066
  update_time_ms: 2.876
timesteps_total: 1749000
training_iteration: 106

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9915254237288136
  reward for individual goal_min: 0.5
episode_len_mean: 47.70028818443804
episode_reward_max: 2.0
episode_reward_mean: 1.9913544668587897
episode_reward_min: 1.0
episodes_this_iter: 347
episodes_total: 32555
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9971181556195965
  agent_1: 0.9942363112391931
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.78314137458801
time_total_s: 6319.676159143448
timers:
  learn_throughput: 461.158
  learn_time_ms: 35779.456
  load_throughput: 4501057.917
  load_time_ms: 3.666
  training_iteration_time_ms: 47084.904
  update_time_ms: 2.376
timesteps_total: 2277000
training_iteration: 138

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8362831858407079
  reward for individual goal_min: 0.5
episode_len_mean: 77.61137440758294
episode_reward_max: 2.0
episode_reward_mean: 1.8246445497630333
episode_reward_min: 1.0
episodes_this_iter: 211
episodes_total: 22383
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8246445497630331
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.19240403175354
time_total_s: 6356.80153799057
timers:
  learn_throughput: 443.714
  learn_time_ms: 37186.135
  load_throughput: 4783054.53
  load_time_ms: 3.45
  training_iteration_time_ms: 48509.843
  update_time_ms: 2.62
timesteps_total: 2062500
training_iteration: 125

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3355263157894737
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7272727272727273
  reward for individual goal_min: 0.0
episode_len_mean: 197.23
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 10513
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.43890976905823
time_total_s: 6371.1825885772705
timers:
  learn_throughput: 477.649
  learn_time_ms: 34544.211
  load_throughput: 4870233.357
  load_time_ms: 3.388
  training_iteration_time_ms: 46018.609
  update_time_ms: 2.488
timesteps_total: 2178000
training_iteration: 132

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 74.63333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.8666666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9333333333333333
  agent_1: 0.9333333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.275
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 179.83
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 12387
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.08730745315552
time_total_s: 6366.375993967056
timers:
  learn_throughput: 530.652
  learn_time_ms: 31093.844
  load_throughput: 5276055.196
  load_time_ms: 3.127
  training_iteration_time_ms: 41642.334
  update_time_ms: 2.446
timesteps_total: 2310000
training_iteration: 140

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19ecg2ejpn/checkpoint_000140/checkpoint-140
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7794117647058824
  reward for individual goal_min: 0.0
episode_len_mean: 209.86
episode_reward_max: 2.0
episode_reward_mean: 1.06
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 10986
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.34
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.67936372756958
time_total_s: 6387.779173612595
timers:
  learn_throughput: 506.297
  learn_time_ms: 32589.543
  load_throughput: 4663509.592
  load_time_ms: 3.538
  training_iteration_time_ms: 43512.5
  update_time_ms: 2.533
timesteps_total: 2260500
training_iteration: 137

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20422535211267606
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.0
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 9652
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.019126415252686
time_total_s: 6369.050511598587
timers:
  learn_throughput: 417.675
  learn_time_ms: 39504.389
  load_throughput: 4318143.11
  load_time_ms: 3.821
  training_iteration_time_ms: 52111.045
  update_time_ms: 2.79
timesteps_total: 1848000
training_iteration: 112

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2848101265822785
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9710144927536232
  reward for individual goal_min: 0.0
episode_len_mean: 180.14
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 9643
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.6342248916626
time_total_s: 6365.920999765396
timers:
  learn_throughput: 408.08
  learn_time_ms: 40433.227
  load_throughput: 4375115.595
  load_time_ms: 3.771
  training_iteration_time_ms: 53184.716
  update_time_ms: 2.734
timesteps_total: 1831500
training_iteration: 111

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20454545454545456
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 172.58
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 10601
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.17957139015198
time_total_s: 6392.095952033997
timers:
  learn_throughput: 455.808
  learn_time_ms: 36199.448
  load_throughput: 4871501.798
  load_time_ms: 3.387
  training_iteration_time_ms: 48082.829
  update_time_ms: 2.617
timesteps_total: 2029500
training_iteration: 123

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.6
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 9676
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.63314366340637
time_total_s: 6390.01385307312
timers:
  learn_throughput: 448.272
  learn_time_ms: 36807.972
  load_throughput: 4762875.921
  load_time_ms: 3.464
  training_iteration_time_ms: 48682.729
  update_time_ms: 2.575
timesteps_total: 1963500
training_iteration: 119

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20394736842105263
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 180.1
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 10639
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.62782859802246
time_total_s: 6389.4605004787445
timers:
  learn_throughput: 461.868
  learn_time_ms: 35724.473
  load_throughput: 4821509.308
  load_time_ms: 3.422
  training_iteration_time_ms: 47404.134
  update_time_ms: 2.583
timesteps_total: 2029500
training_iteration: 123

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35064935064935066
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9791666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 168.78
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 10525
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.25131392478943
time_total_s: 6409.856429100037
timers:
  learn_throughput: 436.278
  learn_time_ms: 37819.883
  load_throughput: 4840393.912
  load_time_ms: 3.409
  training_iteration_time_ms: 50168.407
  update_time_ms: 2.64
timesteps_total: 1963500
training_iteration: 119

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8076923076923077
  reward for individual goal_min: 0.0
episode_len_mean: 186.2
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 9693
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.200103521347046
time_total_s: 6410.804870605469
timers:
  learn_throughput: 433.422
  learn_time_ms: 38069.102
  load_throughput: 4621128.205
  load_time_ms: 3.571
  training_iteration_time_ms: 50631.535
  update_time_ms: 2.616
timesteps_total: 1996500
training_iteration: 121

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2826086956521739
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8164556962025317
  reward for individual goal_min: 0.0
episode_len_mean: 194.95
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 10595
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.852877616882324
time_total_s: 6414.035466194153
timers:
  learn_throughput: 482.467
  learn_time_ms: 34199.249
  load_throughput: 4870987.486
  load_time_ms: 3.387
  training_iteration_time_ms: 45579.735
  update_time_ms: 2.465
timesteps_total: 2194500
training_iteration: 133

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973404255319149
  reward for individual goal_min: 0.5
episode_len_mean: 45.728767123287675
episode_reward_max: 2.0
episode_reward_mean: 1.9972602739726026
episode_reward_min: 1.0
episodes_this_iter: 365
episodes_total: 32920
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9972602739726028
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 47.64132571220398
time_total_s: 6367.317484855652
timers:
  learn_throughput: 461.775
  learn_time_ms: 35731.713
  load_throughput: 4463838.697
  load_time_ms: 3.696
  training_iteration_time_ms: 47030.367
  update_time_ms: 2.364
timesteps_total: 2293500
training_iteration: 139

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8301886792452831
  reward for individual goal_min: 0.5
episode_len_mean: 78.24882629107981
episode_reward_max: 2.0
episode_reward_mean: 1.8309859154929577
episode_reward_min: 1.0
episodes_this_iter: 213
episodes_total: 22596
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8403755868544601
  agent_1: 0.9906103286384976
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.57044792175293
time_total_s: 6405.371985912323
timers:
  learn_throughput: 444.01
  learn_time_ms: 37161.354
  load_throughput: 4797211.759
  load_time_ms: 3.439
  training_iteration_time_ms: 48468.041
  update_time_ms: 2.645
timesteps_total: 2079000
training_iteration: 126

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33116883116883117
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.94
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 12485
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.83530879020691
time_total_s: 6410.211302757263
timers:
  learn_throughput: 530.902
  learn_time_ms: 31079.164
  load_throughput: 5345332.201
  load_time_ms: 3.087
  training_iteration_time_ms: 41633.377
  update_time_ms: 2.437
timesteps_total: 2326500
training_iteration: 141

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9940119760479041
  reward for individual goal_min: 0.5
episode_len_mean: 47.25214899713467
episode_reward_max: 2.0
episode_reward_mean: 1.994269340974212
episode_reward_min: 1.0
episodes_this_iter: 349
episodes_total: 22316
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.994269340974212
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 59.17738080024719
time_total_s: 6400.714198350906
timers:
  learn_throughput: 380.746
  learn_time_ms: 43335.921
  load_throughput: 3870344.442
  load_time_ms: 4.263
  training_iteration_time_ms: 56496.55
  update_time_ms: 2.669
timesteps_total: 1930500
training_iteration: 117

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2564102564102564
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7916666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 199.39
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 11068
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.42
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.935975551605225
time_total_s: 6433.7151491642
timers:
  learn_throughput: 505.215
  learn_time_ms: 32659.373
  load_throughput: 4686024.132
  load_time_ms: 3.521
  training_iteration_time_ms: 43618.743
  update_time_ms: 2.573
timesteps_total: 2277000
training_iteration: 138

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8385416666666666
  reward for individual goal_min: 0.5
episode_len_mean: 77.49074074074075
episode_reward_max: 2.0
episode_reward_mean: 1.8564814814814814
episode_reward_min: 1.0
episodes_this_iter: 216
episodes_total: 15446
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9907407407407407
  agent_1: 0.8657407407407407
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.19240736961365
time_total_s: 6430.689574480057
timers:
  learn_throughput: 370.366
  learn_time_ms: 44550.577
  load_throughput: 3897042.334
  load_time_ms: 4.234
  training_iteration_time_ms: 58416.823
  update_time_ms: 2.829
timesteps_total: 1765500
training_iteration: 107

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21428571428571427
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 186.21
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 9731
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.999579191207886
time_total_s: 6418.920578956604
timers:
  learn_throughput: 406.68
  learn_time_ms: 40572.442
  load_throughput: 4415761.11
  load_time_ms: 3.737
  training_iteration_time_ms: 53386.99
  update_time_ms: 2.766
timesteps_total: 1848000
training_iteration: 112

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23417721518987342
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9662162162162162
  reward for individual goal_min: 0.0
episode_len_mean: 185.99
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 9740
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.78705859184265
time_total_s: 6423.83757019043
timers:
  learn_throughput: 415.773
  learn_time_ms: 39685.158
  load_throughput: 4353072.423
  load_time_ms: 3.79
  training_iteration_time_ms: 52441.948
  update_time_ms: 2.811
timesteps_total: 1864500
training_iteration: 113

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3424657534246575
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 162.18
episode_reward_max: 2.0
episode_reward_mean: 1.41
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 10701
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.21058797836304
time_total_s: 6441.30654001236
timers:
  learn_throughput: 457.359
  learn_time_ms: 36076.72
  load_throughput: 4867390.335
  load_time_ms: 3.39
  training_iteration_time_ms: 47909.023
  update_time_ms: 2.623
timesteps_total: 2046000
training_iteration: 124

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 189.52
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 10726
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.46048283576965
time_total_s: 6437.920983314514
timers:
  learn_throughput: 459.255
  learn_time_ms: 35927.726
  load_throughput: 4882603.076
  load_time_ms: 3.379
  training_iteration_time_ms: 47677.703
  update_time_ms: 2.591
timesteps_total: 2046000
training_iteration: 124

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25675675675675674
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7564102564102564
  reward for individual goal_min: 0.0
episode_len_mean: 216.75
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 10671
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.45125365257263
time_total_s: 6458.4867198467255
timers:
  learn_throughput: 483.933
  learn_time_ms: 34095.65
  load_throughput: 4920197.075
  load_time_ms: 3.354
  training_iteration_time_ms: 45426.523
  update_time_ms: 2.458
timesteps_total: 2211000
training_iteration: 134

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7905405405405406
  reward for individual goal_min: 0.0
episode_len_mean: 198.93
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 9777
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.81525421142578
time_total_s: 6459.6201248168945
timers:
  learn_throughput: 432.512
  learn_time_ms: 38149.208
  load_throughput: 4575180.876
  load_time_ms: 3.606
  training_iteration_time_ms: 50724.001
  update_time_ms: 2.603
timesteps_total: 2013000
training_iteration: 122

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15584415584415584
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 200.75
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 12565
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.38691687583923
time_total_s: 6452.598219633102
timers:
  learn_throughput: 531.198
  learn_time_ms: 31061.881
  load_throughput: 5320511.094
  load_time_ms: 3.101
  training_iteration_time_ms: 41688.888
  update_time_ms: 2.442
timesteps_total: 2343000
training_iteration: 142

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.55
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18831168831168832
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7972972972972973
  reward for individual goal_min: 0.0
episode_len_mean: 207.89
episode_reward_max: 2.0
episode_reward_mean: 1.03
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 11146
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.37
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.6025025844574
time_total_s: 6478.317651748657
timers:
  learn_throughput: 501.855
  learn_time_ms: 32878.005
  load_throughput: 4713182.552
  load_time_ms: 3.501
  training_iteration_time_ms: 43870.773
  update_time_ms: 2.58
timesteps_total: 2293500
training_iteration: 139

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 98.18333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.7666666666666666
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9
  agent_1: 0.8666666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 45.49008498583569
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 353
episodes_total: 33273
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 50.91319990158081
time_total_s: 6418.230684757233
timers:
  learn_throughput: 462.563
  learn_time_ms: 35670.797
  load_throughput: 4435089.014
  load_time_ms: 3.72
  training_iteration_time_ms: 46946.575
  update_time_ms: 2.355
timesteps_total: 2310000
training_iteration: 140

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-172kuwc_58/checkpoint_000140/checkpoint-140
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8235294117647058
  reward for individual goal_min: 0.5
episode_len_mean: 75.95327102803738
episode_reward_max: 2.0
episode_reward_mean: 1.8317757009345794
episode_reward_min: 1.0
episodes_this_iter: 214
episodes_total: 22810
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8317757009345794
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.49032950401306
time_total_s: 6453.862315416336
timers:
  learn_throughput: 446.203
  learn_time_ms: 36978.722
  load_throughput: 4758193.144
  load_time_ms: 3.468
  training_iteration_time_ms: 48269.291
  update_time_ms: 2.635
timesteps_total: 2095500
training_iteration: 127

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2361111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9885057471264368
  reward for individual goal_min: 0.5
episode_len_mean: 171.47
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 9772
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.029059410095215
time_total_s: 6451.042912483215
timers:
  learn_throughput: 448.652
  learn_time_ms: 36776.808
  load_throughput: 4761630.648
  load_time_ms: 3.465
  training_iteration_time_ms: 48673.737
  update_time_ms: 2.572
timesteps_total: 1980000
training_iteration: 120

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9333333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 101.23333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.8333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.95
  agent_1: 0.8833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29577464788732394
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 156.99056603773585
episode_reward_max: 2.0
episode_reward_mean: 1.4339622641509433
episode_reward_min: 0.0
episodes_this_iter: 106
episodes_total: 10631
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7641509433962265
  agent_1: 0.6698113207547169
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.29468774795532
time_total_s: 6472.151116847992
timers:
  learn_throughput: 440.729
  learn_time_ms: 37437.972
  load_throughput: 4811085.111
  load_time_ms: 3.43
  training_iteration_time_ms: 49766.153
  update_time_ms: 2.633
timesteps_total: 1980000
training_iteration: 120

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9970414201183432
  reward for individual goal_min: 0.5
episode_len_mean: 47.214285714285715
episode_reward_max: 2.0
episode_reward_mean: 1.997142857142857
episode_reward_min: 1.0
episodes_this_iter: 350
episodes_total: 22666
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9971428571428571
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 59.73204064369202
time_total_s: 6460.446238994598
timers:
  learn_throughput: 377.943
  learn_time_ms: 43657.429
  load_throughput: 3829312.499
  load_time_ms: 4.309
  training_iteration_time_ms: 56880.075
  update_time_ms: 2.672
timesteps_total: 1947000
training_iteration: 118

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.797752808988764
  reward for individual goal_min: 0.5
episode_len_mean: 91.18435754189944
episode_reward_max: 2.0
episode_reward_mean: 1.7988826815642458
episode_reward_min: 1.0
episodes_this_iter: 179
episodes_total: 15625
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9888268156424581
  agent_1: 0.8100558659217877
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.75589036941528
time_total_s: 6488.445464849472
timers:
  learn_throughput: 369.83
  learn_time_ms: 44615.093
  load_throughput: 3978614.85
  load_time_ms: 4.147
  training_iteration_time_ms: 58467.369
  update_time_ms: 2.727
timesteps_total: 1782000
training_iteration: 108

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9782608695652174
  reward for individual goal_min: 0.0
episode_len_mean: 180.86
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 9824
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.228981494903564
time_total_s: 6472.149560451508
timers:
  learn_throughput: 406.905
  learn_time_ms: 40550.006
  load_throughput: 4408672.353
  load_time_ms: 3.743
  training_iteration_time_ms: 53372.785
  update_time_ms: 2.766
timesteps_total: 1864500
training_iteration: 113

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30666666666666664
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9788732394366197
  reward for individual goal_min: 0.0
episode_len_mean: 170.89
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 9837
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.61967444419861
time_total_s: 6479.457244634628
timers:
  learn_throughput: 410.875
  learn_time_ms: 40158.18
  load_throughput: 4363364.543
  load_time_ms: 3.781
  training_iteration_time_ms: 52977.227
  update_time_ms: 2.79
timesteps_total: 1881000
training_iteration: 114

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22435897435897437
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 187.21
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 10788
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.834322452545166
time_total_s: 6490.140862464905
timers:
  learn_throughput: 457.065
  learn_time_ms: 36099.87
  load_throughput: 4863832.676
  load_time_ms: 3.392
  training_iteration_time_ms: 47927.519
  update_time_ms: 2.607
timesteps_total: 2062500
training_iteration: 125

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9929577464788732
  reward for individual goal_min: 0.5
episode_len_mean: 182.3
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 12656
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.20021986961365
time_total_s: 6494.798439502716
timers:
  learn_throughput: 529.688
  learn_time_ms: 31150.395
  load_throughput: 5305134.955
  load_time_ms: 3.11
  training_iteration_time_ms: 41877.435
  update_time_ms: 2.453
timesteps_total: 2359500
training_iteration: 143

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3561643835616438
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7635135135135135
  reward for individual goal_min: 0.0
episode_len_mean: 194.1
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 10757
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.195740938186646
time_total_s: 6503.682460784912
timers:
  learn_throughput: 483.661
  learn_time_ms: 34114.785
  load_throughput: 4927062.743
  load_time_ms: 3.349
  training_iteration_time_ms: 45387.822
  update_time_ms: 2.441
timesteps_total: 2227500
training_iteration: 135

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3618421052631579
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 160.90384615384616
episode_reward_max: 2.0
episode_reward_mean: 1.4134615384615385
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 10830
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6634615384615384
  agent_1: 0.75
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.37413287162781
time_total_s: 6488.295116186142
timers:
  learn_throughput: 456.518
  learn_time_ms: 36143.141
  load_throughput: 4859427.031
  load_time_ms: 3.395
  training_iteration_time_ms: 47871.73
  update_time_ms: 2.615
timesteps_total: 2062500
training_iteration: 125

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.4015151515151515
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7560975609756098
  reward for individual goal_min: 0.0
episode_len_mean: 185.48
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 9866
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.76
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.561742544174194
time_total_s: 6512.181867361069
timers:
  learn_throughput: 430.072
  learn_time_ms: 38365.647
  load_throughput: 4606977.5
  load_time_ms: 3.582
  training_iteration_time_ms: 51022.949
  update_time_ms: 2.596
timesteps_total: 2029500
training_iteration: 123

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9941176470588236
  reward for individual goal_min: 0.5
episode_len_mean: 46.93314763231198
episode_reward_max: 2.0
episode_reward_mean: 1.9944289693593316
episode_reward_min: 1.0
episodes_this_iter: 359
episodes_total: 33632
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9944289693593314
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 48.13240337371826
time_total_s: 6466.363088130951
timers:
  learn_throughput: 461.205
  learn_time_ms: 35775.872
  load_throughput: 4434321.742
  load_time_ms: 3.721
  training_iteration_time_ms: 47066.767
  update_time_ms: 2.382
timesteps_total: 2326500
training_iteration: 141

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8451327433628318
  reward for individual goal_min: 0.5
episode_len_mean: 77.02777777777777
episode_reward_max: 2.0
episode_reward_mean: 1.837962962962963
episode_reward_min: 1.0
episodes_this_iter: 216
episodes_total: 23026
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8379629629629629
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.76649212837219
time_total_s: 6502.628807544708
timers:
  learn_throughput: 446.884
  learn_time_ms: 36922.355
  load_throughput: 4798375.905
  load_time_ms: 3.439
  training_iteration_time_ms: 48201.495
  update_time_ms: 2.644
timesteps_total: 2112000
training_iteration: 128

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 109.28333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.7333333333333334
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.95
  agent_1: 0.7833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22023809523809523
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.85
  reward for individual goal_min: 0.0
episode_len_mean: 204.67
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 11230
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.74293494224548
time_total_s: 6533.060586690903
timers:
  learn_throughput: 499.801
  learn_time_ms: 33013.11
  load_throughput: 4726541.183
  load_time_ms: 3.491
  training_iteration_time_ms: 44043.276
  update_time_ms: 2.608
timesteps_total: 2310000
training_iteration: 140

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29605263157894735
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9785714285714285
  reward for individual goal_min: 0.0
episode_len_mean: 173.84
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 9865
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.45137405395508
time_total_s: 6504.49428653717
timers:
  learn_throughput: 444.091
  learn_time_ms: 37154.559
  load_throughput: 4726702.592
  load_time_ms: 3.491
  training_iteration_time_ms: 49231.225
  update_time_ms: 2.546
timesteps_total: 1996500
training_iteration: 121

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9615384615384616
  reward for individual goal_min: 0.0
episode_len_mean: 177.36
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 10720
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.389341592788696
time_total_s: 6523.540458440781
timers:
  learn_throughput: 441.352
  learn_time_ms: 37385.103
  load_throughput: 4839581.538
  load_time_ms: 3.409
  training_iteration_time_ms: 49733.699
  update_time_ms: 2.615
timesteps_total: 1996500
training_iteration: 121

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24305555555555555
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.86
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 9916
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.38909864425659
time_total_s: 6523.538659095764
timers:
  learn_throughput: 408.708
  learn_time_ms: 40371.133
  load_throughput: 4434492.224
  load_time_ms: 3.721
  training_iteration_time_ms: 53208.618
  update_time_ms: 2.749
timesteps_total: 1881000
training_iteration: 114

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 47.15229885057471
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 348
episodes_total: 23014
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 60.3960223197937
time_total_s: 6520.842261314392
timers:
  learn_throughput: 375.466
  learn_time_ms: 43945.396
  load_throughput: 3786881.457
  load_time_ms: 4.357
  training_iteration_time_ms: 57177.292
  update_time_ms: 2.663
timesteps_total: 1963500
training_iteration: 119

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2905405405405405
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.53
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 10885
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.946335792541504
time_total_s: 6539.087198257446
timers:
  learn_throughput: 457.131
  learn_time_ms: 36094.719
  load_throughput: 4881432.138
  load_time_ms: 3.38
  training_iteration_time_ms: 47981.338
  update_time_ms: 2.604
timesteps_total: 2079000
training_iteration: 126

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9662162162162162
  reward for individual goal_min: 0.0
episode_len_mean: 173.81
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 9935
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.030553340911865
time_total_s: 6532.48779797554
timers:
  learn_throughput: 412.351
  learn_time_ms: 40014.409
  load_throughput: 4419031.856
  load_time_ms: 3.734
  training_iteration_time_ms: 52835.571
  update_time_ms: 2.802
timesteps_total: 1897500
training_iteration: 115

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8297872340425532
  reward for individual goal_min: 0.5
episode_len_mean: 82.79896907216495
episode_reward_max: 2.0
episode_reward_mean: 1.8350515463917525
episode_reward_min: 1.0
episodes_this_iter: 194
episodes_total: 15819
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9896907216494846
  agent_1: 0.845360824742268
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.14272737503052
time_total_s: 6547.588192224503
timers:
  learn_throughput: 369.548
  learn_time_ms: 44649.188
  load_throughput: 3890929.413
  load_time_ms: 4.241
  training_iteration_time_ms: 58471.516
  update_time_ms: 2.731
timesteps_total: 1798500
training_iteration: 109

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22435897435897437
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 185.5
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 12746
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.28411102294922
time_total_s: 6537.082550525665
timers:
  learn_throughput: 530.2
  learn_time_ms: 31120.36
  load_throughput: 5321165.634
  load_time_ms: 3.101
  training_iteration_time_ms: 41811.653
  update_time_ms: 2.45
timesteps_total: 2376000
training_iteration: 144

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3356164383561644
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7564102564102564
  reward for individual goal_min: 0.0
episode_len_mean: 197.46
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 10839
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.246609926223755
time_total_s: 6549.929070711136
timers:
  learn_throughput: 483.189
  learn_time_ms: 34148.131
  load_throughput: 4891230.193
  load_time_ms: 3.373
  training_iteration_time_ms: 45472.361
  update_time_ms: 2.411
timesteps_total: 2244000
training_iteration: 136

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.273972602739726
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 165.92
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 10929
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.10754418373108
time_total_s: 6536.402660369873
timers:
  learn_throughput: 455.979
  learn_time_ms: 36185.913
  load_throughput: 4825476.3
  load_time_ms: 3.419
  training_iteration_time_ms: 48016.525
  update_time_ms: 2.641
timesteps_total: 2079000
training_iteration: 126

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23376623376623376
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.879746835443038
  reward for individual goal_min: 0.0
episode_len_mean: 197.4
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 11312
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.78983497619629
time_total_s: 6575.850421667099
timers:
  learn_throughput: 500.893
  learn_time_ms: 32941.173
  load_throughput: 4710167.224
  load_time_ms: 3.503
  training_iteration_time_ms: 43956.404
  update_time_ms: 2.603
timesteps_total: 2326500
training_iteration: 141

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3227848101265823
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8141025641025641
  reward for individual goal_min: 0.0
episode_len_mean: 190.92
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 9952
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.72850275039673
time_total_s: 6562.910370111465
timers:
  learn_throughput: 429.294
  learn_time_ms: 38435.167
  load_throughput: 4580389.167
  load_time_ms: 3.602
  training_iteration_time_ms: 51118.178
  update_time_ms: 2.581
timesteps_total: 2046000
training_iteration: 124

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8080808080808081
  reward for individual goal_min: 0.5
episode_len_mean: 79.00956937799043
episode_reward_max: 2.0
episode_reward_mean: 1.8181818181818181
episode_reward_min: 1.0
episodes_this_iter: 209
episodes_total: 23235
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8229665071770335
  agent_1: 0.9952153110047847
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.13602018356323
time_total_s: 6550.7648277282715
timers:
  learn_throughput: 446.264
  learn_time_ms: 36973.64
  load_throughput: 4798109.765
  load_time_ms: 3.439
  training_iteration_time_ms: 48259.006
  update_time_ms: 2.638
timesteps_total: 2128500
training_iteration: 129

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 45.4478021978022
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 364
episodes_total: 33996
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.50746726989746
time_total_s: 6514.870555400848
timers:
  learn_throughput: 459.147
  learn_time_ms: 35936.175
  load_throughput: 4406286.435
  load_time_ms: 3.745
  training_iteration_time_ms: 47268.913
  update_time_ms: 2.399
timesteps_total: 2343000
training_iteration: 142

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24285714285714285
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.5
episode_len_mean: 169.5
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 9964
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.44063973426819
time_total_s: 6552.934926271439
timers:
  learn_throughput: 442.143
  learn_time_ms: 37318.21
  load_throughput: 4751887.613
  load_time_ms: 3.472
  training_iteration_time_ms: 49441.912
  update_time_ms: 2.544
timesteps_total: 2013000
training_iteration: 122

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1736111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.55
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 10814
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.77453804016113
time_total_s: 6576.314996480942
timers:
  learn_throughput: 438.82
  learn_time_ms: 37600.812
  load_throughput: 4819326.885
  load_time_ms: 3.424
  training_iteration_time_ms: 50003.13
  update_time_ms: 2.608
timesteps_total: 2013000
training_iteration: 122

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21951219512195122
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 190.82
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 12832
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.69876503944397
time_total_s: 6577.781315565109
timers:
  learn_throughput: 529.328
  learn_time_ms: 31171.598
  load_throughput: 5284838.414
  load_time_ms: 3.122
  training_iteration_time_ms: 41846.754
  update_time_ms: 2.445
timesteps_total: 2392500
training_iteration: 145

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2088607594936709
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 186.57
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 10008
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.083985567092896
time_total_s: 6575.622644662857
timers:
  learn_throughput: 409.047
  learn_time_ms: 40337.683
  load_throughput: 4504368.337
  load_time_ms: 3.663
  training_iteration_time_ms: 53122.816
  update_time_ms: 2.742
timesteps_total: 1897500
training_iteration: 115

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32432432432432434
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.48
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 10974
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.76
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.550978660583496
time_total_s: 6587.63817691803
timers:
  learn_throughput: 456.411
  learn_time_ms: 36151.651
  load_throughput: 4892302.08
  load_time_ms: 3.373
  training_iteration_time_ms: 48037.255
  update_time_ms: 2.612
timesteps_total: 2095500
training_iteration: 127

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2012987012987013
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 186.9
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 10023
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.09005522727966
time_total_s: 6584.57785320282
timers:
  learn_throughput: 409.589
  learn_time_ms: 40284.324
  load_throughput: 4431879.607
  load_time_ms: 3.723
  training_iteration_time_ms: 53098.463
  update_time_ms: 2.797
timesteps_total: 1914000
training_iteration: 116

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8116883116883117
  reward for individual goal_min: 0.0
episode_len_mean: 179.92
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 10930
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.798895835876465
time_total_s: 6595.727966547012
timers:
  learn_throughput: 483.889
  learn_time_ms: 34098.739
  load_throughput: 4908889.567
  load_time_ms: 3.361
  training_iteration_time_ms: 45443.059
  update_time_ms: 2.42
timesteps_total: 2260500
training_iteration: 137

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8675213675213675
  reward for individual goal_min: 0.5
episode_len_mean: 80.30331753554502
episode_reward_max: 2.0
episode_reward_mean: 1.853080568720379
episode_reward_min: 1.0
episodes_this_iter: 211
episodes_total: 16030
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.990521327014218
  agent_1: 0.8625592417061612
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.11661148071289
time_total_s: 6606.704803705215
timers:
  learn_throughput: 369.369
  learn_time_ms: 44670.832
  load_throughput: 3851133.036
  load_time_ms: 4.284
  training_iteration_time_ms: 58506.341
  update_time_ms: 2.762
timesteps_total: 1815000
training_iteration: 110

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.96666666666667
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27941176470588236
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.31
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 11028
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.02387809753418
time_total_s: 6585.426538467407
timers:
  learn_throughput: 453.869
  learn_time_ms: 36354.075
  load_throughput: 4818689.319
  load_time_ms: 3.424
  training_iteration_time_ms: 48238.325
  update_time_ms: 2.601
timesteps_total: 2095500
training_iteration: 127

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 47.48414985590778
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 347
episodes_total: 23361
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 64.6008517742157
time_total_s: 6585.443113088608
timers:
  learn_throughput: 373.474
  learn_time_ms: 44179.733
  load_throughput: 3766846.792
  load_time_ms: 4.38
  training_iteration_time_ms: 57449.04
  update_time_ms: 2.663
timesteps_total: 1980000
training_iteration: 120

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-184s1jdk42/checkpoint_000120/checkpoint-120
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17105263157894737
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8636363636363636
  reward for individual goal_min: 0.0
episode_len_mean: 202.16
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 11392
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.43
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.07755374908447
time_total_s: 6618.9279754161835
timers:
  learn_throughput: 499.806
  learn_time_ms: 33012.813
  load_throughput: 4746835.672
  load_time_ms: 3.476
  training_iteration_time_ms: 44050.603
  update_time_ms: 2.586
timesteps_total: 2343000
training_iteration: 142

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8362068965517241
  reward for individual goal_min: 0.5
episode_len_mean: 79.74761904761905
episode_reward_max: 2.0
episode_reward_mean: 1.819047619047619
episode_reward_min: 1.0
episodes_this_iter: 210
episodes_total: 23445
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.819047619047619
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.64081025123596
time_total_s: 6598.405637979507
timers:
  learn_throughput: 446.858
  learn_time_ms: 36924.517
  load_throughput: 4778628.957
  load_time_ms: 3.453
  training_iteration_time_ms: 48253.747
  update_time_ms: 2.626
timesteps_total: 2145000
training_iteration: 130

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971751412429378
  reward for individual goal_min: 0.5
episode_len_mean: 46.78632478632478
episode_reward_max: 2.0
episode_reward_mean: 1.997150997150997
episode_reward_min: 1.0
episodes_this_iter: 351
episodes_total: 34347
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9971509971509972
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.053832054138184
time_total_s: 6561.924387454987
timers:
  learn_throughput: 459.64
  learn_time_ms: 35897.648
  load_throughput: 4420669.047
  load_time_ms: 3.732
  training_iteration_time_ms: 47218.958
  update_time_ms: 2.43
timesteps_total: 2359500
training_iteration: 143

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3246753246753247
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7866666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 185.82
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 10044
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.62966251373291
time_total_s: 6615.540032625198
timers:
  learn_throughput: 427.73
  learn_time_ms: 38575.77
  load_throughput: 4555964.767
  load_time_ms: 3.622
  training_iteration_time_ms: 51343.527
  update_time_ms: 2.588
timesteps_total: 2062500
training_iteration: 125

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29012345679012347
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.31
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 10055
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.19828820228577
time_total_s: 6606.133214473724
timers:
  learn_throughput: 436.571
  learn_time_ms: 37794.521
  load_throughput: 4717616.311
  load_time_ms: 3.498
  training_iteration_time_ms: 50060.903
  update_time_ms: 2.558
timesteps_total: 2029500
training_iteration: 123

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3157894736842105
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.9
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 12927
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.54656386375427
time_total_s: 6619.3278794288635
timers:
  learn_throughput: 530.781
  learn_time_ms: 31086.251
  load_throughput: 5282256.824
  load_time_ms: 3.124
  training_iteration_time_ms: 41746.202
  update_time_ms: 2.418
timesteps_total: 2409000
training_iteration: 146

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.0
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 10906
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.34978127479553
time_total_s: 6631.664777755737
timers:
  learn_throughput: 432.392
  learn_time_ms: 38159.824
  load_throughput: 4861543.476
  load_time_ms: 3.394
  training_iteration_time_ms: 50569.018
  update_time_ms: 2.603
timesteps_total: 2029500
training_iteration: 123

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3108108108108108
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.6
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 11070
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.77
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.99851393699646
time_total_s: 6636.636690855026
timers:
  learn_throughput: 454.885
  learn_time_ms: 36272.917
  load_throughput: 4880709.193
  load_time_ms: 3.381
  training_iteration_time_ms: 48174.314
  update_time_ms: 2.626
timesteps_total: 2112000
training_iteration: 128

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2676056338028169
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 167.26
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 10105
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.921955823898315
time_total_s: 6627.544600486755
timers:
  learn_throughput: 410.953
  learn_time_ms: 40150.539
  load_throughput: 4536344.365
  load_time_ms: 3.637
  training_iteration_time_ms: 52872.169
  update_time_ms: 2.741
timesteps_total: 1914000
training_iteration: 116

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22535211267605634
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8614457831325302
  reward for individual goal_min: 0.0
episode_len_mean: 190.78
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 11016
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.25605797767639
time_total_s: 6638.984024524689
timers:
  learn_throughput: 486.113
  learn_time_ms: 33942.73
  load_throughput: 4950642.097
  load_time_ms: 3.333
  training_iteration_time_ms: 45261.612
  update_time_ms: 2.448
timesteps_total: 2277000
training_iteration: 138

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21428571428571427
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9814814814814815
  reward for individual goal_min: 0.0
episode_len_mean: 185.22
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 10114
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.750784397125244
time_total_s: 6635.328637599945
timers:
  learn_throughput: 410.953
  learn_time_ms: 40150.601
  load_throughput: 4417029.359
  load_time_ms: 3.736
  training_iteration_time_ms: 52932.344
  update_time_ms: 2.799
timesteps_total: 1930500
training_iteration: 117

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2012987012987013
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.855072463768116
  reward for individual goal_min: 0.0
episode_len_mean: 208.16
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 11472
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.37
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.780686140060425
time_total_s: 6660.708661556244
timers:
  learn_throughput: 501.628
  learn_time_ms: 32892.894
  load_throughput: 4734139.344
  load_time_ms: 3.485
  training_iteration_time_ms: 43903.77
  update_time_ms: 2.574
timesteps_total: 2359500
training_iteration: 143

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2894736842105263
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.32
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 11122
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.62199354171753
time_total_s: 6635.048532009125
timers:
  learn_throughput: 450.388
  learn_time_ms: 36635.095
  load_throughput: 4809914.791
  load_time_ms: 3.43
  training_iteration_time_ms: 48607.816
  update_time_ms: 2.61
timesteps_total: 2112000
training_iteration: 128

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8021978021978022
  reward for individual goal_min: 0.5
episode_len_mean: 87.9090909090909
episode_reward_max: 2.0
episode_reward_mean: 1.8074866310160427
episode_reward_min: 1.0
episodes_this_iter: 187
episodes_total: 16217
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9946524064171123
  agent_1: 0.8128342245989305
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.04265069961548
time_total_s: 6662.747454404831
timers:
  learn_throughput: 371.325
  learn_time_ms: 44435.485
  load_throughput: 4010362.06
  load_time_ms: 4.114
  training_iteration_time_ms: 58201.795
  update_time_ms: 2.785
timesteps_total: 1831500
training_iteration: 111

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 45.612637362637365
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 364
episodes_total: 34711
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.77007508277893
time_total_s: 6609.6944625377655
timers:
  learn_throughput: 458.806
  learn_time_ms: 35962.948
  load_throughput: 4432305.367
  load_time_ms: 3.723
  training_iteration_time_ms: 47301.908
  update_time_ms: 2.423
timesteps_total: 2376000
training_iteration: 144

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8760330578512396
  reward for individual goal_min: 0.5
episode_len_mean: 68.97008547008546
episode_reward_max: 2.0
episode_reward_mean: 1.8717948717948718
episode_reward_min: 1.0
episodes_this_iter: 234
episodes_total: 23679
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8717948717948718
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 49.005988121032715
time_total_s: 6647.41162610054
timers:
  learn_throughput: 445.915
  learn_time_ms: 37002.535
  load_throughput: 4766090.424
  load_time_ms: 3.462
  training_iteration_time_ms: 48346.232
  update_time_ms: 2.647
timesteps_total: 2161500
training_iteration: 131

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9942196531791907
  reward for individual goal_min: 0.5
episode_len_mean: 49.83783783783784
episode_reward_max: 2.0
episode_reward_mean: 1.993993993993994
episode_reward_min: 1.0
episodes_this_iter: 333
episodes_total: 23694
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.993993993993994
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 57.4180109500885
time_total_s: 6642.861124038696
timers:
  learn_throughput: 372.067
  learn_time_ms: 44346.816
  load_throughput: 3749499.713
  load_time_ms: 4.401
  training_iteration_time_ms: 57662.196
  update_time_ms: 2.641
timesteps_total: 1996500
training_iteration: 121

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24342105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8733333333333333
  reward for individual goal_min: 0.0
episode_len_mean: 199.4
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 10126
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.64343452453613
time_total_s: 6666.1834671497345
timers:
  learn_throughput: 429.208
  learn_time_ms: 38442.883
  load_throughput: 4488825.353
  load_time_ms: 3.676
  training_iteration_time_ms: 51151.388
  update_time_ms: 2.561
timesteps_total: 2079000
training_iteration: 126

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2945205479452055
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.36
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 13020
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.186009645462036
time_total_s: 6661.513889074326
timers:
  learn_throughput: 529.019
  learn_time_ms: 31189.818
  load_throughput: 5353063.899
  load_time_ms: 3.082
  training_iteration_time_ms: 41828.352
  update_time_ms: 2.421
timesteps_total: 2425500
training_iteration: 147

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32051282051282054
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 167.72
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 10152
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.902085304260254
time_total_s: 6657.035299777985
timers:
  learn_throughput: 436.986
  learn_time_ms: 37758.606
  load_throughput: 4668354.143
  load_time_ms: 3.534
  training_iteration_time_ms: 50146.45
  update_time_ms: 2.594
timesteps_total: 2046000
training_iteration: 124

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34210526315789475
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9797297297297297
  reward for individual goal_min: 0.0
episode_len_mean: 161.98039215686273
episode_reward_max: 2.0
episode_reward_mean: 1.392156862745098
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 11008
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7254901960784313
  agent_1: 0.6666666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.49356770515442
time_total_s: 6682.158345460892
timers:
  learn_throughput: 431.008
  learn_time_ms: 38282.356
  load_throughput: 4892578.773
  load_time_ms: 3.372
  training_iteration_time_ms: 50764.303
  update_time_ms: 2.596
timesteps_total: 2046000
training_iteration: 124

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8309859154929577
  reward for individual goal_min: 0.0
episode_len_mean: 203.63
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 11096
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.47503662109375
time_total_s: 6684.4590611457825
timers:
  learn_throughput: 490.084
  learn_time_ms: 33667.714
  load_throughput: 5007707.436
  load_time_ms: 3.295
  training_iteration_time_ms: 44923.496
  update_time_ms: 2.454
timesteps_total: 2293500
training_iteration: 139

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20253164556962025
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 188.27
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 11160
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.722095012664795
time_total_s: 6686.358785867691
timers:
  learn_throughput: 452.847
  learn_time_ms: 36436.112
  load_throughput: 4841511.372
  load_time_ms: 3.408
  training_iteration_time_ms: 48431.982
  update_time_ms: 2.645
timesteps_total: 2128500
training_iteration: 129

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16891891891891891
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9883720930232558
  reward for individual goal_min: 0.0
episode_len_mean: 177.65
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 10198
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.66593360900879
time_total_s: 6677.210534095764
timers:
  learn_throughput: 412.96
  learn_time_ms: 39955.491
  load_throughput: 4542746.416
  load_time_ms: 3.632
  training_iteration_time_ms: 52519.952
  update_time_ms: 2.731
timesteps_total: 1930500
training_iteration: 117

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24324324324324326
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8376623376623377
  reward for individual goal_min: 0.0
episode_len_mean: 197.25
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 11558
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.681989431381226
time_total_s: 6702.390650987625
timers:
  learn_throughput: 505.113
  learn_time_ms: 32665.979
  load_throughput: 4759633.017
  load_time_ms: 3.467
  training_iteration_time_ms: 43655.091
  update_time_ms: 2.575
timesteps_total: 2376000
training_iteration: 144

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2808219178082192
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 167.89
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 10210
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.886998653411865
time_total_s: 6687.215636253357
timers:
  learn_throughput: 411.417
  learn_time_ms: 40105.338
  load_throughput: 4425304.917
  load_time_ms: 3.729
  training_iteration_time_ms: 52809.242
  update_time_ms: 2.795
timesteps_total: 1947000
training_iteration: 118

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24342105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9941860465116279
  reward for individual goal_min: 0.5
episode_len_mean: 175.64
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 11217
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.13790249824524
time_total_s: 6683.18643450737
timers:
  learn_throughput: 450.684
  learn_time_ms: 36611.057
  load_throughput: 4736763.949
  load_time_ms: 3.483
  training_iteration_time_ms: 48627.736
  update_time_ms: 2.62
timesteps_total: 2128500
training_iteration: 129

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.3254593175853
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 381
episodes_total: 35092
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.33722639083862
time_total_s: 6657.031688928604
timers:
  learn_throughput: 459.086
  learn_time_ms: 35941.004
  load_throughput: 4421714.096
  load_time_ms: 3.732
  training_iteration_time_ms: 47251.101
  update_time_ms: 2.42
timesteps_total: 2392500
training_iteration: 145

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8119266055045872
  reward for individual goal_min: 0.5
episode_len_mean: 83.46534653465346
episode_reward_max: 2.0
episode_reward_mean: 1.797029702970297
episode_reward_min: 1.0
episodes_this_iter: 202
episodes_total: 23881
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.801980198019802
  agent_1: 0.995049504950495
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.60301089286804
time_total_s: 6696.014636993408
timers:
  learn_throughput: 445.388
  learn_time_ms: 37046.369
  load_throughput: 4687833.42
  load_time_ms: 3.52
  training_iteration_time_ms: 48394.984
  update_time_ms: 2.636
timesteps_total: 2178000
training_iteration: 132

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2945205479452055
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 168.78
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 13118
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.88701868057251
time_total_s: 6704.400907754898
timers:
  learn_throughput: 525.413
  learn_time_ms: 31403.887
  load_throughput: 5322761.75
  load_time_ms: 3.1
  training_iteration_time_ms: 42085.617
  update_time_ms: 2.429
timesteps_total: 2442000
training_iteration: 148

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8225806451612904
  reward for individual goal_min: 0.5
episode_len_mean: 81.60696517412936
episode_reward_max: 2.0
episode_reward_mean: 1.835820895522388
episode_reward_min: 1.0
episodes_this_iter: 201
episodes_total: 16418
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.835820895522388
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 59.3189332485199
time_total_s: 6722.066387653351
timers:
  learn_throughput: 370.58
  learn_time_ms: 44524.845
  load_throughput: 4037054.606
  load_time_ms: 4.087
  training_iteration_time_ms: 58280.706
  update_time_ms: 2.796
timesteps_total: 1848000
training_iteration: 112

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8357142857142857
  reward for individual goal_min: 0.0
episode_len_mean: 182.97
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 10218
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.78158736228943
time_total_s: 6717.965054512024
timers:
  learn_throughput: 428.833
  learn_time_ms: 38476.546
  load_throughput: 4510210.045
  load_time_ms: 3.658
  training_iteration_time_ms: 51199.761
  update_time_ms: 2.584
timesteps_total: 2095500
training_iteration: 127

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973118279569892
  reward for individual goal_min: 0.5
episode_len_mean: 47.6242774566474
episode_reward_max: 2.0
episode_reward_mean: 1.9971098265895955
episode_reward_min: 1.0
episodes_this_iter: 346
episodes_total: 24040
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9971098265895953
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 58.02050590515137
time_total_s: 6700.881629943848
timers:
  learn_throughput: 370.338
  learn_time_ms: 44553.871
  load_throughput: 3726824.666
  load_time_ms: 4.427
  training_iteration_time_ms: 57874.083
  update_time_ms: 2.618
timesteps_total: 2013000
training_iteration: 122

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26785714285714285
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 185.16
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 10238
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.930267095565796
time_total_s: 6706.96556687355
timers:
  learn_throughput: 438.994
  learn_time_ms: 37585.916
  load_throughput: 4628267.159
  load_time_ms: 3.565
  training_iteration_time_ms: 50014.159
  update_time_ms: 2.581
timesteps_total: 2062500
training_iteration: 125

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22972972972972974
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8271604938271605
  reward for individual goal_min: 0.0
episode_len_mean: 190.19
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 11645
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.299065589904785
time_total_s: 6743.68971657753
timers:
  learn_throughput: 507.459
  learn_time_ms: 32514.973
  load_throughput: 4777408.412
  load_time_ms: 3.454
  training_iteration_time_ms: 43443.28
  update_time_ms: 2.56
timesteps_total: 2392500
training_iteration: 145

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23648648648648649
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.32
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 11102
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.52872180938721
time_total_s: 6730.687067270279
timers:
  learn_throughput: 433.107
  learn_time_ms: 38096.778
  load_throughput: 4870816.072
  load_time_ms: 3.388
  training_iteration_time_ms: 50581.637
  update_time_ms: 2.622
timesteps_total: 2062500
training_iteration: 125

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3026315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 174.72
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 11255
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.01635718345642
time_total_s: 6735.3751430511475
timers:
  learn_throughput: 451.044
  learn_time_ms: 36581.775
  load_throughput: 4843781.741
  load_time_ms: 3.406
  training_iteration_time_ms: 48627.48
  update_time_ms: 2.67
timesteps_total: 2145000
training_iteration: 130

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3170731707317073
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.06
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 10287
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.56677031517029
time_total_s: 6727.777304410934
timers:
  learn_throughput: 414.521
  learn_time_ms: 39804.953
  load_throughput: 4509475.33
  load_time_ms: 3.659
  training_iteration_time_ms: 52407.371
  update_time_ms: 2.745
timesteps_total: 1947000
training_iteration: 118

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8
  reward for individual goal_min: 0.5
episode_len_mean: 119.28333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.7
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.85
  agent_1: 0.85
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23880597014925373
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8055555555555556
  reward for individual goal_min: 0.0
episode_len_mean: 202.04
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 11179
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.00749135017395
time_total_s: 6740.466552495956
timers:
  learn_throughput: 490.661
  learn_time_ms: 33628.124
  load_throughput: 5072936.623
  load_time_ms: 3.253
  training_iteration_time_ms: 44840.365
  update_time_ms: 2.451
timesteps_total: 2310000
training_iteration: 140

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19istq_qwi/checkpoint_000140/checkpoint-140
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 190.66
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 10295
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.44004273414612
time_total_s: 6736.655678987503
timers:
  learn_throughput: 412.822
  learn_time_ms: 39968.776
  load_throughput: 4400738.649
  load_time_ms: 3.749
  training_iteration_time_ms: 52706.922
  update_time_ms: 2.765
timesteps_total: 1963500
training_iteration: 119

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.12
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 11309
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.67723250389099
time_total_s: 6732.863667011261
timers:
  learn_throughput: 450.173
  learn_time_ms: 36652.613
  load_throughput: 4724863.182
  load_time_ms: 3.492
  training_iteration_time_ms: 48647.261
  update_time_ms: 2.622
timesteps_total: 2145000
training_iteration: 130

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3116883116883117
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 169.83
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 13216
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.523526191711426
time_total_s: 6744.9244339466095
timers:
  learn_throughput: 526.825
  learn_time_ms: 31319.722
  load_throughput: 5275773.649
  load_time_ms: 3.128
  training_iteration_time_ms: 41949.007
  update_time_ms: 2.41
timesteps_total: 2458500
training_iteration: 149

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.480211081794195
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 379
episodes_total: 35471
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.631601095199585
time_total_s: 6704.663290023804
timers:
  learn_throughput: 458.52
  learn_time_ms: 35985.382
  load_throughput: 4438787.008
  load_time_ms: 3.717
  training_iteration_time_ms: 47306.06
  update_time_ms: 2.433
timesteps_total: 2409000
training_iteration: 146

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8362831858407079
  reward for individual goal_min: 0.5
episode_len_mean: 75.97235023041475
episode_reward_max: 2.0
episode_reward_mean: 1.8294930875576036
episode_reward_min: 1.0
episodes_this_iter: 217
episodes_total: 24098
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8294930875576036
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.1229202747345
time_total_s: 6744.137557268143
timers:
  learn_throughput: 447.068
  learn_time_ms: 36907.148
  load_throughput: 4693301.505
  load_time_ms: 3.516
  training_iteration_time_ms: 48250.98
  update_time_ms: 2.58
timesteps_total: 2194500
training_iteration: 133

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29605263157894735
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.86
  reward for individual goal_min: 0.0
episode_len_mean: 190.67
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 10303
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.62824511528015
time_total_s: 6766.593299627304
timers:
  learn_throughput: 430.223
  learn_time_ms: 38352.169
  load_throughput: 4532214.96
  load_time_ms: 3.641
  training_iteration_time_ms: 51027.171
  update_time_ms: 2.574
timesteps_total: 2112000
training_iteration: 128

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31756756756756754
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 169.98
episode_reward_max: 2.0
episode_reward_mean: 1.41
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 10335
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.73484802246094
time_total_s: 6754.700414896011
timers:
  learn_throughput: 439.676
  learn_time_ms: 37527.649
  load_throughput: 4649442.115
  load_time_ms: 3.549
  training_iteration_time_ms: 49985.698
  update_time_ms: 2.585
timesteps_total: 2079000
training_iteration: 126

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8061224489795918
  reward for individual goal_min: 0.5
episode_len_mean: 92.35555555555555
episode_reward_max: 2.0
episode_reward_mean: 1.788888888888889
episode_reward_min: 1.0
episodes_this_iter: 180
episodes_total: 16598
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.7888888888888889
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 57.5875461101532
time_total_s: 6779.653933763504
timers:
  learn_throughput: 369.413
  learn_time_ms: 44665.429
  load_throughput: 3986269.073
  load_time_ms: 4.139
  training_iteration_time_ms: 58394.767
  update_time_ms: 2.886
timesteps_total: 1864500
training_iteration: 113

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3356164383561644
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.815068493150685
  reward for individual goal_min: 0.0
episode_len_mean: 186.42
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 11734
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.0937385559082
time_total_s: 6786.783455133438
timers:
  learn_throughput: 508.912
  learn_time_ms: 32422.1
  load_throughput: 4789608.836
  load_time_ms: 3.445
  training_iteration_time_ms: 43294.697
  update_time_ms: 2.556
timesteps_total: 2409000
training_iteration: 146

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2631578947368421
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 179.76
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 11194
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.4271285533905
time_total_s: 6780.114195823669
timers:
  learn_throughput: 435.047
  learn_time_ms: 37926.916
  load_throughput: 4856698.855
  load_time_ms: 3.397
  training_iteration_time_ms: 50456.554
  update_time_ms: 2.615
timesteps_total: 2079000
training_iteration: 126

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971590909090909
  reward for individual goal_min: 0.5
episode_len_mean: 49.038690476190474
episode_reward_max: 2.0
episode_reward_mean: 1.9970238095238095
episode_reward_min: 1.0
episodes_this_iter: 336
episodes_total: 24376
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9970238095238095
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 58.48718214035034
time_total_s: 6759.368812084198
timers:
  learn_throughput: 369.297
  learn_time_ms: 44679.479
  load_throughput: 3711295.73
  load_time_ms: 4.446
  training_iteration_time_ms: 58023.093
  update_time_ms: 2.652
timesteps_total: 2029500
training_iteration: 123

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3235294117647059
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7905405405405406
  reward for individual goal_min: 0.0
episode_len_mean: 184.16
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 11267
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.174758195877075
time_total_s: 6784.6413106918335
timers:
  learn_throughput: 492.155
  learn_time_ms: 33526.03
  load_throughput: 5120719.798
  load_time_ms: 3.222
  training_iteration_time_ms: 44655.071
  update_time_ms: 2.453
timesteps_total: 2326500
training_iteration: 141

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 166.96
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 11351
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.56993222236633
time_total_s: 6784.945075273514
timers:
  learn_throughput: 448.251
  learn_time_ms: 36809.759
  load_throughput: 4823458.37
  load_time_ms: 3.421
  training_iteration_time_ms: 48955.721
  update_time_ms: 2.679
timesteps_total: 2161500
training_iteration: 131

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25925925925925924
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 184.58
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 10379
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.40756916999817
time_total_s: 6779.184873580933
timers:
  learn_throughput: 416.359
  learn_time_ms: 39629.238
  load_throughput: 4523032.521
  load_time_ms: 3.648
  training_iteration_time_ms: 52142.406
  update_time_ms: 2.718
timesteps_total: 1963500
training_iteration: 119

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2602739726027397
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.0
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 13312
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.20036745071411
time_total_s: 6786.124801397324
timers:
  learn_throughput: 527.914
  learn_time_ms: 31255.083
  load_throughput: 5290454.007
  load_time_ms: 3.119
  training_iteration_time_ms: 41938.838
  update_time_ms: 2.426
timesteps_total: 2475000
training_iteration: 150

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3116883116883117
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.69
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 11406
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.38387894630432
time_total_s: 6783.247545957565
timers:
  learn_throughput: 448.767
  learn_time_ms: 36767.425
  load_throughput: 4730709.066
  load_time_ms: 3.488
  training_iteration_time_ms: 48785.255
  update_time_ms: 2.625
timesteps_total: 2161500
training_iteration: 131

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 74.41666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.85
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9166666666666666
  agent_1: 0.9333333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24305555555555555
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.975
  reward for individual goal_min: 0.0
episode_len_mean: 173.26
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 10391
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.236093521118164
time_total_s: 6796.891772508621
timers:
  learn_throughput: 414.033
  learn_time_ms: 39851.918
  load_throughput: 4387319.467
  load_time_ms: 3.761
  training_iteration_time_ms: 52535.422
  update_time_ms: 2.719
timesteps_total: 1980000
training_iteration: 120

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971428571428571
  reward for individual goal_min: 0.5
episode_len_mean: 44.83468834688347
episode_reward_max: 2.0
episode_reward_mean: 1.997289972899729
episode_reward_min: 1.0
episodes_this_iter: 369
episodes_total: 35840
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.997289972899729
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 47.56320762634277
time_total_s: 6752.2264976501465
timers:
  learn_throughput: 457.372
  learn_time_ms: 36075.64
  load_throughput: 4415592.065
  load_time_ms: 3.737
  training_iteration_time_ms: 47415.866
  update_time_ms: 2.464
timesteps_total: 2425500
training_iteration: 147

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8382352941176471
  reward for individual goal_min: 0.5
episode_len_mean: 70.27272727272727
episode_reward_max: 2.0
episode_reward_mean: 1.8571428571428572
episode_reward_min: 1.0
episodes_this_iter: 231
episodes_total: 24329
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8571428571428571
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 49.30948615074158
time_total_s: 6793.447043418884
timers:
  learn_throughput: 445.534
  learn_time_ms: 37034.175
  load_throughput: 4705171.567
  load_time_ms: 3.507
  training_iteration_time_ms: 48416.448
  update_time_ms: 2.594
timesteps_total: 2211000
training_iteration: 134

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24285714285714285
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8375
  reward for individual goal_min: 0.0
episode_len_mean: 186.66
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 11821
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.07073783874512
time_total_s: 6828.854192972183
timers:
  learn_throughput: 512.253
  learn_time_ms: 32210.662
  load_throughput: 4807141.736
  load_time_ms: 3.432
  training_iteration_time_ms: 43033.73
  update_time_ms: 2.56
timesteps_total: 2425500
training_iteration: 147

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3092105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7763157894736842
  reward for individual goal_min: 0.0
episode_len_mean: 191.45
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 10388
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.62725067138672
time_total_s: 6817.220550298691
timers:
  learn_throughput: 431.879
  learn_time_ms: 38205.177
  load_throughput: 4544804.86
  load_time_ms: 3.631
  training_iteration_time_ms: 50893.259
  update_time_ms: 2.567
timesteps_total: 2128500
training_iteration: 129

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18421052631578946
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 186.38
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 10422
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.67680263519287
time_total_s: 6804.377217531204
timers:
  learn_throughput: 438.63
  learn_time_ms: 37617.162
  load_throughput: 4677156.644
  load_time_ms: 3.528
  training_iteration_time_ms: 50012.653
  update_time_ms: 2.591
timesteps_total: 2095500
training_iteration: 127

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30666666666666664
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.726027397260274
  reward for individual goal_min: 0.0
episode_len_mean: 205.42
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 11347
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.81452560424805
time_total_s: 6829.4558362960815
timers:
  learn_throughput: 493.306
  learn_time_ms: 33447.816
  load_throughput: 5139429.514
  load_time_ms: 3.21
  training_iteration_time_ms: 44592.779
  update_time_ms: 2.441
timesteps_total: 2343000
training_iteration: 142

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8465346534653465
  reward for individual goal_min: 0.5
episode_len_mean: 80.96019900497512
episode_reward_max: 2.0
episode_reward_mean: 1.845771144278607
episode_reward_min: 1.0
episodes_this_iter: 201
episodes_total: 16799
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9950248756218906
  agent_1: 0.8507462686567164
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.039814472198486
time_total_s: 6839.6937482357025
timers:
  learn_throughput: 369.649
  learn_time_ms: 44636.999
  load_throughput: 4028711.739
  load_time_ms: 4.096
  training_iteration_time_ms: 58403.346
  update_time_ms: 2.932
timesteps_total: 1881000
training_iteration: 114

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2635135135135135
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9652777777777778
  reward for individual goal_min: 0.0
episode_len_mean: 175.13
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 11291
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.2704656124115
time_total_s: 6834.384661436081
timers:
  learn_throughput: 428.842
  learn_time_ms: 38475.685
  load_throughput: 4830730.829
  load_time_ms: 3.416
  training_iteration_time_ms: 51033.746
  update_time_ms: 2.6
timesteps_total: 2095500
training_iteration: 127

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3918918918918919
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 167.04
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 11444
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.53788185119629
time_total_s: 6835.48295712471
timers:
  learn_throughput: 447.095
  learn_time_ms: 36904.872
  load_throughput: 4795815.53
  load_time_ms: 3.44
  training_iteration_time_ms: 49119.153
  update_time_ms: 2.686
timesteps_total: 2178000
training_iteration: 132

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2708333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 158.7766990291262
episode_reward_max: 2.0
episode_reward_mean: 1.3883495145631068
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 13415
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7184466019417476
  agent_1: 0.6699029126213593
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.442975759506226
time_total_s: 6828.56777715683
timers:
  learn_throughput: 529.743
  learn_time_ms: 31147.208
  load_throughput: 5234789.87
  load_time_ms: 3.152
  training_iteration_time_ms: 41799.326
  update_time_ms: 2.465
timesteps_total: 2491500
training_iteration: 151

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 47.15669515669516
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 351
episodes_total: 24727
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.639914751052856
time_total_s: 6818.008726835251
timers:
  learn_throughput: 365.657
  learn_time_ms: 45124.3
  load_throughput: 3712111.912
  load_time_ms: 4.445
  training_iteration_time_ms: 58598.531
  update_time_ms: 2.648
timesteps_total: 2046000
training_iteration: 124

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 68.75
episode_reward_max: 2.0
episode_reward_mean: 1.9666666666666666
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9666666666666667
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18840579710144928
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9819277108433735
  reward for individual goal_min: 0.0
episode_len_mean: 173.88
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 10474
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.36396932601929
time_total_s: 6838.548842906952
timers:
  learn_throughput: 419.893
  learn_time_ms: 39295.684
  load_throughput: 4504896.11
  load_time_ms: 3.663
  training_iteration_time_ms: 51709.307
  update_time_ms: 2.716
timesteps_total: 1980000
training_iteration: 120

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3115942028985507
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 159.51960784313727
episode_reward_max: 2.0
episode_reward_mean: 1.3823529411764706
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 11508
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6666666666666666
  agent_1: 0.7156862745098039
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.366395235061646
time_total_s: 6835.613941192627
timers:
  learn_throughput: 443.428
  learn_time_ms: 37210.131
  load_throughput: 4724605.134
  load_time_ms: 3.492
  training_iteration_time_ms: 49339.764
  update_time_ms: 2.654
timesteps_total: 2178000
training_iteration: 132

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.11764705882353
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 374
episodes_total: 36214
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.2630980014801
time_total_s: 6800.489595651627
timers:
  learn_throughput: 456.037
  learn_time_ms: 36181.25
  load_throughput: 4423268.46
  load_time_ms: 3.73
  training_iteration_time_ms: 47563.078
  update_time_ms: 2.467
timesteps_total: 2442000
training_iteration: 148

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24324324324324326
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9746835443037974
  reward for individual goal_min: 0.0
episode_len_mean: 172.99
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 10487
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.51277732849121
time_total_s: 6848.404549837112
timers:
  learn_throughput: 417.431
  learn_time_ms: 39527.532
  load_throughput: 4379794.952
  load_time_ms: 3.767
  training_iteration_time_ms: 52149.229
  update_time_ms: 2.707
timesteps_total: 1996500
training_iteration: 121

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8558558558558559
  reward for individual goal_min: 0.5
episode_len_mean: 73.20264317180617
episode_reward_max: 2.0
episode_reward_mean: 1.8590308370044053
episode_reward_min: 1.0
episodes_this_iter: 227
episodes_total: 24556
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8634361233480177
  agent_1: 0.9955947136563876
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.70792579650879
time_total_s: 6843.154969215393
timers:
  learn_throughput: 444.907
  learn_time_ms: 37086.434
  load_throughput: 4680351.402
  load_time_ms: 3.525
  training_iteration_time_ms: 48555.677
  update_time_ms: 2.596
timesteps_total: 2227500
training_iteration: 135

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31756756756756754
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8529411764705882
  reward for individual goal_min: 0.0
episode_len_mean: 184.61
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 11906
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.06216764450073
time_total_s: 6873.916360616684
timers:
  learn_throughput: 512.78
  learn_time_ms: 32177.532
  load_throughput: 4793357.483
  load_time_ms: 3.442
  training_iteration_time_ms: 42946.366
  update_time_ms: 2.508
timesteps_total: 2442000
training_iteration: 148

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3525641025641026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.78
  reward for individual goal_min: 0.0
episode_len_mean: 193.8
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 10470
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.84670948982239
time_total_s: 6866.067259788513
timers:
  learn_throughput: 434.397
  learn_time_ms: 37983.675
  load_throughput: 4573397.038
  load_time_ms: 3.608
  training_iteration_time_ms: 50608.8
  update_time_ms: 2.555
timesteps_total: 2145000
training_iteration: 130

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 170.08
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 10519
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.472455739974976
time_total_s: 6855.849673271179
timers:
  learn_throughput: 438.102
  learn_time_ms: 37662.499
  load_throughput: 4649036.081
  load_time_ms: 3.549
  training_iteration_time_ms: 50074.908
  update_time_ms: 2.581
timesteps_total: 2112000
training_iteration: 128

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.38311688311688313
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.75
  reward for individual goal_min: 0.0
episode_len_mean: 199.14
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 11431
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.710041761398315
time_total_s: 6875.16587805748
timers:
  learn_throughput: 489.597
  learn_time_ms: 33701.219
  load_throughput: 5153129.658
  load_time_ms: 3.202
  training_iteration_time_ms: 44878.747
  update_time_ms: 2.429
timesteps_total: 2359500
training_iteration: 143

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23717948717948717
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.86
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 13507
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 39.36459231376648
time_total_s: 6867.932369470596
timers:
  learn_throughput: 533.446
  learn_time_ms: 30930.95
  load_throughput: 5260174.817
  load_time_ms: 3.137
  training_iteration_time_ms: 41497.054
  update_time_ms: 2.453
timesteps_total: 2508000
training_iteration: 152

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28846153846153844
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9855072463768116
  reward for individual goal_min: 0.0
episode_len_mean: 181.08
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 11534
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.264918088912964
time_total_s: 6884.747875213623
timers:
  learn_throughput: 445.684
  learn_time_ms: 37021.707
  load_throughput: 4811921.403
  load_time_ms: 3.429
  training_iteration_time_ms: 49227.462
  update_time_ms: 2.672
timesteps_total: 2194500
training_iteration: 133

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2462686567164179
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 165.68
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 11390
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.58141040802002
time_total_s: 6886.966071844101
timers:
  learn_throughput: 424.421
  learn_time_ms: 38876.521
  load_throughput: 4828809.579
  load_time_ms: 3.417
  training_iteration_time_ms: 51446.369
  update_time_ms: 2.59
timesteps_total: 2112000
training_iteration: 128

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8137254901960784
  reward for individual goal_min: 0.5
episode_len_mean: 89.22105263157894
episode_reward_max: 2.0
episode_reward_mean: 1.8
episode_reward_min: 1.0
episodes_this_iter: 190
episodes_total: 16989
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9894736842105263
  agent_1: 0.8105263157894737
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.991613149642944
time_total_s: 6897.6853613853455
timers:
  learn_throughput: 370.508
  learn_time_ms: 44533.425
  load_throughput: 3970465.973
  load_time_ms: 4.156
  training_iteration_time_ms: 58299.128
  update_time_ms: 2.895
timesteps_total: 1897500
training_iteration: 115

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.68079096045198
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 354
episodes_total: 25081
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.88984537124634
time_total_s: 6876.898572206497
timers:
  learn_throughput: 365.149
  learn_time_ms: 45187.038
  load_throughput: 3678842.435
  load_time_ms: 4.485
  training_iteration_time_ms: 58708.675
  update_time_ms: 2.638
timesteps_total: 2062500
training_iteration: 125

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23076923076923078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9710144927536232
  reward for individual goal_min: 0.0
episode_len_mean: 187.62
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 10561
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.32233905792236
time_total_s: 6890.871181964874
timers:
  learn_throughput: 420.971
  learn_time_ms: 39195.109
  load_throughput: 4549136.659
  load_time_ms: 3.627
  training_iteration_time_ms: 51578.236
  update_time_ms: 2.726
timesteps_total: 1996500
training_iteration: 121

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9946808510638298
  reward for individual goal_min: 0.5
episode_len_mean: 46.14325842696629
episode_reward_max: 2.0
episode_reward_mean: 1.9943820224719102
episode_reward_min: 1.0
episodes_this_iter: 356
episodes_total: 36570
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9971910112359551
  agent_1: 0.9971910112359551
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.88481640815735
time_total_s: 6847.374412059784
timers:
  learn_throughput: 456.82
  learn_time_ms: 36119.233
  load_throughput: 4417621.458
  load_time_ms: 3.735
  training_iteration_time_ms: 47487.708
  update_time_ms: 2.468
timesteps_total: 2458500
training_iteration: 149

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26973684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8169014084507042
  reward for individual goal_min: 0.0
episode_len_mean: 189.81
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 11998
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.15497398376465
time_total_s: 6917.071334600449
timers:
  learn_throughput: 514.957
  learn_time_ms: 32041.524
  load_throughput: 4762646.48
  load_time_ms: 3.464
  training_iteration_time_ms: 42801.75
  update_time_ms: 2.506
timesteps_total: 2458500
training_iteration: 149

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3148148148148148
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.64
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 11605
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.09836769104004
time_total_s: 6886.712308883667
timers:
  learn_throughput: 440.495
  learn_time_ms: 37457.899
  load_throughput: 4775166.875
  load_time_ms: 3.455
  training_iteration_time_ms: 49686.736
  update_time_ms: 2.649
timesteps_total: 2194500
training_iteration: 133

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1956521739130435
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.974025974025974
  reward for individual goal_min: 0.0
episode_len_mean: 170.29
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 10580
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.06615161895752
time_total_s: 6899.47070145607
timers:
  learn_throughput: 419.26
  learn_time_ms: 39355.04
  load_throughput: 4453411.583
  load_time_ms: 3.705
  training_iteration_time_ms: 51954.086
  update_time_ms: 2.743
timesteps_total: 2013000
training_iteration: 122

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8285714285714286
  reward for individual goal_min: 0.5
episode_len_mean: 77.0324074074074
episode_reward_max: 2.0
episode_reward_mean: 1.8333333333333333
episode_reward_min: 1.0
episodes_this_iter: 216
episodes_total: 24772
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8333333333333334
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.99079632759094
time_total_s: 6892.145765542984
timers:
  learn_throughput: 444.356
  learn_time_ms: 37132.413
  load_throughput: 4681111.194
  load_time_ms: 3.525
  training_iteration_time_ms: 48597.662
  update_time_ms: 2.582
timesteps_total: 2244000
training_iteration: 136

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30714285714285716
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7671232876712328
  reward for individual goal_min: 0.0
episode_len_mean: 185.03
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 10561
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.23425364494324
time_total_s: 6918.301513433456
timers:
  learn_throughput: 433.563
  learn_time_ms: 38056.748
  load_throughput: 4619030.889
  load_time_ms: 3.572
  training_iteration_time_ms: 50712.288
  update_time_ms: 2.554
timesteps_total: 2161500
training_iteration: 131

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 166.78
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 13606
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.19383955001831
time_total_s: 6910.126209020615
timers:
  learn_throughput: 532.917
  learn_time_ms: 30961.692
  load_throughput: 5270510.251
  load_time_ms: 3.131
  training_iteration_time_ms: 41496.689
  update_time_ms: 2.446
timesteps_total: 2524500
training_iteration: 153

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3269230769230769
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8
  reward for individual goal_min: 0.0
episode_len_mean: 195.05
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 11518
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.80552363395691
time_total_s: 6918.971401691437
timers:
  learn_throughput: 490.841
  learn_time_ms: 33615.78
  load_throughput: 5113984.349
  load_time_ms: 3.226
  training_iteration_time_ms: 44814.545
  update_time_ms: 2.448
timesteps_total: 2376000
training_iteration: 144

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2323943661971831
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9857142857142858
  reward for individual goal_min: 0.5
episode_len_mean: 179.8
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 10609
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.171239376068115
time_total_s: 6908.020912647247
timers:
  learn_throughput: 435.327
  learn_time_ms: 37902.572
  load_throughput: 4632759.599
  load_time_ms: 3.562
  training_iteration_time_ms: 50428.927
  update_time_ms: 2.579
timesteps_total: 2128500
training_iteration: 129

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2469879518072289
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9928571428571429
  reward for individual goal_min: 0.5
episode_len_mean: 196.96
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 11619
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.28338956832886
time_total_s: 6932.031264781952
timers:
  learn_throughput: 447.392
  learn_time_ms: 36880.396
  load_throughput: 4778134.066
  load_time_ms: 3.453
  training_iteration_time_ms: 49034.762
  update_time_ms: 2.677
timesteps_total: 2211000
training_iteration: 134

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24358974358974358
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.48
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 11478
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.99906873703003
time_total_s: 6942.965140581131
timers:
  learn_throughput: 418.441
  learn_time_ms: 39432.075
  load_throughput: 4815839.115
  load_time_ms: 3.426
  training_iteration_time_ms: 51920.986
  update_time_ms: 2.638
timesteps_total: 2128500
training_iteration: 129

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2746478873239437
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7894736842105263
  reward for individual goal_min: 0.0
episode_len_mean: 190.0
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 12082
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.99136543273926
time_total_s: 6960.062700033188
timers:
  learn_throughput: 517.457
  learn_time_ms: 31886.723
  load_throughput: 4636701.528
  load_time_ms: 3.559
  training_iteration_time_ms: 42663.859
  update_time_ms: 2.477
timesteps_total: 2475000
training_iteration: 150

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8232323232323232
  reward for individual goal_min: 0.5
episode_len_mean: 82.595
episode_reward_max: 2.0
episode_reward_mean: 1.825
episode_reward_min: 1.0
episodes_this_iter: 200
episodes_total: 17189
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.995
  agent_1: 0.83
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.211068868637085
time_total_s: 6955.8964302539825
timers:
  learn_throughput: 369.701
  learn_time_ms: 44630.69
  load_throughput: 4088691.851
  load_time_ms: 4.036
  training_iteration_time_ms: 58478.308
  update_time_ms: 2.937
timesteps_total: 1914000
training_iteration: 116

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.276881720430104
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 372
episodes_total: 36942
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.5833694934845
time_total_s: 6894.957781553268
timers:
  learn_throughput: 456.271
  learn_time_ms: 36162.684
  load_throughput: 4406454.768
  load_time_ms: 3.745
  training_iteration_time_ms: 47570.494
  update_time_ms: 2.489
timesteps_total: 2475000
training_iteration: 150

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2826086956521739
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939759036144579
  reward for individual goal_min: 0.5
episode_len_mean: 159.07843137254903
episode_reward_max: 2.0
episode_reward_mean: 1.4313725490196079
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 10663
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7450980392156863
  agent_1: 0.6862745098039216
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.05427050590515
time_total_s: 6939.925452470779
timers:
  learn_throughput: 424.57
  learn_time_ms: 38862.818
  load_throughput: 4501643.477
  load_time_ms: 3.665
  training_iteration_time_ms: 51183.516
  update_time_ms: 2.713
timesteps_total: 2013000
training_iteration: 122

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32941176470588235
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 180.88
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 11695
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.47942638397217
time_total_s: 6936.191735267639
timers:
  learn_throughput: 439.38
  learn_time_ms: 37552.951
  load_throughput: 4750158.965
  load_time_ms: 3.474
  training_iteration_time_ms: 49788.535
  update_time_ms: 2.662
timesteps_total: 2211000
training_iteration: 134

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8636363636363636
  reward for individual goal_min: 0.5
episode_len_mean: 73.10714285714286
episode_reward_max: 2.0
episode_reward_mean: 1.8526785714285714
episode_reward_min: 1.0
episodes_this_iter: 224
episodes_total: 24996
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8526785714285714
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.81227517127991
time_total_s: 6940.958040714264
timers:
  learn_throughput: 443.944
  learn_time_ms: 37166.882
  load_throughput: 4672893.228
  load_time_ms: 3.531
  training_iteration_time_ms: 48629.891
  update_time_ms: 2.591
timesteps_total: 2260500
training_iteration: 137

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9527027027027027
  reward for individual goal_min: 0.0
episode_len_mean: 184.67
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 10672
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.52696776390076
time_total_s: 6948.997669219971
timers:
  learn_throughput: 423.306
  learn_time_ms: 38978.907
  load_throughput: 4436396.831
  load_time_ms: 3.719
  training_iteration_time_ms: 51428.215
  update_time_ms: 2.781
timesteps_total: 2029500
training_iteration: 123

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9942857142857143
  reward for individual goal_min: 0.5
episode_len_mean: 49.93636363636364
episode_reward_max: 2.0
episode_reward_mean: 1.993939393939394
episode_reward_min: 1.0
episodes_this_iter: 330
episodes_total: 25411
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.996969696969697
  agent_1: 0.996969696969697
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.15223836898804
time_total_s: 6935.050810575485
timers:
  learn_throughput: 365.141
  learn_time_ms: 45187.962
  load_throughput: 3658906.23
  load_time_ms: 4.51
  training_iteration_time_ms: 58724.138
  update_time_ms: 2.634
timesteps_total: 2079000
training_iteration: 126

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3026315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9929577464788732
  reward for individual goal_min: 0.5
episode_len_mean: 173.09
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 13703
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.4034366607666
time_total_s: 6950.529645681381
timers:
  learn_throughput: 535.755
  learn_time_ms: 30797.643
  load_throughput: 5259255.409
  load_time_ms: 3.137
  training_iteration_time_ms: 41308.525
  update_time_ms: 2.444
timesteps_total: 2541000
training_iteration: 154

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2565789473684211
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8066666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 191.97
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 11605
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.62225914001465
time_total_s: 6962.593660831451
timers:
  learn_throughput: 493.316
  learn_time_ms: 33447.13
  load_throughput: 5098387.075
  load_time_ms: 3.236
  training_iteration_time_ms: 44657.085
  update_time_ms: 2.444
timesteps_total: 2392500
training_iteration: 145

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2911392405063291
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7435897435897436
  reward for individual goal_min: 0.0
episode_len_mean: 189.59
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 10646
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.39188528060913
time_total_s: 6973.693398714066
timers:
  learn_throughput: 426.794
  learn_time_ms: 38660.362
  load_throughput: 4649598.302
  load_time_ms: 3.549
  training_iteration_time_ms: 51370.005
  update_time_ms: 2.583
timesteps_total: 2178000
training_iteration: 132

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.5
episode_len_mean: 181.12
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 10698
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.772491216659546
time_total_s: 6957.793403863907
timers:
  learn_throughput: 434.065
  learn_time_ms: 38012.722
  load_throughput: 4593370.458
  load_time_ms: 3.592
  training_iteration_time_ms: 50638.341
  update_time_ms: 2.591
timesteps_total: 2145000
training_iteration: 130

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2411764705882353
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 192.72
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 11705
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.48
  agent_1: 0.78
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.35979747772217
time_total_s: 6981.391062259674
timers:
  learn_throughput: 446.927
  learn_time_ms: 36918.776
  load_throughput: 4814298.057
  load_time_ms: 3.427
  training_iteration_time_ms: 49087.231
  update_time_ms: 2.672
timesteps_total: 2227500
training_iteration: 135

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2971014492753623
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.74375
  reward for individual goal_min: 0.0
episode_len_mean: 198.63
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 12167
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.41
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.67010045051575
time_total_s: 7001.732800483704
timers:
  learn_throughput: 519.406
  learn_time_ms: 31767.068
  load_throughput: 4656794.224
  load_time_ms: 3.543
  training_iteration_time_ms: 42551.739
  update_time_ms: 2.464
timesteps_total: 2491500
training_iteration: 151

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2222222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 189.26
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 11564
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.30621314048767
time_total_s: 6991.271353721619
timers:
  learn_throughput: 418.641
  learn_time_ms: 39413.253
  load_throughput: 4820233.19
  load_time_ms: 3.423
  training_iteration_time_ms: 51872.472
  update_time_ms: 2.641
timesteps_total: 2145000
training_iteration: 130

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23529411764705882
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 157.5728155339806
episode_reward_max: 2.0
episode_reward_mean: 1.4368932038834952
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 13806
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6796116504854369
  agent_1: 0.7572815533980582
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 39.44645118713379
time_total_s: 6989.976096868515
timers:
  learn_throughput: 537.778
  learn_time_ms: 30681.782
  load_throughput: 5314872.362
  load_time_ms: 3.104
  training_iteration_time_ms: 41183.16
  update_time_ms: 2.45
timesteps_total: 2557500
training_iteration: 155

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21428571428571427
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9807692307692307
  reward for individual goal_min: 0.5
episode_len_mean: 187.22
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 10751
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.89688754081726
time_total_s: 6986.822340011597
timers:
  learn_throughput: 429.655
  learn_time_ms: 38402.92
  load_throughput: 4523387.278
  load_time_ms: 3.648
  training_iteration_time_ms: 50550.12
  update_time_ms: 2.702
timesteps_total: 2029500
training_iteration: 123

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971264367816092
  reward for individual goal_min: 0.5
episode_len_mean: 44.285333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.9973333333333334
episode_reward_min: 1.0
episodes_this_iter: 375
episodes_total: 37317
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973333333333333
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 47.72909450531006
time_total_s: 6942.6868760585785
timers:
  learn_throughput: 456.978
  learn_time_ms: 36106.737
  load_throughput: 4397439.032
  load_time_ms: 3.752
  training_iteration_time_ms: 47529.684
  update_time_ms: 2.492
timesteps_total: 2491500
training_iteration: 151

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8451327433628318
  reward for individual goal_min: 0.5
episode_len_mean: 73.8711111111111
episode_reward_max: 2.0
episode_reward_mean: 1.8444444444444446
episode_reward_min: 1.0
episodes_this_iter: 225
episodes_total: 25221
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8444444444444444
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 49.20817852020264
time_total_s: 6990.166219234467
timers:
  learn_throughput: 443.804
  learn_time_ms: 37178.608
  load_throughput: 4653788.002
  load_time_ms: 3.545
  training_iteration_time_ms: 48673.809
  update_time_ms: 2.578
timesteps_total: 2277000
training_iteration: 138

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26582278481012656
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9810126582278481
  reward for individual goal_min: 0.0
episode_len_mean: 170.87
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 10767
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.536319971084595
time_total_s: 6998.533989191055
timers:
  learn_throughput: 428.7
  learn_time_ms: 38488.437
  load_throughput: 4461622.806
  load_time_ms: 3.698
  training_iteration_time_ms: 50820.07
  update_time_ms: 2.788
timesteps_total: 2046000
training_iteration: 124

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25882352941176473
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9722222222222222
  reward for individual goal_min: 0.0
episode_len_mean: 181.06
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 11788
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.404277086257935
time_total_s: 6987.596012353897
timers:
  learn_throughput: 438.834
  learn_time_ms: 37599.608
  load_throughput: 4762777.587
  load_time_ms: 3.464
  training_iteration_time_ms: 49891.931
  update_time_ms: 2.656
timesteps_total: 2227500
training_iteration: 135

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8263888888888888
  reward for individual goal_min: 0.0
episode_len_mean: 200.55
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 11683
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.69763922691345
time_total_s: 7007.291300058365
timers:
  learn_throughput: 495.465
  learn_time_ms: 33302.065
  load_throughput: 5169604.769
  load_time_ms: 3.192
  training_iteration_time_ms: 44502.196
  update_time_ms: 2.478
timesteps_total: 2409000
training_iteration: 146

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8295454545454546
  reward for individual goal_min: 0.5
episode_len_mean: 80.07920792079207
episode_reward_max: 2.0
episode_reward_mean: 1.8514851485148516
episode_reward_min: 1.0
episodes_this_iter: 202
episodes_total: 17391
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8514851485148515
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 58.59659457206726
time_total_s: 7014.49302482605
timers:
  learn_throughput: 371.482
  learn_time_ms: 44416.645
  load_throughput: 3926315.145
  load_time_ms: 4.202
  training_iteration_time_ms: 58318.934
  update_time_ms: 2.929
timesteps_total: 1930500
training_iteration: 117

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9967948717948718
  reward for individual goal_min: 0.5
episode_len_mean: 45.91899441340782
episode_reward_max: 2.0
episode_reward_mean: 1.9972067039106145
episode_reward_min: 1.0
episodes_this_iter: 358
episodes_total: 25769
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9972067039106145
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 60.617079973220825
time_total_s: 6995.667890548706
timers:
  learn_throughput: 364.093
  learn_time_ms: 45318.12
  load_throughput: 3665708.792
  load_time_ms: 4.501
  training_iteration_time_ms: 58854.513
  update_time_ms: 2.641
timesteps_total: 2095500
training_iteration: 127

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36538461538461536
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7697368421052632
  reward for individual goal_min: 0.0
episode_len_mean: 187.53
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 10733
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.05103874206543
time_total_s: 7024.744437456131
timers:
  learn_throughput: 427.62
  learn_time_ms: 38585.663
  load_throughput: 4584940.971
  load_time_ms: 3.599
  training_iteration_time_ms: 51218.784
  update_time_ms: 2.608
timesteps_total: 2194500
training_iteration: 133

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33116883116883117
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9857142857142858
  reward for individual goal_min: 0.5
episode_len_mean: 174.0
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 10794
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.07639527320862
time_total_s: 7008.8697991371155
timers:
  learn_throughput: 435.632
  learn_time_ms: 37876.028
  load_throughput: 4640494.585
  load_time_ms: 3.556
  training_iteration_time_ms: 50400.918
  update_time_ms: 2.59
timesteps_total: 2161500
training_iteration: 131

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24342105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.06
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 11794
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.11140251159668
time_total_s: 7030.502464771271
timers:
  learn_throughput: 445.894
  learn_time_ms: 37004.311
  load_throughput: 4801505.266
  load_time_ms: 3.436
  training_iteration_time_ms: 49103.734
  update_time_ms: 2.666
timesteps_total: 2244000
training_iteration: 136

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2808219178082192
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8051948051948052
  reward for individual goal_min: 0.0
episode_len_mean: 181.83
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 12258
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.876225233078
time_total_s: 7046.609025716782
timers:
  learn_throughput: 517.791
  learn_time_ms: 31866.136
  load_throughput: 4642611.443
  load_time_ms: 3.554
  training_iteration_time_ms: 42731.378
  update_time_ms: 2.453
timesteps_total: 2508000
training_iteration: 152

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2391304347826087
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9882352941176471
  reward for individual goal_min: 0.0
episode_len_mean: 158.02857142857144
episode_reward_max: 2.0
episode_reward_mean: 1.4285714285714286
episode_reward_min: 0.0
episodes_this_iter: 105
episodes_total: 13911
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7619047619047619
  agent_1: 0.6666666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 38.86334276199341
time_total_s: 7028.839439630508
timers:
  learn_throughput: 541.596
  learn_time_ms: 30465.495
  load_throughput: 5341412.882
  load_time_ms: 3.089
  training_iteration_time_ms: 40914.706
  update_time_ms: 2.482
timesteps_total: 2574000
training_iteration: 156

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26282051282051283
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.971830985915493
  reward for individual goal_min: 0.0
episode_len_mean: 179.39
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 11655
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.099353551864624
time_total_s: 7041.370707273483
timers:
  learn_throughput: 419.31
  learn_time_ms: 39350.397
  load_throughput: 4792959.118
  load_time_ms: 3.443
  training_iteration_time_ms: 51743.49
  update_time_ms: 2.659
timesteps_total: 2161500
training_iteration: 131

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22602739726027396
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 179.5
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 10844
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.59082055091858
time_total_s: 7034.413160562515
timers:
  learn_throughput: 432.667
  learn_time_ms: 38135.561
  load_throughput: 4626472.621
  load_time_ms: 3.566
  training_iteration_time_ms: 50170.315
  update_time_ms: 2.708
timesteps_total: 2046000
training_iteration: 124

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.895890410958906
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 365
episodes_total: 37682
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.56231713294983
time_total_s: 6990.249193191528
timers:
  learn_throughput: 458.335
  learn_time_ms: 35999.886
  load_throughput: 4417198.514
  load_time_ms: 3.735
  training_iteration_time_ms: 47435.287
  update_time_ms: 2.809
timesteps_total: 2508000
training_iteration: 152

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24305555555555555
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7321428571428571
  reward for individual goal_min: 0.0
episode_len_mean: 195.33
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 11771
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.573479890823364
time_total_s: 7051.864779949188
timers:
  learn_throughput: 496.742
  learn_time_ms: 33216.451
  load_throughput: 5156470.062
  load_time_ms: 3.2
  training_iteration_time_ms: 44379.699
  update_time_ms: 2.466
timesteps_total: 2425500
training_iteration: 147

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8589743589743589
  reward for individual goal_min: 0.5
episode_len_mean: 73.40178571428571
episode_reward_max: 2.0
episode_reward_mean: 1.8526785714285714
episode_reward_min: 1.0
episodes_this_iter: 224
episodes_total: 25445
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8526785714285714
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 46.71771430969238
time_total_s: 7036.883933544159
timers:
  learn_throughput: 445.655
  learn_time_ms: 37024.147
  load_throughput: 4671000.871
  load_time_ms: 3.532
  training_iteration_time_ms: 48532.177
  update_time_ms: 2.556
timesteps_total: 2293500
training_iteration: 139

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2847222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9743589743589743
  reward for individual goal_min: 0.0
episode_len_mean: 167.85
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 10865
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.86406469345093
time_total_s: 7045.398053884506
timers:
  learn_throughput: 433.501
  learn_time_ms: 38062.168
  load_throughput: 4462572.204
  load_time_ms: 3.697
  training_iteration_time_ms: 50203.538
  update_time_ms: 2.773
timesteps_total: 2062500
training_iteration: 125

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 165.74
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 11888
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.5999870300293
time_total_s: 7038.195999383926
timers:
  learn_throughput: 435.873
  learn_time_ms: 37855.078
  load_throughput: 4749767.748
  load_time_ms: 3.474
  training_iteration_time_ms: 50141.168
  update_time_ms: 2.658
timesteps_total: 2244000
training_iteration: 136

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8020833333333334
  reward for individual goal_min: 0.5
episode_len_mean: 90.5792349726776
episode_reward_max: 2.0
episode_reward_mean: 1.7923497267759563
episode_reward_min: 1.0
episodes_this_iter: 183
episodes_total: 17574
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9890710382513661
  agent_1: 0.8032786885245902
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.664997816085815
time_total_s: 7071.158022642136
timers:
  learn_throughput: 371.82
  learn_time_ms: 44376.296
  load_throughput: 3897437.376
  load_time_ms: 4.234
  training_iteration_time_ms: 58209.512
  update_time_ms: 2.932
timesteps_total: 1947000
training_iteration: 118

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22972972972972974
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8095238095238095
  reward for individual goal_min: 0.0
episode_len_mean: 200.71
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 12342
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 39.13432765007019
time_total_s: 7085.743353366852
timers:
  learn_throughput: 521.177
  learn_time_ms: 31659.097
  load_throughput: 4621189.92
  load_time_ms: 3.571
  training_iteration_time_ms: 42467.07
  update_time_ms: 2.474
timesteps_total: 2524500
training_iteration: 153

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3611111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8642857142857143
  reward for individual goal_min: 0.0
episode_len_mean: 188.09
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 10825
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.86131954193115
time_total_s: 7075.605756998062
timers:
  learn_throughput: 427.148
  learn_time_ms: 38628.294
  load_throughput: 4594010.78
  load_time_ms: 3.592
  training_iteration_time_ms: 51232.204
  update_time_ms: 2.61
timesteps_total: 2211000
training_iteration: 134

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3227848101265823
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.49
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 10888
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.42920923233032
time_total_s: 7060.299008369446
timers:
  learn_throughput: 433.046
  learn_time_ms: 38102.211
  load_throughput: 4589654.015
  load_time_ms: 3.595
  training_iteration_time_ms: 50699.273
  update_time_ms: 2.6
timesteps_total: 2178000
training_iteration: 132

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9788732394366197
  reward for individual goal_min: 0.0
episode_len_mean: 177.08
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 14004
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.72516584396362
time_total_s: 7069.564605474472
timers:
  learn_throughput: 543.845
  learn_time_ms: 30339.523
  load_throughput: 5346034.159
  load_time_ms: 3.086
  training_iteration_time_ms: 40768.601
  update_time_ms: 2.476
timesteps_total: 2590500
training_iteration: 157

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.57464788732394
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 355
episodes_total: 26124
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.142953872680664
time_total_s: 7054.810844421387
timers:
  learn_throughput: 364.365
  learn_time_ms: 45284.318
  load_throughput: 3663244.548
  load_time_ms: 4.504
  training_iteration_time_ms: 58795.796
  update_time_ms: 2.621
timesteps_total: 2112000
training_iteration: 128

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.19
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 11890
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.63290977478027
time_total_s: 7081.135374546051
timers:
  learn_throughput: 444.235
  learn_time_ms: 37142.485
  load_throughput: 4807843.05
  load_time_ms: 3.432
  training_iteration_time_ms: 49312.047
  update_time_ms: 2.645
timesteps_total: 2260500
training_iteration: 137

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2323943661971831
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.15
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 11749
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.824167013168335
time_total_s: 7089.194874286652
timers:
  learn_throughput: 423.412
  learn_time_ms: 38969.121
  load_throughput: 4833396.143
  load_time_ms: 3.414
  training_iteration_time_ms: 51248.249
  update_time_ms: 2.69
timesteps_total: 2178000
training_iteration: 132

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9850746268656716
  reward for individual goal_min: 0.5
episode_len_mean: 182.76
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 10935
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.22283387184143
time_total_s: 7082.635994434357
timers:
  learn_throughput: 436.157
  learn_time_ms: 37830.438
  load_throughput: 4632356.473
  load_time_ms: 3.562
  training_iteration_time_ms: 49784.417
  update_time_ms: 2.738
timesteps_total: 2062500
training_iteration: 125

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.37857142857142856
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8661971830985915
  reward for individual goal_min: 0.0
episode_len_mean: 163.04
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 11871
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.09554171562195
time_total_s: 7096.96032166481
timers:
  learn_throughput: 494.45
  learn_time_ms: 33370.385
  load_throughput: 5170068.206
  load_time_ms: 3.191
  training_iteration_time_ms: 44563.734
  update_time_ms: 2.458
timesteps_total: 2442000
training_iteration: 148

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972527472527473
  reward for individual goal_min: 0.5
episode_len_mean: 44.98365122615804
episode_reward_max: 2.0
episode_reward_mean: 1.997275204359673
episode_reward_min: 1.0
episodes_this_iter: 367
episodes_total: 38049
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.997275204359673
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.736419916152954
time_total_s: 7038.985613107681
timers:
  learn_throughput: 456.746
  learn_time_ms: 36125.097
  load_throughput: 4409458.869
  load_time_ms: 3.742
  training_iteration_time_ms: 47602.743
  update_time_ms: 2.805
timesteps_total: 2524500
training_iteration: 153

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9927536231884058
  reward for individual goal_min: 0.5
episode_len_mean: 182.44
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 10956
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.52198147773743
time_total_s: 7094.920035362244
timers:
  learn_throughput: 436.864
  learn_time_ms: 37769.17
  load_throughput: 4491272.373
  load_time_ms: 3.674
  training_iteration_time_ms: 49946.75
  update_time_ms: 2.752
timesteps_total: 2079000
training_iteration: 126

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 74.68333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.8333333333333333
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8333333333333334
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.839622641509434
  reward for individual goal_min: 0.5
episode_len_mean: 76.19905213270142
episode_reward_max: 2.0
episode_reward_mean: 1.838862559241706
episode_reward_min: 1.0
episodes_this_iter: 211
episodes_total: 25656
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8388625592417062
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 54.92150950431824
time_total_s: 7091.805443048477
timers:
  learn_throughput: 444.398
  learn_time_ms: 37128.898
  load_throughput: 4676113.758
  load_time_ms: 3.529
  training_iteration_time_ms: 48580.063
  update_time_ms: 2.543
timesteps_total: 2310000
training_iteration: 140

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3525641025641026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 164.15
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 11987
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.77195906639099
time_total_s: 7088.967958450317
timers:
  learn_throughput: 434.258
  learn_time_ms: 37995.825
  load_throughput: 4719514.451
  load_time_ms: 3.496
  training_iteration_time_ms: 50316.01
  update_time_ms: 2.663
timesteps_total: 2260500
training_iteration: 137

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32894736842105265
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 183.95
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 12433
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.625564098358154
time_total_s: 7129.36891746521
timers:
  learn_throughput: 518.311
  learn_time_ms: 31834.147
  load_throughput: 4633752.21
  load_time_ms: 3.561
  training_iteration_time_ms: 42661.335
  update_time_ms: 2.473
timesteps_total: 2541000
training_iteration: 154

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28169014084507044
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 166.11881188118812
episode_reward_max: 2.0
episode_reward_mean: 1.396039603960396
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 14105
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6633663366336634
  agent_1: 0.7326732673267327
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.58584690093994
time_total_s: 7111.150452375412
timers:
  learn_throughput: 545.873
  learn_time_ms: 30226.786
  load_throughput: 5348513.134
  load_time_ms: 3.085
  training_iteration_time_ms: 40638.634
  update_time_ms: 2.477
timesteps_total: 2607000
training_iteration: 158

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7857142857142857
  reward for individual goal_min: 0.5
episode_len_mean: 88.61413043478261
episode_reward_max: 2.0
episode_reward_mean: 1.8043478260869565
episode_reward_min: 1.0
episodes_this_iter: 184
episodes_total: 17758
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9836956521739131
  agent_1: 0.8206521739130435
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.460280895233154
time_total_s: 7129.618303537369
timers:
  learn_throughput: 372.196
  learn_time_ms: 44331.43
  load_throughput: 3931288.862
  load_time_ms: 4.197
  training_iteration_time_ms: 58140.78
  update_time_ms: 2.92
timesteps_total: 1963500
training_iteration: 119

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8269230769230769
  reward for individual goal_min: 0.0
episode_len_mean: 192.96
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 10907
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.83270812034607
time_total_s: 7125.438465118408
timers:
  learn_throughput: 429.021
  learn_time_ms: 38459.681
  load_throughput: 4640681.289
  load_time_ms: 3.556
  training_iteration_time_ms: 50952.501
  update_time_ms: 2.609
timesteps_total: 2227500
training_iteration: 135

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21428571428571427
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 174.85
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 10983
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.74348497390747
time_total_s: 7114.042493343353
timers:
  learn_throughput: 432.379
  learn_time_ms: 38160.958
  load_throughput: 4539051.867
  load_time_ms: 3.635
  training_iteration_time_ms: 50753.448
  update_time_ms: 2.583
timesteps_total: 2194500
training_iteration: 133

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22941176470588234
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 198.94
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 11974
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.35578370094299
time_total_s: 7129.491158246994
timers:
  learn_throughput: 444.685
  learn_time_ms: 37104.909
  load_throughput: 4826485.898
  load_time_ms: 3.419
  training_iteration_time_ms: 49247.906
  update_time_ms: 2.635
timesteps_total: 2277000
training_iteration: 138

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9968944099378882
  reward for individual goal_min: 0.5
episode_len_mean: 46.111111111111114
episode_reward_max: 2.0
episode_reward_mean: 1.9972222222222222
episode_reward_min: 1.0
episodes_this_iter: 360
episodes_total: 26484
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972222222222222
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 60.34998893737793
time_total_s: 7115.160833358765
timers:
  learn_throughput: 364.434
  learn_time_ms: 45275.712
  load_throughput: 3672049.537
  load_time_ms: 4.493
  training_iteration_time_ms: 58790.05
  update_time_ms: 2.646
timesteps_total: 2128500
training_iteration: 129

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2236842105263158
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 189.94
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 11020
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.899924755096436
time_total_s: 7130.535919189453
timers:
  learn_throughput: 440.681
  learn_time_ms: 37442.045
  load_throughput: 4638379.657
  load_time_ms: 3.557
  training_iteration_time_ms: 49382.556
  update_time_ms: 2.762
timesteps_total: 2079000
training_iteration: 126

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 185.77
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 11839
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.59772515296936
time_total_s: 7142.792599439621
timers:
  learn_throughput: 425.637
  learn_time_ms: 38765.432
  load_throughput: 4820736.84
  load_time_ms: 3.423
  training_iteration_time_ms: 51073.238
  update_time_ms: 2.693
timesteps_total: 2194500
training_iteration: 133

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3263888888888889
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7847222222222222
  reward for individual goal_min: 0.0
episode_len_mean: 188.91
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 11955
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.495749711990356
time_total_s: 7142.4560713768005
timers:
  learn_throughput: 494.378
  learn_time_ms: 33375.244
  load_throughput: 5127473.013
  load_time_ms: 3.218
  training_iteration_time_ms: 44566.212
  update_time_ms: 2.465
timesteps_total: 2458500
training_iteration: 149

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.37903225806452
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 372
episodes_total: 38421
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.82070779800415
time_total_s: 7086.806320905685
timers:
  learn_throughput: 456.497
  learn_time_ms: 36144.851
  load_throughput: 4391439.721
  load_time_ms: 3.757
  training_iteration_time_ms: 47606.839
  update_time_ms: 2.821
timesteps_total: 2541000
training_iteration: 154

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22151898734177214
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9857142857142858
  reward for individual goal_min: 0.5
episode_len_mean: 190.77
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 11044
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.0974338054657
time_total_s: 7142.017469167709
timers:
  learn_throughput: 441.354
  learn_time_ms: 37384.96
  load_throughput: 4462226.922
  load_time_ms: 3.698
  training_iteration_time_ms: 49581.712
  update_time_ms: 2.747
timesteps_total: 2095500
training_iteration: 127

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8577586206896551
  reward for individual goal_min: 0.5
episode_len_mean: 71.40833333333333
episode_reward_max: 2.0
episode_reward_mean: 1.8625
episode_reward_min: 1.0
episodes_this_iter: 240
episodes_total: 25896
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8666666666666667
  agent_1: 0.9958333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.136991024017334
time_total_s: 7141.9424340724945
timers:
  learn_throughput: 443.303
  learn_time_ms: 37220.585
  load_throughput: 4704467.904
  load_time_ms: 3.507
  training_iteration_time_ms: 48692.937
  update_time_ms: 2.558
timesteps_total: 2326500
training_iteration: 141

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2261904761904762
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 186.87
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 12077
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.91757822036743
time_total_s: 7139.885536670685
timers:
  learn_throughput: 432.576
  learn_time_ms: 38143.618
  load_throughput: 4733394.616
  load_time_ms: 3.486
  training_iteration_time_ms: 50445.774
  update_time_ms: 2.687
timesteps_total: 2277000
training_iteration: 138

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23780487804878048
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8214285714285714
  reward for individual goal_min: 0.0
episode_len_mean: 194.52
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 12520
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.77722239494324
time_total_s: 7173.146139860153
timers:
  learn_throughput: 514.774
  learn_time_ms: 32052.888
  load_throughput: 4614257.349
  load_time_ms: 3.576
  training_iteration_time_ms: 42909.124
  update_time_ms: 2.46
timesteps_total: 2557500
training_iteration: 155

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3141025641025641
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9769230769230769
  reward for individual goal_min: 0.0
episode_len_mean: 178.01
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 14195
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.17989659309387
time_total_s: 7152.330348968506
timers:
  learn_throughput: 545.548
  learn_time_ms: 30244.827
  load_throughput: 5392816.645
  load_time_ms: 3.06
  training_iteration_time_ms: 40704.117
  update_time_ms: 2.493
timesteps_total: 2623500
training_iteration: 159

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30405405405405406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8
  reward for individual goal_min: 0.0
episode_len_mean: 202.17
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 10989
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.674638509750366
time_total_s: 7175.113103628159
timers:
  learn_throughput: 430.03
  learn_time_ms: 38369.396
  load_throughput: 4679307.09
  load_time_ms: 3.526
  training_iteration_time_ms: 50855.564
  update_time_ms: 2.655
timesteps_total: 2244000
training_iteration: 136

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2878787878787879
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 158.71
episode_reward_max: 2.0
episode_reward_mean: 1.46
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 12074
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.87791848182678
time_total_s: 7177.369076728821
timers:
  learn_throughput: 446.234
  learn_time_ms: 36976.151
  load_throughput: 4880743.614
  load_time_ms: 3.381
  training_iteration_time_ms: 49063.479
  update_time_ms: 2.606
timesteps_total: 2293500
training_iteration: 139

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25735294117647056
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.98
  reward for individual goal_min: 0.0
episode_len_mean: 164.58
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 11082
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.402504682540894
time_total_s: 7164.444998025894
timers:
  learn_throughput: 432.267
  learn_time_ms: 38170.892
  load_throughput: 4548867.549
  load_time_ms: 3.627
  training_iteration_time_ms: 50703.551
  update_time_ms: 2.576
timesteps_total: 2211000
training_iteration: 134

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24305555555555555
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8181818181818182
  reward for individual goal_min: 0.0
episode_len_mean: 193.09
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 12041
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.49
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.885071754455566
time_total_s: 7187.341143131256
timers:
  learn_throughput: 493.934
  learn_time_ms: 33405.298
  load_throughput: 5103537.949
  load_time_ms: 3.233
  training_iteration_time_ms: 44652.191
  update_time_ms: 2.507
timesteps_total: 2475000
training_iteration: 150

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 182.41
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 11112
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.6035041809082
time_total_s: 7179.139423370361
timers:
  learn_throughput: 442.898
  learn_time_ms: 37254.661
  load_throughput: 4631457.444
  load_time_ms: 3.563
  training_iteration_time_ms: 49276.595
  update_time_ms: 2.75
timesteps_total: 2095500
training_iteration: 127

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8
  reward for individual goal_min: 0.5
episode_len_mean: 92.38333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.8
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9666666666666667
  agent_1: 0.8333333333333334
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8043478260869565
  reward for individual goal_min: 0.5
episode_len_mean: 92.20879120879121
episode_reward_max: 2.0
episode_reward_mean: 1.8021978021978022
episode_reward_min: 1.0
episodes_this_iter: 182
episodes_total: 17940
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.967032967032967
  agent_1: 0.8351648351648352
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 68.91114115715027
time_total_s: 7198.529444694519
timers:
  learn_throughput: 373.219
  learn_time_ms: 44209.986
  load_throughput: 3972289.148
  load_time_ms: 4.154
  training_iteration_time_ms: 58013.794
  update_time_ms: 2.885
timesteps_total: 1980000
training_iteration: 120

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25949367088607594
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9637681159420289
  reward for individual goal_min: 0.0
episode_len_mean: 185.55
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 11927
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.40199255943298
time_total_s: 7195.194591999054
timers:
  learn_throughput: 423.16
  learn_time_ms: 38992.352
  load_throughput: 4823525.607
  load_time_ms: 3.421
  training_iteration_time_ms: 51264.529
  update_time_ms: 2.686
timesteps_total: 2211000
training_iteration: 134

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.96632124352332
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 386
episodes_total: 38807
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.15376830101013
time_total_s: 7134.960089206696
timers:
  learn_throughput: 455.589
  learn_time_ms: 36216.84
  load_throughput: 4422477.011
  load_time_ms: 3.731
  training_iteration_time_ms: 47688.058
  update_time_ms: 2.783
timesteps_total: 2557500
training_iteration: 155

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21052631578947367
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 185.8
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 11133
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.21640968322754
time_total_s: 7190.233878850937
timers:
  learn_throughput: 444.652
  learn_time_ms: 37107.702
  load_throughput: 4453325.611
  load_time_ms: 3.705
  training_iteration_time_ms: 49214.897
  update_time_ms: 2.746
timesteps_total: 2112000
training_iteration: 128

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9945652173913043
  reward for individual goal_min: 0.5
episode_len_mean: 46.349295774647885
episode_reward_max: 2.0
episode_reward_mean: 1.9943661971830986
episode_reward_min: 1.0
episodes_this_iter: 355
episodes_total: 26839
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9971830985915493
  agent_1: 0.9971830985915493
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.39627480506897
time_total_s: 7174.557108163834
timers:
  learn_throughput: 364.537
  learn_time_ms: 45262.933
  load_throughput: 3661848.967
  load_time_ms: 4.506
  training_iteration_time_ms: 58796.891
  update_time_ms: 2.642
timesteps_total: 2145000
training_iteration: 130

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2571428571428571
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8611111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 189.9
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 12607
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 39.88839888572693
time_total_s: 7213.03453874588
timers:
  learn_throughput: 519.232
  learn_time_ms: 31777.681
  load_throughput: 4600087.474
  load_time_ms: 3.587
  training_iteration_time_ms: 42588.52
  update_time_ms: 2.466
timesteps_total: 2574000
training_iteration: 156

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 82.859375
episode_reward_max: 2.0
episode_reward_mean: 1.8177083333333333
episode_reward_min: 1.0
episodes_this_iter: 192
episodes_total: 26088
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8229166666666666
  agent_1: 0.9947916666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.08480453491211
time_total_s: 7189.027238607407
timers:
  learn_throughput: 444.843
  learn_time_ms: 37091.759
  load_throughput: 4765795.034
  load_time_ms: 3.462
  training_iteration_time_ms: 48541.771
  update_time_ms: 2.559
timesteps_total: 2343000
training_iteration: 142

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2916666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 153.9245283018868
episode_reward_max: 2.0
episode_reward_mean: 1.4245283018867925
episode_reward_min: 0.0
episodes_this_iter: 106
episodes_total: 12183
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6981132075471698
  agent_1: 0.7264150943396226
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.05001664161682
time_total_s: 7188.935553312302
timers:
  learn_throughput: 431.749
  learn_time_ms: 38216.666
  load_throughput: 4741696.996
  load_time_ms: 3.48
  training_iteration_time_ms: 50536.803
  update_time_ms: 2.678
timesteps_total: 2293500
training_iteration: 139

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9333333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 84.01666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.8333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9333333333333333
  agent_1: 0.9
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2468354430379747
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9855072463768116
  reward for individual goal_min: 0.5
episode_len_mean: 191.69
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 14279
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.51499795913696
time_total_s: 7202.845346927643
timers:
  learn_throughput: 545.717
  learn_time_ms: 30235.444
  load_throughput: 5343144.924
  load_time_ms: 3.088
  training_iteration_time_ms: 40635.918
  update_time_ms: 2.49
timesteps_total: 2640000
training_iteration: 160

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26973684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.89375
  reward for individual goal_min: 0.0
episode_len_mean: 190.7
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 11078
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.10838007926941
time_total_s: 7225.221483707428
timers:
  learn_throughput: 431.648
  learn_time_ms: 38225.588
  load_throughput: 4664075.32
  load_time_ms: 3.538
  training_iteration_time_ms: 50688.696
  update_time_ms: 2.645
timesteps_total: 2260500
training_iteration: 137

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2565789473684211
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9791666666666666
  reward for individual goal_min: 0.5
episode_len_mean: 187.7
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 11167
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.84534549713135
time_total_s: 7214.2903435230255
timers:
  learn_throughput: 431.742
  learn_time_ms: 38217.244
  load_throughput: 4555664.859
  load_time_ms: 3.622
  training_iteration_time_ms: 50694.835
  update_time_ms: 2.581
timesteps_total: 2227500
training_iteration: 135

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3208955223880597
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8
  reward for individual goal_min: 0.0
episode_len_mean: 174.75
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 12135
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.45842385292053
time_total_s: 7230.799566984177
timers:
  learn_throughput: 495.016
  learn_time_ms: 33332.288
  load_throughput: 5095421.587
  load_time_ms: 3.238
  training_iteration_time_ms: 44580.367
  update_time_ms: 2.513
timesteps_total: 2491500
training_iteration: 151

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 82.58333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.8333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9166666666666666
  agent_1: 0.9166666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2463768115942029
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 167.67326732673268
episode_reward_max: 2.0
episode_reward_mean: 1.3663366336633664
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 12175
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6435643564356436
  agent_1: 0.7227722772277227
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.732019662857056
time_total_s: 7237.101096391678
timers:
  learn_throughput: 445.715
  learn_time_ms: 37019.208
  load_throughput: 4880365.008
  load_time_ms: 3.381
  training_iteration_time_ms: 49064.754
  update_time_ms: 2.593
timesteps_total: 2310000
training_iteration: 140

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1962025316455696
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 194.3
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 11198
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.99139356613159
time_total_s: 7227.130816936493
timers:
  learn_throughput: 444.518
  learn_time_ms: 37118.879
  load_throughput: 4678484.627
  load_time_ms: 3.527
  training_iteration_time_ms: 49018.785
  update_time_ms: 2.734
timesteps_total: 2112000
training_iteration: 128

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.863013698630137
  reward for individual goal_min: 0.0
episode_len_mean: 193.42
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 12693
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.93680143356323
time_total_s: 7253.971340179443
timers:
  learn_throughput: 521.128
  learn_time_ms: 31662.1
  load_throughput: 4650566.89
  load_time_ms: 3.548
  training_iteration_time_ms: 42475.045
  update_time_ms: 2.481
timesteps_total: 2590500
training_iteration: 157

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2676056338028169
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.5
episode_len_mean: 169.58
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 12027
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.28795623779297
time_total_s: 7244.482548236847
timers:
  learn_throughput: 422.868
  learn_time_ms: 39019.248
  load_throughput: 4815369.993
  load_time_ms: 3.427
  training_iteration_time_ms: 51340.624
  update_time_ms: 2.67
timesteps_total: 2227500
training_iteration: 135

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.36388140161725
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 371
episodes_total: 39178
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.89236330986023
time_total_s: 7182.852452516556
timers:
  learn_throughput: 455.428
  learn_time_ms: 36229.617
  load_throughput: 4422363.97
  load_time_ms: 3.731
  training_iteration_time_ms: 47713.983
  update_time_ms: 2.761
timesteps_total: 2574000
training_iteration: 156

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27710843373493976
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9930555555555556
  reward for individual goal_min: 0.5
episode_len_mean: 181.86
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 11226
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.56287717819214
time_total_s: 7239.796756029129
timers:
  learn_throughput: 443.608
  learn_time_ms: 37194.981
  load_throughput: 4486584.593
  load_time_ms: 3.678
  training_iteration_time_ms: 49226.907
  update_time_ms: 2.786
timesteps_total: 2128500
training_iteration: 129

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.5
episode_len_mean: 162.7450980392157
episode_reward_max: 2.0
episode_reward_mean: 1.4019607843137254
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 14381
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7156862745098039
  agent_1: 0.6862745098039216
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 39.800909757614136
time_total_s: 7242.646256685257
timers:
  learn_throughput: 548.795
  learn_time_ms: 30065.882
  load_throughput: 5390002.57
  load_time_ms: 3.061
  training_iteration_time_ms: 40371.866
  update_time_ms: 2.458
timesteps_total: 2656500
training_iteration: 161

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8316326530612245
  reward for individual goal_min: 0.5
episode_len_mean: 82.99497487437186
episode_reward_max: 2.0
episode_reward_mean: 1.8341708542713568
episode_reward_min: 1.0
episodes_this_iter: 199
episodes_total: 18139
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9899497487437185
  agent_1: 0.8442211055276382
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.674519062042236
time_total_s: 7257.203963756561
timers:
  learn_throughput: 371.219
  learn_time_ms: 44448.178
  load_throughput: 3899369.844
  load_time_ms: 4.231
  training_iteration_time_ms: 58275.92
  update_time_ms: 2.925
timesteps_total: 1996500
training_iteration: 121

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8708333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 71.07725321888412
episode_reward_max: 2.0
episode_reward_mean: 1.8669527896995708
episode_reward_min: 1.0
episodes_this_iter: 233
episodes_total: 26321
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8669527896995708
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 49.07267642021179
time_total_s: 7238.099915027618
timers:
  learn_throughput: 443.963
  learn_time_ms: 37165.256
  load_throughput: 4759436.62
  load_time_ms: 3.467
  training_iteration_time_ms: 48636.29
  update_time_ms: 2.547
timesteps_total: 2359500
training_iteration: 143

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 45.11538461538461
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 364
episodes_total: 27203
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 60.12318444252014
time_total_s: 7234.680292606354
timers:
  learn_throughput: 362.601
  learn_time_ms: 45504.52
  load_throughput: 3653555.625
  load_time_ms: 4.516
  training_iteration_time_ms: 59066.799
  update_time_ms: 2.635
timesteps_total: 2161500
training_iteration: 131

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 62.833333333333336
episode_reward_max: 2.0
episode_reward_mean: 1.9833333333333334
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9833333333333333
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22916666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9929577464788732
  reward for individual goal_min: 0.5
episode_len_mean: 174.06
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 12277
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.40933060646057
time_total_s: 7246.344883918762
timers:
  learn_throughput: 431.292
  learn_time_ms: 38257.143
  load_throughput: 4777936.139
  load_time_ms: 3.453
  training_iteration_time_ms: 50541.508
  update_time_ms: 2.676
timesteps_total: 2310000
training_iteration: 140

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-28met_xp4k/checkpoint_000140/checkpoint-140
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3055555555555556
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.89375
  reward for individual goal_min: 0.0
episode_len_mean: 174.28
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 11171
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.19866991043091
time_total_s: 7273.420153617859
timers:
  learn_throughput: 431.711
  learn_time_ms: 38220.029
  load_throughput: 4658486.931
  load_time_ms: 3.542
  training_iteration_time_ms: 50645.728
  update_time_ms: 2.687
timesteps_total: 2277000
training_iteration: 138

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8783783783783784
  reward for individual goal_min: 0.0
episode_len_mean: 183.05
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 12227
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.35345411300659
time_total_s: 7276.153021097183
timers:
  learn_throughput: 493.861
  learn_time_ms: 33410.19
  load_throughput: 5062471.91
  load_time_ms: 3.259
  training_iteration_time_ms: 44634.189
  update_time_ms: 2.529
timesteps_total: 2508000
training_iteration: 152

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21621621621621623
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9753086419753086
  reward for individual goal_min: 0.0
episode_len_mean: 179.18
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 11261
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.25859093666077
time_total_s: 7261.548934459686
timers:
  learn_throughput: 431.814
  learn_time_ms: 38210.924
  load_throughput: 4557224.812
  load_time_ms: 3.621
  training_iteration_time_ms: 50647.331
  update_time_ms: 2.594
timesteps_total: 2244000
training_iteration: 136

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24691358024691357
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 206.09
episode_reward_max: 2.0
episode_reward_mean: 1.04
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 12774
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.81215739250183
time_total_s: 7298.783497571945
timers:
  learn_throughput: 521.66
  learn_time_ms: 31629.797
  load_throughput: 4658267.437
  load_time_ms: 3.542
  training_iteration_time_ms: 42449.985
  update_time_ms: 2.548
timesteps_total: 2607000
training_iteration: 158

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29605263157894735
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.98
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 11295
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.01142930984497
time_total_s: 7275.142246246338
timers:
  learn_throughput: 448.03
  learn_time_ms: 36827.858
  load_throughput: 4674471.365
  load_time_ms: 3.53
  training_iteration_time_ms: 48678.912
  update_time_ms: 2.741
timesteps_total: 2128500
training_iteration: 129

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3767123287671233
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 154.9904761904762
episode_reward_max: 2.0
episode_reward_mean: 1.4761904761904763
episode_reward_min: 0.0
episodes_this_iter: 105
episodes_total: 12280
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7428571428571429
  agent_1: 0.7333333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.56150197982788
time_total_s: 7286.662598371506
timers:
  learn_throughput: 445.377
  learn_time_ms: 37047.229
  load_throughput: 4896698.271
  load_time_ms: 3.37
  training_iteration_time_ms: 49063.661
  update_time_ms: 2.578
timesteps_total: 2326500
training_iteration: 141

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.827195467422094
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 353
episodes_total: 39531
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 46.97300839424133
time_total_s: 7229.825460910797
timers:
  learn_throughput: 456.077
  learn_time_ms: 36178.087
  load_throughput: 4448974.061
  load_time_ms: 3.709
  training_iteration_time_ms: 47655.242
  update_time_ms: 2.735
timesteps_total: 2590500
training_iteration: 157

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1987179487179487
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.98
  reward for individual goal_min: 0.0
episode_len_mean: 191.61
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 14466
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.05903387069702
time_total_s: 7283.705290555954
timers:
  learn_throughput: 546.261
  learn_time_ms: 30205.323
  load_throughput: 5408241.068
  load_time_ms: 3.051
  training_iteration_time_ms: 40541.388
  update_time_ms: 2.467
timesteps_total: 2673000
training_iteration: 162

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9791666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 180.52
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 12116
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.03754162788391
time_total_s: 7296.520089864731
timers:
  learn_throughput: 419.685
  learn_time_ms: 39315.193
  load_throughput: 4873594.456
  load_time_ms: 3.386
  training_iteration_time_ms: 51601.375
  update_time_ms: 2.687
timesteps_total: 2244000
training_iteration: 136

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2635135135135135
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 168.73
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 11323
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.25469470024109
time_total_s: 7290.05145072937
timers:
  learn_throughput: 443.711
  learn_time_ms: 37186.35
  load_throughput: 4518661.757
  load_time_ms: 3.652
  training_iteration_time_ms: 49277.798
  update_time_ms: 2.779
timesteps_total: 2145000
training_iteration: 130

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.835
  reward for individual goal_min: 0.5
episode_len_mean: 74.6289592760181
episode_reward_max: 2.0
episode_reward_mean: 1.8506787330316743
episode_reward_min: 1.0
episodes_this_iter: 221
episodes_total: 26542
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8597285067873304
  agent_1: 0.9909502262443439
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.33757448196411
time_total_s: 7286.4374895095825
timers:
  learn_throughput: 445.082
  learn_time_ms: 37071.808
  load_throughput: 4773948.098
  load_time_ms: 3.456
  training_iteration_time_ms: 48539.237
  update_time_ms: 2.533
timesteps_total: 2376000
training_iteration: 144

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8125
  reward for individual goal_min: 0.5
episode_len_mean: 90.25555555555556
episode_reward_max: 2.0
episode_reward_mean: 1.8
episode_reward_min: 1.0
episodes_this_iter: 180
episodes_total: 18319
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9888888888888889
  agent_1: 0.8111111111111111
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.27703857421875
time_total_s: 7314.48100233078
timers:
  learn_throughput: 372.813
  learn_time_ms: 44258.137
  load_throughput: 3956121.509
  load_time_ms: 4.171
  training_iteration_time_ms: 58072.496
  update_time_ms: 2.91
timesteps_total: 2013000
training_iteration: 122

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3051948051948052
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.58
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 12376
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.9954617023468
time_total_s: 7295.340345621109
timers:
  learn_throughput: 432.395
  learn_time_ms: 38159.562
  load_throughput: 4727865.063
  load_time_ms: 3.49
  training_iteration_time_ms: 50403.038
  update_time_ms: 2.7
timesteps_total: 2326500
training_iteration: 141

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9886363636363636
  reward for individual goal_min: 0.5
episode_len_mean: 47.99418604651163
episode_reward_max: 2.0
episode_reward_mean: 1.9883720930232558
episode_reward_min: 1.0
episodes_this_iter: 344
episodes_total: 27547
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9883720930232558
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 58.118577003479004
time_total_s: 7292.798869609833
timers:
  learn_throughput: 362.624
  learn_time_ms: 45501.719
  load_throughput: 3636985.558
  load_time_ms: 4.537
  training_iteration_time_ms: 59077.452
  update_time_ms: 2.972
timesteps_total: 2178000
training_iteration: 132

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24305555555555555
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8223684210526315
  reward for individual goal_min: 0.0
episode_len_mean: 186.43
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 12314
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.79489350318909
time_total_s: 7320.947914600372
timers:
  learn_throughput: 495.269
  learn_time_ms: 33315.204
  load_throughput: 5051755.259
  load_time_ms: 3.266
  training_iteration_time_ms: 44542.509
  update_time_ms: 2.535
timesteps_total: 2524500
training_iteration: 153

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2916666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8076923076923077
  reward for individual goal_min: 0.0
episode_len_mean: 202.54
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 11254
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.2346613407135
time_total_s: 7324.654814958572
timers:
  learn_throughput: 431.723
  learn_time_ms: 38218.956
  load_throughput: 4625204.908
  load_time_ms: 3.567
  training_iteration_time_ms: 50706.622
  update_time_ms: 2.709
timesteps_total: 2293500
training_iteration: 139

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2733333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9810126582278481
  reward for individual goal_min: 0.0
episode_len_mean: 174.71
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 11358
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.263697147369385
time_total_s: 7310.812631607056
timers:
  learn_throughput: 432.387
  learn_time_ms: 38160.276
  load_throughput: 4541255.955
  load_time_ms: 3.633
  training_iteration_time_ms: 50606.106
  update_time_ms: 2.597
timesteps_total: 2260500
training_iteration: 137

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24050632911392406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8223684210526315
  reward for individual goal_min: 0.0
episode_len_mean: 208.13
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 12855
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.41
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.195647954940796
time_total_s: 7342.979145526886
timers:
  learn_throughput: 520.496
  learn_time_ms: 31700.545
  load_throughput: 4650379.39
  load_time_ms: 3.548
  training_iteration_time_ms: 42553.89
  update_time_ms: 2.549
timesteps_total: 2623500
training_iteration: 159

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26785714285714285
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 187.06
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 11381
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.07523560523987
time_total_s: 7319.217481851578
timers:
  learn_throughput: 454.213
  learn_time_ms: 36326.557
  load_throughput: 4674060.947
  load_time_ms: 3.53
  training_iteration_time_ms: 48028.093
  update_time_ms: 2.748
timesteps_total: 2145000
training_iteration: 130

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3076923076923077
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9810126582278481
  reward for individual goal_min: 0.0
episode_len_mean: 170.85
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 14560
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 39.78115153312683
time_total_s: 7323.486442089081
timers:
  learn_throughput: 549.036
  learn_time_ms: 30052.685
  load_throughput: 5370094.279
  load_time_ms: 3.073
  training_iteration_time_ms: 40299.971
  update_time_ms: 2.468
timesteps_total: 2689500
training_iteration: 163

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 165.27
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 12379
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.47856402397156
time_total_s: 7336.141162395477
timers:
  learn_throughput: 445.751
  learn_time_ms: 37016.152
  load_throughput: 4901518.914
  load_time_ms: 3.366
  training_iteration_time_ms: 48957.573
  update_time_ms: 2.546
timesteps_total: 2343000
training_iteration: 142

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.97808219178082
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 365
episodes_total: 39896
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.32131743431091
time_total_s: 7277.146778345108
timers:
  learn_throughput: 457.132
  learn_time_ms: 36094.646
  load_throughput: 4464846.647
  load_time_ms: 3.696
  training_iteration_time_ms: 47560.771
  update_time_ms: 2.724
timesteps_total: 2607000
training_iteration: 158

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19863013698630136
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9746835443037974
  reward for individual goal_min: 0.0
episode_len_mean: 181.87
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 11412
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.65286922454834
time_total_s: 7336.7043199539185
timers:
  learn_throughput: 447.523
  learn_time_ms: 36869.65
  load_throughput: 4554345.732
  load_time_ms: 3.623
  training_iteration_time_ms: 48792.347
  update_time_ms: 2.762
timesteps_total: 2161500
training_iteration: 131

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22535211267605634
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9802631578947368
  reward for individual goal_min: 0.0
episode_len_mean: 174.17
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 12211
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.81195831298828
time_total_s: 7347.332048177719
timers:
  learn_throughput: 423.035
  learn_time_ms: 39003.864
  load_throughput: 4909899.54
  load_time_ms: 3.361
  training_iteration_time_ms: 51255.448
  update_time_ms: 2.699
timesteps_total: 2260500
training_iteration: 137

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8349056603773585
  reward for individual goal_min: 0.5
episode_len_mean: 75.99547511312217
episode_reward_max: 2.0
episode_reward_mean: 1.841628959276018
episode_reward_min: 1.0
episodes_this_iter: 221
episodes_total: 26763
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8416289592760181
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.13750624656677
time_total_s: 7333.574995756149
timers:
  learn_throughput: 446.902
  learn_time_ms: 36920.82
  load_throughput: 4756754.141
  load_time_ms: 3.469
  training_iteration_time_ms: 48294.156
  update_time_ms: 2.55
timesteps_total: 2392500
training_iteration: 145

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.07
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 12468
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.173866271972656
time_total_s: 7342.514211893082
timers:
  learn_throughput: 436.932
  learn_time_ms: 37763.305
  load_throughput: 4734236.5
  load_time_ms: 3.485
  training_iteration_time_ms: 49883.604
  update_time_ms: 2.68
timesteps_total: 2343000
training_iteration: 142

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3157894736842105
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8380281690140845
  reward for individual goal_min: 0.0
episode_len_mean: 189.32
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 12401
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.02704954147339
time_total_s: 7362.974964141846
timers:
  learn_throughput: 497.067
  learn_time_ms: 33194.748
  load_throughput: 5082772.661
  load_time_ms: 3.246
  training_iteration_time_ms: 44364.414
  update_time_ms: 2.518
timesteps_total: 2541000
training_iteration: 154

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8348623853211009
  reward for individual goal_min: 0.5
episode_len_mean: 90.91803278688525
episode_reward_max: 2.0
episode_reward_mean: 1.8032786885245902
episode_reward_min: 1.0
episodes_this_iter: 183
episodes_total: 18502
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8032786885245902
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 56.57887887954712
time_total_s: 7371.059881210327
timers:
  learn_throughput: 373.58
  learn_time_ms: 44167.245
  load_throughput: 4029438.897
  load_time_ms: 4.095
  training_iteration_time_ms: 57971.434
  update_time_ms: 2.838
timesteps_total: 2029500
training_iteration: 123

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25675675675675674
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 176.33
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 14654
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.10552263259888
time_total_s: 7363.59196472168
timers:
  learn_throughput: 548.997
  learn_time_ms: 30054.823
  load_throughput: 5409466.995
  load_time_ms: 3.05
  training_iteration_time_ms: 40270.183
  update_time_ms: 2.452
timesteps_total: 2706000
training_iteration: 164

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2922077922077922
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9662162162162162
  reward for individual goal_min: 0.0
episode_len_mean: 177.81
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 11448
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.40216422080994
time_total_s: 7359.214795827866
timers:
  learn_throughput: 434.899
  learn_time_ms: 37939.834
  load_throughput: 4553986.102
  load_time_ms: 3.623
  training_iteration_time_ms: 50299.147
  update_time_ms: 2.605
timesteps_total: 2277000
training_iteration: 138

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2054794520547945
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.19
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 1.0
episodes_this_iter: 95
episodes_total: 11476
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.3192880153656
time_total_s: 7364.536769866943
timers:
  learn_throughput: 460.386
  learn_time_ms: 35839.485
  load_throughput: 4701016.608
  load_time_ms: 3.51
  training_iteration_time_ms: 47327.876
  update_time_ms: 2.71
timesteps_total: 2161500
training_iteration: 131

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972067039106145
  reward for individual goal_min: 0.5
episode_len_mean: 46.95467422096317
episode_reward_max: 2.0
episode_reward_mean: 1.9971671388101984
episode_reward_min: 1.0
episodes_this_iter: 353
episodes_total: 27900
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9971671388101983
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 58.29795575141907
time_total_s: 7351.096825361252
timers:
  learn_throughput: 362.757
  learn_time_ms: 45485.056
  load_throughput: 3645779.85
  load_time_ms: 4.526
  training_iteration_time_ms: 59057.913
  update_time_ms: 2.972
timesteps_total: 2194500
training_iteration: 133

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2152777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.13
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 12474
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.50203251838684
time_total_s: 7383.643194913864
timers:
  learn_throughput: 446.682
  learn_time_ms: 36939.023
  load_throughput: 4863695.947
  load_time_ms: 3.392
  training_iteration_time_ms: 48781.553
  update_time_ms: 2.546
timesteps_total: 2359500
training_iteration: 143

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 104.46666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.7666666666666666
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9666666666666667
  agent_1: 0.8
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24096385542168675
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8666666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 203.57
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 12937
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.19976329803467
time_total_s: 7398.178908824921
timers:
  learn_throughput: 517.643
  learn_time_ms: 31875.265
  load_throughput: 4770558.562
  load_time_ms: 3.459
  training_iteration_time_ms: 42758.52
  update_time_ms: 2.574
timesteps_total: 2640000
training_iteration: 160

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8166666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 117.9
episode_reward_max: 2.0
episode_reward_mean: 1.75
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9333333333333333
  agent_1: 0.8166666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8157894736842105
  reward for individual goal_min: 0.0
episode_len_mean: 180.64
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 11342
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.18711757659912
time_total_s: 7387.8419325351715
timers:
  learn_throughput: 430.131
  learn_time_ms: 38360.389
  load_throughput: 4625946.9
  load_time_ms: 3.567
  training_iteration_time_ms: 50825.172
  update_time_ms: 2.703
timesteps_total: 2310000
training_iteration: 140

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19mcvnks1j/checkpoint_000140/checkpoint-140
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.53083109919571
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 373
episodes_total: 40269
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.8567328453064
time_total_s: 7325.003511190414
timers:
  learn_throughput: 456.28
  learn_time_ms: 36162.025
  load_throughput: 4441293.767
  load_time_ms: 3.715
  training_iteration_time_ms: 47657.015
  update_time_ms: 2.734
timesteps_total: 2623500
training_iteration: 159

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 159.03846153846155
episode_reward_max: 2.0
episode_reward_mean: 1.4230769230769231
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 11516
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7788461538461539
  agent_1: 0.6442307692307693
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.928069829940796
time_total_s: 7382.632389783859
timers:
  learn_throughput: 452.411
  learn_time_ms: 36471.263
  load_throughput: 4592242.704
  load_time_ms: 3.593
  training_iteration_time_ms: 48278.454
  update_time_ms: 2.726
timesteps_total: 2178000
training_iteration: 132

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2839506172839506
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.9
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 12302
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.96468472480774
time_total_s: 7395.296732902527
timers:
  learn_throughput: 427.546
  learn_time_ms: 38592.316
  load_throughput: 4900790.007
  load_time_ms: 3.367
  training_iteration_time_ms: 50793.894
  update_time_ms: 2.689
timesteps_total: 2277000
training_iteration: 138

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8405172413793104
  reward for individual goal_min: 0.5
episode_len_mean: 79.86057692307692
episode_reward_max: 2.0
episode_reward_mean: 1.8221153846153846
episode_reward_min: 1.0
episodes_this_iter: 208
episodes_total: 26971
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8269230769230769
  agent_1: 0.9951923076923077
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.212459087371826
time_total_s: 7379.787454843521
timers:
  learn_throughput: 450.02
  learn_time_ms: 36665.003
  load_throughput: 4803938.332
  load_time_ms: 3.435
  training_iteration_time_ms: 48016.547
  update_time_ms: 2.579
timesteps_total: 2409000
training_iteration: 146

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30666666666666664
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 159.21904761904761
episode_reward_max: 2.0
episode_reward_mean: 1.457142857142857
episode_reward_min: 0.0
episodes_this_iter: 105
episodes_total: 12573
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7238095238095238
  agent_1: 0.7333333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.78474259376526
time_total_s: 7389.298954486847
timers:
  learn_throughput: 440.44
  learn_time_ms: 37462.543
  load_throughput: 4760680.746
  load_time_ms: 3.466
  training_iteration_time_ms: 49452.352
  update_time_ms: 2.694
timesteps_total: 2359500
training_iteration: 143

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7784810126582279
  reward for individual goal_min: 0.0
episode_len_mean: 191.98
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 12485
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.26499962806702
time_total_s: 7407.239963769913
timers:
  learn_throughput: 496.346
  learn_time_ms: 33242.951
  load_throughput: 5145811.287
  load_time_ms: 3.206
  training_iteration_time_ms: 44428.78
  update_time_ms: 2.523
timesteps_total: 2557500
training_iteration: 155

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2945205479452055
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9928571428571429
  reward for individual goal_min: 0.5
episode_len_mean: 168.58
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 14749
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.63656258583069
time_total_s: 7406.22852730751
timers:
  learn_throughput: 544.096
  learn_time_ms: 30325.534
  load_throughput: 5392396.447
  load_time_ms: 3.06
  training_iteration_time_ms: 40589.363
  update_time_ms: 2.437
timesteps_total: 2722500
training_iteration: 165

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.07
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 11568
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.591442823410034
time_total_s: 7411.128212690353
timers:
  learn_throughput: 461.96
  learn_time_ms: 35717.38
  load_throughput: 4764449.83
  load_time_ms: 3.463
  training_iteration_time_ms: 47081.688
  update_time_ms: 2.71
timesteps_total: 2178000
training_iteration: 132

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2654320987654321
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9714285714285714
  reward for individual goal_min: 0.0
episode_len_mean: 185.76
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 11537
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.19199752807617
time_total_s: 7408.406793355942
timers:
  learn_throughput: 437.498
  learn_time_ms: 37714.451
  load_throughput: 4587524.344
  load_time_ms: 3.597
  training_iteration_time_ms: 50000.984
  update_time_ms: 2.604
timesteps_total: 2293500
training_iteration: 139

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8425925925925926
  reward for individual goal_min: 0.5
episode_len_mean: 83.92857142857143
episode_reward_max: 2.0
episode_reward_mean: 1.8265306122448979
episode_reward_min: 1.0
episodes_this_iter: 196
episodes_total: 18698
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.826530612244898
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 58.95880722999573
time_total_s: 7430.018688440323
timers:
  learn_throughput: 373.899
  learn_time_ms: 44129.562
  load_throughput: 4025150.116
  load_time_ms: 4.099
  training_iteration_time_ms: 57863.833
  update_time_ms: 2.799
timesteps_total: 2046000
training_iteration: 124

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18831168831168832
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8766233766233766
  reward for individual goal_min: 0.0
episode_len_mean: 205.05
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 13018
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.23055124282837
time_total_s: 7441.409460067749
timers:
  learn_throughput: 515.961
  learn_time_ms: 31979.167
  load_throughput: 4744069.811
  load_time_ms: 3.478
  training_iteration_time_ms: 42914.706
  update_time_ms: 2.579
timesteps_total: 2656500
training_iteration: 161

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 185.97
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 12560
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.89397621154785
time_total_s: 7431.537171125412
timers:
  learn_throughput: 445.655
  learn_time_ms: 37024.179
  load_throughput: 4896871.511
  load_time_ms: 3.369
  training_iteration_time_ms: 48842.77
  update_time_ms: 2.688
timesteps_total: 2376000
training_iteration: 144

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36363636363636365
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9925373134328358
  reward for individual goal_min: 0.5
episode_len_mean: 165.3960396039604
episode_reward_max: 2.0
episode_reward_mean: 1.3564356435643565
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 11617
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6732673267326733
  agent_1: 0.6831683168316832
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.610225200653076
time_total_s: 7430.242614984512
timers:
  learn_throughput: 453.566
  learn_time_ms: 36378.372
  load_throughput: 4653975.777
  load_time_ms: 3.545
  training_iteration_time_ms: 48087.047
  update_time_ms: 2.68
timesteps_total: 2194500
training_iteration: 133

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8701298701298701
  reward for individual goal_min: 0.0
episode_len_mean: 183.8
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 11432
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.969865798950195
time_total_s: 7438.811798334122
timers:
  learn_throughput: 430.205
  learn_time_ms: 38353.825
  load_throughput: 4668417.126
  load_time_ms: 3.534
  training_iteration_time_ms: 50698.861
  update_time_ms: 2.702
timesteps_total: 2326500
training_iteration: 141

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 45.11780821917808
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 365
episodes_total: 28265
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 60.57979679107666
time_total_s: 7411.6766221523285
timers:
  learn_throughput: 361.407
  learn_time_ms: 45654.905
  load_throughput: 3636354.924
  load_time_ms: 4.538
  training_iteration_time_ms: 59251.885
  update_time_ms: 2.966
timesteps_total: 2211000
training_iteration: 134

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.95
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.36388140161725
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 371
episodes_total: 40640
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 52.218132972717285
time_total_s: 7377.221644163132
timers:
  learn_throughput: 455.6
  learn_time_ms: 36215.999
  load_throughput: 4433497.931
  load_time_ms: 3.722
  training_iteration_time_ms: 47702.654
  update_time_ms: 2.729
timesteps_total: 2640000
training_iteration: 160

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-172kuwc_58/checkpoint_000160/checkpoint-160
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.13380281690140844
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9888888888888889
  reward for individual goal_min: 0.0
episode_len_mean: 178.11
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 12395
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.48306894302368
time_total_s: 7444.7798018455505
timers:
  learn_throughput: 434.534
  learn_time_ms: 37971.713
  load_throughput: 4961075.858
  load_time_ms: 3.326
  training_iteration_time_ms: 50142.187
  update_time_ms: 2.651
timesteps_total: 2293500
training_iteration: 139

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.861904761904762
  reward for individual goal_min: 0.5
episode_len_mean: 69.93562231759657
episode_reward_max: 2.0
episode_reward_mean: 1.8755364806866952
episode_reward_min: 1.0
episodes_this_iter: 233
episodes_total: 27204
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8755364806866953
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 49.075146436691284
time_total_s: 7428.862601280212
timers:
  learn_throughput: 450.491
  learn_time_ms: 36626.665
  load_throughput: 4817682.979
  load_time_ms: 3.425
  training_iteration_time_ms: 48042.494
  update_time_ms: 2.593
timesteps_total: 2425500
training_iteration: 147

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2708333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8355263157894737
  reward for individual goal_min: 0.0
episode_len_mean: 187.65
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 12573
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.15376329421997
time_total_s: 7453.393727064133
timers:
  learn_throughput: 494.512
  learn_time_ms: 33366.234
  load_throughput: 5164512.436
  load_time_ms: 3.195
  training_iteration_time_ms: 44574.586
  update_time_ms: 2.524
timesteps_total: 2574000
training_iteration: 156

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32098765432098764
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.91
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 12667
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.847275733947754
time_total_s: 7438.146230220795
timers:
  learn_throughput: 441.102
  learn_time_ms: 37406.354
  load_throughput: 4798941.551
  load_time_ms: 3.438
  training_iteration_time_ms: 49389.282
  update_time_ms: 2.681
timesteps_total: 2376000
training_iteration: 144

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2974683544303797
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 174.43
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 14844
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.8092520236969
time_total_s: 7449.037779331207
timers:
  learn_throughput: 538.019
  learn_time_ms: 30668.071
  load_throughput: 5326243.785
  load_time_ms: 3.098
  training_iteration_time_ms: 40983.967
  update_time_ms: 2.416
timesteps_total: 2739000
training_iteration: 166

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2986111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 165.5
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 11667
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.79683756828308
time_total_s: 7460.9250502586365
timers:
  learn_throughput: 458.856
  learn_time_ms: 35958.974
  load_throughput: 4760058.601
  load_time_ms: 3.466
  training_iteration_time_ms: 47371.985
  update_time_ms: 2.7
timesteps_total: 2194500
training_iteration: 133

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8051948051948052
  reward for individual goal_min: 0.0
episode_len_mean: 191.34
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 13106
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.85775065422058
time_total_s: 7485.26721072197
timers:
  learn_throughput: 516.816
  learn_time_ms: 31926.254
  load_throughput: 4737931.374
  load_time_ms: 3.483
  training_iteration_time_ms: 42812.955
  update_time_ms: 2.594
timesteps_total: 2673000
training_iteration: 162

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21830985915492956
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.35
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 12654
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.21806859970093
time_total_s: 7478.755239725113
timers:
  learn_throughput: 447.535
  learn_time_ms: 36868.586
  load_throughput: 4906070.806
  load_time_ms: 3.363
  training_iteration_time_ms: 48628.383
  update_time_ms: 2.682
timesteps_total: 2392500
training_iteration: 145

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8486238532110092
  reward for individual goal_min: 0.5
episode_len_mean: 83.64102564102564
episode_reward_max: 2.0
episode_reward_mean: 1.8307692307692307
episode_reward_min: 1.0
episodes_this_iter: 195
episodes_total: 18893
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9897435897435898
  agent_1: 0.841025641025641
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.853511333465576
time_total_s: 7487.872199773788
timers:
  learn_throughput: 374.094
  learn_time_ms: 44106.556
  load_throughput: 4116391.927
  load_time_ms: 4.008
  training_iteration_time_ms: 57849.335
  update_time_ms: 2.822
timesteps_total: 2062500
training_iteration: 125

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27702702702702703
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9794520547945206
  reward for individual goal_min: 0.5
episode_len_mean: 173.77
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 11710
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.64313793182373
time_total_s: 7478.885752916336
timers:
  learn_throughput: 454.245
  learn_time_ms: 36323.978
  load_throughput: 4658769.169
  load_time_ms: 3.542
  training_iteration_time_ms: 47997.482
  update_time_ms: 2.672
timesteps_total: 2211000
training_iteration: 134

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3466666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8918918918918919
  reward for individual goal_min: 0.0
episode_len_mean: 178.25
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 11524
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.142467975616455
time_total_s: 7486.954266309738
timers:
  learn_throughput: 437.534
  learn_time_ms: 37711.315
  load_throughput: 4649941.948
  load_time_ms: 3.548
  training_iteration_time_ms: 49973.952
  update_time_ms: 2.697
timesteps_total: 2343000
training_iteration: 142

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 100.18333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.6666666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8333333333333334
  agent_1: 0.8333333333333334
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28289473684210525
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9857142857142858
  reward for individual goal_min: 0.0
episode_len_mean: 173.29
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 11630
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.72398519515991
time_total_s: 7472.130778551102
timers:
  learn_throughput: 435.639
  learn_time_ms: 37875.386
  load_throughput: 4591694.268
  load_time_ms: 3.593
  training_iteration_time_ms: 50176.421
  update_time_ms: 2.624
timesteps_total: 2310000
training_iteration: 140

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.42368421052632
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 380
episodes_total: 41020
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.18340253829956
time_total_s: 7425.405046701431
timers:
  learn_throughput: 455.126
  learn_time_ms: 36253.696
  load_throughput: 4412579.604
  load_time_ms: 3.739
  training_iteration_time_ms: 47747.543
  update_time_ms: 2.713
timesteps_total: 2656500
training_iteration: 161

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.98989898989899
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8285714285714286
  reward for individual goal_min: 0.5
episode_len_mean: 81.07843137254902
episode_reward_max: 2.0
episode_reward_mean: 1.8137254901960784
episode_reward_min: 0.0
episodes_this_iter: 204
episodes_total: 27408
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8186274509803921
  agent_1: 0.9950980392156863
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.83798789978027
time_total_s: 7476.700589179993
timers:
  learn_throughput: 451.611
  learn_time_ms: 36535.854
  load_throughput: 4773651.733
  load_time_ms: 3.456
  training_iteration_time_ms: 47906.098
  update_time_ms: 2.636
timesteps_total: 2442000
training_iteration: 148

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.18156424581006
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 358
episodes_total: 28623
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.51837706565857
time_total_s: 7470.194999217987
timers:
  learn_throughput: 361.259
  learn_time_ms: 45673.638
  load_throughput: 3612740.38
  load_time_ms: 4.567
  training_iteration_time_ms: 59214.595
  update_time_ms: 2.928
timesteps_total: 2227500
training_iteration: 135

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33544303797468356
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.782608695652174
  reward for individual goal_min: 0.0
episode_len_mean: 195.73
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 12657
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.18266463279724
time_total_s: 7500.57639169693
timers:
  learn_throughput: 491.304
  learn_time_ms: 33584.078
  load_throughput: 5169566.153
  load_time_ms: 3.192
  training_iteration_time_ms: 44835.59
  update_time_ms: 2.536
timesteps_total: 2590500
training_iteration: 157

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.310126582278481
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.53
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 14939
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.45018744468689
time_total_s: 7492.487966775894
timers:
  learn_throughput: 534.675
  learn_time_ms: 30859.853
  load_throughput: 5313607.334
  load_time_ms: 3.105
  training_iteration_time_ms: 41256.443
  update_time_ms: 2.4
timesteps_total: 2755500
training_iteration: 167

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.7
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 12764
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.393924713134766
time_total_s: 7488.540154933929
timers:
  learn_throughput: 441.46
  learn_time_ms: 37375.971
  load_throughput: 4785733.668
  load_time_ms: 3.448
  training_iteration_time_ms: 49288.074
  update_time_ms: 2.659
timesteps_total: 2392500
training_iteration: 145

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95
  reward for individual goal_min: 0.5
episode_len_mean: 90.13333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.8166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9
  agent_1: 0.9166666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9855072463768116
  reward for individual goal_min: 0.5
episode_len_mean: 190.32
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 12480
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.76057291030884
time_total_s: 7508.540374755859
timers:
  learn_throughput: 430.746
  learn_time_ms: 38305.663
  load_throughput: 4952236.256
  load_time_ms: 3.332
  training_iteration_time_ms: 50468.202
  update_time_ms: 2.639
timesteps_total: 2310000
training_iteration: 140

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7857142857142857
  reward for individual goal_min: 0.0
episode_len_mean: 212.58
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 13182
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.42
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.88622236251831
time_total_s: 7530.153433084488
timers:
  learn_throughput: 509.343
  learn_time_ms: 32394.665
  load_throughput: 4742086.885
  load_time_ms: 3.479
  training_iteration_time_ms: 43388.297
  update_time_ms: 2.849
timesteps_total: 2689500
training_iteration: 163

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1962025316455696
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9814814814814815
  reward for individual goal_min: 0.0
episode_len_mean: 190.92
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 11751
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.51825571060181
time_total_s: 7508.443305969238
timers:
  learn_throughput: 459.2
  learn_time_ms: 35932.032
  load_throughput: 4683518.831
  load_time_ms: 3.523
  training_iteration_time_ms: 47364.571
  update_time_ms: 2.702
timesteps_total: 2211000
training_iteration: 134

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2716049382716049
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 185.34
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 12742
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.169835567474365
time_total_s: 7526.925075292587
timers:
  learn_throughput: 448.398
  learn_time_ms: 36797.701
  load_throughput: 4905375.314
  load_time_ms: 3.364
  training_iteration_time_ms: 48534.631
  update_time_ms: 2.722
timesteps_total: 2409000
training_iteration: 146

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3246753246753247
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 165.9009900990099
episode_reward_max: 2.0
episode_reward_mean: 1.3564356435643565
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 11811
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6831683168316832
  agent_1: 0.6732673267326733
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.332024812698364
time_total_s: 7528.217777729034
timers:
  learn_throughput: 452.582
  learn_time_ms: 36457.447
  load_throughput: 4702390.129
  load_time_ms: 3.509
  training_iteration_time_ms: 48244.31
  update_time_ms: 2.705
timesteps_total: 2227500
training_iteration: 135

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31690140845070425
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9671052631578947
  reward for individual goal_min: 0.0
episode_len_mean: 171.25
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 11726
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.62852430343628
time_total_s: 7521.759302854538
timers:
  learn_throughput: 436.447
  learn_time_ms: 37805.284
  load_throughput: 4518337.24
  load_time_ms: 3.652
  training_iteration_time_ms: 50031.597
  update_time_ms: 2.642
timesteps_total: 2326500
training_iteration: 141

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34210526315789475
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8787878787878788
  reward for individual goal_min: 0.0
episode_len_mean: 182.62
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 11617
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.02803182601929
time_total_s: 7538.982298135757
timers:
  learn_throughput: 436.836
  learn_time_ms: 37771.61
  load_throughput: 4627864.814
  load_time_ms: 3.565
  training_iteration_time_ms: 50071.48
  update_time_ms: 2.695
timesteps_total: 2359500
training_iteration: 143

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971098265895953
  reward for individual goal_min: 0.5
episode_len_mean: 44.997275204359674
episode_reward_max: 2.0
episode_reward_mean: 1.997275204359673
episode_reward_min: 1.0
episodes_this_iter: 367
episodes_total: 41387
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.997275204359673
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.1559534072876
time_total_s: 7473.561000108719
timers:
  learn_throughput: 454.37
  learn_time_ms: 36314.009
  load_throughput: 4398473.125
  load_time_ms: 3.751
  training_iteration_time_ms: 47806.315
  update_time_ms: 2.392
timesteps_total: 2673000
training_iteration: 162

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8125
  reward for individual goal_min: 0.5
episode_len_mean: 88.63020833333333
episode_reward_max: 2.0
episode_reward_mean: 1.8125
episode_reward_min: 1.0
episodes_this_iter: 192
episodes_total: 19085
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.984375
  agent_1: 0.828125
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.83539652824402
time_total_s: 7546.7075963020325
timers:
  learn_throughput: 373.109
  learn_time_ms: 44222.961
  load_throughput: 4149812.975
  load_time_ms: 3.976
  training_iteration_time_ms: 57912.273
  update_time_ms: 2.783
timesteps_total: 2079000
training_iteration: 126

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21518987341772153
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 186.98
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 15026
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.82012391090393
time_total_s: 7536.308090686798
timers:
  learn_throughput: 531.972
  learn_time_ms: 31016.69
  load_throughput: 5316301.344
  load_time_ms: 3.104
  training_iteration_time_ms: 41479.677
  update_time_ms: 2.405
timesteps_total: 2772000
training_iteration: 168

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8689320388349514
  reward for individual goal_min: 0.5
episode_len_mean: 64.11811023622047
episode_reward_max: 2.0
episode_reward_mean: 1.8937007874015748
episode_reward_min: 1.0
episodes_this_iter: 254
episodes_total: 27662
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8937007874015748
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 51.070204973220825
time_total_s: 7527.7707941532135
timers:
  learn_throughput: 446.668
  learn_time_ms: 36940.216
  load_throughput: 4747910.346
  load_time_ms: 3.475
  training_iteration_time_ms: 48340.458
  update_time_ms: 2.648
timesteps_total: 2458500
training_iteration: 149

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8205128205128205
  reward for individual goal_min: 0.0
episode_len_mean: 186.44
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 12745
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.87986874580383
time_total_s: 7546.456260442734
timers:
  learn_throughput: 490.832
  learn_time_ms: 33616.359
  load_throughput: 5108774.665
  load_time_ms: 3.23
  training_iteration_time_ms: 44914.112
  update_time_ms: 2.55
timesteps_total: 2607000
training_iteration: 158

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 163.16
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 12864
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.09657144546509
time_total_s: 7536.6367263793945
timers:
  learn_throughput: 443.038
  learn_time_ms: 37242.857
  load_throughput: 4771940.121
  load_time_ms: 3.458
  training_iteration_time_ms: 49037.745
  update_time_ms: 2.635
timesteps_total: 2409000
training_iteration: 146

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24358974358974358
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9726027397260274
  reward for individual goal_min: 0.0
episode_len_mean: 186.33
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 12570
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.46682548522949
time_total_s: 7558.007200241089
timers:
  learn_throughput: 431.606
  learn_time_ms: 38229.327
  load_throughput: 4994948.9
  load_time_ms: 3.303
  training_iteration_time_ms: 50404.529
  update_time_ms: 2.631
timesteps_total: 2326500
training_iteration: 141

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.11204481792717
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 357
episodes_total: 28980
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.72113084793091
time_total_s: 7528.916130065918
timers:
  learn_throughput: 360.78
  learn_time_ms: 45734.232
  load_throughput: 3586751.732
  load_time_ms: 4.6
  training_iteration_time_ms: 59270.877
  update_time_ms: 2.921
timesteps_total: 2244000
training_iteration: 136

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35714285714285715
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8666666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 173.21
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 13277
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.134135723114014
time_total_s: 7575.287568807602
timers:
  learn_throughput: 506.631
  learn_time_ms: 32568.112
  load_throughput: 4706547.517
  load_time_ms: 3.506
  training_iteration_time_ms: 43539.283
  update_time_ms: 2.856
timesteps_total: 2706000
training_iteration: 164

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33116883116883117
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.21
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 11849
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.73887658119202
time_total_s: 7556.18218255043
timers:
  learn_throughput: 460.113
  learn_time_ms: 35860.764
  load_throughput: 4688246.261
  load_time_ms: 3.519
  training_iteration_time_ms: 47315.989
  update_time_ms: 2.662
timesteps_total: 2227500
training_iteration: 135

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2792207792207792
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9930555555555556
  reward for individual goal_min: 0.5
episode_len_mean: 183.37
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 12832
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.07113432884216
time_total_s: 7573.996209621429
timers:
  learn_throughput: 451.659
  learn_time_ms: 36532.003
  load_throughput: 4918798.269
  load_time_ms: 3.354
  training_iteration_time_ms: 48178.578
  update_time_ms: 2.728
timesteps_total: 2425500
training_iteration: 147

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30434782608695654
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 160.18
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 11911
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.609619140625
time_total_s: 7576.827396869659
timers:
  learn_throughput: 452.758
  learn_time_ms: 36443.342
  load_throughput: 4694861.609
  load_time_ms: 3.514
  training_iteration_time_ms: 48153.103
  update_time_ms: 2.71
timesteps_total: 2244000
training_iteration: 136

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9
  reward for individual goal_min: 0.0
episode_len_mean: 196.84
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 11701
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.614803314208984
time_total_s: 7587.597101449966
timers:
  learn_throughput: 439.018
  learn_time_ms: 37583.846
  load_throughput: 4618445.214
  load_time_ms: 3.573
  training_iteration_time_ms: 49846.544
  update_time_ms: 2.684
timesteps_total: 2376000
training_iteration: 144

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2077922077922078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9805194805194806
  reward for individual goal_min: 0.0
episode_len_mean: 181.23
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 11815
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.18351864814758
time_total_s: 7571.942821502686
timers:
  learn_throughput: 437.767
  learn_time_ms: 37691.321
  load_throughput: 4526641.812
  load_time_ms: 3.645
  training_iteration_time_ms: 49907.445
  update_time_ms: 2.64
timesteps_total: 2343000
training_iteration: 142

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28378378378378377
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 179.69
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 15119
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.0435585975647
time_total_s: 7579.351649284363
timers:
  learn_throughput: 528.715
  learn_time_ms: 31207.742
  load_throughput: 5278348.905
  load_time_ms: 3.126
  training_iteration_time_ms: 41665.883
  update_time_ms: 2.399
timesteps_total: 2788500
training_iteration: 169

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971751412429378
  reward for individual goal_min: 0.5
episode_len_mean: 45.09315068493151
episode_reward_max: 2.0
episode_reward_mean: 1.9972602739726026
episode_reward_min: 1.0
episodes_this_iter: 365
episodes_total: 41752
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9972602739726028
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 48.08407926559448
time_total_s: 7521.645079374313
timers:
  learn_throughput: 455.121
  learn_time_ms: 36254.133
  load_throughput: 4398556.992
  load_time_ms: 3.751
  training_iteration_time_ms: 47741.124
  update_time_ms: 2.365
timesteps_total: 2689500
training_iteration: 163

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28205128205128205
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8525641025641025
  reward for individual goal_min: 0.0
episode_len_mean: 193.47
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 12831
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.18589425086975
time_total_s: 7591.6421546936035
timers:
  learn_throughput: 491.276
  learn_time_ms: 33586.015
  load_throughput: 5106399.858
  load_time_ms: 3.231
  training_iteration_time_ms: 44882.729
  update_time_ms: 2.542
timesteps_total: 2623500
training_iteration: 159

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8252427184466019
  reward for individual goal_min: 0.5
episode_len_mean: 78.85714285714286
episode_reward_max: 2.0
episode_reward_mean: 1.8285714285714285
episode_reward_min: 1.0
episodes_this_iter: 210
episodes_total: 27872
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8333333333333334
  agent_1: 0.9952380952380953
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.830116987228394
time_total_s: 7575.600911140442
timers:
  learn_throughput: 447.506
  learn_time_ms: 36871.039
  load_throughput: 4684628.444
  load_time_ms: 3.522
  training_iteration_time_ms: 48311.491
  update_time_ms: 2.662
timesteps_total: 2475000
training_iteration: 150

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8691588785046729
  reward for individual goal_min: 0.5
episode_len_mean: 74.78703703703704
episode_reward_max: 2.0
episode_reward_mean: 1.8703703703703705
episode_reward_min: 1.0
episodes_this_iter: 216
episodes_total: 19301
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8703703703703703
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 59.15465712547302
time_total_s: 7605.8622534275055
timers:
  learn_throughput: 372.402
  learn_time_ms: 44306.918
  load_throughput: 4263002.938
  load_time_ms: 3.871
  training_iteration_time_ms: 57967.599
  update_time_ms: 2.803
timesteps_total: 2095500
training_iteration: 127

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9929577464788732
  reward for individual goal_min: 0.5
episode_len_mean: 183.52
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 12955
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.89628219604492
time_total_s: 7584.533008575439
timers:
  learn_throughput: 445.437
  learn_time_ms: 37042.251
  load_throughput: 4759142.054
  load_time_ms: 3.467
  training_iteration_time_ms: 48750.368
  update_time_ms: 2.633
timesteps_total: 2425500
training_iteration: 147

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24675324675324675
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8782051282051282
  reward for individual goal_min: 0.0
episode_len_mean: 194.32
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 13363
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.220945596694946
time_total_s: 7618.508514404297
timers:
  learn_throughput: 506.946
  learn_time_ms: 32547.846
  load_throughput: 4705299.529
  load_time_ms: 3.507
  training_iteration_time_ms: 43483.419
  update_time_ms: 2.878
timesteps_total: 2722500
training_iteration: 165

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28846153846153844
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.14
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 12663
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.945451974868774
time_total_s: 7610.952652215958
timers:
  learn_throughput: 426.617
  learn_time_ms: 38676.357
  load_throughput: 4976057.752
  load_time_ms: 3.316
  training_iteration_time_ms: 50916.665
  update_time_ms: 2.598
timesteps_total: 2343000
training_iteration: 142

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35135135135135137
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 158.63106796116506
episode_reward_max: 2.0
episode_reward_mean: 1.4271844660194175
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 11952
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7378640776699029
  agent_1: 0.6893203883495146
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.666765451431274
time_total_s: 7605.848948001862
timers:
  learn_throughput: 457.254
  learn_time_ms: 36084.938
  load_throughput: 4734009.809
  load_time_ms: 3.485
  training_iteration_time_ms: 47492.411
  update_time_ms: 2.639
timesteps_total: 2244000
training_iteration: 136

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 47.47838616714697
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 347
episodes_total: 29327
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.873783111572266
time_total_s: 7587.78991317749
timers:
  learn_throughput: 361.844
  learn_time_ms: 45599.72
  load_throughput: 3545117.742
  load_time_ms: 4.654
  training_iteration_time_ms: 59110.44
  update_time_ms: 2.913
timesteps_total: 2260500
training_iteration: 137

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24285714285714285
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 165.62
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 12932
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.54437017440796
time_total_s: 7621.540579795837
timers:
  learn_throughput: 451.594
  learn_time_ms: 36537.223
  load_throughput: 4913176.106
  load_time_ms: 3.358
  training_iteration_time_ms: 48097.326
  update_time_ms: 2.713
timesteps_total: 2442000
training_iteration: 148

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24675324675324675
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 184.4
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 15209
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.5966854095459
time_total_s: 7623.948334693909
timers:
  learn_throughput: 523.558
  learn_time_ms: 31515.115
  load_throughput: 5257257.804
  load_time_ms: 3.139
  training_iteration_time_ms: 42073.659
  update_time_ms: 2.411
timesteps_total: 2805000
training_iteration: 170

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3116883116883117
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9873417721518988
  reward for individual goal_min: 0.0
episode_len_mean: 164.36190476190475
episode_reward_max: 2.0
episode_reward_mean: 1.3714285714285714
episode_reward_min: 0.0
episodes_this_iter: 105
episodes_total: 12016
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.638095238095238
  agent_1: 0.7333333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.07545828819275
time_total_s: 7626.902855157852
timers:
  learn_throughput: 448.755
  learn_time_ms: 36768.398
  load_throughput: 4729868.436
  load_time_ms: 3.488
  training_iteration_time_ms: 48450.781
  update_time_ms: 2.71
timesteps_total: 2260500
training_iteration: 137

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2608695652173913
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8846153846153846
  reward for individual goal_min: 0.0
episode_len_mean: 190.94
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 11787
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.21756362915039
time_total_s: 7635.814665079117
timers:
  learn_throughput: 440.349
  learn_time_ms: 37470.258
  load_throughput: 4588041.368
  load_time_ms: 3.596
  training_iteration_time_ms: 49684.689
  update_time_ms: 2.694
timesteps_total: 2392500
training_iteration: 145

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20238095238095238
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 196.56
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 11902
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.81927132606506
time_total_s: 7622.762092828751
timers:
  learn_throughput: 440.55
  learn_time_ms: 37453.189
  load_throughput: 4599934.596
  load_time_ms: 3.587
  training_iteration_time_ms: 49615.168
  update_time_ms: 2.656
timesteps_total: 2359500
training_iteration: 143

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.16
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 375
episodes_total: 42127
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.186439037323
time_total_s: 7569.831518411636
timers:
  learn_throughput: 454.983
  learn_time_ms: 36265.102
  load_throughput: 4389684.884
  load_time_ms: 3.759
  training_iteration_time_ms: 47778.242
  update_time_ms: 2.354
timesteps_total: 2706000
training_iteration: 164

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.860655737704918
  reward for individual goal_min: 0.5
episode_len_mean: 73.73127753303964
episode_reward_max: 2.0
episode_reward_mean: 1.8502202643171806
episode_reward_min: 1.0
episodes_this_iter: 227
episodes_total: 28099
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8590308370044053
  agent_1: 0.9911894273127754
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.856626987457275
time_total_s: 7624.457538127899
timers:
  learn_throughput: 449.229
  learn_time_ms: 36729.635
  load_throughput: 4662409.96
  load_time_ms: 3.539
  training_iteration_time_ms: 48183.953
  update_time_ms: 2.649
timesteps_total: 2491500
training_iteration: 151

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 116.4
episode_reward_max: 2.0
episode_reward_mean: 1.7833333333333334
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9
  agent_1: 0.8833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8467741935483871
  reward for individual goal_min: 0.0
episode_len_mean: 191.49
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 12917
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.565231800079346
time_total_s: 7648.207386493683
timers:
  learn_throughput: 490.269
  learn_time_ms: 33654.968
  load_throughput: 5117539.062
  load_time_ms: 3.224
  training_iteration_time_ms: 44918.879
  update_time_ms: 2.511
timesteps_total: 2640000
training_iteration: 160

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19istq_qwi/checkpoint_000160/checkpoint-160
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2974683544303797
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7857142857142857
  reward for individual goal_min: 0.0
episode_len_mean: 187.43
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 13453
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.00759696960449
time_total_s: 7663.516111373901
timers:
  learn_throughput: 500.361
  learn_time_ms: 32976.214
  load_throughput: 4714691.664
  load_time_ms: 3.5
  training_iteration_time_ms: 43995.27
  update_time_ms: 2.86
timesteps_total: 2739000
training_iteration: 166

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 152.54128440366972
episode_reward_max: 2.0
episode_reward_mean: 1.4587155963302751
episode_reward_min: 0.0
episodes_this_iter: 109
episodes_total: 13064
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7155963302752294
  agent_1: 0.7431192660550459
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.01440691947937
time_total_s: 7632.547415494919
timers:
  learn_throughput: 448.133
  learn_time_ms: 36819.404
  load_throughput: 4769013.479
  load_time_ms: 3.46
  training_iteration_time_ms: 48459.69
  update_time_ms: 2.616
timesteps_total: 2442000
training_iteration: 148

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8010204081632653
  reward for individual goal_min: 0.5
episode_len_mean: 91.25842696629213
episode_reward_max: 2.0
episode_reward_mean: 1.7808988764044944
episode_reward_min: 1.0
episodes_this_iter: 178
episodes_total: 19479
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9943820224719101
  agent_1: 0.7865168539325843
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.50310564041138
time_total_s: 7663.365359067917
timers:
  learn_throughput: 371.97
  learn_time_ms: 44358.459
  load_throughput: 4296055.422
  load_time_ms: 3.841
  training_iteration_time_ms: 58051.416
  update_time_ms: 2.874
timesteps_total: 2112000
training_iteration: 128

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.273972602739726
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.57
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 12763
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.26412582397461
time_total_s: 7661.216778039932
timers:
  learn_throughput: 429.266
  learn_time_ms: 38437.683
  load_throughput: 4959653.714
  load_time_ms: 3.327
  training_iteration_time_ms: 50582.958
  update_time_ms: 2.605
timesteps_total: 2359500
training_iteration: 143

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30714285714285716
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 162.05
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 12052
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.48699879646301
time_total_s: 7656.335946798325
timers:
  learn_throughput: 454.285
  learn_time_ms: 36320.777
  load_throughput: 4747226.407
  load_time_ms: 3.476
  training_iteration_time_ms: 47680.633
  update_time_ms: 2.655
timesteps_total: 2260500
training_iteration: 137

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27710843373493976
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.83
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 13018
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.059855699539185
time_total_s: 7671.600435495377
timers:
  learn_throughput: 449.482
  learn_time_ms: 36708.882
  load_throughput: 4857516.986
  load_time_ms: 3.397
  training_iteration_time_ms: 48315.736
  update_time_ms: 2.757
timesteps_total: 2458500
training_iteration: 149

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31756756756756754
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 165.26
episode_reward_max: 2.0
episode_reward_mean: 1.41
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 15308
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.96680784225464
time_total_s: 7665.915142536163
timers:
  learn_throughput: 520.308
  learn_time_ms: 31711.956
  load_throughput: 5216599.405
  load_time_ms: 3.163
  training_iteration_time_ms: 42290.499
  update_time_ms: 2.41
timesteps_total: 2821500
training_iteration: 171

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.616216216216216
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 370
episodes_total: 29697
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.26946830749512
time_total_s: 7647.059381484985
timers:
  learn_throughput: 361.719
  learn_time_ms: 45615.458
  load_throughput: 3533370.91
  load_time_ms: 4.67
  training_iteration_time_ms: 59122.252
  update_time_ms: 2.925
timesteps_total: 2277000
training_iteration: 138

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.189873417721519
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 188.09
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 12099
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.42304253578186
time_total_s: 7675.325897693634
timers:
  learn_throughput: 448.855
  learn_time_ms: 36760.219
  load_throughput: 4754956.611
  load_time_ms: 3.47
  training_iteration_time_ms: 48471.258
  update_time_ms: 2.708
timesteps_total: 2277000
training_iteration: 138

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9133333333333333
  reward for individual goal_min: 0.0
episode_len_mean: 191.37
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 11875
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.439821004867554
time_total_s: 7684.254486083984
timers:
  learn_throughput: 441.13
  learn_time_ms: 37403.981
  load_throughput: 4558996.054
  load_time_ms: 3.619
  training_iteration_time_ms: 49561.262
  update_time_ms: 2.662
timesteps_total: 2409000
training_iteration: 146

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2077922077922078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 192.59
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 11987
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.22248077392578
time_total_s: 7670.984573602676
timers:
  learn_throughput: 443.162
  learn_time_ms: 37232.426
  load_throughput: 4610353.474
  load_time_ms: 3.579
  training_iteration_time_ms: 49396.971
  update_time_ms: 2.63
timesteps_total: 2376000
training_iteration: 144

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.53367875647668
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 386
episodes_total: 42513
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.75284385681152
time_total_s: 7617.584362268448
timers:
  learn_throughput: 455.304
  learn_time_ms: 36239.503
  load_throughput: 4392721.918
  load_time_ms: 3.756
  training_iteration_time_ms: 47738.445
  update_time_ms: 2.366
timesteps_total: 2722500
training_iteration: 165

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8041237113402062
  reward for individual goal_min: 0.5
episode_len_mean: 79.32057416267942
episode_reward_max: 2.0
episode_reward_mean: 1.8181818181818181
episode_reward_min: 1.0
episodes_this_iter: 209
episodes_total: 28308
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8229665071770335
  agent_1: 0.9952153110047847
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.75783443450928
time_total_s: 7673.215372562408
timers:
  learn_throughput: 447.471
  learn_time_ms: 36873.916
  load_throughput: 4658643.726
  load_time_ms: 3.542
  training_iteration_time_ms: 48350.678
  update_time_ms: 2.643
timesteps_total: 2508000
training_iteration: 152

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.825
  reward for individual goal_min: 0.0
episode_len_mean: 205.24
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 13533
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.4
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.195048332214355
time_total_s: 7706.711159706116
timers:
  learn_throughput: 497.046
  learn_time_ms: 33196.099
  load_throughput: 4644917.278
  load_time_ms: 3.552
  training_iteration_time_ms: 44221.242
  update_time_ms: 2.856
timesteps_total: 2755500
training_iteration: 167

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2682926829268293
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8571428571428571
  reward for individual goal_min: 0.0
episode_len_mean: 198.22
episode_reward_max: 2.0
episode_reward_mean: 1.06
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 13000
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.55254626274109
time_total_s: 7696.759932756424
timers:
  learn_throughput: 483.857
  learn_time_ms: 34100.953
  load_throughput: 5082138.131
  load_time_ms: 3.247
  training_iteration_time_ms: 45427.99
  update_time_ms: 2.492
timesteps_total: 2656500
training_iteration: 161

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30666666666666664
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 163.57
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 13164
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.77
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.61479640007019
time_total_s: 7681.162211894989
timers:
  learn_throughput: 448.38
  learn_time_ms: 36799.103
  load_throughput: 4805472.763
  load_time_ms: 3.434
  training_iteration_time_ms: 48416.556
  update_time_ms: 2.613
timesteps_total: 2458500
training_iteration: 149

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2465753424657534
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9859154929577465
  reward for individual goal_min: 0.0
episode_len_mean: 172.08
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 12858
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.086604833602905
time_total_s: 7712.303382873535
timers:
  learn_throughput: 430.415
  learn_time_ms: 38335.088
  load_throughput: 4819796.778
  load_time_ms: 3.423
  training_iteration_time_ms: 50451.272
  update_time_ms: 2.612
timesteps_total: 2376000
training_iteration: 144

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8118811881188119
  reward for individual goal_min: 0.5
episode_len_mean: 86.35384615384615
episode_reward_max: 2.0
episode_reward_mean: 1.8051282051282052
episode_reward_min: 1.0
episodes_this_iter: 195
episodes_total: 19674
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9846153846153847
  agent_1: 0.8205128205128205
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.2294180393219
time_total_s: 7722.594777107239
timers:
  learn_throughput: 371.662
  learn_time_ms: 44395.152
  load_throughput: 4296828.943
  load_time_ms: 3.84
  training_iteration_time_ms: 58128.444
  update_time_ms: 2.853
timesteps_total: 2128500
training_iteration: 129

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2866666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.972972972972973
  reward for individual goal_min: 0.0
episode_len_mean: 163.96116504854368
episode_reward_max: 2.0
episode_reward_mean: 1.3398058252427185
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 12155
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6796116504854369
  agent_1: 0.6601941747572816
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.00450944900513
time_total_s: 7707.34045624733
timers:
  learn_throughput: 451.799
  learn_time_ms: 36520.647
  load_throughput: 4751365.624
  load_time_ms: 3.473
  training_iteration_time_ms: 47982.048
  update_time_ms: 2.648
timesteps_total: 2277000
training_iteration: 138

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 154.09345794392524
episode_reward_max: 2.0
episode_reward_mean: 1.4579439252336448
episode_reward_min: 0.0
episodes_this_iter: 107
episodes_total: 15415
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7757009345794392
  agent_1: 0.6822429906542056
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.926459074020386
time_total_s: 7709.841601610184
timers:
  learn_throughput: 517.027
  learn_time_ms: 31913.214
  load_throughput: 5192255.509
  load_time_ms: 3.178
  training_iteration_time_ms: 42577.156
  update_time_ms: 2.406
timesteps_total: 2838000
training_iteration: 172

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 163.87
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 13118
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.00418663024902
time_total_s: 7719.604622125626
timers:
  learn_throughput: 449.981
  learn_time_ms: 36668.248
  load_throughput: 4783781.901
  load_time_ms: 3.449
  training_iteration_time_ms: 48213.245
  update_time_ms: 2.756
timesteps_total: 2475000
training_iteration: 150

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8974358974358975
  reward for individual goal_min: 0.0
episode_len_mean: 192.45
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 11961
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.884499311447144
time_total_s: 7733.1389853954315
timers:
  learn_throughput: 441.833
  learn_time_ms: 37344.393
  load_throughput: 4556054.747
  load_time_ms: 3.622
  training_iteration_time_ms: 49438.687
  update_time_ms: 2.69
timesteps_total: 2425500
training_iteration: 147

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35542168674698793
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.971830985915493
  reward for individual goal_min: 0.0
episode_len_mean: 169.56435643564356
episode_reward_max: 2.0
episode_reward_mean: 1.316831683168317
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 12200
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6732673267326733
  agent_1: 0.6435643564356436
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.39063358306885
time_total_s: 7725.716531276703
timers:
  learn_throughput: 448.066
  learn_time_ms: 36824.958
  load_throughput: 4767633.613
  load_time_ms: 3.461
  training_iteration_time_ms: 48554.197
  update_time_ms: 2.7
timesteps_total: 2293500
training_iteration: 139

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.57368421052632
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 380
episodes_total: 42893
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.437411308288574
time_total_s: 7665.021773576736
timers:
  learn_throughput: 455.672
  learn_time_ms: 36210.234
  load_throughput: 4342338.259
  load_time_ms: 3.8
  training_iteration_time_ms: 47692.917
  update_time_ms: 2.369
timesteps_total: 2739000
training_iteration: 166

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18181818181818182
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9819277108433735
  reward for individual goal_min: 0.0
episode_len_mean: 165.19
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 12087
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.60486316680908
time_total_s: 7721.5894367694855
timers:
  learn_throughput: 442.85
  learn_time_ms: 37258.652
  load_throughput: 3273498.602
  load_time_ms: 5.04
  training_iteration_time_ms: 49473.021
  update_time_ms: 2.62
timesteps_total: 2392500
training_iteration: 145

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9970588235294118
  reward for individual goal_min: 0.5
episode_len_mean: 46.51685393258427
episode_reward_max: 2.0
episode_reward_mean: 1.997191011235955
episode_reward_min: 1.0
episodes_this_iter: 356
episodes_total: 30053
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9971910112359551
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 59.18761444091797
time_total_s: 7706.246995925903
timers:
  learn_throughput: 362.809
  learn_time_ms: 45478.494
  load_throughput: 3557582.905
  load_time_ms: 4.638
  training_iteration_time_ms: 59006.176
  update_time_ms: 2.952
timesteps_total: 2293500
training_iteration: 139

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2692307692307692
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7631578947368421
  reward for individual goal_min: 0.0
episode_len_mean: 203.74
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 13616
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.40155076980591
time_total_s: 7751.112710475922
timers:
  learn_throughput: 497.5
  learn_time_ms: 33165.812
  load_throughput: 4655040.123
  load_time_ms: 3.545
  training_iteration_time_ms: 44180.165
  update_time_ms: 2.811
timesteps_total: 2772000
training_iteration: 168

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8558558558558559
  reward for individual goal_min: 0.5
episode_len_mean: 69.39830508474576
episode_reward_max: 2.0
episode_reward_mean: 1.8644067796610169
episode_reward_min: 1.0
episodes_this_iter: 236
episodes_total: 28544
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8686440677966102
  agent_1: 0.9957627118644068
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.43046045303345
time_total_s: 7722.645833015442
timers:
  learn_throughput: 446.938
  learn_time_ms: 36917.87
  load_throughput: 4660086.729
  load_time_ms: 3.541
  training_iteration_time_ms: 48386.269
  update_time_ms: 2.639
timesteps_total: 2524500
training_iteration: 153

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2839506172839506
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7014925373134329
  reward for individual goal_min: 0.0
episode_len_mean: 211.49
episode_reward_max: 2.0
episode_reward_mean: 0.98
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 13079
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.48
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.06613898277283
time_total_s: 7744.826071739197
timers:
  learn_throughput: 480.792
  learn_time_ms: 34318.376
  load_throughput: 5061620.309
  load_time_ms: 3.26
  training_iteration_time_ms: 45699.269
  update_time_ms: 2.496
timesteps_total: 2673000
training_iteration: 162

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3227848101265823
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 164.42
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 13263
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.07150912284851
time_total_s: 7730.2337210178375
timers:
  learn_throughput: 449.558
  learn_time_ms: 36702.711
  load_throughput: 4755021.952
  load_time_ms: 3.47
  training_iteration_time_ms: 48351.145
  update_time_ms: 2.587
timesteps_total: 2475000
training_iteration: 150

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2631578947368421
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9857142857142858
  reward for individual goal_min: 0.5
episode_len_mean: 185.49
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 15498
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.82644176483154
time_total_s: 7753.668043375015
timers:
  learn_throughput: 512.691
  learn_time_ms: 32183.097
  load_throughput: 5130399.87
  load_time_ms: 3.216
  training_iteration_time_ms: 42981.623
  update_time_ms: 2.405
timesteps_total: 2854500
training_iteration: 173

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2222222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 189.69
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 12946
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.817877531051636
time_total_s: 7763.121260404587
timers:
  learn_throughput: 428.677
  learn_time_ms: 38490.52
  load_throughput: 4826149.319
  load_time_ms: 3.419
  training_iteration_time_ms: 50604.059
  update_time_ms: 2.624
timesteps_total: 2392500
training_iteration: 145

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2911392405063291
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.96
  reward for individual goal_min: 0.0
episode_len_mean: 177.24
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 12244
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.35644459724426
time_total_s: 7756.696900844574
timers:
  learn_throughput: 449.888
  learn_time_ms: 36675.828
  load_throughput: 4715687.565
  load_time_ms: 3.499
  training_iteration_time_ms: 48116.482
  update_time_ms: 2.638
timesteps_total: 2293500
training_iteration: 139

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2708333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 175.11
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 13209
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.36653113365173
time_total_s: 7767.971153259277
timers:
  learn_throughput: 450.694
  learn_time_ms: 36610.21
  load_throughput: 4795981.705
  load_time_ms: 3.44
  training_iteration_time_ms: 48094.128
  update_time_ms: 2.785
timesteps_total: 2491500
training_iteration: 151

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8434343434343434
  reward for individual goal_min: 0.5
episode_len_mean: 77.35680751173709
episode_reward_max: 2.0
episode_reward_mean: 1.8544600938967135
episode_reward_min: 1.0
episodes_this_iter: 213
episodes_total: 19887
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8544600938967136
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 60.44894003868103
time_total_s: 7783.04371714592
timers:
  learn_throughput: 369.753
  learn_time_ms: 44624.396
  load_throughput: 4337983.264
  load_time_ms: 3.804
  training_iteration_time_ms: 58387.861
  update_time_ms: 2.904
timesteps_total: 2145000
training_iteration: 130

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2948717948717949
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8636363636363636
  reward for individual goal_min: 0.0
episode_len_mean: 193.48
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 12048
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.3253014087677
time_total_s: 7781.464286804199
timers:
  learn_throughput: 441.678
  learn_time_ms: 37357.544
  load_throughput: 4518779.774
  load_time_ms: 3.651
  training_iteration_time_ms: 49451.149
  update_time_ms: 2.702
timesteps_total: 2442000
training_iteration: 148

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.275
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8461538461538461
  reward for individual goal_min: 0.0
episode_len_mean: 205.68
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 13696
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.21803641319275
time_total_s: 7796.330746889114
timers:
  learn_throughput: 495.656
  learn_time_ms: 33289.239
  load_throughput: 4689961.914
  load_time_ms: 3.518
  training_iteration_time_ms: 44282.474
  update_time_ms: 2.809
timesteps_total: 2788500
training_iteration: 169

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9948979591836735
  reward for individual goal_min: 0.5
episode_len_mean: 44.524324324324326
episode_reward_max: 2.0
episode_reward_mean: 1.9945945945945946
episode_reward_min: 1.0
episodes_this_iter: 370
episodes_total: 43263
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9945945945945946
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 47.27839183807373
time_total_s: 7712.30016541481
timers:
  learn_throughput: 455.26
  learn_time_ms: 36243.052
  load_throughput: 4337521.059
  load_time_ms: 3.804
  training_iteration_time_ms: 47723.368
  update_time_ms: 2.368
timesteps_total: 2755500
training_iteration: 167

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24324324324324326
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.5
episode_len_mean: 177.63
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 12179
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.28837180137634
time_total_s: 7772.877808570862
timers:
  learn_throughput: 439.594
  learn_time_ms: 37534.626
  load_throughput: 3275605.769
  load_time_ms: 5.037
  training_iteration_time_ms: 49876.02
  update_time_ms: 2.628
timesteps_total: 2409000
training_iteration: 146

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2808219178082192
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8243243243243243
  reward for individual goal_min: 0.0
episode_len_mean: 193.66
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 13164
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.49
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.377399921417236
time_total_s: 7788.203471660614
timers:
  learn_throughput: 481.873
  learn_time_ms: 34241.42
  load_throughput: 5091073.446
  load_time_ms: 3.241
  training_iteration_time_ms: 45557.462
  update_time_ms: 2.499
timesteps_total: 2689500
training_iteration: 163

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8598130841121495
  reward for individual goal_min: 0.5
episode_len_mean: 72.51131221719457
episode_reward_max: 2.0
episode_reward_mean: 1.8642533936651584
episode_reward_min: 1.0
episodes_this_iter: 221
episodes_total: 28765
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8642533936651584
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.05932259559631
time_total_s: 7770.705155611038
timers:
  learn_throughput: 446.815
  learn_time_ms: 36928.074
  load_throughput: 4641583.903
  load_time_ms: 3.555
  training_iteration_time_ms: 48358.265
  update_time_ms: 2.642
timesteps_total: 2541000
training_iteration: 154

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9166666666666666
  reward for individual goal_min: 0.5
episode_len_mean: 83.53333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.85
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9333333333333333
  agent_1: 0.9166666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 164.09
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 12294
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.41205811500549
time_total_s: 7785.128589391708
timers:
  learn_throughput: 448.892
  learn_time_ms: 36757.191
  load_throughput: 4721704.032
  load_time_ms: 3.495
  training_iteration_time_ms: 48426.265
  update_time_ms: 2.699
timesteps_total: 2310000
training_iteration: 140

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.55
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3181818181818182
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 164.03960396039605
episode_reward_max: 2.0
episode_reward_mean: 1.3762376237623761
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 13364
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6732673267326733
  agent_1: 0.7029702970297029
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.47202968597412
time_total_s: 7780.705750703812
timers:
  learn_throughput: 448.815
  learn_time_ms: 36763.494
  load_throughput: 4784906.454
  load_time_ms: 3.448
  training_iteration_time_ms: 48498.556
  update_time_ms: 2.561
timesteps_total: 2491500
training_iteration: 151

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972375690607734
  reward for individual goal_min: 0.5
episode_len_mean: 46.422096317280456
episode_reward_max: 2.0
episode_reward_mean: 1.9971671388101984
episode_reward_min: 1.0
episodes_this_iter: 353
episodes_total: 30406
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9971671388101983
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 62.16651439666748
time_total_s: 7768.413510322571
timers:
  learn_throughput: 363.796
  learn_time_ms: 45355.14
  load_throughput: 3560987.728
  load_time_ms: 4.634
  training_iteration_time_ms: 58833.632
  update_time_ms: 2.959
timesteps_total: 2310000
training_iteration: 140

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-184s1jdk42/checkpoint_000140/checkpoint-140
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 171.6
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 15597
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.76143288612366
time_total_s: 7795.429476261139
timers:
  learn_throughput: 510.083
  learn_time_ms: 32347.66
  load_throughput: 5107228.905
  load_time_ms: 3.231
  training_iteration_time_ms: 43147.036
  update_time_ms: 2.425
timesteps_total: 2871000
training_iteration: 174

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3092105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 170.37
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 13042
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.22462606430054
time_total_s: 7813.345886468887
timers:
  learn_throughput: 430.7
  learn_time_ms: 38309.69
  load_throughput: 4818051.922
  load_time_ms: 3.425
  training_iteration_time_ms: 50423.312
  update_time_ms: 2.601
timesteps_total: 2409000
training_iteration: 146

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16025641025641027
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935897435897436
  reward for individual goal_min: 0.5
episode_len_mean: 195.52
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 13297
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.83598017692566
time_total_s: 7814.807133436203
timers:
  learn_throughput: 453.094
  learn_time_ms: 36416.32
  load_throughput: 4813494.418
  load_time_ms: 3.428
  training_iteration_time_ms: 47830.212
  update_time_ms: 2.791
timesteps_total: 2508000
training_iteration: 152

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 61.8
episode_reward_max: 2.0
episode_reward_mean: 1.9833333333333334
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9833333333333333
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18055555555555555
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939024390243902
  reward for individual goal_min: 0.5
episode_len_mean: 178.45
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 12337
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.509730100631714
time_total_s: 7811.206630945206
timers:
  learn_throughput: 446.688
  learn_time_ms: 36938.556
  load_throughput: 4760451.516
  load_time_ms: 3.466
  training_iteration_time_ms: 48437.848
  update_time_ms: 2.62
timesteps_total: 2310000
training_iteration: 140

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-2933yh83nn/checkpoint_000140/checkpoint-140
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2905405405405405
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9459459459459459
  reward for individual goal_min: 0.0
episode_len_mean: 183.31
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 12138
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.42204260826111
time_total_s: 7829.88632941246
timers:
  learn_throughput: 443.469
  learn_time_ms: 37206.639
  load_throughput: 4572611.382
  load_time_ms: 3.608
  training_iteration_time_ms: 49169.793
  update_time_ms: 2.681
timesteps_total: 2458500
training_iteration: 149

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.38235294117647056
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8461538461538461
  reward for individual goal_min: 0.0
episode_len_mean: 188.05
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 13785
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.70652723312378
time_total_s: 7842.037274122238
timers:
  learn_throughput: 494.843
  learn_time_ms: 33343.915
  load_throughput: 4654038.372
  load_time_ms: 3.545
  training_iteration_time_ms: 44349.507
  update_time_ms: 2.804
timesteps_total: 2805000
training_iteration: 170

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.74338624338624
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 378
episodes_total: 43641
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.406707763671875
time_total_s: 7759.706873178482
timers:
  learn_throughput: 455.29
  learn_time_ms: 36240.62
  load_throughput: 4308062.972
  load_time_ms: 3.83
  training_iteration_time_ms: 47721.739
  update_time_ms: 2.384
timesteps_total: 2772000
training_iteration: 168

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28378378378378377
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8873239436619719
  reward for individual goal_min: 0.0
episode_len_mean: 193.1
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 13249
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.22377038002014
time_total_s: 7834.427242040634
timers:
  learn_throughput: 476.534
  learn_time_ms: 34624.987
  load_throughput: 5049727.908
  load_time_ms: 3.268
  training_iteration_time_ms: 45977.225
  update_time_ms: 2.507
timesteps_total: 2706000
training_iteration: 164

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8263157894736842
  reward for individual goal_min: 0.5
episode_len_mean: 80.6747572815534
episode_reward_max: 2.0
episode_reward_mean: 1.8398058252427185
episode_reward_min: 1.0
episodes_this_iter: 206
episodes_total: 20093
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8398058252427184
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 57.10538172721863
time_total_s: 7840.149098873138
timers:
  learn_throughput: 371.322
  learn_time_ms: 44435.805
  load_throughput: 4366778.519
  load_time_ms: 3.779
  training_iteration_time_ms: 58231.72
  update_time_ms: 2.806
timesteps_total: 2161500
training_iteration: 131

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8760330578512396
  reward for individual goal_min: 0.5
episode_len_mean: 71.91025641025641
episode_reward_max: 2.0
episode_reward_mean: 1.8717948717948718
episode_reward_min: 1.0
episodes_this_iter: 234
episodes_total: 28999
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8717948717948718
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.33458375930786
time_total_s: 7819.039739370346
timers:
  learn_throughput: 446.045
  learn_time_ms: 36991.79
  load_throughput: 4698431.458
  load_time_ms: 3.512
  training_iteration_time_ms: 48477.756
  update_time_ms: 2.643
timesteps_total: 2557500
training_iteration: 155

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2721518987341772
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9850746268656716
  reward for individual goal_min: 0.0
episode_len_mean: 185.89
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 12268
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.123233795166016
time_total_s: 7824.001042366028
timers:
  learn_throughput: 437.888
  learn_time_ms: 37680.859
  load_throughput: 3265343.468
  load_time_ms: 5.053
  training_iteration_time_ms: 50062.047
  update_time_ms: 2.637
timesteps_total: 2425500
training_iteration: 147

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22535211267605634
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9938271604938271
  reward for individual goal_min: 0.5
episode_len_mean: 167.21
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 12391
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.50196146965027
time_total_s: 7833.630550861359
timers:
  learn_throughput: 447.274
  learn_time_ms: 36890.138
  load_throughput: 4692346.851
  load_time_ms: 3.516
  training_iteration_time_ms: 48610.837
  update_time_ms: 2.735
timesteps_total: 2326500
training_iteration: 141

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3291139240506329
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 171.55
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 15694
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.89093327522278
time_total_s: 7837.320409536362
timers:
  learn_throughput: 510.878
  learn_time_ms: 32297.324
  load_throughput: 5090998.543
  load_time_ms: 3.241
  training_iteration_time_ms: 43072.545
  update_time_ms: 2.442
timesteps_total: 2887500
training_iteration: 175

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23972602739726026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.68
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 13463
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.08407735824585
time_total_s: 7829.7898280620575
timers:
  learn_throughput: 446.927
  learn_time_ms: 36918.807
  load_throughput: 4785535.11
  load_time_ms: 3.448
  training_iteration_time_ms: 48690.082
  update_time_ms: 2.837
timesteps_total: 2508000
training_iteration: 152

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9970059880239521
  reward for individual goal_min: 0.5
episode_len_mean: 45.96388888888889
episode_reward_max: 2.0
episode_reward_mean: 1.9972222222222222
episode_reward_min: 1.0
episodes_this_iter: 360
episodes_total: 30766
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9972222222222222
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 59.07990837097168
time_total_s: 7827.4934186935425
timers:
  learn_throughput: 364.539
  learn_time_ms: 45262.654
  load_throughput: 3544627.488
  load_time_ms: 4.655
  training_iteration_time_ms: 58729.167
  update_time_ms: 2.974
timesteps_total: 2326500
training_iteration: 141

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19444444444444445
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.9
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 13391
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.16426396369934
time_total_s: 7862.971397399902
timers:
  learn_throughput: 452.91
  learn_time_ms: 36431.086
  load_throughput: 4837889.969
  load_time_ms: 3.411
  training_iteration_time_ms: 47896.325
  update_time_ms: 2.806
timesteps_total: 2524500
training_iteration: 153

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9810126582278481
  reward for individual goal_min: 0.0
episode_len_mean: 166.05
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 13141
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.190255641937256
time_total_s: 7863.536142110825
timers:
  learn_throughput: 431.227
  learn_time_ms: 38262.92
  load_throughput: 4780444.432
  load_time_ms: 3.452
  training_iteration_time_ms: 50360.931
  update_time_ms: 2.59
timesteps_total: 2425500
training_iteration: 147

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2236842105263158
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 179.61
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 12425
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.568809270858765
time_total_s: 7858.775440216064
timers:
  learn_throughput: 444.719
  learn_time_ms: 37102.094
  load_throughput: 4778628.957
  load_time_ms: 3.453
  training_iteration_time_ms: 48662.726
  update_time_ms: 2.632
timesteps_total: 2326500
training_iteration: 141

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2792207792207792
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7803030303030303
  reward for individual goal_min: 0.0
episode_len_mean: 195.14
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 13868
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.43
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.67339825630188
time_total_s: 7888.71067237854
timers:
  learn_throughput: 491.14
  learn_time_ms: 33595.337
  load_throughput: 4644075.695
  load_time_ms: 3.553
  training_iteration_time_ms: 44693.412
  update_time_ms: 2.814
timesteps_total: 2821500
training_iteration: 171

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22857142857142856
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8950617283950617
  reward for individual goal_min: 0.0
episode_len_mean: 194.53
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 12222
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.234955072402954
time_total_s: 7878.121284484863
timers:
  learn_throughput: 445.731
  learn_time_ms: 37017.845
  load_throughput: 4513092.439
  load_time_ms: 3.656
  training_iteration_time_ms: 48989.93
  update_time_ms: 2.702
timesteps_total: 2475000
training_iteration: 150

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.315363881401616
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 371
episodes_total: 44012
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.2158465385437
time_total_s: 7807.922719717026
timers:
  learn_throughput: 454.944
  learn_time_ms: 36268.214
  load_throughput: 4287325.982
  load_time_ms: 3.849
  training_iteration_time_ms: 47758.172
  update_time_ms: 2.36
timesteps_total: 2788500
training_iteration: 169

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31690140845070425
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8092105263157895
  reward for individual goal_min: 0.0
episode_len_mean: 188.32
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 13335
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.88047480583191
time_total_s: 7881.307716846466
timers:
  learn_throughput: 472.975
  learn_time_ms: 34885.551
  load_throughput: 5013330.243
  load_time_ms: 3.291
  training_iteration_time_ms: 46238.653
  update_time_ms: 2.505
timesteps_total: 2722500
training_iteration: 165

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8648648648648649
  reward for individual goal_min: 0.5
episode_len_mean: 71.19650655021834
episode_reward_max: 2.0
episode_reward_mean: 1.8689956331877728
episode_reward_min: 1.0
episodes_this_iter: 229
episodes_total: 29228
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.868995633187773
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.120561361312866
time_total_s: 7867.160300731659
timers:
  learn_throughput: 444.539
  learn_time_ms: 37117.077
  load_throughput: 4638566.191
  load_time_ms: 3.557
  training_iteration_time_ms: 48668.087
  update_time_ms: 2.616
timesteps_total: 2574000
training_iteration: 156

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23493975903614459
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 193.62
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 15776
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.58025407791138
time_total_s: 7879.900663614273
timers:
  learn_throughput: 511.709
  learn_time_ms: 32244.901
  load_throughput: 5138933.393
  load_time_ms: 3.211
  training_iteration_time_ms: 43049.698
  update_time_ms: 2.448
timesteps_total: 2904000
training_iteration: 176

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2710843373493976
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.43
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 12480
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.48547887802124
time_total_s: 7883.11602973938
timers:
  learn_throughput: 443.6
  learn_time_ms: 37195.662
  load_throughput: 4655697.756
  load_time_ms: 3.544
  training_iteration_time_ms: 48966.853
  update_time_ms: 2.727
timesteps_total: 2343000
training_iteration: 142

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2328767123287671
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9810126582278481
  reward for individual goal_min: 0.0
episode_len_mean: 178.06
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 12360
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.327420711517334
time_total_s: 7875.328463077545
timers:
  learn_throughput: 434.918
  learn_time_ms: 37938.161
  load_throughput: 3257796.189
  load_time_ms: 5.065
  training_iteration_time_ms: 50354.617
  update_time_ms: 2.66
timesteps_total: 2442000
training_iteration: 148

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8693693693693694
  reward for individual goal_min: 0.5
episode_len_mean: 79.42233009708738
episode_reward_max: 2.0
episode_reward_mean: 1.8592233009708738
episode_reward_min: 1.0
episodes_this_iter: 206
episodes_total: 20299
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9951456310679612
  agent_1: 0.8640776699029126
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.11248207092285
time_total_s: 7899.261580944061
timers:
  learn_throughput: 369.66
  learn_time_ms: 44635.567
  load_throughput: 4388515.771
  load_time_ms: 3.76
  training_iteration_time_ms: 58414.435
  update_time_ms: 2.826
timesteps_total: 2178000
training_iteration: 132

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28289473684210525
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.46
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 13561
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.94203519821167
time_total_s: 7877.731863260269
timers:
  learn_throughput: 445.768
  learn_time_ms: 37014.798
  load_throughput: 4733135.635
  load_time_ms: 3.486
  training_iteration_time_ms: 48805.84
  update_time_ms: 2.86
timesteps_total: 2524500
training_iteration: 153

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26282051282051283
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9859154929577465
  reward for individual goal_min: 0.0
episode_len_mean: 180.22
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 13480
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.73167324066162
time_total_s: 7911.703070640564
timers:
  learn_throughput: 451.919
  learn_time_ms: 36511.003
  load_throughput: 4833868.784
  load_time_ms: 3.413
  training_iteration_time_ms: 47980.081
  update_time_ms: 2.669
timesteps_total: 2541000
training_iteration: 154

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24358974358974358
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.0
episode_len_mean: 180.05
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 13231
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.36615324020386
time_total_s: 7916.902295351028
timers:
  learn_throughput: 425.564
  learn_time_ms: 38772.064
  load_throughput: 4792925.924
  load_time_ms: 3.443
  training_iteration_time_ms: 50900.9
  update_time_ms: 2.613
timesteps_total: 2442000
training_iteration: 148

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9969512195121951
  reward for individual goal_min: 0.5
episode_len_mean: 45.470914127423825
episode_reward_max: 2.0
episode_reward_mean: 1.997229916897507
episode_reward_min: 1.0
episodes_this_iter: 361
episodes_total: 31127
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.997229916897507
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 58.30770230293274
time_total_s: 7885.801120996475
timers:
  learn_throughput: 364.611
  learn_time_ms: 45253.714
  load_throughput: 3509201.523
  load_time_ms: 4.702
  training_iteration_time_ms: 58747.24
  update_time_ms: 2.628
timesteps_total: 2343000
training_iteration: 142

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8827160493827161
  reward for individual goal_min: 0.0
episode_len_mean: 199.69
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 13948
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.55629539489746
time_total_s: 7932.2669677734375
timers:
  learn_throughput: 491.713
  learn_time_ms: 33556.152
  load_throughput: 4619339.198
  load_time_ms: 3.572
  training_iteration_time_ms: 44663.375
  update_time_ms: 2.817
timesteps_total: 2838000
training_iteration: 172

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2597402597402597
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9652777777777778
  reward for individual goal_min: 0.0
episode_len_mean: 183.02
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 12514
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.46216917037964
time_total_s: 7909.237609386444
timers:
  learn_throughput: 441.322
  learn_time_ms: 37387.672
  load_throughput: 4745891.663
  load_time_ms: 3.477
  training_iteration_time_ms: 49049.998
  update_time_ms: 2.608
timesteps_total: 2343000
training_iteration: 142

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.821917808219178
  reward for individual goal_min: 0.0
episode_len_mean: 193.8
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 12309
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.87021207809448
time_total_s: 7926.991496562958
timers:
  learn_throughput: 447.805
  learn_time_ms: 36846.382
  load_throughput: 4509093.373
  load_time_ms: 3.659
  training_iteration_time_ms: 48779.951
  update_time_ms: 2.692
timesteps_total: 2491500
training_iteration: 151

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30405405405405406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7628205128205128
  reward for individual goal_min: 0.0
episode_len_mean: 192.16
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 13422
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.77164077758789
time_total_s: 7928.079357624054
timers:
  learn_throughput: 472.32
  learn_time_ms: 34933.965
  load_throughput: 4966630.018
  load_time_ms: 3.322
  training_iteration_time_ms: 46300.497
  update_time_ms: 2.509
timesteps_total: 2739000
training_iteration: 166

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9944444444444445
  reward for individual goal_min: 0.5
episode_len_mean: 44.205882352941174
episode_reward_max: 2.0
episode_reward_mean: 1.9946524064171123
episode_reward_min: 1.0
episodes_this_iter: 374
episodes_total: 44386
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973262032085561
  agent_1: 0.9973262032085561
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.09354782104492
time_total_s: 7856.016267538071
timers:
  learn_throughput: 455.033
  learn_time_ms: 36261.087
  load_throughput: 4316257.905
  load_time_ms: 3.823
  training_iteration_time_ms: 47763.238
  update_time_ms: 2.356
timesteps_total: 2805000
training_iteration: 170

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25882352941176473
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 197.05
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 15862
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.84475922584534
time_total_s: 7921.745422840118
timers:
  learn_throughput: 513.765
  learn_time_ms: 32115.836
  load_throughput: 5085275.0
  load_time_ms: 3.245
  training_iteration_time_ms: 42889.397
  update_time_ms: 2.49
timesteps_total: 2920500
training_iteration: 177

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8119266055045872
  reward for individual goal_min: 0.5
episode_len_mean: 89.16842105263157
episode_reward_max: 2.0
episode_reward_mean: 1.7842105263157895
episode_reward_min: 1.0
episodes_this_iter: 190
episodes_total: 29418
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7894736842105263
  agent_1: 0.9947368421052631
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.639914989471436
time_total_s: 7911.80021572113
timers:
  learn_throughput: 448.194
  learn_time_ms: 36814.416
  load_throughput: 4611059.985
  load_time_ms: 3.578
  training_iteration_time_ms: 48225.445
  update_time_ms: 2.61
timesteps_total: 2590500
training_iteration: 157

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2631578947368421
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 175.25
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 12577
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.69141101837158
time_total_s: 7930.8074407577515
timers:
  learn_throughput: 443.177
  learn_time_ms: 37231.184
  load_throughput: 4590171.52
  load_time_ms: 3.595
  training_iteration_time_ms: 48975.05
  update_time_ms: 2.705
timesteps_total: 2359500
training_iteration: 143

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24342105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.02
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 12454
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.9670684337616
time_total_s: 7926.295531511307
timers:
  learn_throughput: 433.758
  learn_time_ms: 38039.683
  load_throughput: 3238920.578
  load_time_ms: 5.094
  training_iteration_time_ms: 50532.137
  update_time_ms: 2.663
timesteps_total: 2458500
training_iteration: 149

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.37662337662337664
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 168.03
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 13658
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.262696504592896
time_total_s: 7927.994559764862
timers:
  learn_throughput: 444.0
  learn_time_ms: 37162.128
  load_throughput: 4702262.325
  load_time_ms: 3.509
  training_iteration_time_ms: 48947.372
  update_time_ms: 2.884
timesteps_total: 2541000
training_iteration: 154

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7873563218390804
  reward for individual goal_min: 0.5
episode_len_mean: 89.5081081081081
episode_reward_max: 2.0
episode_reward_mean: 1.8
episode_reward_min: 1.0
episodes_this_iter: 185
episodes_total: 20484
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9945945945945946
  agent_1: 0.8054054054054054
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.15353584289551
time_total_s: 7957.415116786957
timers:
  learn_throughput: 368.855
  learn_time_ms: 44733.018
  load_throughput: 4226321.588
  load_time_ms: 3.904
  training_iteration_time_ms: 58571.675
  update_time_ms: 2.805
timesteps_total: 2194500
training_iteration: 133

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3013698630136986
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 163.2970297029703
episode_reward_max: 2.0
episode_reward_mean: 1.3564356435643565
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 13581
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6534653465346535
  agent_1: 0.7029702970297029
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.905617475509644
time_total_s: 7959.608688116074
timers:
  learn_throughput: 451.346
  learn_time_ms: 36557.316
  load_throughput: 4822046.823
  load_time_ms: 3.422
  training_iteration_time_ms: 48048.96
  update_time_ms: 2.717
timesteps_total: 2557500
training_iteration: 155

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2564102564102564
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8414634146341463
  reward for individual goal_min: 0.0
episode_len_mean: 201.06
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 14032
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.42194485664368
time_total_s: 7973.688912630081
timers:
  learn_throughput: 495.396
  learn_time_ms: 33306.709
  load_throughput: 4663446.742
  load_time_ms: 3.538
  training_iteration_time_ms: 44316.877
  update_time_ms: 2.569
timesteps_total: 2854500
training_iteration: 173

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2532467532467532
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 175.74
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 13326
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.74359726905823
time_total_s: 7966.645892620087
timers:
  learn_throughput: 425.77
  learn_time_ms: 38753.292
  load_throughput: 4708885.275
  load_time_ms: 3.504
  training_iteration_time_ms: 50927.069
  update_time_ms: 2.631
timesteps_total: 2458500
training_iteration: 149

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26506024096385544
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9788732394366197
  reward for individual goal_min: 0.0
episode_len_mean: 191.36
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 12603
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.8504638671875
time_total_s: 7957.088073253632
timers:
  learn_throughput: 443.089
  learn_time_ms: 37238.561
  load_throughput: 4720287.012
  load_time_ms: 3.496
  training_iteration_time_ms: 48855.232
  update_time_ms: 2.621
timesteps_total: 2359500
training_iteration: 143

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3170731707317073
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8266666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 200.99
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 13503
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.129433155059814
time_total_s: 7974.208790779114
timers:
  learn_throughput: 473.551
  learn_time_ms: 34843.157
  load_throughput: 4934791.965
  load_time_ms: 3.344
  training_iteration_time_ms: 46194.806
  update_time_ms: 2.527
timesteps_total: 2755500
training_iteration: 167

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 179.04
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 15953
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.862037897109985
time_total_s: 7965.607460737228
timers:
  learn_throughput: 513.124
  learn_time_ms: 32155.956
  load_throughput: 5104893.19
  load_time_ms: 3.232
  training_iteration_time_ms: 42893.392
  update_time_ms: 2.497
timesteps_total: 2937000
training_iteration: 178

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 199.89
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 12389
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.97189211845398
time_total_s: 7975.963388681412
timers:
  learn_throughput: 446.407
  learn_time_ms: 36961.78
  load_throughput: 4510621.59
  load_time_ms: 3.658
  training_iteration_time_ms: 48863.036
  update_time_ms: 2.704
timesteps_total: 2508000
training_iteration: 152

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9947643979057592
  reward for individual goal_min: 0.5
episode_len_mean: 45.28260869565217
episode_reward_max: 2.0
episode_reward_mean: 1.9945652173913044
episode_reward_min: 1.0
episodes_this_iter: 368
episodes_total: 31495
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972826086956522
  agent_1: 0.9972826086956522
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.481104612350464
time_total_s: 7943.282225608826
timers:
  learn_throughput: 364.944
  learn_time_ms: 45212.455
  load_throughput: 3521522.468
  load_time_ms: 4.685
  training_iteration_time_ms: 58665.13
  update_time_ms: 2.634
timesteps_total: 2359500
training_iteration: 143

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.083109919571044
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 373
episodes_total: 44759
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.66102385520935
time_total_s: 7904.67729139328
timers:
  learn_throughput: 454.548
  learn_time_ms: 36299.83
  load_throughput: 4310907.517
  load_time_ms: 3.828
  training_iteration_time_ms: 47811.305
  update_time_ms: 2.341
timesteps_total: 2821500
training_iteration: 171

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8918918918918919
  reward for individual goal_min: 0.5
episode_len_mean: 64.832
episode_reward_max: 2.0
episode_reward_mean: 1.904
episode_reward_min: 1.0
episodes_this_iter: 250
episodes_total: 29668
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.904
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 49.873953342437744
time_total_s: 7961.674169063568
timers:
  learn_throughput: 446.463
  learn_time_ms: 36957.12
  load_throughput: 4718709.968
  load_time_ms: 3.497
  training_iteration_time_ms: 48427.792
  update_time_ms: 2.563
timesteps_total: 2607000
training_iteration: 158

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939759036144579
  reward for individual goal_min: 0.5
episode_len_mean: 179.37
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 12669
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.23151731491089
time_total_s: 7976.038958072662
timers:
  learn_throughput: 445.89
  learn_time_ms: 37004.675
  load_throughput: 4581905.43
  load_time_ms: 3.601
  training_iteration_time_ms: 48633.858
  update_time_ms: 2.71
timesteps_total: 2376000
training_iteration: 144

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 185.88
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 12545
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.79662108421326
time_total_s: 7977.09215259552
timers:
  learn_throughput: 434.4
  learn_time_ms: 37983.412
  load_throughput: 3230303.211
  load_time_ms: 5.108
  training_iteration_time_ms: 50459.227
  update_time_ms: 2.634
timesteps_total: 2475000
training_iteration: 150

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35714285714285715
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 165.8
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 13755
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.76
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.063220500946045
time_total_s: 7978.057780265808
timers:
  learn_throughput: 444.27
  learn_time_ms: 37139.609
  load_throughput: 4694543.136
  load_time_ms: 3.515
  training_iteration_time_ms: 48914.299
  update_time_ms: 2.885
timesteps_total: 2557500
training_iteration: 155

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2692307692307692
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8378378378378378
  reward for individual goal_min: 0.0
episode_len_mean: 197.84
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 14114
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.80524039268494
time_total_s: 8019.494153022766
timers:
  learn_throughput: 494.86
  learn_time_ms: 33342.745
  load_throughput: 4709301.832
  load_time_ms: 3.504
  training_iteration_time_ms: 44384.084
  update_time_ms: 2.566
timesteps_total: 2871000
training_iteration: 174

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21153846153846154
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.21
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 13672
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.47498607635498
time_total_s: 8007.083674192429
timers:
  learn_throughput: 451.586
  learn_time_ms: 36537.91
  load_throughput: 4825846.437
  load_time_ms: 3.419
  training_iteration_time_ms: 47979.43
  update_time_ms: 2.672
timesteps_total: 2574000
training_iteration: 156

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8490566037735849
  reward for individual goal_min: 0.5
episode_len_mean: 77.44811320754717
episode_reward_max: 2.0
episode_reward_mean: 1.849056603773585
episode_reward_min: 1.0
episodes_this_iter: 212
episodes_total: 20696
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9858490566037735
  agent_1: 0.8632075471698113
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.825681924819946
time_total_s: 8016.240798711777
timers:
  learn_throughput: 368.775
  learn_time_ms: 44742.669
  load_throughput: 4256291.075
  load_time_ms: 3.877
  training_iteration_time_ms: 58557.798
  update_time_ms: 2.799
timesteps_total: 2211000
training_iteration: 134

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28160919540229884
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9710144927536232
  reward for individual goal_min: 0.0
episode_len_mean: 190.4
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 12688
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.90542411804199
time_total_s: 8004.993497371674
timers:
  learn_throughput: 441.855
  learn_time_ms: 37342.544
  load_throughput: 4729383.593
  load_time_ms: 3.489
  training_iteration_time_ms: 48894.133
  update_time_ms: 2.615
timesteps_total: 2376000
training_iteration: 144

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.59
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 16049
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.343353033065796
time_total_s: 8007.950813770294
timers:
  learn_throughput: 514.139
  learn_time_ms: 32092.512
  load_throughput: 5100265.751
  load_time_ms: 3.235
  training_iteration_time_ms: 42823.366
  update_time_ms: 2.501
timesteps_total: 2953500
training_iteration: 179

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2152777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 176.01
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 13421
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.61794877052307
time_total_s: 8020.26384139061
timers:
  learn_throughput: 424.036
  learn_time_ms: 38911.764
  load_throughput: 4734463.212
  load_time_ms: 3.485
  training_iteration_time_ms: 51132.014
  update_time_ms: 2.645
timesteps_total: 2475000
training_iteration: 150

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2986111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7697368421052632
  reward for individual goal_min: 0.0
episode_len_mean: 204.04
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 13584
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.2746901512146
time_total_s: 8020.483480930328
timers:
  learn_throughput: 472.841
  learn_time_ms: 34895.456
  load_throughput: 4990122.723
  load_time_ms: 3.307
  training_iteration_time_ms: 46234.348
  update_time_ms: 2.501
timesteps_total: 2772000
training_iteration: 168

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27848101265822783
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8661971830985915
  reward for individual goal_min: 0.0
episode_len_mean: 194.38
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 12476
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.79616975784302
time_total_s: 8025.759558439255
timers:
  learn_throughput: 448.757
  learn_time_ms: 36768.214
  load_throughput: 4581602.097
  load_time_ms: 3.601
  training_iteration_time_ms: 48639.886
  update_time_ms: 2.69
timesteps_total: 2524500
training_iteration: 153

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.336898395721924
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 374
episodes_total: 45133
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.81225323677063
time_total_s: 7952.489544630051
timers:
  learn_throughput: 454.945
  learn_time_ms: 36268.102
  load_throughput: 4272873.072
  load_time_ms: 3.862
  training_iteration_time_ms: 47776.957
  update_time_ms: 2.337
timesteps_total: 2838000
training_iteration: 172

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9902912621359223
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8472222222222222
  reward for individual goal_min: 0.5
episode_len_mean: 79.3175355450237
episode_reward_max: 2.0
episode_reward_mean: 1.8341232227488151
episode_reward_min: 0.0
episodes_this_iter: 211
episodes_total: 29879
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8388625592417062
  agent_1: 0.995260663507109
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.1755530834198
time_total_s: 8008.849722146988
timers:
  learn_throughput: 450.066
  learn_time_ms: 36661.296
  load_throughput: 4742541.836
  load_time_ms: 3.479
  training_iteration_time_ms: 48039.454
  update_time_ms: 2.558
timesteps_total: 2623500
training_iteration: 159

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 154.32075471698113
episode_reward_max: 2.0
episode_reward_mean: 1.4245283018867925
episode_reward_min: 0.0
episodes_this_iter: 106
episodes_total: 12775
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7264150943396226
  agent_1: 0.6981132075471698
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.207114458084106
time_total_s: 8024.2460725307465
timers:
  learn_throughput: 446.519
  learn_time_ms: 36952.499
  load_throughput: 4540362.148
  load_time_ms: 3.634
  training_iteration_time_ms: 48521.056
  update_time_ms: 2.687
timesteps_total: 2392500
training_iteration: 145

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.38544474393531
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 371
episodes_total: 31866
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.12979030609131
time_total_s: 8001.412015914917
timers:
  learn_throughput: 366.364
  learn_time_ms: 45037.193
  load_throughput: 3503747.266
  load_time_ms: 4.709
  training_iteration_time_ms: 58419.789
  update_time_ms: 2.615
timesteps_total: 2376000
training_iteration: 144

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27564102564102566
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.37
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 12635
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.76
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.356144189834595
time_total_s: 8027.448296785355
timers:
  learn_throughput: 433.744
  learn_time_ms: 38040.872
  load_throughput: 3261373.334
  load_time_ms: 5.059
  training_iteration_time_ms: 50532.083
  update_time_ms: 2.622
timesteps_total: 2491500
training_iteration: 151

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25903614457831325
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9852941176470589
  reward for individual goal_min: 0.0
episode_len_mean: 186.2
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 13845
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.14837861061096
time_total_s: 8029.206158876419
timers:
  learn_throughput: 441.566
  learn_time_ms: 37366.99
  load_throughput: 4688690.939
  load_time_ms: 3.519
  training_iteration_time_ms: 49219.592
  update_time_ms: 2.894
timesteps_total: 2574000
training_iteration: 156

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21686746987951808
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 198.84
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 13755
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.68718504905701
time_total_s: 8051.770859241486
timers:
  learn_throughput: 453.09
  learn_time_ms: 36416.645
  load_throughput: 4833531.174
  load_time_ms: 3.414
  training_iteration_time_ms: 47741.123
  update_time_ms: 2.689
timesteps_total: 2590500
training_iteration: 157

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2357142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.86875
  reward for individual goal_min: 0.0
episode_len_mean: 182.29
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 14207
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.09916973114014
time_total_s: 8064.593322753906
timers:
  learn_throughput: 492.996
  learn_time_ms: 33468.816
  load_throughput: 4678326.494
  load_time_ms: 3.527
  training_iteration_time_ms: 44572.051
  update_time_ms: 2.561
timesteps_total: 2887500
training_iteration: 175

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22857142857142856
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.0
episode_len_mean: 165.84
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 12788
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.529481172561646
time_total_s: 8053.522978544235
timers:
  learn_throughput: 440.341
  learn_time_ms: 37470.938
  load_throughput: 4726412.064
  load_time_ms: 3.491
  training_iteration_time_ms: 48973.34
  update_time_ms: 2.619
timesteps_total: 2392500
training_iteration: 145

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3048780487804878
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7676056338028169
  reward for individual goal_min: 0.0
episode_len_mean: 209.22
episode_reward_max: 2.0
episode_reward_mean: 1.06
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 13663
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.49
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.13608908653259
time_total_s: 8065.619570016861
timers:
  learn_throughput: 472.926
  learn_time_ms: 34889.189
  load_throughput: 4979853.208
  load_time_ms: 3.313
  training_iteration_time_ms: 46229.424
  update_time_ms: 2.498
timesteps_total: 2788500
training_iteration: 169

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1643835616438356
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9805194805194806
  reward for individual goal_min: 0.0
episode_len_mean: 186.88
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 13511
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.872169494628906
time_total_s: 8070.136010885239
timers:
  learn_throughput: 424.019
  learn_time_ms: 38913.311
  load_throughput: 4687992.196
  load_time_ms: 3.52
  training_iteration_time_ms: 51172.931
  update_time_ms: 2.634
timesteps_total: 2491500
training_iteration: 151

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8076923076923077
  reward for individual goal_min: 0.5
episode_len_mean: 83.16915422885572
episode_reward_max: 2.0
episode_reward_mean: 1.8258706467661692
episode_reward_min: 1.0
episodes_this_iter: 201
episodes_total: 20897
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8258706467661692
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 58.359699964523315
time_total_s: 8074.6004986763
timers:
  learn_throughput: 368.43
  learn_time_ms: 44784.663
  load_throughput: 4217718.729
  load_time_ms: 3.912
  training_iteration_time_ms: 58608.999
  update_time_ms: 2.742
timesteps_total: 2227500
training_iteration: 135

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95
  reward for individual goal_min: 0.5
episode_len_mean: 88.1
episode_reward_max: 2.0
episode_reward_mean: 1.85
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9333333333333333
  agent_1: 0.9166666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23484848484848486
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9802631578947368
  reward for individual goal_min: 0.0
episode_len_mean: 165.97029702970298
episode_reward_max: 2.0
episode_reward_mean: 1.3564356435643565
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 16150
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6831683168316832
  agent_1: 0.6732673267326733
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.82959794998169
time_total_s: 8061.780411720276
timers:
  learn_throughput: 516.255
  learn_time_ms: 31960.95
  load_throughput: 5061176.109
  load_time_ms: 3.26
  training_iteration_time_ms: 42734.072
  update_time_ms: 2.491
timesteps_total: 2970000
training_iteration: 180

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29605263157894735
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8376623376623377
  reward for individual goal_min: 0.0
episode_len_mean: 193.23
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 12561
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.283687114715576
time_total_s: 8073.04324555397
timers:
  learn_throughput: 449.586
  learn_time_ms: 36700.444
  load_throughput: 4603973.975
  load_time_ms: 3.584
  training_iteration_time_ms: 48506.958
  update_time_ms: 2.718
timesteps_total: 2541000
training_iteration: 154

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9922279792746114
  reward for individual goal_min: 0.5
episode_len_mean: 46.134453781512605
episode_reward_max: 2.0
episode_reward_mean: 1.9915966386554622
episode_reward_min: 1.0
episodes_this_iter: 357
episodes_total: 45490
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9915966386554622
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 47.236560344696045
time_total_s: 7999.726104974747
timers:
  learn_throughput: 455.784
  learn_time_ms: 36201.333
  load_throughput: 4256840.862
  load_time_ms: 3.876
  training_iteration_time_ms: 47692.525
  update_time_ms: 2.335
timesteps_total: 2854500
training_iteration: 173

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3150684931506849
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 149.49107142857142
episode_reward_max: 2.0
episode_reward_mean: 1.375
episode_reward_min: 0.0
episodes_this_iter: 112
episodes_total: 12887
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7321428571428571
  agent_1: 0.6428571428571429
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.406452655792236
time_total_s: 8077.652525186539
timers:
  learn_throughput: 442.156
  learn_time_ms: 37317.138
  load_throughput: 4530464.463
  load_time_ms: 3.642
  training_iteration_time_ms: 49000.375
  update_time_ms: 2.675
timesteps_total: 2409000
training_iteration: 146

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9
  reward for individual goal_min: 0.5
episode_len_mean: 64.5
episode_reward_max: 2.0
episode_reward_mean: 1.9
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8842592592592593
  reward for individual goal_min: 0.5
episode_len_mean: 67.05327868852459
episode_reward_max: 2.0
episode_reward_mean: 1.8975409836065573
episode_reward_min: 1.0
episodes_this_iter: 244
episodes_total: 30123
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9016393442622951
  agent_1: 0.9959016393442623
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.630393266677856
time_total_s: 8066.480115413666
timers:
  learn_throughput: 447.304
  learn_time_ms: 36887.686
  load_throughput: 4802338.23
  load_time_ms: 3.436
  training_iteration_time_ms: 48319.612
  update_time_ms: 2.555
timesteps_total: 2640000
training_iteration: 160

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-18g5xov65h/checkpoint_000160/checkpoint-160
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972972972972973
  reward for individual goal_min: 0.5
episode_len_mean: 45.80609418282548
episode_reward_max: 2.0
episode_reward_mean: 1.997229916897507
episode_reward_min: 1.0
episodes_this_iter: 361
episodes_total: 32227
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.997229916897507
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 55.96755647659302
time_total_s: 8057.37957239151
timers:
  learn_throughput: 367.776
  learn_time_ms: 44864.228
  load_throughput: 3522436.581
  load_time_ms: 4.684
  training_iteration_time_ms: 58164.565
  update_time_ms: 2.618
timesteps_total: 2392500
training_iteration: 145

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28846153846153844
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.85
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 13943
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.2260000705719
time_total_s: 8077.432158946991
timers:
  learn_throughput: 441.602
  learn_time_ms: 37363.962
  load_throughput: 4673682.163
  load_time_ms: 3.53
  training_iteration_time_ms: 49252.549
  update_time_ms: 2.893
timesteps_total: 2590500
training_iteration: 157

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23076923076923078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9921875
  reward for individual goal_min: 0.5
episode_len_mean: 188.55
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 12721
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.437047719955444
time_total_s: 8081.88534450531
timers:
  learn_throughput: 429.248
  learn_time_ms: 38439.351
  load_throughput: 3274164.546
  load_time_ms: 5.039
  training_iteration_time_ms: 50957.388
  update_time_ms: 2.619
timesteps_total: 2508000
training_iteration: 152

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2361111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.855072463768116
  reward for individual goal_min: 0.0
episode_len_mean: 179.48
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 14299
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.94018578529358
time_total_s: 8111.5335085392
timers:
  learn_throughput: 491.716
  learn_time_ms: 33555.931
  load_throughput: 4663761.01
  load_time_ms: 3.538
  training_iteration_time_ms: 44765.086
  update_time_ms: 2.589
timesteps_total: 2904000
training_iteration: 176

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2571428571428571
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 167.47
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 13854
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.03116488456726
time_total_s: 8099.802024126053
timers:
  learn_throughput: 453.038
  learn_time_ms: 36420.772
  load_throughput: 4861748.391
  load_time_ms: 3.394
  training_iteration_time_ms: 47789.781
  update_time_ms: 2.703
timesteps_total: 2607000
training_iteration: 158

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2721518987341772
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8157894736842105
  reward for individual goal_min: 0.0
episode_len_mean: 209.46
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 13744
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.46
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.773764848709106
time_total_s: 8111.39333486557
timers:
  learn_throughput: 472.689
  learn_time_ms: 34906.65
  load_throughput: 5004086.508
  load_time_ms: 3.297
  training_iteration_time_ms: 46281.884
  update_time_ms: 2.494
timesteps_total: 2805000
training_iteration: 170

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 170.63
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 16245
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.237186431884766
time_total_s: 8104.017598152161
timers:
  learn_throughput: 516.382
  learn_time_ms: 31953.067
  load_throughput: 5043582.09
  load_time_ms: 3.271
  training_iteration_time_ms: 42760.604
  update_time_ms: 2.496
timesteps_total: 2986500
training_iteration: 181

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23648648648648649
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9753086419753086
  reward for individual goal_min: 0.0
episode_len_mean: 173.04
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 12883
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.76
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.42929673194885
time_total_s: 8101.952275276184
timers:
  learn_throughput: 441.21
  learn_time_ms: 37397.121
  load_throughput: 4665049.949
  load_time_ms: 3.537
  training_iteration_time_ms: 48849.727
  update_time_ms: 2.625
timesteps_total: 2409000
training_iteration: 146

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28378378378378377
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9928571428571429
  reward for individual goal_min: 0.5
episode_len_mean: 172.36
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 13605
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.187586545944214
time_total_s: 8118.323597431183
timers:
  learn_throughput: 428.058
  learn_time_ms: 38546.17
  load_throughput: 4700952.743
  load_time_ms: 3.51
  training_iteration_time_ms: 50697.2
  update_time_ms: 2.654
timesteps_total: 2508000
training_iteration: 152

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3051948051948052
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8653846153846154
  reward for individual goal_min: 0.0
episode_len_mean: 187.26
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 12647
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.96869683265686
time_total_s: 8120.011942386627
timers:
  learn_throughput: 451.302
  learn_time_ms: 36560.886
  load_throughput: 4607008.168
  load_time_ms: 3.582
  training_iteration_time_ms: 48382.451
  update_time_ms: 2.715
timesteps_total: 2557500
training_iteration: 155

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9940476190476191
  reward for individual goal_min: 0.5
episode_len_mean: 45.81388888888889
episode_reward_max: 2.0
episode_reward_mean: 1.9944444444444445
episode_reward_min: 1.0
episodes_this_iter: 360
episodes_total: 45850
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9944444444444445
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 47.42814302444458
time_total_s: 8047.154247999191
timers:
  learn_throughput: 456.554
  learn_time_ms: 36140.3
  load_throughput: 4254041.049
  load_time_ms: 3.879
  training_iteration_time_ms: 47616.949
  update_time_ms: 2.328
timesteps_total: 2871000
training_iteration: 174

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.826530612244898
  reward for individual goal_min: 0.5
episode_len_mean: 87.94021739130434
episode_reward_max: 2.0
episode_reward_mean: 1.815217391304348
episode_reward_min: 1.0
episodes_this_iter: 184
episodes_total: 21081
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9891304347826086
  agent_1: 0.8260869565217391
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.854429960250854
time_total_s: 8131.454928636551
timers:
  learn_throughput: 369.743
  learn_time_ms: 44625.59
  load_throughput: 4233897.355
  load_time_ms: 3.897
  training_iteration_time_ms: 58411.132
  update_time_ms: 2.783
timesteps_total: 2244000
training_iteration: 136

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3223684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 156.29807692307693
episode_reward_max: 2.0
episode_reward_mean: 1.3846153846153846
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 12991
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6730769230769231
  agent_1: 0.7115384615384616
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.092281341552734
time_total_s: 8128.744806528091
timers:
  learn_throughput: 441.179
  learn_time_ms: 37399.82
  load_throughput: 4494510.031
  load_time_ms: 3.671
  training_iteration_time_ms: 49101.998
  update_time_ms: 2.683
timesteps_total: 2425500
training_iteration: 147

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8782608695652174
  reward for individual goal_min: 0.5
episode_len_mean: 69.06382978723404
episode_reward_max: 2.0
episode_reward_mean: 1.8808510638297873
episode_reward_min: 1.0
episodes_this_iter: 235
episodes_total: 30358
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8808510638297873
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 49.484102725982666
time_total_s: 8115.964218139648
timers:
  learn_throughput: 446.446
  learn_time_ms: 36958.518
  load_throughput: 4824467.124
  load_time_ms: 3.42
  training_iteration_time_ms: 48381.755
  update_time_ms: 2.545
timesteps_total: 2656500
training_iteration: 161

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2671232876712329
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8175675675675675
  reward for individual goal_min: 0.0
episode_len_mean: 179.98
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 14392
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.98974251747131
time_total_s: 8157.523251056671
timers:
  learn_throughput: 489.235
  learn_time_ms: 33726.13
  load_throughput: 4702613.801
  load_time_ms: 3.509
  training_iteration_time_ms: 45044.539
  update_time_ms: 2.608
timesteps_total: 2920500
training_iteration: 177

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2642857142857143
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9733333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 166.46
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 14039
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.47038459777832
time_total_s: 8127.902543544769
timers:
  learn_throughput: 439.345
  learn_time_ms: 37555.894
  load_throughput: 4641864.096
  load_time_ms: 3.555
  training_iteration_time_ms: 49498.749
  update_time_ms: 2.876
timesteps_total: 2607000
training_iteration: 158

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28205128205128205
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.09
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 13948
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.3466317653656
time_total_s: 8146.1486558914185
timers:
  learn_throughput: 456.857
  learn_time_ms: 36116.329
  load_throughput: 4905966.469
  load_time_ms: 3.363
  training_iteration_time_ms: 47418.281
  update_time_ms: 2.678
timesteps_total: 2623500
training_iteration: 159

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20394736842105263
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9797297297297297
  reward for individual goal_min: 0.0
episode_len_mean: 183.79
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 12811
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.93821930885315
time_total_s: 8131.823563814163
timers:
  learn_throughput: 429.933
  learn_time_ms: 38378.044
  load_throughput: 3161839.007
  load_time_ms: 5.218
  training_iteration_time_ms: 50869.214
  update_time_ms: 2.589
timesteps_total: 2524500
training_iteration: 153

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 45.319559228650135
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 363
episodes_total: 32590
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.720484495162964
time_total_s: 8115.100056886673
timers:
  learn_throughput: 368.432
  learn_time_ms: 44784.324
  load_throughput: 3529586.585
  load_time_ms: 4.675
  training_iteration_time_ms: 58064.373
  update_time_ms: 2.625
timesteps_total: 2409000
training_iteration: 146

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2962962962962963
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9859154929577465
  reward for individual goal_min: 0.0
episode_len_mean: 177.53
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 16335
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.434704542160034
time_total_s: 8146.452302694321
timers:
  learn_throughput: 518.32
  learn_time_ms: 31833.645
  load_throughput: 5042479.635
  load_time_ms: 3.272
  training_iteration_time_ms: 42611.449
  update_time_ms: 2.498
timesteps_total: 3003000
training_iteration: 182

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23295454545454544
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7777777777777778
  reward for individual goal_min: 0.0
episode_len_mean: 221.62
episode_reward_max: 2.0
episode_reward_mean: 0.95
episode_reward_min: 0.0
episodes_this_iter: 75
episodes_total: 13819
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.46
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.010101556777954
time_total_s: 8157.403436422348
timers:
  learn_throughput: 475.724
  learn_time_ms: 34683.957
  load_throughput: 5009737.446
  load_time_ms: 3.294
  training_iteration_time_ms: 46027.944
  update_time_ms: 2.512
timesteps_total: 2821500
training_iteration: 171

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 170.86
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 12979
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.14867615699768
time_total_s: 8152.100951433182
timers:
  learn_throughput: 441.711
  learn_time_ms: 37354.767
  load_throughput: 4648317.885
  load_time_ms: 3.55
  training_iteration_time_ms: 48815.887
  update_time_ms: 2.625
timesteps_total: 2425500
training_iteration: 147

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2671232876712329
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.926829268292683
  reward for individual goal_min: 0.0
episode_len_mean: 188.12
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 12733
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.423930168151855
time_total_s: 8166.435872554779
timers:
  learn_throughput: 453.269
  learn_time_ms: 36402.204
  load_throughput: 4656042.304
  load_time_ms: 3.544
  training_iteration_time_ms: 48180.634
  update_time_ms: 2.721
timesteps_total: 2574000
training_iteration: 156

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24615384615384617
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.5
episode_len_mean: 162.45544554455446
episode_reward_max: 2.0
episode_reward_mean: 1.4059405940594059
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 13706
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7425742574257426
  agent_1: 0.6633663366336634
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.9155638217926
time_total_s: 8168.2391612529755
timers:
  learn_throughput: 428.181
  learn_time_ms: 38535.149
  load_throughput: 4714113.592
  load_time_ms: 3.5
  training_iteration_time_ms: 50662.602
  update_time_ms: 2.654
timesteps_total: 2524500
training_iteration: 153

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.27748691099477
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 382
episodes_total: 46232
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.838985204696655
time_total_s: 8094.993233203888
timers:
  learn_throughput: 456.791
  learn_time_ms: 36121.577
  load_throughput: 4247305.221
  load_time_ms: 3.885
  training_iteration_time_ms: 47625.346
  update_time_ms: 2.329
timesteps_total: 2887500
training_iteration: 175

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8021978021978022
  reward for individual goal_min: 0.5
episode_len_mean: 88.23655913978494
episode_reward_max: 2.0
episode_reward_mean: 1.8064516129032258
episode_reward_min: 1.0
episodes_this_iter: 186
episodes_total: 21267
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8064516129032258
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 57.73212456703186
time_total_s: 8189.187053203583
timers:
  learn_throughput: 369.902
  learn_time_ms: 44606.445
  load_throughput: 4252054.633
  load_time_ms: 3.88
  training_iteration_time_ms: 58269.721
  update_time_ms: 2.789
timesteps_total: 2260500
training_iteration: 137

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.904
  reward for individual goal_min: 0.5
episode_len_mean: 68.02390438247012
episode_reward_max: 2.0
episode_reward_mean: 1.904382470119522
episode_reward_min: 1.0
episodes_this_iter: 251
episodes_total: 30609
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9043824701195219
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 50.06673502922058
time_total_s: 8166.030953168869
timers:
  learn_throughput: 445.295
  learn_time_ms: 37054.057
  load_throughput: 4823559.226
  load_time_ms: 3.421
  training_iteration_time_ms: 48511.266
  update_time_ms: 2.563
timesteps_total: 2673000
training_iteration: 162

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2792207792207792
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 162.54368932038835
episode_reward_max: 2.0
episode_reward_mean: 1.3203883495145632
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 13094
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6796116504854369
  agent_1: 0.6407766990291263
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.430097818374634
time_total_s: 8181.174904346466
timers:
  learn_throughput: 437.246
  learn_time_ms: 37736.23
  load_throughput: 4483997.408
  load_time_ms: 3.68
  training_iteration_time_ms: 49502.854
  update_time_ms: 2.679
timesteps_total: 2442000
training_iteration: 148

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35074626865671643
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8924050632911392
  reward for individual goal_min: 0.0
episode_len_mean: 168.96
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 14489
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.76
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.62905693054199
time_total_s: 8201.152307987213
timers:
  learn_throughput: 489.864
  learn_time_ms: 33682.789
  load_throughput: 4695243.833
  load_time_ms: 3.514
  training_iteration_time_ms: 44967.377
  update_time_ms: 2.611
timesteps_total: 2937000
training_iteration: 178

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2866666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 164.47058823529412
episode_reward_max: 2.0
episode_reward_mean: 1.3823529411764706
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 14141
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7647058823529411
  agent_1: 0.6176470588235294
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.85362410545349
time_total_s: 8178.756167650223
timers:
  learn_throughput: 437.28
  learn_time_ms: 37733.217
  load_throughput: 4605260.72
  load_time_ms: 3.583
  training_iteration_time_ms: 49722.636
  update_time_ms: 2.888
timesteps_total: 2623500
training_iteration: 159

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2631578947368421
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 179.22
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 12903
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.660271644592285
time_total_s: 8182.4838354587555
timers:
  learn_throughput: 426.247
  learn_time_ms: 38709.985
  load_throughput: 3144141.638
  load_time_ms: 5.248
  training_iteration_time_ms: 51112.934
  update_time_ms: 2.613
timesteps_total: 2541000
training_iteration: 154

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27380952380952384
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.99375
  reward for individual goal_min: 0.5
episode_len_mean: 183.35
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 16423
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.85205340385437
time_total_s: 8188.304356098175
timers:
  learn_throughput: 520.573
  learn_time_ms: 31695.861
  load_throughput: 5108963.236
  load_time_ms: 3.23
  training_iteration_time_ms: 42414.132
  update_time_ms: 2.495
timesteps_total: 3019500
training_iteration: 183

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 82.81666666666666
episode_reward_max: 2.0
episode_reward_mean: 1.9166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.95
  agent_1: 0.9666666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2894736842105263
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.16
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 14044
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.32810091972351
time_total_s: 8201.476756811142
timers:
  learn_throughput: 460.625
  learn_time_ms: 35820.919
  load_throughput: 4978957.531
  load_time_ms: 3.314
  training_iteration_time_ms: 47080.234
  update_time_ms: 2.684
timesteps_total: 2640000
training_iteration: 160

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-18pddilqzj/checkpoint_000160/checkpoint-160
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2865853658536585
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.73125
  reward for individual goal_min: 0.0
episode_len_mean: 210.13
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 13895
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.465569257736206
time_total_s: 8203.869005680084
timers:
  learn_throughput: 477.694
  learn_time_ms: 34540.976
  load_throughput: 5036791.29
  load_time_ms: 3.276
  training_iteration_time_ms: 45867.999
  update_time_ms: 2.504
timesteps_total: 2838000
training_iteration: 172

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.91008174386921
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 367
episodes_total: 32957
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 56.77873706817627
time_total_s: 8171.878793954849
timers:
  learn_throughput: 369.619
  learn_time_ms: 44640.536
  load_throughput: 3550209.865
  load_time_ms: 4.648
  training_iteration_time_ms: 57853.956
  update_time_ms: 2.637
timesteps_total: 2425500
training_iteration: 147

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2191780821917808
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8625
  reward for individual goal_min: 0.0
episode_len_mean: 198.08
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 12819
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.90388631820679
time_total_s: 8212.339758872986
timers:
  learn_throughput: 456.241
  learn_time_ms: 36165.093
  load_throughput: 4700633.444
  load_time_ms: 3.51
  training_iteration_time_ms: 47882.572
  update_time_ms: 2.682
timesteps_total: 2590500
training_iteration: 157

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3670886075949367
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.974025974025974
  reward for individual goal_min: 0.0
episode_len_mean: 154.9532710280374
episode_reward_max: 2.0
episode_reward_mean: 1.4299065420560748
episode_reward_min: 0.0
episodes_this_iter: 107
episodes_total: 13086
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7383177570093458
  agent_1: 0.6915887850467289
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.263988971710205
time_total_s: 8203.364940404892
timers:
  learn_throughput: 441.163
  learn_time_ms: 37401.144
  load_throughput: 4610937.098
  load_time_ms: 3.578
  training_iteration_time_ms: 48841.572
  update_time_ms: 2.622
timesteps_total: 2442000
training_iteration: 148

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28378378378378377
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9846153846153847
  reward for individual goal_min: 0.0
episode_len_mean: 173.81
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 13803
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.117557525634766
time_total_s: 8215.35671877861
timers:
  learn_throughput: 432.053
  learn_time_ms: 38189.765
  load_throughput: 4871981.922
  load_time_ms: 3.387
  training_iteration_time_ms: 50265.931
  update_time_ms: 2.675
timesteps_total: 2541000
training_iteration: 154

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.09866666666667
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 375
episodes_total: 46607
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.46336317062378
time_total_s: 8142.456596374512
timers:
  learn_throughput: 457.008
  learn_time_ms: 36104.409
  load_throughput: 4295548.783
  load_time_ms: 3.841
  training_iteration_time_ms: 47628.053
  update_time_ms: 2.325
timesteps_total: 2904000
training_iteration: 176

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23684210526315788
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8716216216216216
  reward for individual goal_min: 0.0
episode_len_mean: 195.28
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 14569
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.56100058555603
time_total_s: 8244.71330857277
timers:
  learn_throughput: 491.851
  learn_time_ms: 33546.732
  load_throughput: 4679813.365
  load_time_ms: 3.526
  training_iteration_time_ms: 44801.437
  update_time_ms: 2.617
timesteps_total: 2953500
training_iteration: 179

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8717948717948718
  reward for individual goal_min: 0.5
episode_len_mean: 73.95813953488373
episode_reward_max: 2.0
episode_reward_mean: 1.8604651162790697
episode_reward_min: 1.0
episodes_this_iter: 215
episodes_total: 30824
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8604651162790697
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.26776432991028
time_total_s: 8214.29871749878
timers:
  learn_throughput: 446.414
  learn_time_ms: 36961.214
  load_throughput: 4801072.239
  load_time_ms: 3.437
  training_iteration_time_ms: 48395.741
  update_time_ms: 2.557
timesteps_total: 2689500
training_iteration: 163

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20253164556962025
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.41
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 13181
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.18845510482788
time_total_s: 8233.363359451294
timers:
  learn_throughput: 435.675
  learn_time_ms: 37872.296
  load_throughput: 4432078.285
  load_time_ms: 3.723
  training_iteration_time_ms: 49682.672
  update_time_ms: 2.686
timesteps_total: 2458500
training_iteration: 149

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2465753424657534
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.06
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 16516
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.096827268600464
time_total_s: 8231.401183366776
timers:
  learn_throughput: 519.435
  learn_time_ms: 31765.296
  load_throughput: 5105119.134
  load_time_ms: 3.232
  training_iteration_time_ms: 42547.519
  update_time_ms: 2.498
timesteps_total: 3036000
training_iteration: 184

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8
  reward for individual goal_min: 0.5
episode_len_mean: 87.40932642487047
episode_reward_max: 2.0
episode_reward_mean: 1.8031088082901554
episode_reward_min: 1.0
episodes_this_iter: 193
episodes_total: 21460
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9896373056994818
  agent_1: 0.8134715025906736
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.51827263832092
time_total_s: 8246.705325841904
timers:
  learn_throughput: 370.295
  learn_time_ms: 44559.066
  load_throughput: 4117028.62
  load_time_ms: 4.008
  training_iteration_time_ms: 58271.125
  update_time_ms: 2.733
timesteps_total: 2277000
training_iteration: 138

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19480519480519481
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 184.94
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 14133
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.622127294540405
time_total_s: 8245.098884105682
timers:
  learn_throughput: 466.065
  learn_time_ms: 35402.802
  load_throughput: 5011805.397
  load_time_ms: 3.292
  training_iteration_time_ms: 46605.718
  update_time_ms: 2.672
timesteps_total: 2656500
training_iteration: 161

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.208955223880597
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 169.8
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 13000
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.956560134887695
time_total_s: 8233.440395593643
timers:
  learn_throughput: 425.886
  learn_time_ms: 38742.787
  load_throughput: 4374534.835
  load_time_ms: 3.772
  training_iteration_time_ms: 51148.099
  update_time_ms: 2.632
timesteps_total: 2557500
training_iteration: 155

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21686746987951808
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.863013698630137
  reward for individual goal_min: 0.0
episode_len_mean: 207.6
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 13976
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.269349575042725
time_total_s: 8250.138355255127
timers:
  learn_throughput: 474.569
  learn_time_ms: 34768.393
  load_throughput: 4961431.521
  load_time_ms: 3.326
  training_iteration_time_ms: 46157.281
  update_time_ms: 2.517
timesteps_total: 2854500
training_iteration: 173

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 64.91666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.9333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9666666666666667
  agent_1: 0.9666666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2702702702702703
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 168.37
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 14238
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.347325801849365
time_total_s: 8235.103493452072
timers:
  learn_throughput: 438.291
  learn_time_ms: 37646.256
  load_throughput: 4621560.242
  load_time_ms: 3.57
  training_iteration_time_ms: 49600.293
  update_time_ms: 2.882
timesteps_total: 2640000
training_iteration: 160

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25675675675675674
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9102564102564102
  reward for individual goal_min: 0.0
episode_len_mean: 188.05
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 12906
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.20404314994812
time_total_s: 8258.543802022934
timers:
  learn_throughput: 458.155
  learn_time_ms: 36014.01
  load_throughput: 4775068.032
  load_time_ms: 3.455
  training_iteration_time_ms: 47670.552
  update_time_ms: 2.622
timesteps_total: 2607000
training_iteration: 158

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973262032085561
  reward for individual goal_min: 0.5
episode_len_mean: 45.095628415300546
episode_reward_max: 2.0
episode_reward_mean: 1.9972677595628416
episode_reward_min: 1.0
episodes_this_iter: 366
episodes_total: 33323
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9972677595628415
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 54.15991973876953
time_total_s: 8226.038713693619
timers:
  learn_throughput: 372.817
  learn_time_ms: 44257.596
  load_throughput: 3577296.275
  load_time_ms: 4.612
  training_iteration_time_ms: 57343.54
  update_time_ms: 2.628
timesteps_total: 2442000
training_iteration: 148

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21428571428571427
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9746835443037974
  reward for individual goal_min: 0.0
episode_len_mean: 182.89
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 13893
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.87074589729309
time_total_s: 8265.227464675903
timers:
  learn_throughput: 432.75
  learn_time_ms: 38128.293
  load_throughput: 4873011.076
  load_time_ms: 3.386
  training_iteration_time_ms: 50171.643
  update_time_ms: 2.67
timesteps_total: 2557500
training_iteration: 155

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2714285714285714
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 165.81
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 13185
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.61351466178894
time_total_s: 8254.97845506668
timers:
  learn_throughput: 438.653
  learn_time_ms: 37615.156
  load_throughput: 4629815.292
  load_time_ms: 3.564
  training_iteration_time_ms: 49067.423
  update_time_ms: 2.626
timesteps_total: 2458500
training_iteration: 149

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9888268156424581
  reward for individual goal_min: 0.5
episode_len_mean: 45.21487603305785
episode_reward_max: 2.0
episode_reward_mean: 1.9889807162534436
episode_reward_min: 1.0
episodes_this_iter: 363
episodes_total: 46970
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9889807162534435
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 47.56390690803528
time_total_s: 8190.020503282547
timers:
  learn_throughput: 457.119
  learn_time_ms: 36095.669
  load_throughput: 4278764.212
  load_time_ms: 3.856
  training_iteration_time_ms: 47646.581
  update_time_ms: 2.319
timesteps_total: 2920500
training_iteration: 177

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23972602739726026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.84
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 16613
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.373420000076294
time_total_s: 8274.774603366852
timers:
  learn_throughput: 516.977
  learn_time_ms: 31916.321
  load_throughput: 5074759.373
  load_time_ms: 3.251
  training_iteration_time_ms: 42695.659
  update_time_ms: 2.49
timesteps_total: 3052500
training_iteration: 185

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 95.48333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.8
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9666666666666667
  agent_1: 0.8333333333333334
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9053030303030303
  reward for individual goal_min: 0.5
episode_len_mean: 64.7624521072797
episode_reward_max: 2.0
episode_reward_mean: 1.9042145593869733
episode_reward_min: 1.0
episodes_this_iter: 261
episodes_total: 31085
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9042145593869731
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 50.3336398601532
time_total_s: 8264.632357358932
timers:
  learn_throughput: 444.303
  learn_time_ms: 37136.78
  load_throughput: 4783319.003
  load_time_ms: 3.449
  training_iteration_time_ms: 48622.061
  update_time_ms: 2.552
timesteps_total: 2706000
training_iteration: 164

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8181818181818182
  reward for individual goal_min: 0.0
episode_len_mean: 195.96
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 14656
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.58226823806763
time_total_s: 8299.295576810837
timers:
  learn_throughput: 491.905
  learn_time_ms: 33543.039
  load_throughput: 4700729.229
  load_time_ms: 3.51
  training_iteration_time_ms: 44809.394
  update_time_ms: 2.611
timesteps_total: 2970000
training_iteration: 180

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30864197530864196
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.5
episode_len_mean: 179.62
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 14226
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.97835087776184
time_total_s: 8290.077234983444
timers:
  learn_throughput: 467.474
  learn_time_ms: 35296.09
  load_throughput: 5008178.542
  load_time_ms: 3.295
  training_iteration_time_ms: 46419.696
  update_time_ms: 2.668
timesteps_total: 2673000
training_iteration: 162

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.57
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 13274
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.25699591636658
time_total_s: 8287.62035536766
timers:
  learn_throughput: 430.256
  learn_time_ms: 38349.303
  load_throughput: 4448287.751
  load_time_ms: 3.709
  training_iteration_time_ms: 50210.612
  update_time_ms: 2.713
timesteps_total: 2475000
training_iteration: 150

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.325
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.75
  reward for individual goal_min: 0.0
episode_len_mean: 206.86
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 14060
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.933740854263306
time_total_s: 8297.07209610939
timers:
  learn_throughput: 474.044
  learn_time_ms: 34806.924
  load_throughput: 4975342.277
  load_time_ms: 3.316
  training_iteration_time_ms: 46228.286
  update_time_ms: 2.51
timesteps_total: 2871000
training_iteration: 174

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2112676056338028
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9493670886075949
  reward for individual goal_min: 0.0
episode_len_mean: 177.65
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 13093
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.28655481338501
time_total_s: 8283.726950407028
timers:
  learn_throughput: 426.624
  learn_time_ms: 38675.786
  load_throughput: 4354277.517
  load_time_ms: 3.789
  training_iteration_time_ms: 51048.081
  update_time_ms: 2.615
timesteps_total: 2574000
training_iteration: 156

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3051948051948052
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 155.6320754716981
episode_reward_max: 2.0
episode_reward_mean: 1.4150943396226414
episode_reward_min: 0.0
episodes_this_iter: 106
episodes_total: 14344
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6886792452830188
  agent_1: 0.7264150943396226
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.30104422569275
time_total_s: 8285.404537677765
timers:
  learn_throughput: 437.973
  learn_time_ms: 37673.521
  load_throughput: 4654977.501
  load_time_ms: 3.545
  training_iteration_time_ms: 49583.305
  update_time_ms: 2.88
timesteps_total: 2656500
training_iteration: 161

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8556701030927835
  reward for individual goal_min: 0.5
episode_len_mean: 76.01860465116279
episode_reward_max: 2.0
episode_reward_mean: 1.869767441860465
episode_reward_min: 1.0
episodes_this_iter: 215
episodes_total: 21675
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8697674418604651
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 61.31980013847351
time_total_s: 8308.025125980377
timers:
  learn_throughput: 368.798
  learn_time_ms: 44739.893
  load_throughput: 4092052.293
  load_time_ms: 4.032
  training_iteration_time_ms: 58478.931
  update_time_ms: 2.825
timesteps_total: 2293500
training_iteration: 139

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8037974683544303
  reward for individual goal_min: 0.0
episode_len_mean: 201.61
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 12989
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.43
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.75509238243103
time_total_s: 8306.298894405365
timers:
  learn_throughput: 458.75
  learn_time_ms: 35967.3
  load_throughput: 4777936.139
  load_time_ms: 3.453
  training_iteration_time_ms: 47603.923
  update_time_ms: 2.618
timesteps_total: 2623500
training_iteration: 159

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23648648648648649
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9788732394366197
  reward for individual goal_min: 0.0
episode_len_mean: 178.78
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 13983
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.426873445510864
time_total_s: 8314.654338121414
timers:
  learn_throughput: 433.749
  learn_time_ms: 38040.458
  load_throughput: 4852306.117
  load_time_ms: 3.4
  training_iteration_time_ms: 50091.724
  update_time_ms: 2.696
timesteps_total: 2574000
training_iteration: 156

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3246753246753247
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 169.07
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 13284
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.716734886169434
time_total_s: 8306.69518995285
timers:
  learn_throughput: 434.751
  learn_time_ms: 37952.769
  load_throughput: 4616288.746
  load_time_ms: 3.574
  training_iteration_time_ms: 49510.182
  update_time_ms: 2.628
timesteps_total: 2475000
training_iteration: 150

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9942196531791907
  reward for individual goal_min: 0.5
episode_len_mean: 44.716981132075475
episode_reward_max: 2.0
episode_reward_mean: 1.994609164420485
episode_reward_min: 1.0
episodes_this_iter: 371
episodes_total: 47341
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973045822102425
  agent_1: 0.9973045822102425
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.837687730789185
time_total_s: 8238.858191013336
timers:
  learn_throughput: 456.028
  learn_time_ms: 36181.983
  load_throughput: 4290488.962
  load_time_ms: 3.846
  training_iteration_time_ms: 47799.657
  update_time_ms: 2.298
timesteps_total: 2937000
training_iteration: 178

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.028645833333336
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 384
episodes_total: 33707
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.19991970062256
time_total_s: 8283.238633394241
timers:
  learn_throughput: 373.431
  learn_time_ms: 44184.886
  load_throughput: 3566217.459
  load_time_ms: 4.627
  training_iteration_time_ms: 57144.987
  update_time_ms: 2.608
timesteps_total: 2458500
training_iteration: 149

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2558139534883721
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 199.07
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 16698
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.3763689994812
time_total_s: 8317.150972366333
timers:
  learn_throughput: 516.808
  learn_time_ms: 31926.725
  load_throughput: 5060287.943
  load_time_ms: 3.261
  training_iteration_time_ms: 42675.326
  update_time_ms: 2.49
timesteps_total: 3069000
training_iteration: 186

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2777777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8904109589041096
  reward for individual goal_min: 0.0
episode_len_mean: 184.26
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 14743
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.5526819229126
time_total_s: 8342.84825873375
timers:
  learn_throughput: 495.313
  learn_time_ms: 33312.302
  load_throughput: 4741534.562
  load_time_ms: 3.48
  training_iteration_time_ms: 44497.211
  update_time_ms: 2.616
timesteps_total: 2986500
training_iteration: 181

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9915254237288136
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.912
  reward for individual goal_min: 0.5
episode_len_mean: 67.17695473251028
episode_reward_max: 2.0
episode_reward_mean: 1.9012345679012346
episode_reward_min: 0.0
episodes_this_iter: 243
episodes_total: 31328
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9053497942386831
  agent_1: 0.9958847736625515
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.421953439712524
time_total_s: 8314.054310798645
timers:
  learn_throughput: 442.635
  learn_time_ms: 37276.767
  load_throughput: 4572460.325
  load_time_ms: 3.609
  training_iteration_time_ms: 48730.626
  update_time_ms: 2.546
timesteps_total: 2722500
training_iteration: 165

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.84
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 14319
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.21658492088318
time_total_s: 8336.293819904327
timers:
  learn_throughput: 469.645
  learn_time_ms: 35132.936
  load_throughput: 5032103.484
  load_time_ms: 3.279
  training_iteration_time_ms: 46225.047
  update_time_ms: 2.636
timesteps_total: 2689500
training_iteration: 163

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36153846153846153
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8506493506493507
  reward for individual goal_min: 0.0
episode_len_mean: 174.35
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 14155
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.12823843955994
time_total_s: 8342.20033454895
timers:
  learn_throughput: 476.058
  learn_time_ms: 34659.613
  load_throughput: 4957735.114
  load_time_ms: 3.328
  training_iteration_time_ms: 46053.034
  update_time_ms: 2.53
timesteps_total: 2887500
training_iteration: 175

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2702702702702703
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.98
  reward for individual goal_min: 0.0
episode_len_mean: 174.95
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 13187
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.187443017959595
time_total_s: 8331.914393424988
timers:
  learn_throughput: 428.785
  learn_time_ms: 38480.857
  load_throughput: 4349761.852
  load_time_ms: 3.793
  training_iteration_time_ms: 50754.607
  update_time_ms: 2.606
timesteps_total: 2590500
training_iteration: 157

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.972972972972973
  reward for individual goal_min: 0.0
episode_len_mean: 156.23853211009174
episode_reward_max: 2.0
episode_reward_mean: 1.385321100917431
episode_reward_min: 0.0
episodes_this_iter: 109
episodes_total: 13383
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6697247706422018
  agent_1: 0.7155963302752294
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.70466589927673
time_total_s: 8340.325021266937
timers:
  learn_throughput: 426.264
  learn_time_ms: 38708.414
  load_throughput: 4441037.264
  load_time_ms: 3.715
  training_iteration_time_ms: 50630.624
  update_time_ms: 2.708
timesteps_total: 2491500
training_iteration: 151

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 169.94
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 14438
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.02463412284851
time_total_s: 8332.429171800613
timers:
  learn_throughput: 440.034
  learn_time_ms: 37497.067
  load_throughput: 4680034.894
  load_time_ms: 3.526
  training_iteration_time_ms: 49377.288
  update_time_ms: 2.625
timesteps_total: 2673000
training_iteration: 162

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1858974358974359
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 187.71
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 14071
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.197973012924194
time_total_s: 8361.852311134338
timers:
  learn_throughput: 437.08
  learn_time_ms: 37750.495
  load_throughput: 4836300.971
  load_time_ms: 3.412
  training_iteration_time_ms: 49792.362
  update_time_ms: 2.728
timesteps_total: 2590500
training_iteration: 157

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 121.46666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.6666666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8666666666666667
  agent_1: 0.8
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29577464788732394
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.845679012345679
  reward for individual goal_min: 0.0
episode_len_mean: 179.91
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 13083
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.286983013153076
time_total_s: 8366.585877418518
timers:
  learn_throughput: 458.608
  learn_time_ms: 35978.424
  load_throughput: 4824769.832
  load_time_ms: 3.42
  training_iteration_time_ms: 47569.315
  update_time_ms: 2.593
timesteps_total: 2640000
training_iteration: 160

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.199475065616795
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 381
episodes_total: 47722
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.342960596084595
time_total_s: 8287.20115160942
timers:
  learn_throughput: 455.887
  learn_time_ms: 36193.172
  load_throughput: 4350226.67
  load_time_ms: 3.793
  training_iteration_time_ms: 47811.863
  update_time_ms: 2.305
timesteps_total: 2953500
training_iteration: 179

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18571428571428572
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 177.8
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 16792
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.563631534576416
time_total_s: 8359.71460390091
timers:
  learn_throughput: 514.974
  learn_time_ms: 32040.448
  load_throughput: 5069517.852
  load_time_ms: 3.255
  training_iteration_time_ms: 42747.112
  update_time_ms: 2.474
timesteps_total: 3085500
training_iteration: 187

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2876712328767123
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 163.22
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 13382
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.76
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.54687428474426
time_total_s: 8358.242064237595
timers:
  learn_throughput: 430.431
  learn_time_ms: 38333.679
  load_throughput: 4578328.658
  load_time_ms: 3.604
  training_iteration_time_ms: 49907.935
  update_time_ms: 2.637
timesteps_total: 2491500
training_iteration: 151

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24647887323943662
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7828947368421053
  reward for individual goal_min: 0.0
episode_len_mean: 201.92
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 14824
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.34391903877258
time_total_s: 8386.192177772522
timers:
  learn_throughput: 495.077
  learn_time_ms: 33328.124
  load_throughput: 4816241.292
  load_time_ms: 3.426
  training_iteration_time_ms: 44476.037
  update_time_ms: 2.623
timesteps_total: 3003000
training_iteration: 182

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.85
  reward for individual goal_min: 0.5
episode_len_mean: 79.06666666666666
episode_reward_max: 2.0
episode_reward_mean: 1.85
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.85
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8623853211009175
  reward for individual goal_min: 0.5
episode_len_mean: 81.2156862745098
episode_reward_max: 2.0
episode_reward_mean: 1.8529411764705883
episode_reward_min: 1.0
episodes_this_iter: 204
episodes_total: 21879
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9950980392156863
  agent_1: 0.8578431372549019
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 72.04596543312073
time_total_s: 8380.071091413498
timers:
  learn_throughput: 367.818
  learn_time_ms: 44859.086
  load_throughput: 3779374.491
  load_time_ms: 4.366
  training_iteration_time_ms: 58633.776
  update_time_ms: 2.798
timesteps_total: 2310000
training_iteration: 140

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-19wecnkw4d/checkpoint_000140/checkpoint-140
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 45.66298342541437
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 362
episodes_total: 34069
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 55.4145393371582
time_total_s: 8338.6531727314
timers:
  learn_throughput: 374.59
  learn_time_ms: 44048.131
  load_throughput: 3584763.799
  load_time_ms: 4.603
  training_iteration_time_ms: 56918.982
  update_time_ms: 2.594
timesteps_total: 2475000
training_iteration: 150

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23837209302325582
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 202.54
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 14400
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.43084359169006
time_total_s: 8382.724663496017
timers:
  learn_throughput: 472.399
  learn_time_ms: 34928.119
  load_throughput: 5074164.045
  load_time_ms: 3.252
  training_iteration_time_ms: 45994.804
  update_time_ms: 2.655
timesteps_total: 2706000
training_iteration: 164

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9915966386554622
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8803418803418803
  reward for individual goal_min: 0.5
episode_len_mean: 70.17796610169492
episode_reward_max: 2.0
episode_reward_mean: 1.8728813559322033
episode_reward_min: 0.0
episodes_this_iter: 236
episodes_total: 31564
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8771186440677966
  agent_1: 0.9957627118644068
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.0600905418396
time_total_s: 8364.114401340485
timers:
  learn_throughput: 440.364
  learn_time_ms: 37469.015
  load_throughput: 4613826.677
  load_time_ms: 3.576
  training_iteration_time_ms: 48924.3
  update_time_ms: 2.562
timesteps_total: 2739000
training_iteration: 166

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2945205479452055
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8421052631578947
  reward for individual goal_min: 0.0
episode_len_mean: 180.39
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 14248
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.597193002700806
time_total_s: 8388.797527551651
timers:
  learn_throughput: 475.829
  learn_time_ms: 34676.295
  load_throughput: 4954256.998
  load_time_ms: 3.33
  training_iteration_time_ms: 46035.383
  update_time_ms: 2.497
timesteps_total: 2904000
training_iteration: 176

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9927536231884058
  reward for individual goal_min: 0.5
episode_len_mean: 180.09
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 13277
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.645843267440796
time_total_s: 8381.560236692429
timers:
  learn_throughput: 429.975
  learn_time_ms: 38374.341
  load_throughput: 4364327.624
  load_time_ms: 3.781
  training_iteration_time_ms: 50586.57
  update_time_ms: 2.581
timesteps_total: 2607000
training_iteration: 158

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2905405405405405
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9875
  reward for individual goal_min: 0.0
episode_len_mean: 167.48
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 13481
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.04952669143677
time_total_s: 8390.374547958374
timers:
  learn_throughput: 425.609
  learn_time_ms: 38768.007
  load_throughput: 4408672.353
  load_time_ms: 3.743
  training_iteration_time_ms: 50686.466
  update_time_ms: 2.734
timesteps_total: 2508000
training_iteration: 152

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33766233766233766
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 162.7403846153846
episode_reward_max: 2.0
episode_reward_mean: 1.4134615384615385
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 14542
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7019230769230769
  agent_1: 0.7115384615384616
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.794336795806885
time_total_s: 8381.22350859642
timers:
  learn_throughput: 438.624
  learn_time_ms: 37617.673
  load_throughput: 4721832.894
  load_time_ms: 3.494
  training_iteration_time_ms: 49462.266
  update_time_ms: 2.605
timesteps_total: 2689500
training_iteration: 163

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21794871794871795
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9782608695652174
  reward for individual goal_min: 0.0
episode_len_mean: 193.1
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 14158
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.28903007507324
time_total_s: 8410.141341209412
timers:
  learn_throughput: 442.688
  learn_time_ms: 37272.33
  load_throughput: 4863832.676
  load_time_ms: 3.392
  training_iteration_time_ms: 49285.038
  update_time_ms: 2.704
timesteps_total: 2607000
training_iteration: 158

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21518987341772153
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 184.22
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 16884
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.07950806617737
time_total_s: 8401.794111967087
timers:
  learn_throughput: 517.392
  learn_time_ms: 31890.723
  load_throughput: 5063286.753
  load_time_ms: 3.259
  training_iteration_time_ms: 42569.196
  update_time_ms: 2.473
timesteps_total: 3102000
training_iteration: 188

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2986111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.86875
  reward for individual goal_min: 0.0
episode_len_mean: 187.0
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 13171
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.3661789894104
time_total_s: 8412.952056407928
timers:
  learn_throughput: 461.133
  learn_time_ms: 35781.414
  load_throughput: 4816140.742
  load_time_ms: 3.426
  training_iteration_time_ms: 47318.748
  update_time_ms: 2.592
timesteps_total: 2656500
training_iteration: 161

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3433734939759036
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7847222222222222
  reward for individual goal_min: 0.0
episode_len_mean: 192.76
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 14912
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.32344055175781
time_total_s: 8432.51561832428
timers:
  learn_throughput: 489.119
  learn_time_ms: 33734.12
  load_throughput: 4786197.033
  load_time_ms: 3.447
  training_iteration_time_ms: 44966.182
  update_time_ms: 2.645
timesteps_total: 3019500
training_iteration: 183

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.766666666666666
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.994413407821229
  reward for individual goal_min: 0.5
episode_len_mean: 44.06951871657754
episode_reward_max: 2.0
episode_reward_mean: 1.9946524064171123
episode_reward_min: 1.0
episodes_this_iter: 374
episodes_total: 48096
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9946524064171123
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 51.79462265968323
time_total_s: 8338.995774269104
timers:
  learn_throughput: 456.156
  learn_time_ms: 36171.856
  load_throughput: 4350117.292
  load_time_ms: 3.793
  training_iteration_time_ms: 47791.569
  update_time_ms: 2.316
timesteps_total: 2970000
training_iteration: 180

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-172kuwc_58/checkpoint_000180/checkpoint-180
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2777777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 166.43
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 13481
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.849334955215454
time_total_s: 8411.09139919281
timers:
  learn_throughput: 428.748
  learn_time_ms: 38484.158
  load_throughput: 4486526.421
  load_time_ms: 3.678
  training_iteration_time_ms: 50146.461
  update_time_ms: 2.664
timesteps_total: 2508000
training_iteration: 152

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19078947368421054
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939759036144579
  reward for individual goal_min: 0.5
episode_len_mean: 185.1
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 14489
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.32631969451904
time_total_s: 8428.050983190536
timers:
  learn_throughput: 475.196
  learn_time_ms: 34722.511
  load_throughput: 5056664.499
  load_time_ms: 3.263
  training_iteration_time_ms: 45737.158
  update_time_ms: 2.639
timesteps_total: 2722500
training_iteration: 165

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.233870967741936
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 372
episodes_total: 34441
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 56.34809327125549
time_total_s: 8395.001266002655
timers:
  learn_throughput: 376.336
  learn_time_ms: 43843.83
  load_throughput: 3615609.297
  load_time_ms: 4.564
  training_iteration_time_ms: 56645.583
  update_time_ms: 2.588
timesteps_total: 2491500
training_iteration: 151

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8307692307692308
  reward for individual goal_min: 0.0
episode_len_mean: 197.06
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 14328
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.26439714431763
time_total_s: 8434.061924695969
timers:
  learn_throughput: 477.029
  learn_time_ms: 34589.065
  load_throughput: 4988755.803
  load_time_ms: 3.307
  training_iteration_time_ms: 45948.892
  update_time_ms: 2.532
timesteps_total: 2920500
training_iteration: 177

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8364485981308412
  reward for individual goal_min: 0.5
episode_len_mean: 85.48969072164948
episode_reward_max: 2.0
episode_reward_mean: 1.8195876288659794
episode_reward_min: 1.0
episodes_this_iter: 194
episodes_total: 22073
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9896907216494846
  agent_1: 0.8298969072164949
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.52733874320984
time_total_s: 8439.598430156708
timers:
  learn_throughput: 365.991
  learn_time_ms: 45083.091
  load_throughput: 3746820.136
  load_time_ms: 4.404
  training_iteration_time_ms: 58876.144
  update_time_ms: 2.821
timesteps_total: 2326500
training_iteration: 141

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9280575539568345
  reward for individual goal_min: 0.5
episode_len_mean: 60.194945848375454
episode_reward_max: 2.0
episode_reward_mean: 1.9277978339350181
episode_reward_min: 1.0
episodes_this_iter: 277
episodes_total: 31841
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.927797833935018
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 51.625853538513184
time_total_s: 8415.740254878998
timers:
  learn_throughput: 433.806
  learn_time_ms: 38035.415
  load_throughput: 4629288.81
  load_time_ms: 3.564
  training_iteration_time_ms: 49620.592
  update_time_ms: 2.576
timesteps_total: 2755500
training_iteration: 167

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935897435897436
  reward for individual goal_min: 0.5
episode_len_mean: 176.86
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 13369
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.872424840927124
time_total_s: 8429.432661533356
timers:
  learn_throughput: 432.09
  learn_time_ms: 38186.52
  load_throughput: 4377634.006
  load_time_ms: 3.769
  training_iteration_time_ms: 50277.123
  update_time_ms: 2.577
timesteps_total: 2623500
training_iteration: 159

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2972972972972973
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9873417721518988
  reward for individual goal_min: 0.0
episode_len_mean: 159.53398058252426
episode_reward_max: 2.0
episode_reward_mean: 1.3980582524271845
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 14645
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6310679611650486
  agent_1: 0.7669902912621359
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.255605697631836
time_total_s: 8429.479114294052
timers:
  learn_throughput: 440.584
  learn_time_ms: 37450.322
  load_throughput: 4718999.55
  load_time_ms: 3.497
  training_iteration_time_ms: 49261.352
  update_time_ms: 2.606
timesteps_total: 2706000
training_iteration: 164

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3076923076923077
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9852941176470589
  reward for individual goal_min: 0.0
episode_len_mean: 172.89
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 13574
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.734760999679565
time_total_s: 8444.109308958054
timers:
  learn_throughput: 420.973
  learn_time_ms: 39194.928
  load_throughput: 4395651.478
  load_time_ms: 3.754
  training_iteration_time_ms: 51290.668
  update_time_ms: 2.744
timesteps_total: 2524500
training_iteration: 153

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 179.43
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 14252
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.57889914512634
time_total_s: 8456.720240354538
timers:
  learn_throughput: 446.108
  learn_time_ms: 36986.547
  load_throughput: 4890124.221
  load_time_ms: 3.374
  training_iteration_time_ms: 48968.73
  update_time_ms: 2.72
timesteps_total: 2623500
training_iteration: 159

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2905405405405405
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9857142857142858
  reward for individual goal_min: 0.0
episode_len_mean: 173.53
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 16980
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.902740240097046
time_total_s: 8446.696852207184
timers:
  learn_throughput: 514.321
  learn_time_ms: 32081.146
  load_throughput: 5072713.519
  load_time_ms: 3.253
  training_iteration_time_ms: 42825.38
  update_time_ms: 2.499
timesteps_total: 3118500
training_iteration: 189

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31097560975609756
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 189.87
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 13256
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.18094849586487
time_total_s: 8462.133004903793
timers:
  learn_throughput: 461.02
  learn_time_ms: 35790.173
  load_throughput: 4872462.14
  load_time_ms: 3.386
  training_iteration_time_ms: 47339.513
  update_time_ms: 2.567
timesteps_total: 2673000
training_iteration: 162

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2848101265822785
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8417721518987342
  reward for individual goal_min: 0.0
episode_len_mean: 199.62
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 14994
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.53900957107544
time_total_s: 8475.054627895355
timers:
  learn_throughput: 493.517
  learn_time_ms: 33433.523
  load_throughput: 4706387.482
  load_time_ms: 3.506
  training_iteration_time_ms: 44639.469
  update_time_ms: 2.636
timesteps_total: 3036000
training_iteration: 184

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972067039106145
  reward for individual goal_min: 0.5
episode_len_mean: 43.635883905013195
episode_reward_max: 2.0
episode_reward_mean: 1.9973614775725594
episode_reward_min: 1.0
episodes_this_iter: 379
episodes_total: 48475
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973614775725593
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 48.223055362701416
time_total_s: 8387.218829631805
timers:
  learn_throughput: 456.653
  learn_time_ms: 36132.445
  load_throughput: 4325403.034
  load_time_ms: 3.815
  training_iteration_time_ms: 47747.657
  update_time_ms: 2.32
timesteps_total: 2986500
training_iteration: 181

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.69
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 13575
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.49993133544922
time_total_s: 8462.59133052826
timers:
  learn_throughput: 425.923
  learn_time_ms: 38739.421
  load_throughput: 4464529.814
  load_time_ms: 3.696
  training_iteration_time_ms: 50511.244
  update_time_ms: 2.699
timesteps_total: 2524500
training_iteration: 153

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 175.6
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 14580
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.645779848098755
time_total_s: 8475.696763038635
timers:
  learn_throughput: 475.472
  learn_time_ms: 34702.379
  load_throughput: 5022170.812
  load_time_ms: 3.285
  training_iteration_time_ms: 45753.962
  update_time_ms: 2.643
timesteps_total: 2739000
training_iteration: 166

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34782608695652173
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.855072463768116
  reward for individual goal_min: 0.0
episode_len_mean: 183.71
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 14417
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.11007475852966
time_total_s: 8480.171999454498
timers:
  learn_throughput: 477.355
  learn_time_ms: 34565.46
  load_throughput: 4963317.388
  load_time_ms: 3.324
  training_iteration_time_ms: 45932.284
  update_time_ms: 2.538
timesteps_total: 2937000
training_iteration: 178

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9
  reward for individual goal_min: 0.5
episode_len_mean: 66.15510204081633
episode_reward_max: 2.0
episode_reward_mean: 1.9020408163265305
episode_reward_min: 1.0
episodes_this_iter: 245
episodes_total: 32086
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9020408163265307
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 50.42144966125488
time_total_s: 8466.161704540253
timers:
  learn_throughput: 433.132
  learn_time_ms: 38094.662
  load_throughput: 4561279.684
  load_time_ms: 3.617
  training_iteration_time_ms: 49675.145
  update_time_ms: 2.59
timesteps_total: 2772000
training_iteration: 168

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.16577540106952
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 374
episodes_total: 34815
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 56.74107575416565
time_total_s: 8451.74234175682
timers:
  learn_throughput: 377.329
  learn_time_ms: 43728.377
  load_throughput: 3636221.181
  load_time_ms: 4.538
  training_iteration_time_ms: 56488.077
  update_time_ms: 2.595
timesteps_total: 2508000
training_iteration: 152

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15492957746478872
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9943181818181818
  reward for individual goal_min: 0.5
episode_len_mean: 178.92
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 14732
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.874865770339966
time_total_s: 8474.353980064392
timers:
  learn_throughput: 445.344
  learn_time_ms: 37049.999
  load_throughput: 4727380.63
  load_time_ms: 3.49
  training_iteration_time_ms: 48742.678
  update_time_ms: 2.606
timesteps_total: 2722500
training_iteration: 165

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8495145631067961
  reward for individual goal_min: 0.5
episode_len_mean: 80.76616915422886
episode_reward_max: 2.0
episode_reward_mean: 1.845771144278607
episode_reward_min: 1.0
episodes_this_iter: 201
episodes_total: 22274
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.845771144278607
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 59.590996980667114
time_total_s: 8499.189427137375
timers:
  learn_throughput: 365.363
  learn_time_ms: 45160.603
  load_throughput: 3675735.667
  load_time_ms: 4.489
  training_iteration_time_ms: 58924.092
  update_time_ms: 2.799
timesteps_total: 2343000
training_iteration: 142

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.39
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 17075
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.75431251525879
time_total_s: 8490.451164722443
timers:
  learn_throughput: 513.602
  learn_time_ms: 32126.061
  load_throughput: 5144548.962
  load_time_ms: 3.207
  training_iteration_time_ms: 42830.577
  update_time_ms: 2.503
timesteps_total: 3135000
training_iteration: 190

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9333333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 95.31666666666666
episode_reward_max: 2.0
episode_reward_mean: 1.7666666666666666
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9166666666666666
  agent_1: 0.85
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2702702702702703
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.18
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 13463
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.749372243881226
time_total_s: 8488.182033777237
timers:
  learn_throughput: 435.721
  learn_time_ms: 37868.249
  load_throughput: 4401522.336
  load_time_ms: 3.749
  training_iteration_time_ms: 49881.257
  update_time_ms: 2.568
timesteps_total: 2640000
training_iteration: 160

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3092105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 176.33
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 13667
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.21115493774414
time_total_s: 8498.320463895798
timers:
  learn_throughput: 413.576
  learn_time_ms: 39895.932
  load_throughput: 4382540.766
  load_time_ms: 3.765
  training_iteration_time_ms: 52188.483
  update_time_ms: 2.746
timesteps_total: 2541000
training_iteration: 154

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36666666666666664
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9054054054054054
  reward for individual goal_min: 0.0
episode_len_mean: 171.99
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 15088
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.904731035232544
time_total_s: 8518.959358930588
timers:
  learn_throughput: 494.742
  learn_time_ms: 33350.721
  load_throughput: 4764121.846
  load_time_ms: 3.463
  training_iteration_time_ms: 44520.064
  update_time_ms: 2.617
timesteps_total: 3052500
training_iteration: 185

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29012345679012347
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9214285714285714
  reward for individual goal_min: 0.0
episode_len_mean: 193.6
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 13343
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.7785120010376
time_total_s: 8511.911516904831
timers:
  learn_throughput: 460.372
  learn_time_ms: 35840.613
  load_throughput: 4828304.24
  load_time_ms: 3.417
  training_iteration_time_ms: 47337.989
  update_time_ms: 2.555
timesteps_total: 2689500
training_iteration: 163

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 69.13333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.9166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9666666666666667
  agent_1: 0.95
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2714285714285714
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9753086419753086
  reward for individual goal_min: 0.0
episode_len_mean: 165.79
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 14352
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.523494243621826
time_total_s: 8514.24373459816
timers:
  learn_throughput: 450.17
  learn_time_ms: 36652.843
  load_throughput: 4904610.5
  load_time_ms: 3.364
  training_iteration_time_ms: 48517.915
  update_time_ms: 2.739
timesteps_total: 2640000
training_iteration: 160

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19igysrqn6/checkpoint_000160/checkpoint-160
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.859693877551024
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 392
episodes_total: 48867
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.02753019332886
time_total_s: 8435.246359825134
timers:
  learn_throughput: 456.297
  learn_time_ms: 36160.693
  load_throughput: 4367743.108
  load_time_ms: 3.778
  training_iteration_time_ms: 47768.648
  update_time_ms: 2.319
timesteps_total: 3003000
training_iteration: 182

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23684210526315788
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 180.3
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 14670
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.147029399871826
time_total_s: 8521.843792438507
timers:
  learn_throughput: 474.3
  learn_time_ms: 34788.118
  load_throughput: 5001446.535
  load_time_ms: 3.299
  training_iteration_time_ms: 45899.925
  update_time_ms: 2.616
timesteps_total: 2755500
training_iteration: 167

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2426470588235294
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9802631578947368
  reward for individual goal_min: 0.0
episode_len_mean: 170.72
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 13673
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.771326303482056
time_total_s: 8514.362656831741
timers:
  learn_throughput: 423.546
  learn_time_ms: 38956.798
  load_throughput: 4425021.963
  load_time_ms: 3.729
  training_iteration_time_ms: 50897.437
  update_time_ms: 2.708
timesteps_total: 2541000
training_iteration: 154

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33974358974358976
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8666666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 186.52
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 14505
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.651490688323975
time_total_s: 8526.823490142822
timers:
  learn_throughput: 475.366
  learn_time_ms: 34710.081
  load_throughput: 4939088.632
  load_time_ms: 3.341
  training_iteration_time_ms: 46083.893
  update_time_ms: 2.514
timesteps_total: 2953500
training_iteration: 179

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8903508771929824
  reward for individual goal_min: 0.5
episode_len_mean: 70.1344537815126
episode_reward_max: 2.0
episode_reward_mean: 1.8949579831932772
episode_reward_min: 1.0
episodes_this_iter: 238
episodes_total: 32324
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8949579831932774
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 50.16981291770935
time_total_s: 8516.331517457962
timers:
  learn_throughput: 430.727
  learn_time_ms: 38307.335
  load_throughput: 4544685.479
  load_time_ms: 3.631
  training_iteration_time_ms: 49973.526
  update_time_ms: 2.618
timesteps_total: 2788500
training_iteration: 169

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 191.26
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 17160
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.68152141571045
time_total_s: 8532.132686138153
timers:
  learn_throughput: 514.416
  learn_time_ms: 32075.235
  load_throughput: 5173159.913
  load_time_ms: 3.19
  training_iteration_time_ms: 42775.394
  update_time_ms: 2.501
timesteps_total: 3151500
training_iteration: 191

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.63
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 14828
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.8505973815918
time_total_s: 8524.204577445984
timers:
  learn_throughput: 446.636
  learn_time_ms: 36942.854
  load_throughput: 4762384.288
  load_time_ms: 3.465
  training_iteration_time_ms: 48612.806
  update_time_ms: 2.624
timesteps_total: 2739000
training_iteration: 166

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.474393530997304
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 371
episodes_total: 35186
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 56.913907289505005
time_total_s: 8508.656249046326
timers:
  learn_throughput: 377.899
  learn_time_ms: 43662.419
  load_throughput: 3616440.624
  load_time_ms: 4.562
  training_iteration_time_ms: 56431.587
  update_time_ms: 2.567
timesteps_total: 2524500
training_iteration: 153

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2948717948717949
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8450704225352113
  reward for individual goal_min: 0.0
episode_len_mean: 192.87
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 15175
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.47866106033325
time_total_s: 8563.438019990921
timers:
  learn_throughput: 497.083
  learn_time_ms: 33193.632
  load_throughput: 4767108.16
  load_time_ms: 3.461
  training_iteration_time_ms: 44274.067
  update_time_ms: 2.614
timesteps_total: 3069000
training_iteration: 186

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.189873417721519
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9810126582278481
  reward for individual goal_min: 0.0
episode_len_mean: 190.46
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 13550
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.052732944488525
time_total_s: 8538.234766721725
timers:
  learn_throughput: 436.42
  learn_time_ms: 37807.632
  load_throughput: 4414775.198
  load_time_ms: 3.737
  training_iteration_time_ms: 49850.903
  update_time_ms: 2.571
timesteps_total: 2656500
training_iteration: 161

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8365384615384616
  reward for individual goal_min: 0.5
episode_len_mean: 80.7846889952153
episode_reward_max: 2.0
episode_reward_mean: 1.8373205741626795
episode_reward_min: 1.0
episodes_this_iter: 209
episodes_total: 22483
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9952153110047847
  agent_1: 0.8421052631578947
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.088170766830444
time_total_s: 8560.277597904205
timers:
  learn_throughput: 363.363
  learn_time_ms: 45409.171
  load_throughput: 3644647.048
  load_time_ms: 4.527
  training_iteration_time_ms: 59216.911
  update_time_ms: 2.844
timesteps_total: 2359500
training_iteration: 143

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28378378378378377
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9533333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 167.78
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 13765
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.18725323677063
time_total_s: 8552.507717132568
timers:
  learn_throughput: 409.087
  learn_time_ms: 40333.747
  load_throughput: 4357649.844
  load_time_ms: 3.786
  training_iteration_time_ms: 52786.806
  update_time_ms: 2.76
timesteps_total: 2557500
training_iteration: 155

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24305555555555555
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8918918918918919
  reward for individual goal_min: 0.0
episode_len_mean: 193.1
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 13429
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.87752890586853
time_total_s: 8560.7890458107
timers:
  learn_throughput: 458.753
  learn_time_ms: 35967.053
  load_throughput: 4802504.858
  load_time_ms: 3.436
  training_iteration_time_ms: 47497.391
  update_time_ms: 2.545
timesteps_total: 2706000
training_iteration: 164

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3148148148148148
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 176.21
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 14445
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.41432809829712
time_total_s: 8562.658062696457
timers:
  learn_throughput: 450.105
  learn_time_ms: 36658.086
  load_throughput: 4974913.09
  load_time_ms: 3.317
  training_iteration_time_ms: 48371.636
  update_time_ms: 2.754
timesteps_total: 2656500
training_iteration: 161

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.62015503875969
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 387
episodes_total: 49254
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.78834080696106
time_total_s: 8483.034700632095
timers:
  learn_throughput: 455.673
  learn_time_ms: 36210.184
  load_throughput: 4384956.598
  load_time_ms: 3.763
  training_iteration_time_ms: 47823.306
  update_time_ms: 2.33
timesteps_total: 3019500
training_iteration: 183

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2922077922077922
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9850746268656716
  reward for individual goal_min: 0.0
episode_len_mean: 182.28
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 14762
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.011898040771484
time_total_s: 8569.855690479279
timers:
  learn_throughput: 474.44
  learn_time_ms: 34777.811
  load_throughput: 5006874.159
  load_time_ms: 3.295
  training_iteration_time_ms: 45898.011
  update_time_ms: 2.62
timesteps_total: 2772000
training_iteration: 168

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26811594202898553
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9788732394366197
  reward for individual goal_min: 0.0
episode_len_mean: 160.57843137254903
episode_reward_max: 2.0
episode_reward_mean: 1.3431372549019607
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 13775
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.696078431372549
  agent_1: 0.6470588235294118
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.22611618041992
time_total_s: 8568.588773012161
timers:
  learn_throughput: 419.48
  learn_time_ms: 39334.415
  load_throughput: 4410133.248
  load_time_ms: 3.741
  training_iteration_time_ms: 51466.972
  update_time_ms: 2.711
timesteps_total: 2557500
training_iteration: 155

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28289473684210525
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.0
episode_len_mean: 172.89
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 17257
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.93119239807129
time_total_s: 8576.063878536224
timers:
  learn_throughput: 512.458
  learn_time_ms: 32197.755
  load_throughput: 5178579.307
  load_time_ms: 3.186
  training_iteration_time_ms: 42924.965
  update_time_ms: 2.55
timesteps_total: 3168000
training_iteration: 192

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8166666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 129.25
episode_reward_max: 2.0
episode_reward_mean: 1.65
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8166666666666667
  agent_1: 0.8333333333333334
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2948717948717949
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8571428571428571
  reward for individual goal_min: 0.0
episode_len_mean: 198.34
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 14587
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.391584396362305
time_total_s: 8587.215074539185
timers:
  learn_throughput: 472.702
  learn_time_ms: 34905.713
  load_throughput: 4895070.414
  load_time_ms: 3.371
  training_iteration_time_ms: 46291.014
  update_time_ms: 2.507
timesteps_total: 2970000
training_iteration: 180

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 188.54
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 14916
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.87371301651001
time_total_s: 8571.078290462494
timers:
  learn_throughput: 447.353
  learn_time_ms: 36883.657
  load_throughput: 4787786.395
  load_time_ms: 3.446
  training_iteration_time_ms: 48477.631
  update_time_ms: 2.62
timesteps_total: 2755500
training_iteration: 167

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9507042253521126
  reward for individual goal_min: 0.5
episode_len_mean: 54.95681063122924
episode_reward_max: 2.0
episode_reward_mean: 1.9534883720930232
episode_reward_min: 1.0
episodes_this_iter: 301
episodes_total: 32625
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9534883720930233
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 53.4688994884491
time_total_s: 8569.800416946411
timers:
  learn_throughput: 428.138
  learn_time_ms: 38538.974
  load_throughput: 4538337.487
  load_time_ms: 3.636
  training_iteration_time_ms: 50254.541
  update_time_ms: 2.608
timesteps_total: 2805000
training_iteration: 170

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31756756756756754
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7916666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 199.78
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 15258
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.21453261375427
time_total_s: 8608.652552604675
timers:
  learn_throughput: 497.52
  learn_time_ms: 33164.502
  load_throughput: 4765466.865
  load_time_ms: 3.462
  training_iteration_time_ms: 44196.491
  update_time_ms: 2.63
timesteps_total: 3085500
training_iteration: 187

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22972972972972974
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9810126582278481
  reward for individual goal_min: 0.0
episode_len_mean: 178.61
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 13642
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.48932671546936
time_total_s: 8586.724093437195
timers:
  learn_throughput: 441.788
  learn_time_ms: 37348.267
  load_throughput: 4425135.14
  load_time_ms: 3.729
  training_iteration_time_ms: 49255.859
  update_time_ms: 2.572
timesteps_total: 2673000
training_iteration: 162

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.49189189189189
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 370
episodes_total: 35556
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 56.26942324638367
time_total_s: 8564.92567229271
timers:
  learn_throughput: 379.158
  learn_time_ms: 43517.426
  load_throughput: 3610214.977
  load_time_ms: 4.57
  training_iteration_time_ms: 56245.366
  update_time_ms: 2.567
timesteps_total: 2541000
training_iteration: 154

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30128205128205127
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8767123287671232
  reward for individual goal_min: 0.0
episode_len_mean: 188.13
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 13515
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.68600034713745
time_total_s: 8607.475046157837
timers:
  learn_throughput: 458.713
  learn_time_ms: 35970.223
  load_throughput: 4786263.235
  load_time_ms: 3.447
  training_iteration_time_ms: 47469.363
  update_time_ms: 2.541
timesteps_total: 2722500
training_iteration: 165

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2357142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 168.54
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 14545
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.098697900772095
time_total_s: 8609.756760597229
timers:
  learn_throughput: 451.33
  learn_time_ms: 36558.609
  load_throughput: 4968876.571
  load_time_ms: 3.321
  training_iteration_time_ms: 48263.132
  update_time_ms: 2.747
timesteps_total: 2673000
training_iteration: 162

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2088607594936709
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.961038961038961
  reward for individual goal_min: 0.0
episode_len_mean: 182.53
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 13852
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.208537578582764
time_total_s: 8606.716254711151
timers:
  learn_throughput: 408.246
  learn_time_ms: 40416.768
  load_throughput: 4334831.351
  load_time_ms: 3.806
  training_iteration_time_ms: 52867.691
  update_time_ms: 2.769
timesteps_total: 2574000
training_iteration: 156

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25925925925925924
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 189.78
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 14851
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.3070182800293
time_total_s: 8615.162708759308
timers:
  learn_throughput: 475.035
  learn_time_ms: 34734.258
  load_throughput: 4973125.611
  load_time_ms: 3.318
  training_iteration_time_ms: 45794.058
  update_time_ms: 2.599
timesteps_total: 2788500
training_iteration: 169

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971264367816092
  reward for individual goal_min: 0.5
episode_len_mean: 43.38845144356955
episode_reward_max: 2.0
episode_reward_mean: 1.9973753280839894
episode_reward_min: 1.0
episodes_this_iter: 381
episodes_total: 49635
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973753280839895
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 49.027156591415405
time_total_s: 8532.06185722351
timers:
  learn_throughput: 454.06
  learn_time_ms: 36338.821
  load_throughput: 4385706.88
  load_time_ms: 3.762
  training_iteration_time_ms: 47982.875
  update_time_ms: 2.323
timesteps_total: 3036000
training_iteration: 184

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8349514563106796
  reward for individual goal_min: 0.5
episode_len_mean: 80.26086956521739
episode_reward_max: 2.0
episode_reward_mean: 1.8357487922705313
episode_reward_min: 1.0
episodes_this_iter: 207
episodes_total: 22690
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9951690821256038
  agent_1: 0.8405797101449275
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.70553660392761
time_total_s: 8623.983134508133
timers:
  learn_throughput: 360.017
  learn_time_ms: 45831.165
  load_throughput: 3544046.622
  load_time_ms: 4.656
  training_iteration_time_ms: 59704.244
  update_time_ms: 2.957
timesteps_total: 2376000
training_iteration: 144

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9857142857142858
  reward for individual goal_min: 0.0
episode_len_mean: 180.05
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 17351
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.72817134857178
time_total_s: 8619.792049884796
timers:
  learn_throughput: 509.908
  learn_time_ms: 32358.766
  load_throughput: 5138208.465
  load_time_ms: 3.211
  training_iteration_time_ms: 43112.382
  update_time_ms: 2.537
timesteps_total: 3184500
training_iteration: 193

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3475609756097561
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9615384615384616
  reward for individual goal_min: 0.0
episode_len_mean: 179.79
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 13869
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.9678840637207
time_total_s: 8619.556657075882
timers:
  learn_throughput: 417.599
  learn_time_ms: 39511.636
  load_throughput: 4420160.824
  load_time_ms: 3.733
  training_iteration_time_ms: 51720.802
  update_time_ms: 2.719
timesteps_total: 2574000
training_iteration: 156

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21830985915492956
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8445945945945946
  reward for individual goal_min: 0.0
episode_len_mean: 198.3
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 14670
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.85284209251404
time_total_s: 8632.067916631699
timers:
  learn_throughput: 474.371
  learn_time_ms: 34782.866
  load_throughput: 4907218.799
  load_time_ms: 3.362
  training_iteration_time_ms: 46175.368
  update_time_ms: 2.519
timesteps_total: 2986500
training_iteration: 181

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 160.1747572815534
episode_reward_max: 2.0
episode_reward_mean: 1.3980582524271845
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 15019
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6116504854368932
  agent_1: 0.7864077669902912
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.329097509384155
time_total_s: 8618.407387971878
timers:
  learn_throughput: 450.823
  learn_time_ms: 36599.751
  load_throughput: 4851693.809
  load_time_ms: 3.401
  training_iteration_time_ms: 48162.994
  update_time_ms: 2.642
timesteps_total: 2772000
training_iteration: 168

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2708333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8424657534246576
  reward for individual goal_min: 0.0
episode_len_mean: 184.39
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 15346
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.54392337799072
time_total_s: 8654.196475982666
timers:
  learn_throughput: 495.837
  learn_time_ms: 33277.076
  load_throughput: 4743256.936
  load_time_ms: 3.479
  training_iteration_time_ms: 44387.895
  update_time_ms: 2.626
timesteps_total: 3102000
training_iteration: 188

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9323308270676691
  reward for individual goal_min: 0.5
episode_len_mean: 61.20220588235294
episode_reward_max: 2.0
episode_reward_mean: 1.9338235294117647
episode_reward_min: 1.0
episodes_this_iter: 272
episodes_total: 32897
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9338235294117647
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 51.134798765182495
time_total_s: 8620.935215711594
timers:
  learn_throughput: 426.523
  learn_time_ms: 38684.898
  load_throughput: 4516155.337
  load_time_ms: 3.654
  training_iteration_time_ms: 50419.189
  update_time_ms: 2.594
timesteps_total: 2821500
training_iteration: 171

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19230769230769232
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 192.54
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 13726
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.376407861709595
time_total_s: 8634.100501298904
timers:
  learn_throughput: 443.901
  learn_time_ms: 37170.458
  load_throughput: 4660557.467
  load_time_ms: 3.54
  training_iteration_time_ms: 48999.842
  update_time_ms: 2.588
timesteps_total: 2689500
training_iteration: 163

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8581081081081081
  reward for individual goal_min: 0.0
episode_len_mean: 211.15
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 13592
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.22549057006836
time_total_s: 8656.700536727905
timers:
  learn_throughput: 456.355
  learn_time_ms: 36156.03
  load_throughput: 4761401.326
  load_time_ms: 3.465
  training_iteration_time_ms: 47749.748
  update_time_ms: 2.518
timesteps_total: 2739000
training_iteration: 166

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2972972972972973
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 166.22
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 14641
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.501301765441895
time_total_s: 8659.25806236267
timers:
  learn_throughput: 452.154
  learn_time_ms: 36491.984
  load_throughput: 4962854.684
  load_time_ms: 3.325
  training_iteration_time_ms: 48221.694
  update_time_ms: 2.75
timesteps_total: 2689500
training_iteration: 163

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3108108108108108
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 168.6
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 14947
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.226497650146484
time_total_s: 8662.389206409454
timers:
  learn_throughput: 472.511
  learn_time_ms: 34919.828
  load_throughput: 4996824.26
  load_time_ms: 3.302
  training_iteration_time_ms: 46054.215
  update_time_ms: 2.581
timesteps_total: 2805000
training_iteration: 170

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.26809651474531
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 373
episodes_total: 35929
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 56.204970598220825
time_total_s: 8621.13064289093
timers:
  learn_throughput: 379.199
  learn_time_ms: 43512.718
  load_throughput: 3611345.318
  load_time_ms: 4.569
  training_iteration_time_ms: 56268.645
  update_time_ms: 2.587
timesteps_total: 2557500
training_iteration: 155

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24358974358974358
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.37
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 13947
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.93523454666138
time_total_s: 8657.651489257812
timers:
  learn_throughput: 408.098
  learn_time_ms: 40431.483
  load_throughput: 4324997.563
  load_time_ms: 3.815
  training_iteration_time_ms: 52852.082
  update_time_ms: 2.816
timesteps_total: 2590500
training_iteration: 157

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.055837563451774
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 394
episodes_total: 50029
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.554683446884155
time_total_s: 8580.616540670395
timers:
  learn_throughput: 453.205
  learn_time_ms: 36407.329
  load_throughput: 4355483.278
  load_time_ms: 3.788
  training_iteration_time_ms: 48054.037
  update_time_ms: 2.322
timesteps_total: 3052500
training_iteration: 185

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21014492753623187
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.26
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 17448
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.651933908462524
time_total_s: 8662.443983793259
timers:
  learn_throughput: 510.883
  learn_time_ms: 32296.993
  load_throughput: 5115193.91
  load_time_ms: 3.226
  training_iteration_time_ms: 43068.061
  update_time_ms: 2.532
timesteps_total: 3201000
training_iteration: 194

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8618421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 195.13
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 14755
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.13088774681091
time_total_s: 8679.19880437851
timers:
  learn_throughput: 473.643
  learn_time_ms: 34836.336
  load_throughput: 4852170.036
  load_time_ms: 3.401
  training_iteration_time_ms: 46241.826
  update_time_ms: 2.521
timesteps_total: 3003000
training_iteration: 182

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.13
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 13961
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.46372938156128
time_total_s: 8672.020386457443
timers:
  learn_throughput: 415.959
  learn_time_ms: 39667.415
  load_throughput: 4462140.61
  load_time_ms: 3.698
  training_iteration_time_ms: 51952.37
  update_time_ms: 2.706
timesteps_total: 2590500
training_iteration: 157

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.63
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 15112
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.54825448989868
time_total_s: 8665.955642461777
timers:
  learn_throughput: 453.587
  learn_time_ms: 36376.671
  load_throughput: 4860382.617
  load_time_ms: 3.395
  training_iteration_time_ms: 47832.542
  update_time_ms: 2.614
timesteps_total: 2788500
training_iteration: 169

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8935185185185185
  reward for individual goal_min: 0.5
episode_len_mean: 66.99163179916317
episode_reward_max: 2.0
episode_reward_mean: 1.903765690376569
episode_reward_min: 1.0
episodes_this_iter: 239
episodes_total: 22929
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9037656903765691
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 64.49992156028748
time_total_s: 8688.48305606842
timers:
  learn_throughput: 355.848
  learn_time_ms: 46368.146
  load_throughput: 3523512.616
  load_time_ms: 4.683
  training_iteration_time_ms: 60316.677
  update_time_ms: 2.972
timesteps_total: 2392500
training_iteration: 145

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9038461538461539
  reward for individual goal_min: 0.0
episode_len_mean: 183.31
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 15437
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.2714581489563
time_total_s: 8697.467934131622
timers:
  learn_throughput: 495.864
  learn_time_ms: 33275.232
  load_throughput: 4748138.37
  load_time_ms: 3.475
  training_iteration_time_ms: 44359.198
  update_time_ms: 2.624
timesteps_total: 3118500
training_iteration: 189

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9482758620689655
  reward for individual goal_min: 0.5
episode_len_mean: 59.586715867158674
episode_reward_max: 2.0
episode_reward_mean: 1.944649446494465
episode_reward_min: 1.0
episodes_this_iter: 271
episodes_total: 33168
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.948339483394834
  agent_1: 0.996309963099631
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.912625789642334
time_total_s: 8671.847841501236
timers:
  learn_throughput: 425.692
  learn_time_ms: 38760.398
  load_throughput: 4478687.056
  load_time_ms: 3.684
  training_iteration_time_ms: 50503.647
  update_time_ms: 2.583
timesteps_total: 2838000
training_iteration: 172

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23972602739726026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.87
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 13822
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.63117074966431
time_total_s: 8683.731672048569
timers:
  learn_throughput: 445.672
  learn_time_ms: 37022.767
  load_throughput: 4748105.794
  load_time_ms: 3.475
  training_iteration_time_ms: 48897.198
  update_time_ms: 2.572
timesteps_total: 2706000
training_iteration: 164

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3835616438356164
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8732394366197183
  reward for individual goal_min: 0.0
episode_len_mean: 178.24
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 13687
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.14000368118286
time_total_s: 8706.840540409088
timers:
  learn_throughput: 451.809
  learn_time_ms: 36519.892
  load_throughput: 4699994.974
  load_time_ms: 3.511
  training_iteration_time_ms: 48173.342
  update_time_ms: 2.518
timesteps_total: 2755500
training_iteration: 167

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23076923076923078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 183.82
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 15041
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.75844693183899
time_total_s: 8708.147653341293
timers:
  learn_throughput: 470.791
  learn_time_ms: 35047.381
  load_throughput: 4942368.989
  load_time_ms: 3.338
  training_iteration_time_ms: 46267.883
  update_time_ms: 2.56
timesteps_total: 2821500
training_iteration: 171

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2535211267605634
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.63
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 14735
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.350886821746826
time_total_s: 8709.608949184418
timers:
  learn_throughput: 448.476
  learn_time_ms: 36791.294
  load_throughput: 4930643.279
  load_time_ms: 3.346
  training_iteration_time_ms: 48544.836
  update_time_ms: 2.732
timesteps_total: 2706000
training_iteration: 164

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.275
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 179.64
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 17539
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.60847496986389
time_total_s: 8705.052458763123
timers:
  learn_throughput: 513.014
  learn_time_ms: 32162.866
  load_throughput: 5150445.118
  load_time_ms: 3.204
  training_iteration_time_ms: 42991.706
  update_time_ms: 2.537
timesteps_total: 3217500
training_iteration: 195

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.35356200527705
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 379
episodes_total: 50408
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.57284927368164
time_total_s: 8629.189389944077
timers:
  learn_throughput: 452.166
  learn_time_ms: 36491.034
  load_throughput: 4306615.307
  load_time_ms: 3.831
  training_iteration_time_ms: 48165.01
  update_time_ms: 2.337
timesteps_total: 3069000
training_iteration: 186

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27205882352941174
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.975
  reward for individual goal_min: 0.0
episode_len_mean: 159.40196078431373
episode_reward_max: 2.0
episode_reward_mean: 1.3529411764705883
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 14049
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6666666666666666
  agent_1: 0.6862745098039216
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.77964234352112
time_total_s: 8711.431131601334
timers:
  learn_throughput: 407.009
  learn_time_ms: 40539.624
  load_throughput: 4304525.952
  load_time_ms: 3.833
  training_iteration_time_ms: 52986.791
  update_time_ms: 2.811
timesteps_total: 2607000
training_iteration: 158

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.97866666666667
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 375
episodes_total: 36304
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 55.626593351364136
time_total_s: 8676.757236242294
timers:
  learn_throughput: 380.695
  learn_time_ms: 43341.805
  load_throughput: 3619466.855
  load_time_ms: 4.559
  training_iteration_time_ms: 56058.743
  update_time_ms: 2.582
timesteps_total: 2574000
training_iteration: 156

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2721518987341772
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.84375
  reward for individual goal_min: 0.0
episode_len_mean: 198.81
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 14840
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.66553068161011
time_total_s: 8724.86433506012
timers:
  learn_throughput: 474.57
  learn_time_ms: 34768.352
  load_throughput: 4853463.122
  load_time_ms: 3.4
  training_iteration_time_ms: 46181.142
  update_time_ms: 2.511
timesteps_total: 3019500
training_iteration: 183

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2972972972972973
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8625
  reward for individual goal_min: 0.0
episode_len_mean: 179.83
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 15527
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.740631103515625
time_total_s: 8742.208565235138
timers:
  learn_throughput: 496.835
  learn_time_ms: 33210.254
  load_throughput: 4755675.46
  load_time_ms: 3.47
  training_iteration_time_ms: 44254.489
  update_time_ms: 2.65
timesteps_total: 3135000
training_iteration: 190

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28488372093023256
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9652777777777778
  reward for individual goal_min: 0.0
episode_len_mean: 187.14
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 15200
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.050655364990234
time_total_s: 8713.006297826767
timers:
  learn_throughput: 454.599
  learn_time_ms: 36295.738
  load_throughput: 4889986.01
  load_time_ms: 3.374
  training_iteration_time_ms: 47753.031
  update_time_ms: 2.632
timesteps_total: 2805000
training_iteration: 170

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22784810126582278
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.74
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 14052
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.12721538543701
time_total_s: 8724.14760184288
timers:
  learn_throughput: 415.51
  learn_time_ms: 39710.26
  load_throughput: 4478281.318
  load_time_ms: 3.684
  training_iteration_time_ms: 52039.185
  update_time_ms: 2.7
timesteps_total: 2607000
training_iteration: 158

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18055555555555555
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9807692307692307
  reward for individual goal_min: 0.0
episode_len_mean: 182.72
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 13913
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.84243035316467
time_total_s: 8730.574102401733
timers:
  learn_throughput: 448.922
  learn_time_ms: 36754.685
  load_throughput: 4749702.552
  load_time_ms: 3.474
  training_iteration_time_ms: 48485.749
  update_time_ms: 2.574
timesteps_total: 2722500
training_iteration: 165

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9363057324840764
  reward for individual goal_min: 0.5
episode_len_mean: 62.79182156133829
episode_reward_max: 2.0
episode_reward_mean: 1.9256505576208178
episode_reward_min: 1.0
episodes_this_iter: 269
episodes_total: 33437
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9256505576208178
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 50.50044798851013
time_total_s: 8722.348289489746
timers:
  learn_throughput: 423.491
  learn_time_ms: 38961.854
  load_throughput: 4501672.759
  load_time_ms: 3.665
  training_iteration_time_ms: 50725.258
  update_time_ms: 2.591
timesteps_total: 2854500
training_iteration: 173

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8632075471698113
  reward for individual goal_min: 0.5
episode_len_mean: 74.74439461883408
episode_reward_max: 2.0
episode_reward_mean: 1.8699551569506727
episode_reward_min: 1.0
episodes_this_iter: 223
episodes_total: 23152
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8699551569506726
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 64.27388143539429
time_total_s: 8752.756937503815
timers:
  learn_throughput: 351.387
  learn_time_ms: 46956.739
  load_throughput: 3486431.605
  load_time_ms: 4.733
  training_iteration_time_ms: 61057.178
  update_time_ms: 2.951
timesteps_total: 2409000
training_iteration: 146

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26506024096385544
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 191.22
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 15126
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.865413427352905
time_total_s: 8752.013066768646
timers:
  learn_throughput: 472.828
  learn_time_ms: 34896.407
  load_throughput: 4923382.326
  load_time_ms: 3.351
  training_iteration_time_ms: 46156.702
  update_time_ms: 2.549
timesteps_total: 2838000
training_iteration: 172

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3815789473684211
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9041095890410958
  reward for individual goal_min: 0.0
episode_len_mean: 175.87
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 13780
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.377556800842285
time_total_s: 8757.21809720993
timers:
  learn_throughput: 448.221
  learn_time_ms: 36812.164
  load_throughput: 4670969.344
  load_time_ms: 3.532
  training_iteration_time_ms: 48590.718
  update_time_ms: 2.542
timesteps_total: 2772000
training_iteration: 168

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22784810126582278
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9875
  reward for individual goal_min: 0.0
episode_len_mean: 181.09
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 17628
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.90160012245178
time_total_s: 8747.954058885574
timers:
  learn_throughput: 512.082
  learn_time_ms: 32221.43
  load_throughput: 5176487.625
  load_time_ms: 3.187
  training_iteration_time_ms: 43044.14
  update_time_ms: 2.555
timesteps_total: 3234000
training_iteration: 196

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2534246575342466
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.26
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 14833
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.924864292144775
time_total_s: 8760.533813476562
timers:
  learn_throughput: 447.676
  learn_time_ms: 36857.044
  load_throughput: 4960471.347
  load_time_ms: 3.326
  training_iteration_time_ms: 48650.022
  update_time_ms: 2.72
timesteps_total: 2722500
training_iteration: 165

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.5
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 388
episodes_total: 50796
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.017661571502686
time_total_s: 8677.20705151558
timers:
  learn_throughput: 451.39
  learn_time_ms: 36553.751
  load_throughput: 4307258.594
  load_time_ms: 3.831
  training_iteration_time_ms: 48220.014
  update_time_ms: 2.353
timesteps_total: 3085500
training_iteration: 187

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22388059701492538
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8141025641025641
  reward for individual goal_min: 0.0
episode_len_mean: 198.51
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 14920
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.30395865440369
time_total_s: 8769.168293714523
timers:
  learn_throughput: 477.797
  learn_time_ms: 34533.467
  load_throughput: 4857482.891
  load_time_ms: 3.397
  training_iteration_time_ms: 45918.054
  update_time_ms: 2.527
timesteps_total: 3036000
training_iteration: 184

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2987012987012987
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9240506329113924
  reward for individual goal_min: 0.0
episode_len_mean: 164.3495145631068
episode_reward_max: 2.0
episode_reward_mean: 1.2912621359223302
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 14152
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6504854368932039
  agent_1: 0.6407766990291263
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.386388063430786
time_total_s: 8767.817519664764
timers:
  learn_throughput: 403.84
  learn_time_ms: 40857.767
  load_throughput: 4333474.18
  load_time_ms: 3.808
  training_iteration_time_ms: 53406.157
  update_time_ms: 2.785
timesteps_total: 2623500
training_iteration: 159

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.891304347826086
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 368
episodes_total: 36672
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 53.771040201187134
time_total_s: 8730.528276443481
timers:
  learn_throughput: 383.166
  learn_time_ms: 43062.315
  load_throughput: 3628385.771
  load_time_ms: 4.547
  training_iteration_time_ms: 55757.5
  update_time_ms: 2.565
timesteps_total: 2590500
training_iteration: 157

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22535211267605634
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9038461538461539
  reward for individual goal_min: 0.0
episode_len_mean: 185.09
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 15616
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.10856628417969
time_total_s: 8788.317131519318
timers:
  learn_throughput: 493.52
  learn_time_ms: 33433.291
  load_throughput: 4748659.65
  load_time_ms: 3.475
  training_iteration_time_ms: 44510.493
  update_time_ms: 2.646
timesteps_total: 3151500
training_iteration: 191

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23972602739726026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 168.28
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 15298
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.060202836990356
time_total_s: 8761.066500663757
timers:
  learn_throughput: 456.399
  learn_time_ms: 36152.604
  load_throughput: 4867938.129
  load_time_ms: 3.39
  training_iteration_time_ms: 47529.199
  update_time_ms: 2.632
timesteps_total: 2821500
training_iteration: 171

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22972972972972974
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.5
episode_len_mean: 176.88
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 14140
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.465615034103394
time_total_s: 8773.613216876984
timers:
  learn_throughput: 417.344
  learn_time_ms: 39535.685
  load_throughput: 4450719.385
  load_time_ms: 3.707
  training_iteration_time_ms: 51824.374
  update_time_ms: 2.718
timesteps_total: 2623500
training_iteration: 159

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24324324324324326
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9746835443037974
  reward for individual goal_min: 0.0
episode_len_mean: 180.5
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 14002
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.14814805984497
time_total_s: 8775.722250461578
timers:
  learn_throughput: 453.585
  learn_time_ms: 36376.885
  load_throughput: 4798941.551
  load_time_ms: 3.438
  training_iteration_time_ms: 47971.811
  update_time_ms: 2.574
timesteps_total: 2739000
training_iteration: 166

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.69
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 15219
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.13299226760864
time_total_s: 8797.146059036255
timers:
  learn_throughput: 473.093
  learn_time_ms: 34876.839
  load_throughput: 4915793.526
  load_time_ms: 3.357
  training_iteration_time_ms: 46048.483
  update_time_ms: 2.585
timesteps_total: 2854500
training_iteration: 173

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9463087248322147
  reward for individual goal_min: 0.5
episode_len_mean: 58.946236559139784
episode_reward_max: 2.0
episode_reward_mean: 1.9426523297491038
episode_reward_min: 1.0
episodes_this_iter: 279
episodes_total: 33716
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.946236559139785
  agent_1: 0.996415770609319
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.30182123184204
time_total_s: 8773.650110721588
timers:
  learn_throughput: 423.405
  learn_time_ms: 38969.808
  load_throughput: 4480803.885
  load_time_ms: 3.682
  training_iteration_time_ms: 50821.812
  update_time_ms: 2.618
timesteps_total: 2871000
training_iteration: 174

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27710843373493976
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 189.57
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 17714
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.84629154205322
time_total_s: 8791.800350427628
timers:
  learn_throughput: 511.162
  learn_time_ms: 32279.384
  load_throughput: 5164049.994
  load_time_ms: 3.195
  training_iteration_time_ms: 43172.415
  update_time_ms: 2.592
timesteps_total: 3250500
training_iteration: 197

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2597402597402597
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8913043478260869
  reward for individual goal_min: 0.0
episode_len_mean: 195.42
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 13862
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.29638147354126
time_total_s: 8806.514478683472
timers:
  learn_throughput: 446.759
  learn_time_ms: 36932.633
  load_throughput: 4626781.925
  load_time_ms: 3.566
  training_iteration_time_ms: 48744.758
  update_time_ms: 2.562
timesteps_total: 2788500
training_iteration: 169

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8793103448275862
  reward for individual goal_min: 0.5
episode_len_mean: 75.6774193548387
episode_reward_max: 2.0
episode_reward_mean: 1.8709677419354838
episode_reward_min: 1.0
episodes_this_iter: 217
episodes_total: 23369
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8709677419354839
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 63.93643498420715
time_total_s: 8816.693372488022
timers:
  learn_throughput: 348.607
  learn_time_ms: 47331.276
  load_throughput: 3508400.977
  load_time_ms: 4.703
  training_iteration_time_ms: 61676.237
  update_time_ms: 3.064
timesteps_total: 2425500
training_iteration: 147

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23943661971830985
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.73
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 14926
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.02912759780884
time_total_s: 8814.562941074371
timers:
  learn_throughput: 442.2
  learn_time_ms: 37313.411
  load_throughput: 4913106.347
  load_time_ms: 3.358
  training_iteration_time_ms: 49109.935
  update_time_ms: 2.718
timesteps_total: 2739000
training_iteration: 166

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.162234042553195
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 376
episodes_total: 51172
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.41364312171936
time_total_s: 8724.620694637299
timers:
  learn_throughput: 452.212
  learn_time_ms: 36487.3
  load_throughput: 4314885.435
  load_time_ms: 3.824
  training_iteration_time_ms: 48078.176
  update_time_ms: 2.363
timesteps_total: 3102000
training_iteration: 188

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21830985915492956
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8354430379746836
  reward for individual goal_min: 0.0
episode_len_mean: 196.04
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 15006
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.7824170589447
time_total_s: 8815.950710773468
timers:
  learn_throughput: 475.852
  learn_time_ms: 34674.646
  load_throughput: 4855404.044
  load_time_ms: 3.398
  training_iteration_time_ms: 46072.089
  update_time_ms: 2.527
timesteps_total: 3052500
training_iteration: 185

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.37142857142857144
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9041095890410958
  reward for individual goal_min: 0.0
episode_len_mean: 170.72
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 15710
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.98361277580261
time_total_s: 8832.30074429512
timers:
  learn_throughput: 493.16
  learn_time_ms: 33457.687
  load_throughput: 4705363.512
  load_time_ms: 3.507
  training_iteration_time_ms: 44574.104
  update_time_ms: 2.64
timesteps_total: 3168000
training_iteration: 192

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26282051282051283
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.01
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 15394
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.34309411048889
time_total_s: 8808.409594774246
timers:
  learn_throughput: 455.969
  learn_time_ms: 36186.682
  load_throughput: 4821005.496
  load_time_ms: 3.423
  training_iteration_time_ms: 47560.924
  update_time_ms: 2.6
timesteps_total: 2838000
training_iteration: 172

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.59349593495935
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 369
episodes_total: 37041
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.36698389053345
time_total_s: 8787.895260334015
timers:
  learn_throughput: 380.905
  learn_time_ms: 43317.837
  load_throughput: 3595603.354
  load_time_ms: 4.589
  training_iteration_time_ms: 56077.758
  update_time_ms: 2.581
timesteps_total: 2607000
training_iteration: 158

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18666666666666668
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.18
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 14093
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.80450630187988
time_total_s: 8820.526756763458
timers:
  learn_throughput: 457.41
  learn_time_ms: 36072.639
  load_throughput: 4850809.636
  load_time_ms: 3.401
  training_iteration_time_ms: 47633.377
  update_time_ms: 2.546
timesteps_total: 2755500
training_iteration: 167

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9333333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 82.23333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.8666666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9166666666666666
  agent_1: 0.95
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21232876712328766
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9788732394366197
  reward for individual goal_min: 0.0
episode_len_mean: 179.65
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 14236
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.267762184143066
time_total_s: 8831.085281848907
timers:
  learn_throughput: 406.048
  learn_time_ms: 40635.571
  load_throughput: 4263922.221
  load_time_ms: 3.87
  training_iteration_time_ms: 53212.922
  update_time_ms: 2.781
timesteps_total: 2640000
training_iteration: 160

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-29ws591pkw/checkpoint_000160/checkpoint-160
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3014705882352941
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 159.1980198019802
episode_reward_max: 2.0
episode_reward_mean: 1.4356435643564356
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 15320
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7227722772277227
  agent_1: 0.7128712871287128
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.299216508865356
time_total_s: 8844.44527554512
timers:
  learn_throughput: 471.581
  learn_time_ms: 34988.67
  load_throughput: 4889986.01
  load_time_ms: 3.374
  training_iteration_time_ms: 46135.19
  update_time_ms: 2.548
timesteps_total: 2871000
training_iteration: 174

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 77.6
episode_reward_max: 2.0
episode_reward_mean: 1.9166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9666666666666667
  agent_1: 0.95
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21014492753623187
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.07
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 14237
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.995096921920776
time_total_s: 8834.608313798904
timers:
  learn_throughput: 417.233
  learn_time_ms: 39546.263
  load_throughput: 4433867.188
  load_time_ms: 3.721
  training_iteration_time_ms: 51794.291
  update_time_ms: 2.735
timesteps_total: 2640000
training_iteration: 160

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2532467532467532
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.62
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 17807
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.27953505516052
time_total_s: 8837.079885482788
timers:
  learn_throughput: 506.243
  learn_time_ms: 32593.063
  load_throughput: 5172541.276
  load_time_ms: 3.19
  training_iteration_time_ms: 43492.282
  update_time_ms: 2.598
timesteps_total: 3267000
training_iteration: 198

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.91796875
  reward for individual goal_min: 0.5
episode_len_mean: 62.150375939849624
episode_reward_max: 2.0
episode_reward_mean: 1.9210526315789473
episode_reward_min: 1.0
episodes_this_iter: 266
episodes_total: 33982
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9210526315789473
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 50.67958307266235
time_total_s: 8824.32969379425
timers:
  learn_throughput: 423.126
  learn_time_ms: 38995.437
  load_throughput: 4582117.787
  load_time_ms: 3.601
  training_iteration_time_ms: 50946.763
  update_time_ms: 2.619
timesteps_total: 2887500
training_iteration: 175

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2463768115942029
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.881578947368421
  reward for individual goal_min: 0.0
episode_len_mean: 189.61
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 13949
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.64516854286194
time_total_s: 8856.159647226334
timers:
  learn_throughput: 445.001
  learn_time_ms: 37078.537
  load_throughput: 4611305.779
  load_time_ms: 3.578
  training_iteration_time_ms: 48920.456
  update_time_ms: 2.578
timesteps_total: 2805000
training_iteration: 170

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20121951219512196
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 193.08
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 15013
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.19955778121948
time_total_s: 8863.76249885559
timers:
  learn_throughput: 440.208
  learn_time_ms: 37482.298
  load_throughput: 4956172.907
  load_time_ms: 3.329
  training_iteration_time_ms: 49310.232
  update_time_ms: 2.705
timesteps_total: 2755500
training_iteration: 167

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973404255319149
  reward for individual goal_min: 0.5
episode_len_mean: 43.23421052631579
episode_reward_max: 2.0
episode_reward_mean: 1.9973684210526317
episode_reward_min: 1.0
episodes_this_iter: 380
episodes_total: 51552
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973684210526316
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 47.84278893470764
time_total_s: 8772.463483572006
timers:
  learn_throughput: 452.574
  learn_time_ms: 36458.146
  load_throughput: 4309860.502
  load_time_ms: 3.828
  training_iteration_time_ms: 48028.439
  update_time_ms: 2.374
timesteps_total: 3118500
training_iteration: 189

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3223684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8846153846153846
  reward for individual goal_min: 0.0
episode_len_mean: 182.81
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 15799
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.505638122558594
time_total_s: 8875.806382417679
timers:
  learn_throughput: 496.317
  learn_time_ms: 33244.875
  load_throughput: 4731517.646
  load_time_ms: 3.487
  training_iteration_time_ms: 44292.155
  update_time_ms: 2.619
timesteps_total: 3184500
training_iteration: 193

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20833333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8026315789473685
  reward for individual goal_min: 0.0
episode_len_mean: 208.08
episode_reward_max: 2.0
episode_reward_mean: 1.03
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 15085
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.48
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.17847180366516
time_total_s: 8864.129182577133
timers:
  learn_throughput: 473.897
  learn_time_ms: 34817.71
  load_throughput: 4889364.155
  load_time_ms: 3.375
  training_iteration_time_ms: 46230.394
  update_time_ms: 2.549
timesteps_total: 3069000
training_iteration: 186

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23648648648648649
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935897435897436
  reward for individual goal_min: 0.5
episode_len_mean: 178.03
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 15488
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.35958290100098
time_total_s: 8851.769177675247
timers:
  learn_throughput: 461.689
  learn_time_ms: 35738.364
  load_throughput: 4806140.213
  load_time_ms: 3.433
  training_iteration_time_ms: 47017.681
  update_time_ms: 2.571
timesteps_total: 2854500
training_iteration: 173

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8482142857142857
  reward for individual goal_min: 0.5
episode_len_mean: 85.5561224489796
episode_reward_max: 2.0
episode_reward_mean: 1.8265306122448979
episode_reward_min: 1.0
episodes_this_iter: 196
episodes_total: 23565
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.826530612244898
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 63.50529861450195
time_total_s: 8880.198671102524
timers:
  learn_throughput: 345.017
  learn_time_ms: 47823.709
  load_throughput: 3563206.195
  load_time_ms: 4.631
  training_iteration_time_ms: 62274.737
  update_time_ms: 3.089
timesteps_total: 2442000
training_iteration: 148

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3092105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.25
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 14189
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.20671534538269
time_total_s: 8867.733472108841
timers:
  learn_throughput: 459.969
  learn_time_ms: 35871.958
  load_throughput: 4893824.276
  load_time_ms: 3.372
  training_iteration_time_ms: 47389.592
  update_time_ms: 2.567
timesteps_total: 2772000
training_iteration: 168

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2987012987012987
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9929577464788732
  reward for individual goal_min: 0.5
episode_len_mean: 175.63
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 17903
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.46986937522888
time_total_s: 8880.549754858017
timers:
  learn_throughput: 508.167
  learn_time_ms: 32469.645
  load_throughput: 5157238.584
  load_time_ms: 3.199
  training_iteration_time_ms: 43348.961
  update_time_ms: 2.569
timesteps_total: 3283500
training_iteration: 199

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27702702702702703
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.952054794520548
  reward for individual goal_min: 0.0
episode_len_mean: 176.43
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 14332
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.35693430900574
time_total_s: 8885.442216157913
timers:
  learn_throughput: 405.11
  learn_time_ms: 40729.728
  load_throughput: 4265236.17
  load_time_ms: 3.868
  training_iteration_time_ms: 53378.599
  update_time_ms: 2.767
timesteps_total: 2656500
training_iteration: 161

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.3109243697479
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 357
episodes_total: 37398
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.10507035255432
time_total_s: 8847.00033068657
timers:
  learn_throughput: 379.764
  learn_time_ms: 43448.036
  load_throughput: 3570338.637
  load_time_ms: 4.621
  training_iteration_time_ms: 56268.052
  update_time_ms: 2.556
timesteps_total: 2623500
training_iteration: 159

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24342105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.15
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 15414
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.656818866729736
time_total_s: 8893.10209441185
timers:
  learn_throughput: 467.183
  learn_time_ms: 35318.089
  load_throughput: 4865131.987
  load_time_ms: 3.391
  training_iteration_time_ms: 46467.882
  update_time_ms: 2.526
timesteps_total: 2887500
training_iteration: 175

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27631578947368424
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.67
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 14331
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.23638343811035
time_total_s: 8886.844697237015
timers:
  learn_throughput: 417.897
  learn_time_ms: 39483.422
  load_throughput: 4412213.885
  load_time_ms: 3.74
  training_iteration_time_ms: 51863.312
  update_time_ms: 2.736
timesteps_total: 2656500
training_iteration: 161

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9452054794520548
  reward for individual goal_min: 0.5
episode_len_mean: 59.46376811594203
episode_reward_max: 2.0
episode_reward_mean: 1.9420289855072463
episode_reward_min: 1.0
episodes_this_iter: 276
episodes_total: 34258
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9420289855072463
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 51.90232491493225
time_total_s: 8876.232018709183
timers:
  learn_throughput: 421.343
  learn_time_ms: 39160.478
  load_throughput: 4538069.652
  load_time_ms: 3.636
  training_iteration_time_ms: 51129.829
  update_time_ms: 2.639
timesteps_total: 2904000
training_iteration: 176

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22784810126582278
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9013157894736842
  reward for individual goal_min: 0.0
episode_len_mean: 207.49
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 14028
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.821316719055176
time_total_s: 8905.980963945389
timers:
  learn_throughput: 441.868
  learn_time_ms: 37341.469
  load_throughput: 4582937.063
  load_time_ms: 3.6
  training_iteration_time_ms: 49266.013
  update_time_ms: 2.601
timesteps_total: 2821500
training_iteration: 171

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19718309859154928
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.84375
  reward for individual goal_min: 0.0
episode_len_mean: 190.01
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 15888
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.43
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.47783422470093
time_total_s: 8921.28421664238
timers:
  learn_throughput: 493.325
  learn_time_ms: 33446.492
  load_throughput: 4800273.009
  load_time_ms: 3.437
  training_iteration_time_ms: 44585.894
  update_time_ms: 2.638
timesteps_total: 3201000
training_iteration: 194

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8481012658227848
  reward for individual goal_min: 0.0
episode_len_mean: 199.34
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 15171
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.84331250190735
time_total_s: 8908.97249507904
timers:
  learn_throughput: 473.96
  learn_time_ms: 34813.075
  load_throughput: 4834105.14
  load_time_ms: 3.413
  training_iteration_time_ms: 46188.287
  update_time_ms: 2.487
timesteps_total: 3085500
training_iteration: 187

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972972972972973
  reward for individual goal_min: 0.5
episode_len_mean: 44.769647696476966
episode_reward_max: 2.0
episode_reward_mean: 1.997289972899729
episode_reward_min: 1.0
episodes_this_iter: 369
episodes_total: 51921
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.997289972899729
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 48.874109506607056
time_total_s: 8821.337593078613
timers:
  learn_throughput: 452.085
  learn_time_ms: 36497.55
  load_throughput: 4304981.152
  load_time_ms: 3.833
  training_iteration_time_ms: 48127.069
  update_time_ms: 2.351
timesteps_total: 3135000
training_iteration: 190

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15584415584415584
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939024390243902
  reward for individual goal_min: 0.5
episode_len_mean: 191.42
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 15097
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.024845600128174
time_total_s: 8914.787344455719
timers:
  learn_throughput: 436.111
  learn_time_ms: 37834.364
  load_throughput: 4914362.325
  load_time_ms: 3.358
  training_iteration_time_ms: 49583.622
  update_time_ms: 2.72
timesteps_total: 2772000
training_iteration: 168

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23026315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.08
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 15583
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.21799302101135
time_total_s: 8899.987170696259
timers:
  learn_throughput: 461.712
  learn_time_ms: 35736.566
  load_throughput: 4820031.759
  load_time_ms: 3.423
  training_iteration_time_ms: 47014.135
  update_time_ms: 2.539
timesteps_total: 2871000
training_iteration: 174

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16417910447761194
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.26
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 14282
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.74837303161621
time_total_s: 8912.481845140457
timers:
  learn_throughput: 463.684
  learn_time_ms: 35584.542
  load_throughput: 4921841.69
  load_time_ms: 3.352
  training_iteration_time_ms: 47077.287
  update_time_ms: 2.562
timesteps_total: 2788500
training_iteration: 169

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8476190476190476
  reward for individual goal_min: 0.5
episode_len_mean: 83.03626943005182
episode_reward_max: 2.0
episode_reward_mean: 1.83419689119171
episode_reward_min: 1.0
episodes_this_iter: 193
episodes_total: 23758
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9948186528497409
  agent_1: 0.8393782383419689
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.09800982475281
time_total_s: 8942.296680927277
timers:
  learn_throughput: 344.994
  learn_time_ms: 47826.876
  load_throughput: 3441458.813
  load_time_ms: 4.794
  training_iteration_time_ms: 62353.102
  update_time_ms: 3.024
timesteps_total: 2458500
training_iteration: 149

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 68.01666666666667
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3231707317073171
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9794520547945206
  reward for individual goal_min: 0.0
episode_len_mean: 178.73
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 15509
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.92156767845154
time_total_s: 8940.023662090302
timers:
  learn_throughput: 468.497
  learn_time_ms: 35219.011
  load_throughput: 4899159.428
  load_time_ms: 3.368
  training_iteration_time_ms: 46395.457
  update_time_ms: 2.524
timesteps_total: 2904000
training_iteration: 176


custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2876712328767123
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 168.47
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 18000
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.172658920288086
time_total_s: 8930.722413778305
timers:
  learn_throughput: 508.809
  learn_time_ms: 32428.697
  load_throughput: 5142522.887
  load_time_ms: 3.209
  training_iteration_time_ms: 43246.852
  update_time_ms: 2.581
timesteps_total: 3300000
training_iteration: 200

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19ecg2ejpn/checkpoint_000200/checkpoint-200
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9817073170731707
  reward for individual goal_min: 0.0
episode_len_mean: 177.95
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 14423
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.18231415748596
time_total_s: 8937.6245303154
timers:
  learn_throughput: 403.733
  learn_time_ms: 40868.644
  load_throughput: 4280431.47
  load_time_ms: 3.855
  training_iteration_time_ms: 53592.071
  update_time_ms: 2.734
timesteps_total: 2673000
training_iteration: 162

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 184.61
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 14423
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.9784197807312
time_total_s: 8934.823117017746
timers:
  learn_throughput: 421.278
  learn_time_ms: 39166.56
  load_throughput: 4471452.773
  load_time_ms: 3.69
  training_iteration_time_ms: 51376.242
  update_time_ms: 2.721
timesteps_total: 2673000
training_iteration: 162

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8481012658227848
  reward for individual goal_min: 0.0
episode_len_mean: 196.44
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 15972
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.43894839286804
time_total_s: 8964.723165035248
timers:
  learn_throughput: 494.913
  learn_time_ms: 33339.205
  load_throughput: 4792594.008
  load_time_ms: 3.443
  training_iteration_time_ms: 44539.502
  update_time_ms: 2.651
timesteps_total: 3217500
training_iteration: 195

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18493150684931506
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 199.9
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 14110
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.260403871536255
time_total_s: 8955.241367816925
timers:
  learn_throughput: 441.67
  learn_time_ms: 37358.203
  load_throughput: 4530583.098
  load_time_ms: 3.642
  training_iteration_time_ms: 49273.842
  update_time_ms: 2.595
timesteps_total: 2838000
training_iteration: 172

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3424657534246575
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8802816901408451
  reward for individual goal_min: 0.0
episode_len_mean: 191.08
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 15254
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.79117798805237
time_total_s: 8953.763673067093
timers:
  learn_throughput: 475.08
  learn_time_ms: 34731.012
  load_throughput: 4841138.828
  load_time_ms: 3.408
  training_iteration_time_ms: 46056.661
  update_time_ms: 2.478
timesteps_total: 3102000
training_iteration: 188

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 47.36666666666667
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.758620689655174
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 377
episodes_total: 37775
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 63.10890555381775
time_total_s: 8910.109236240387
timers:
  learn_throughput: 378.217
  learn_time_ms: 43625.705
  load_throughput: 3539587.561
  load_time_ms: 4.662
  training_iteration_time_ms: 56516.023
  update_time_ms: 2.581
timesteps_total: 2640000
training_iteration: 160

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-184s1jdk42/checkpoint_000160/checkpoint-160
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9697986577181208
  reward for individual goal_min: 0.5
episode_len_mean: 53.36807817589577
episode_reward_max: 2.0
episode_reward_mean: 1.9706840390879479
episode_reward_min: 1.0
episodes_this_iter: 307
episodes_total: 34565
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9706840390879479
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 53.33885645866394
time_total_s: 8929.570875167847
timers:
  learn_throughput: 419.614
  learn_time_ms: 39321.895
  load_throughput: 4527056.361
  load_time_ms: 3.645
  training_iteration_time_ms: 51300.417
  update_time_ms: 2.626
timesteps_total: 2920500
training_iteration: 177

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.576227390180875
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 387
episodes_total: 52308
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.92491602897644
time_total_s: 8870.26250910759
timers:
  learn_throughput: 451.237
  learn_time_ms: 36566.167
  load_throughput: 4353318.865
  load_time_ms: 3.79
  training_iteration_time_ms: 48197.06
  update_time_ms: 2.353
timesteps_total: 3151500
training_iteration: 191

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2112676056338028
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.98125
  reward for individual goal_min: 0.0
episode_len_mean: 173.02
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 15195
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.093528747558594
time_total_s: 8964.880873203278
timers:
  learn_throughput: 432.753
  learn_time_ms: 38127.955
  load_throughput: 4900200.098
  load_time_ms: 3.367
  training_iteration_time_ms: 49934.588
  update_time_ms: 2.702
timesteps_total: 2788500
training_iteration: 169

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9701492537313433
  reward for individual goal_min: 0.0
episode_len_mean: 183.49
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 15675
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.45623803138733
time_total_s: 8948.443408727646
timers:
  learn_throughput: 457.918
  learn_time_ms: 36032.674
  load_throughput: 4793888.739
  load_time_ms: 3.442
  training_iteration_time_ms: 47372.104
  update_time_ms: 2.565
timesteps_total: 2887500
training_iteration: 175

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 193.83
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 14368
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.62274217605591
time_total_s: 8958.104587316513
timers:
  learn_throughput: 464.235
  learn_time_ms: 35542.325
  load_throughput: 4954753.573
  load_time_ms: 3.33
  training_iteration_time_ms: 46955.599
  update_time_ms: 2.586
timesteps_total: 2805000
training_iteration: 170

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27631578947368424
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.59
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 18093
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.658799171447754
time_total_s: 8972.381212949753
timers:
  learn_throughput: 509.3
  learn_time_ms: 32397.391
  load_throughput: 5113304.223
  load_time_ms: 3.227
  training_iteration_time_ms: 43244.493
  update_time_ms: 2.583
timesteps_total: 3316500
training_iteration: 201

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35443037974683544
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 168.35
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 15606
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.06383943557739
time_total_s: 8985.087501525879
timers:
  learn_throughput: 469.551
  learn_time_ms: 35139.939
  load_throughput: 4873834.712
  load_time_ms: 3.385
  training_iteration_time_ms: 46287.211
  update_time_ms: 2.561
timesteps_total: 2920500
training_iteration: 177

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2916666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 165.32
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 14523
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.406240940093994
time_total_s: 8982.22935795784
timers:
  learn_throughput: 424.432
  learn_time_ms: 38875.519
  load_throughput: 4481500.265
  load_time_ms: 3.682
  training_iteration_time_ms: 50967.057
  update_time_ms: 2.681
timesteps_total: 2689500
training_iteration: 163

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2564102564102564
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7666666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 201.51
episode_reward_max: 2.0
episode_reward_mean: 1.06
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 16055
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.80164408683777
time_total_s: 9008.524809122086
timers:
  learn_throughput: 495.355
  learn_time_ms: 33309.423
  load_throughput: 4769210.668
  load_time_ms: 3.46
  training_iteration_time_ms: 44471.771
  update_time_ms: 2.642
timesteps_total: 3234000
training_iteration: 196

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27631578947368424
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8013698630136986
  reward for individual goal_min: 0.0
episode_len_mean: 204.75
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 15338
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.41290307044983
time_total_s: 8998.176576137543
timers:
  learn_throughput: 477.389
  learn_time_ms: 34563.007
  load_throughput: 4888155.447
  load_time_ms: 3.376
  training_iteration_time_ms: 45832.783
  update_time_ms: 2.498
timesteps_total: 3118500
training_iteration: 189

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32608695652173914
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9578313253012049
  reward for individual goal_min: 0.0
episode_len_mean: 149.09821428571428
episode_reward_max: 2.0
episode_reward_mean: 1.4285714285714286
episode_reward_min: 0.0
episodes_this_iter: 112
episodes_total: 14535
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7678571428571429
  agent_1: 0.6607142857142857
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.206894874572754
time_total_s: 8992.831425189972
timers:
  learn_throughput: 402.24
  learn_time_ms: 41020.269
  load_throughput: 4295868.752
  load_time_ms: 3.841
  training_iteration_time_ms: 53738.933
  update_time_ms: 2.736
timesteps_total: 2689500
training_iteration: 163

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8739130434782608
  reward for individual goal_min: 0.5
episode_len_mean: 72.51948051948052
episode_reward_max: 2.0
episode_reward_mean: 1.8744588744588744
episode_reward_min: 1.0
episodes_this_iter: 231
episodes_total: 23989
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8744588744588745
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 64.76532220840454
time_total_s: 9007.062003135681
timers:
  learn_throughput: 343.144
  learn_time_ms: 48084.727
  load_throughput: 3670102.192
  load_time_ms: 4.496
  training_iteration_time_ms: 62629.697
  update_time_ms: 3.058
timesteps_total: 2475000
training_iteration: 150

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31756756756756754
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8802816901408451
  reward for individual goal_min: 0.0
episode_len_mean: 177.54
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 14207
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.864352226257324
time_total_s: 9006.105720043182
timers:
  learn_throughput: 441.038
  learn_time_ms: 37411.715
  load_throughput: 4546446.985
  load_time_ms: 3.629
  training_iteration_time_ms: 49382.281
  update_time_ms: 2.604
timesteps_total: 2854500
training_iteration: 173

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9602649006622517
  reward for individual goal_min: 0.5
episode_len_mean: 58.30662020905923
episode_reward_max: 2.0
episode_reward_mean: 1.9581881533101044
episode_reward_min: 1.0
episodes_this_iter: 287
episodes_total: 34852
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9651567944250871
  agent_1: 0.9930313588850174
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.53500556945801
time_total_s: 8980.105880737305
timers:
  learn_throughput: 419.381
  learn_time_ms: 39343.655
  load_throughput: 4502756.462
  load_time_ms: 3.664
  training_iteration_time_ms: 51310.687
  update_time_ms: 2.642
timesteps_total: 2937000
training_iteration: 178

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.45283018867924
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 371
episodes_total: 52679
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.94986271858215
time_total_s: 8918.212371826172
timers:
  learn_throughput: 451.417
  learn_time_ms: 36551.596
  load_throughput: 4322728.329
  load_time_ms: 3.817
  training_iteration_time_ms: 48189.011
  update_time_ms: 2.369
timesteps_total: 3168000
training_iteration: 192

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21428571428571427
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.45
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 15770
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.3544762134552
time_total_s: 8994.797884941101
timers:
  learn_throughput: 461.369
  learn_time_ms: 35763.146
  load_throughput: 4765630.944
  load_time_ms: 3.462
  training_iteration_time_ms: 47022.472
  update_time_ms: 2.547
timesteps_total: 2904000
training_iteration: 176

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974619289340102
  reward for individual goal_min: 0.5
episode_len_mean: 44.652054794520545
episode_reward_max: 2.0
episode_reward_mean: 1.9972602739726026
episode_reward_min: 1.0
episodes_this_iter: 365
episodes_total: 38140
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9972602739726028
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 56.879618644714355
time_total_s: 8966.988854885101
timers:
  learn_throughput: 377.719
  learn_time_ms: 43683.22
  load_throughput: 3513013.569
  load_time_ms: 4.697
  training_iteration_time_ms: 56568.993
  update_time_ms: 2.572
timesteps_total: 2656500
training_iteration: 161

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3076923076923077
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 170.68
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 15293
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.11579632759094
time_total_s: 9015.996669530869
timers:
  learn_throughput: 432.035
  learn_time_ms: 38191.39
  load_throughput: 4881053.426
  load_time_ms: 3.38
  training_iteration_time_ms: 50135.067
  update_time_ms: 2.673
timesteps_total: 2805000
training_iteration: 170

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18452380952380953
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935897435897436
  reward for individual goal_min: 0.5
episode_len_mean: 206.8
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 14447
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.288203954696655
time_total_s: 9005.39279127121
timers:
  learn_throughput: 466.316
  learn_time_ms: 35383.743
  load_throughput: 4939652.684
  load_time_ms: 3.34
  training_iteration_time_ms: 46679.184
  update_time_ms: 2.583
timesteps_total: 2821500
training_iteration: 171

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24358974358974358
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 188.28
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 18177
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.336249351501465
time_total_s: 9012.717462301254
timers:
  learn_throughput: 513.367
  learn_time_ms: 32140.765
  load_throughput: 5069777.814
  load_time_ms: 3.255
  training_iteration_time_ms: 42885.057
  update_time_ms: 2.542
timesteps_total: 3333000
training_iteration: 202

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34810126582278483
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9632352941176471
  reward for individual goal_min: 0.0
episode_len_mean: 173.46
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 15698
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.61542558670044
time_total_s: 9032.70292711258
timers:
  learn_throughput: 469.568
  learn_time_ms: 35138.695
  load_throughput: 4858915.264
  load_time_ms: 3.396
  training_iteration_time_ms: 46247.658
  update_time_ms: 2.553
timesteps_total: 2937000
training_iteration: 178

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2916666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8235294117647058
  reward for individual goal_min: 0.0
episode_len_mean: 187.93
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 16143
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.086743116378784
time_total_s: 9051.611552238464
timers:
  learn_throughput: 498.204
  learn_time_ms: 33118.965
  load_throughput: 4752246.546
  load_time_ms: 3.472
  training_iteration_time_ms: 44259.227
  update_time_ms: 2.607
timesteps_total: 3250500
training_iteration: 197

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.18
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 14616
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.70672082901001
time_total_s: 9029.93607878685
timers:
  learn_throughput: 428.107
  learn_time_ms: 38541.805
  load_throughput: 4546656.067
  load_time_ms: 3.629
  training_iteration_time_ms: 50560.828
  update_time_ms: 2.696
timesteps_total: 2706000
training_iteration: 164

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2846153846153846
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7916666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 183.37
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 15423
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.41531300544739
time_total_s: 9045.59188914299
timers:
  learn_throughput: 478.858
  learn_time_ms: 34456.964
  load_throughput: 4888293.555
  load_time_ms: 3.375
  training_iteration_time_ms: 45789.943
  update_time_ms: 2.508
timesteps_total: 3135000
training_iteration: 190

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26582278481012656
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9696969696969697
  reward for individual goal_min: 0.0
episode_len_mean: 181.07
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 14623
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.67827916145325
time_total_s: 9046.509704351425
timers:
  learn_throughput: 402.973
  learn_time_ms: 40945.656
  load_throughput: 4260640.888
  load_time_ms: 3.873
  training_iteration_time_ms: 53685.851
  update_time_ms: 2.73
timesteps_total: 2706000
training_iteration: 164

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34415584415584416
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8666666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 189.58
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 14295
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.49379539489746
time_total_s: 9056.59951543808
timers:
  learn_throughput: 439.559
  learn_time_ms: 37537.589
  load_throughput: 4537950.625
  load_time_ms: 3.636
  training_iteration_time_ms: 49543.83
  update_time_ms: 2.607
timesteps_total: 2871000
training_iteration: 174

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973544973544973
  reward for individual goal_min: 0.5
episode_len_mean: 42.45103092783505
episode_reward_max: 2.0
episode_reward_mean: 1.9974226804123711
episode_reward_min: 1.0
episodes_this_iter: 388
episodes_total: 53067
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9974226804123711
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 48.05167746543884
time_total_s: 8966.26404929161
timers:
  learn_throughput: 451.444
  learn_time_ms: 36549.373
  load_throughput: 4304874.037
  load_time_ms: 3.833
  training_iteration_time_ms: 48215.086
  update_time_ms: 2.651
timesteps_total: 3184500
training_iteration: 193

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25925925925925924
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 184.08
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 15859
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.796963930130005
time_total_s: 9042.594848871231
timers:
  learn_throughput: 460.956
  learn_time_ms: 35795.175
  load_throughput: 4767075.323
  load_time_ms: 3.461
  training_iteration_time_ms: 47115.01
  update_time_ms: 2.55
timesteps_total: 2920500
training_iteration: 177

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9769736842105263
  reward for individual goal_min: 0.5
episode_len_mean: 51.71844660194175
episode_reward_max: 2.0
episode_reward_mean: 1.977346278317152
episode_reward_min: 1.0
episodes_this_iter: 309
episodes_total: 35161
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9805825242718447
  agent_1: 0.9967637540453075
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.98676943778992
time_total_s: 9033.092650175095
timers:
  learn_throughput: 417.335
  learn_time_ms: 39536.564
  load_throughput: 4482951.754
  load_time_ms: 3.681
  training_iteration_time_ms: 51590.392
  update_time_ms: 2.605
timesteps_total: 2953500
training_iteration: 179

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20270270270270271
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.92
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 15379
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.193954944610596
time_total_s: 9065.19062447548
timers:
  learn_throughput: 431.597
  learn_time_ms: 38230.077
  load_throughput: 4817850.674
  load_time_ms: 3.425
  training_iteration_time_ms: 50213.389
  update_time_ms: 2.703
timesteps_total: 2821500
training_iteration: 171

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.175
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 202.45
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 14529
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.2772581577301
time_total_s: 9050.67004942894
timers:
  learn_throughput: 470.065
  learn_time_ms: 35101.519
  load_throughput: 4952378.008
  load_time_ms: 3.332
  training_iteration_time_ms: 46358.216
  update_time_ms: 2.582
timesteps_total: 2838000
training_iteration: 172

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2565789473684211
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9930555555555556
  reward for individual goal_min: 0.5
episode_len_mean: 179.66
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 18269
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.5843551158905
time_total_s: 9055.301817417145
timers:
  learn_throughput: 514.421
  learn_time_ms: 32074.876
  load_throughput: 5093996.379
  load_time_ms: 3.239
  training_iteration_time_ms: 42770.811
  update_time_ms: 2.58
timesteps_total: 3349500
training_iteration: 203

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8571428571428571
  reward for individual goal_min: 0.5
episode_len_mean: 74.99090909090908
episode_reward_max: 2.0
episode_reward_mean: 1.8636363636363635
episode_reward_min: 1.0
episodes_this_iter: 220
episodes_total: 24209
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9954545454545455
  agent_1: 0.8681818181818182
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 64.6719970703125
time_total_s: 9071.734000205994
timers:
  learn_throughput: 340.066
  learn_time_ms: 48519.984
  load_throughput: 3647297.756
  load_time_ms: 4.524
  training_iteration_time_ms: 63142.963
  update_time_ms: 3.11
timesteps_total: 2491500
training_iteration: 151

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.094182825484765
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 361
episodes_total: 38501
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.41684913635254
time_total_s: 9025.405704021454
timers:
  learn_throughput: 376.101
  learn_time_ms: 43871.164
  load_throughput: 3525074.035
  load_time_ms: 4.681
  training_iteration_time_ms: 56736.753
  update_time_ms: 2.57
timesteps_total: 2673000
training_iteration: 162

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2987012987012987
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.23
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 15794
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.13073921203613
time_total_s: 9080.833666324615
timers:
  learn_throughput: 466.105
  learn_time_ms: 35399.734
  load_throughput: 4870199.084
  load_time_ms: 3.388
  training_iteration_time_ms: 46530.192
  update_time_ms: 2.576
timesteps_total: 2953500
training_iteration: 179

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30405405405405406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8618421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 185.4
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 16225
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.09486389160156
time_total_s: 9095.706416130066
timers:
  learn_throughput: 499.661
  learn_time_ms: 33022.374
  load_throughput: 4756688.753
  load_time_ms: 3.469
  training_iteration_time_ms: 44114.512
  update_time_ms: 2.626
timesteps_total: 3267000
training_iteration: 198

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.44
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 14709
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.352155447006226
time_total_s: 9080.288234233856
timers:
  learn_throughput: 431.243
  learn_time_ms: 38261.492
  load_throughput: 4598100.857
  load_time_ms: 3.588
  training_iteration_time_ms: 50173.75
  update_time_ms: 2.681
timesteps_total: 2722500
training_iteration: 165

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2318840579710145
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8066666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 202.16
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 15505
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.409252882003784
time_total_s: 9091.001142024994
timers:
  learn_throughput: 477.91
  learn_time_ms: 34525.313
  load_throughput: 4862875.733
  load_time_ms: 3.393
  training_iteration_time_ms: 45845.571
  update_time_ms: 2.486
timesteps_total: 3151500
training_iteration: 191

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36153846153846153
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8841463414634146
  reward for individual goal_min: 0.0
episode_len_mean: 166.31
episode_reward_max: 2.0
episode_reward_mean: 1.42
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 14394
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.78
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.51939272880554
time_total_s: 9107.118908166885
timers:
  learn_throughput: 435.751
  learn_time_ms: 37865.675
  load_throughput: 4563415.142
  load_time_ms: 3.616
  training_iteration_time_ms: 49926.838
  update_time_ms: 2.634
timesteps_total: 2887500
training_iteration: 175

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2945205479452055
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 156.41904761904763
episode_reward_max: 2.0
episode_reward_mean: 1.361904761904762
episode_reward_min: 0.0
episodes_this_iter: 105
episodes_total: 14728
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6952380952380952
  agent_1: 0.6666666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.403838872909546
time_total_s: 9099.913543224335
timers:
  learn_throughput: 403.425
  learn_time_ms: 40899.782
  load_throughput: 4264368.873
  load_time_ms: 3.869
  training_iteration_time_ms: 53607.117
  update_time_ms: 2.72
timesteps_total: 2722500
training_iteration: 165

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3355263157894737
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 160.15384615384616
episode_reward_max: 2.0
episode_reward_mean: 1.4326923076923077
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 18373
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7115384615384616
  agent_1: 0.7211538461538461
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.75605654716492
time_total_s: 9098.05787396431
timers:
  learn_throughput: 513.612
  learn_time_ms: 32125.395
  load_throughput: 5119091.07
  load_time_ms: 3.223
  training_iteration_time_ms: 42781.068
  update_time_ms: 2.573
timesteps_total: 3366000
training_iteration: 204

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 180.68
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 15950
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.751203298568726
time_total_s: 9091.3460521698
timers:
  learn_throughput: 459.056
  learn_time_ms: 35943.356
  load_throughput: 4722799.585
  load_time_ms: 3.494
  training_iteration_time_ms: 47257.46
  update_time_ms: 2.545
timesteps_total: 2937000
training_iteration: 178

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971910112359551
  reward for individual goal_min: 0.5
episode_len_mean: 44.48525469168901
episode_reward_max: 2.0
episode_reward_mean: 1.997319034852547
episode_reward_min: 1.0
episodes_this_iter: 373
episodes_total: 53440
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973190348525469
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.052050828933716
time_total_s: 9014.316100120544
timers:
  learn_throughput: 452.453
  learn_time_ms: 36467.906
  load_throughput: 4299899.098
  load_time_ms: 3.837
  training_iteration_time_ms: 48117.249
  update_time_ms: 2.67
timesteps_total: 3201000
training_iteration: 194

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22077922077922077
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.4
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 14619
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.83783483505249
time_total_s: 9099.507884263992
timers:
  learn_throughput: 468.693
  learn_time_ms: 35204.295
  load_throughput: 4922051.72
  load_time_ms: 3.352
  training_iteration_time_ms: 46504.012
  update_time_ms: 2.605
timesteps_total: 2854500
training_iteration: 173

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3223684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9929577464788732
  reward for individual goal_min: 0.5
episode_len_mean: 171.35
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 15479
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.26057767868042
time_total_s: 9115.45120215416
timers:
  learn_throughput: 429.194
  learn_time_ms: 38444.121
  load_throughput: 4830798.269
  load_time_ms: 3.416
  training_iteration_time_ms: 50529.384
  update_time_ms: 2.7
timesteps_total: 2838000
training_iteration: 172

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.78333333333333
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9291044776119403
  reward for individual goal_min: 0.5
episode_len_mean: 60.059859154929576
episode_reward_max: 2.0
episode_reward_mean: 1.9330985915492958
episode_reward_min: 1.0
episodes_this_iter: 284
episodes_total: 35445
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9330985915492958
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 55.404210805892944
time_total_s: 9088.496860980988
timers:
  learn_throughput: 419.475
  learn_time_ms: 39334.864
  load_throughput: 4448688.073
  load_time_ms: 3.709
  training_iteration_time_ms: 51339.401
  update_time_ms: 2.756
timesteps_total: 2970000
training_iteration: 180

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-18g5xov65h/checkpoint_000180/checkpoint-180
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24358974358974358
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8732394366197183
  reward for individual goal_min: 0.0
episode_len_mean: 197.11
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 16307
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.27273488044739
time_total_s: 9139.979151010513
timers:
  learn_throughput: 499.038
  learn_time_ms: 33063.618
  load_throughput: 4763957.872
  load_time_ms: 3.464
  training_iteration_time_ms: 44214.697
  update_time_ms: 2.623
timesteps_total: 3283500
training_iteration: 199

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8317307692307693
  reward for individual goal_min: 0.5
episode_len_mean: 84.67357512953367
episode_reward_max: 2.0
episode_reward_mean: 1.8186528497409327
episode_reward_min: 1.0
episodes_this_iter: 193
episodes_total: 24402
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9948186528497409
  agent_1: 0.8238341968911918
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.5374596118927
time_total_s: 9133.271459817886
timers:
  learn_throughput: 339.343
  learn_time_ms: 48623.386
  load_throughput: 3648432.207
  load_time_ms: 4.522
  training_iteration_time_ms: 63337.735
  update_time_ms: 3.18
timesteps_total: 2508000
training_iteration: 152

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972826086956522
  reward for individual goal_min: 0.5
episode_len_mean: 45.91436464088398
episode_reward_max: 2.0
episode_reward_mean: 1.9972375690607735
episode_reward_min: 1.0
episodes_this_iter: 362
episodes_total: 38863
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972375690607734
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 57.71798777580261
time_total_s: 9083.123691797256
timers:
  learn_throughput: 375.397
  learn_time_ms: 43953.524
  load_throughput: 3519409.279
  load_time_ms: 4.688
  training_iteration_time_ms: 56817.12
  update_time_ms: 2.577
timesteps_total: 2689500
training_iteration: 163

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 81.23333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.9333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9666666666666667
  agent_1: 0.9666666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.2
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 15886
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.24613881111145
time_total_s: 9137.079805135727
timers:
  learn_throughput: 466.612
  learn_time_ms: 35361.256
  load_throughput: 4865987.175
  load_time_ms: 3.391
  training_iteration_time_ms: 46446.595
  update_time_ms: 2.608
timesteps_total: 2970000
training_iteration: 180

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-18pddilqzj/checkpoint_000180/checkpoint-180
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8873239436619719
  reward for individual goal_min: 0.0
episode_len_mean: 199.03
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 15592
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.2801148891449
time_total_s: 9138.281256914139
timers:
  learn_throughput: 477.452
  learn_time_ms: 34558.469
  load_throughput: 4922016.713
  load_time_ms: 3.352
  training_iteration_time_ms: 45860.571
  update_time_ms: 2.476
timesteps_total: 3168000
training_iteration: 192

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2605633802816901
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.5
episode_len_mean: 171.63
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 14803
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.03027558326721
time_total_s: 9129.318509817123
timers:
  learn_throughput: 432.993
  learn_time_ms: 38106.865
  load_throughput: 4608143.186
  load_time_ms: 3.581
  training_iteration_time_ms: 49979.747
  update_time_ms: 2.671
timesteps_total: 2739000
training_iteration: 166

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27702702702702703
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9927536231884058
  reward for individual goal_min: 0.5
episode_len_mean: 177.83
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 18467
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.73144292831421
time_total_s: 9141.789316892624
timers:
  learn_throughput: 511.474
  learn_time_ms: 32259.723
  load_throughput: 5101356.017
  load_time_ms: 3.234
  training_iteration_time_ms: 42893.067
  update_time_ms: 2.571
timesteps_total: 3382500
training_iteration: 205

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2785714285714286
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9883720930232558
  reward for individual goal_min: 0.5
episode_len_mean: 156.9433962264151
episode_reward_max: 2.0
episode_reward_mean: 1.3962264150943395
episode_reward_min: 0.0
episodes_this_iter: 106
episodes_total: 14834
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7264150943396226
  agent_1: 0.6698113207547169
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.40719723701477
time_total_s: 9149.32074046135
timers:
  learn_throughput: 407.235
  learn_time_ms: 40517.183
  load_throughput: 4291952.421
  load_time_ms: 3.844
  training_iteration_time_ms: 53126.634
  update_time_ms: 2.699
timesteps_total: 2739000
training_iteration: 166

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2926829268292683
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9342105263157895
  reward for individual goal_min: 0.0
episode_len_mean: 192.18
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 14480
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.94685912132263
time_total_s: 9157.065767288208
timers:
  learn_throughput: 434.37
  learn_time_ms: 37986.069
  load_throughput: 4534055.924
  load_time_ms: 3.639
  training_iteration_time_ms: 49998.964
  update_time_ms: 2.64
timesteps_total: 2904000
training_iteration: 176

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 167.4
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 16047
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.8542959690094
time_total_s: 9139.20034813881
timers:
  learn_throughput: 459.367
  learn_time_ms: 35918.982
  load_throughput: 4722155.081
  load_time_ms: 3.494
  training_iteration_time_ms: 47287.763
  update_time_ms: 2.576
timesteps_total: 2953500
training_iteration: 179

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.191601049868765
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 381
episodes_total: 53821
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.31592416763306
time_total_s: 9061.632024288177
timers:
  learn_throughput: 453.757
  learn_time_ms: 36363.064
  load_throughput: 4282020.542
  load_time_ms: 3.853
  training_iteration_time_ms: 47993.917
  update_time_ms: 2.665
timesteps_total: 3217500
training_iteration: 195

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22794117647058823
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.974025974025974
  reward for individual goal_min: 0.0
episode_len_mean: 172.54
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 15575
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.42583608627319
time_total_s: 9163.877038240433
timers:
  learn_throughput: 430.233
  learn_time_ms: 38351.268
  load_throughput: 4819461.131
  load_time_ms: 3.424
  training_iteration_time_ms: 50421.656
  update_time_ms: 2.692
timesteps_total: 2854500
training_iteration: 173

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.12
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 14714
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.2743034362793
time_total_s: 9148.782187700272
timers:
  learn_throughput: 468.838
  learn_time_ms: 35193.356
  load_throughput: 4874898.988
  load_time_ms: 3.385
  training_iteration_time_ms: 46468.062
  update_time_ms: 2.603
timesteps_total: 2871000
training_iteration: 174

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.96
  reward for individual goal_min: 0.5
episode_len_mean: 55.13468013468013
episode_reward_max: 2.0
episode_reward_mean: 1.9595959595959596
episode_reward_min: 1.0
episodes_this_iter: 297
episodes_total: 35742
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9595959595959596
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 52.12357544898987
time_total_s: 9140.620436429977
timers:
  learn_throughput: 418.373
  learn_time_ms: 39438.513
  load_throughput: 4408953.219
  load_time_ms: 3.742
  training_iteration_time_ms: 51437.189
  update_time_ms: 2.762
timesteps_total: 2986500
training_iteration: 181

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 97.38333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.7833333333333334
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.95
  agent_1: 0.8333333333333334
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2037037037037037
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8421052631578947
  reward for individual goal_min: 0.0
episode_len_mean: 208.28
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 16388
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.38
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.61865520477295
time_total_s: 9191.597806215286
timers:
  learn_throughput: 502.366
  learn_time_ms: 32844.612
  load_throughput: 4749539.568
  load_time_ms: 3.474
  training_iteration_time_ms: 43943.593
  update_time_ms: 2.599
timesteps_total: 3300000
training_iteration: 200

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 191.14
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 15971
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.208600997924805
time_total_s: 9183.288406133652
timers:
  learn_throughput: 465.266
  learn_time_ms: 35463.548
  load_throughput: 4901310.633
  load_time_ms: 3.366
  training_iteration_time_ms: 46491.662
  update_time_ms: 2.614
timesteps_total: 2986500
training_iteration: 181

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32051282051282054
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8863636363636364
  reward for individual goal_min: 0.0
episode_len_mean: 191.71
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 15677
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.83634614944458
time_total_s: 9184.117603063583
timers:
  learn_throughput: 477.309
  learn_time_ms: 34568.806
  load_throughput: 4977095.556
  load_time_ms: 3.315
  training_iteration_time_ms: 45877.862
  update_time_ms: 2.478
timesteps_total: 3184500
training_iteration: 193

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.403693931398415
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 379
episodes_total: 39242
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.09261178970337
time_total_s: 9140.21630358696
timers:
  learn_throughput: 374.77
  learn_time_ms: 44026.986
  load_throughput: 3547862.035
  load_time_ms: 4.651
  training_iteration_time_ms: 56899.02
  update_time_ms: 2.645
timesteps_total: 2706000
training_iteration: 164

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.85
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 14894
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.28570508956909
time_total_s: 9180.604214906693
timers:
  learn_throughput: 433.525
  learn_time_ms: 38060.058
  load_throughput: 4616565.894
  load_time_ms: 3.574
  training_iteration_time_ms: 49862.053
  update_time_ms: 2.682
timesteps_total: 2755500
training_iteration: 167

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2905405405405405
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 168.24
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 18564
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.770092248916626
time_total_s: 9183.55940914154
timers:
  learn_throughput: 512.546
  learn_time_ms: 32192.205
  load_throughput: 5039505.414
  load_time_ms: 3.274
  training_iteration_time_ms: 42779.669
  update_time_ms: 2.576
timesteps_total: 3399000
training_iteration: 206

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8973214285714286
  reward for individual goal_min: 0.5
episode_len_mean: 70.53418803418803
episode_reward_max: 2.0
episode_reward_mean: 1.9017094017094016
episode_reward_min: 1.0
episodes_this_iter: 234
episodes_total: 24636
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9957264957264957
  agent_1: 0.905982905982906
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 66.51047611236572
time_total_s: 9199.781935930252
timers:
  learn_throughput: 336.694
  learn_time_ms: 49005.948
  load_throughput: 3490106.308
  load_time_ms: 4.728
  training_iteration_time_ms: 63878.878
  update_time_ms: 3.153
timesteps_total: 2524500
training_iteration: 153

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28205128205128205
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.86875
  reward for individual goal_min: 0.0
episode_len_mean: 193.21
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 14564
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.216049671173096
time_total_s: 9206.281816959381
timers:
  learn_throughput: 435.319
  learn_time_ms: 37903.199
  load_throughput: 4556114.736
  load_time_ms: 3.622
  training_iteration_time_ms: 49906.387
  update_time_ms: 2.644
timesteps_total: 2920500
training_iteration: 177

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.320754716981135
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 371
episodes_total: 54192
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.376195192337036
time_total_s: 9109.008219480515
timers:
  learn_throughput: 454.691
  learn_time_ms: 36288.418
  load_throughput: 4187198.451
  load_time_ms: 3.941
  training_iteration_time_ms: 47874.184
  update_time_ms: 2.652
timesteps_total: 3234000
training_iteration: 196

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28846153846153844
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9838709677419355
  reward for individual goal_min: 0.0
episode_len_mean: 169.88
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 14929
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.41659426689148
time_total_s: 9202.737334728241
timers:
  learn_throughput: 405.53
  learn_time_ms: 40687.469
  load_throughput: 4331602.679
  load_time_ms: 3.809
  training_iteration_time_ms: 53374.552
  update_time_ms: 2.641
timesteps_total: 2755500
training_iteration: 167

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 73.66666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.9333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9833333333333333
  agent_1: 0.95
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3014705882352941
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 158.82692307692307
episode_reward_max: 2.0
episode_reward_mean: 1.4038461538461537
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 14818
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7115384615384616
  agent_1: 0.6923076923076923
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.80035328865051
time_total_s: 9198.582540988922
timers:
  learn_throughput: 465.657
  learn_time_ms: 35433.822
  load_throughput: 4903012.115
  load_time_ms: 3.365
  training_iteration_time_ms: 46763.701
  update_time_ms: 2.581
timesteps_total: 2887500
training_iteration: 175

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22435897435897437
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.1
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 16139
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.76905536651611
time_total_s: 9194.969403505325
timers:
  learn_throughput: 458.951
  learn_time_ms: 35951.554
  load_throughput: 4740527.715
  load_time_ms: 3.481
  training_iteration_time_ms: 47248.187
  update_time_ms: 2.57
timesteps_total: 2970000
training_iteration: 180

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2716049382716049
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 187.89
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 15664
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.2864933013916
time_total_s: 9214.163531541824
timers:
  learn_throughput: 430.222
  learn_time_ms: 38352.296
  load_throughput: 4827058.192
  load_time_ms: 3.418
  training_iteration_time_ms: 50415.497
  update_time_ms: 2.681
timesteps_total: 2871000
training_iteration: 174

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3014705882352941
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8289473684210527
  reward for individual goal_min: 0.0
episode_len_mean: 178.6
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 16481
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.35019898414612
time_total_s: 9235.948005199432
timers:
  learn_throughput: 504.721
  learn_time_ms: 32691.341
  load_throughput: 4741112.283
  load_time_ms: 3.48
  training_iteration_time_ms: 43767.612
  update_time_ms: 2.607
timesteps_total: 3316500
training_iteration: 201

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9766666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 53.70645161290322
episode_reward_max: 2.0
episode_reward_mean: 1.9774193548387098
episode_reward_min: 1.0
episodes_this_iter: 310
episodes_total: 36052
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9806451612903225
  agent_1: 0.9967741935483871
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.292173862457275
time_total_s: 9194.912610292435
timers:
  learn_throughput: 415.363
  learn_time_ms: 39724.257
  load_throughput: 4396265.786
  load_time_ms: 3.753
  training_iteration_time_ms: 51773.805
  update_time_ms: 2.783
timesteps_total: 3003000
training_iteration: 182

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2898550724637681
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.85625
  reward for individual goal_min: 0.0
episode_len_mean: 178.6
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 15772
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.23783802986145
time_total_s: 9229.355441093445
timers:
  learn_throughput: 475.788
  learn_time_ms: 34679.334
  load_throughput: 4955605.075
  load_time_ms: 3.33
  training_iteration_time_ms: 45971.536
  update_time_ms: 2.473
timesteps_total: 3201000
training_iteration: 194

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24025974025974026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.35
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 16063
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.55608010292053
time_total_s: 9229.844486236572
timers:
  learn_throughput: 461.518
  learn_time_ms: 35751.596
  load_throughput: 4904575.741
  load_time_ms: 3.364
  training_iteration_time_ms: 46760.487
  update_time_ms: 2.646
timesteps_total: 3003000
training_iteration: 182

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2876712328767123
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 164.54455445544554
episode_reward_max: 2.0
episode_reward_mean: 1.396039603960396
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 18665
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7128712871287128
  agent_1: 0.6831683168316832
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.95655274391174
time_total_s: 9225.515961885452
timers:
  learn_throughput: 514.657
  learn_time_ms: 32060.172
  load_throughput: 5061361.183
  load_time_ms: 3.26
  training_iteration_time_ms: 42590.202
  update_time_ms: 2.531
timesteps_total: 3415500
training_iteration: 207

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.225
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9875
  reward for individual goal_min: 0.0
episode_len_mean: 183.56
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 14983
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.697261095047
time_total_s: 9229.30147600174
timers:
  learn_throughput: 436.794
  learn_time_ms: 37775.208
  load_throughput: 4607682.976
  load_time_ms: 3.581
  training_iteration_time_ms: 49519.03
  update_time_ms: 2.695
timesteps_total: 2772000
training_iteration: 168

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.265415549597854
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 373
episodes_total: 39615
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.69546604156494
time_total_s: 9198.911769628525
timers:
  learn_throughput: 372.943
  learn_time_ms: 44242.647
  load_throughput: 3551211.823
  load_time_ms: 4.646
  training_iteration_time_ms: 57148.415
  update_time_ms: 2.624
timesteps_total: 2722500
training_iteration: 165

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.94375
  reward for individual goal_min: 0.0
episode_len_mean: 178.53
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 14657
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.835304975509644
time_total_s: 9254.11712193489
timers:
  learn_throughput: 436.479
  learn_time_ms: 37802.549
  load_throughput: 4558935.989
  load_time_ms: 3.619
  training_iteration_time_ms: 49651.958
  update_time_ms: 2.655
timesteps_total: 2937000
training_iteration: 178

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9947089947089947
  reward for individual goal_min: 0.5
episode_len_mean: 44.152
episode_reward_max: 2.0
episode_reward_mean: 1.9946666666666666
episode_reward_min: 1.0
episodes_this_iter: 375
episodes_total: 54567
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9946666666666667
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.81305718421936
time_total_s: 9156.821276664734
timers:
  learn_throughput: 454.974
  learn_time_ms: 36265.814
  load_throughput: 4181253.421
  load_time_ms: 3.946
  training_iteration_time_ms: 47853.537
  update_time_ms: 2.648
timesteps_total: 3250500
training_iteration: 197

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14285714285714285
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9941176470588236
  reward for individual goal_min: 0.5
episode_len_mean: 181.1
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 15021
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.5340473651886
time_total_s: 9251.27138209343
timers:
  learn_throughput: 409.702
  learn_time_ms: 40273.203
  load_throughput: 4308062.972
  load_time_ms: 3.83
  training_iteration_time_ms: 52850.065
  update_time_ms: 2.652
timesteps_total: 2772000
training_iteration: 168

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23648648648648649
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.79
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 14914
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.86848592758179
time_total_s: 9244.451026916504
timers:
  learn_throughput: 464.809
  learn_time_ms: 35498.42
  load_throughput: 4871330.349
  load_time_ms: 3.387
  training_iteration_time_ms: 46835.645
  update_time_ms: 2.605
timesteps_total: 2904000
training_iteration: 176

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.273972602739726
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9671052631578947
  reward for individual goal_min: 0.0
episode_len_mean: 169.46
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 16233
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.92111253738403
time_total_s: 9243.89051604271
timers:
  learn_throughput: 458.336
  learn_time_ms: 35999.808
  load_throughput: 4722638.442
  load_time_ms: 3.494
  training_iteration_time_ms: 47322.218
  update_time_ms: 2.591
timesteps_total: 2986500
training_iteration: 181

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9
  reward for individual goal_min: 0.5
episode_len_mean: 69.73333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.8958333333333333
episode_reward_min: 1.0
episodes_this_iter: 240
episodes_total: 24876
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9958333333333333
  agent_1: 0.9
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.5286009311676
time_total_s: 9265.31053686142
timers:
  learn_throughput: 336.195
  learn_time_ms: 49078.591
  load_throughput: 3516119.192
  load_time_ms: 4.693
  training_iteration_time_ms: 64060.389
  update_time_ms: 3.052
timesteps_total: 2541000
training_iteration: 154

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3561643835616438
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9923076923076923
  reward for individual goal_min: 0.5
episode_len_mean: 165.82
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 15760
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.84720277786255
time_total_s: 9266.010734319687
timers:
  learn_throughput: 428.359
  learn_time_ms: 38519.069
  load_throughput: 4821106.25
  load_time_ms: 3.422
  training_iteration_time_ms: 50507.516
  update_time_ms: 2.697
timesteps_total: 2887500
training_iteration: 175

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.4315068493150685
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8208955223880597
  reward for individual goal_min: 0.0
episode_len_mean: 174.85
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 16572
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.39257550239563
time_total_s: 9279.340580701828
timers:
  learn_throughput: 504.882
  learn_time_ms: 32680.921
  load_throughput: 4754009.37
  load_time_ms: 3.471
  training_iteration_time_ms: 43708.812
  update_time_ms: 2.593
timesteps_total: 3333000
training_iteration: 202

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2357142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8658536585365854
  reward for individual goal_min: 0.0
episode_len_mean: 186.01
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 15857
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.93806529045105
time_total_s: 9271.293506383896
timers:
  learn_throughput: 481.596
  learn_time_ms: 34261.102
  load_throughput: 4988216.435
  load_time_ms: 3.308
  training_iteration_time_ms: 45498.552
  update_time_ms: 2.463
timesteps_total: 3217500
training_iteration: 195

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9577464788732394
  reward for individual goal_min: 0.0
episode_len_mean: 184.92
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 16152
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.09700632095337
time_total_s: 9276.941492557526
timers:
  learn_throughput: 459.752
  learn_time_ms: 35888.912
  load_throughput: 4858778.811
  load_time_ms: 3.396
  training_iteration_time_ms: 46956.823
  update_time_ms: 2.643
timesteps_total: 3019500
training_iteration: 183

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32051282051282054
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9850746268656716
  reward for individual goal_min: 0.5
episode_len_mean: 177.18
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 18758
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.31989288330078
time_total_s: 9266.835854768753
timers:
  learn_throughput: 519.879
  learn_time_ms: 31738.142
  load_throughput: 5081615.696
  load_time_ms: 3.247
  training_iteration_time_ms: 42193.9
  update_time_ms: 2.548
timesteps_total: 3432000
training_iteration: 208

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9662576687116564
  reward for individual goal_min: 0.5
episode_len_mean: 55.20134228187919
episode_reward_max: 2.0
episode_reward_mean: 1.9630872483221478
episode_reward_min: 1.0
episodes_this_iter: 298
episodes_total: 36350
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9630872483221476
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 52.59765386581421
time_total_s: 9247.510264158249
timers:
  learn_throughput: 413.877
  learn_time_ms: 39866.935
  load_throughput: 4361577.09
  load_time_ms: 3.783
  training_iteration_time_ms: 51982.752
  update_time_ms: 2.789
timesteps_total: 3019500
training_iteration: 183

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2974683544303797
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 184.76
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 15071
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.61088180541992
time_total_s: 9281.91235780716
timers:
  learn_throughput: 434.372
  learn_time_ms: 37985.88
  load_throughput: 4661844.637
  load_time_ms: 3.539
  training_iteration_time_ms: 49833.793
  update_time_ms: 2.667
timesteps_total: 2788500
training_iteration: 169

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8525641025641025
  reward for individual goal_min: 0.0
episode_len_mean: 200.6
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 14740
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.702393531799316
time_total_s: 9303.81951546669
timers:
  learn_throughput: 436.462
  learn_time_ms: 37803.979
  load_throughput: 4587798.048
  load_time_ms: 3.596
  training_iteration_time_ms: 49692.59
  update_time_ms: 2.652
timesteps_total: 2953500
training_iteration: 179

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.994475138121547
  reward for individual goal_min: 0.5
episode_len_mean: 45.0792349726776
episode_reward_max: 2.0
episode_reward_mean: 1.994535519125683
episode_reward_min: 1.0
episodes_this_iter: 366
episodes_total: 54933
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972677595628415
  agent_1: 0.9972677595628415
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.72091889381409
time_total_s: 9204.542195558548
timers:
  learn_throughput: 454.8
  learn_time_ms: 36279.659
  load_throughput: 4180571.456
  load_time_ms: 3.947
  training_iteration_time_ms: 47884.759
  update_time_ms: 2.981
timesteps_total: 3267000
training_iteration: 198

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30864197530864196
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.06
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 15008
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.81233072280884
time_total_s: 9293.263357639313
timers:
  learn_throughput: 460.412
  learn_time_ms: 35837.493
  load_throughput: 4808912.113
  load_time_ms: 3.431
  training_iteration_time_ms: 47236.323
  update_time_ms: 2.624
timesteps_total: 2920500
training_iteration: 177

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3055555555555556
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 163.85
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 15120
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.438695192337036
time_total_s: 9302.710077285767
timers:
  learn_throughput: 414.201
  learn_time_ms: 39835.737
  load_throughput: 4299471.683
  load_time_ms: 3.838
  training_iteration_time_ms: 52355.67
  update_time_ms: 2.639
timesteps_total: 2788500
training_iteration: 169

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21951219512195122
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9753086419753086
  reward for individual goal_min: 0.0
episode_len_mean: 184.63
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 16326
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.00903582572937
time_total_s: 9290.899551868439
timers:
  learn_throughput: 458.735
  learn_time_ms: 35968.462
  load_throughput: 4754433.949
  load_time_ms: 3.47
  training_iteration_time_ms: 47288.806
  update_time_ms: 2.596
timesteps_total: 3003000
training_iteration: 182

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.721518987341774
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 395
episodes_total: 40010
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.34821605682373
time_total_s: 9258.259985685349
timers:
  learn_throughput: 370.443
  learn_time_ms: 44541.234
  load_throughput: 3553107.98
  load_time_ms: 4.644
  training_iteration_time_ms: 57520.057
  update_time_ms: 2.621
timesteps_total: 2739000
training_iteration: 166

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7828947368421053
  reward for individual goal_min: 0.0
episode_len_mean: 200.27
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 16653
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.074156045913696
time_total_s: 9324.414736747742
timers:
  learn_throughput: 503.153
  learn_time_ms: 32793.2
  load_throughput: 4731420.602
  load_time_ms: 3.487
  training_iteration_time_ms: 43865.521
  update_time_ms: 2.581
timesteps_total: 3349500
training_iteration: 203

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29605263157894735
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.39
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 15852
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.89760112762451
time_total_s: 9316.908335447311
timers:
  learn_throughput: 431.532
  learn_time_ms: 38235.869
  load_throughput: 4810048.513
  load_time_ms: 3.43
  training_iteration_time_ms: 50194.808
  update_time_ms: 2.671
timesteps_total: 2904000
training_iteration: 176

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8648648648648649
  reward for individual goal_min: 0.0
episode_len_mean: 196.52
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 15939
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.51199650764465
time_total_s: 9315.80550289154
timers:
  learn_throughput: 487.055
  learn_time_ms: 33877.1
  load_throughput: 5016091.847
  load_time_ms: 3.289
  training_iteration_time_ms: 45131.668
  update_time_ms: 2.464
timesteps_total: 3234000
training_iteration: 196

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 170.97
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 18856
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.50391387939453
time_total_s: 9309.339768648148
timers:
  learn_throughput: 520.353
  learn_time_ms: 31709.265
  load_throughput: 5114097.721
  load_time_ms: 3.226
  training_iteration_time_ms: 42097.157
  update_time_ms: 2.562
timesteps_total: 3448500
training_iteration: 209

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3092105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9675324675324676
  reward for individual goal_min: 0.0
episode_len_mean: 167.71
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 16251
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.35232400894165
time_total_s: 9326.293816566467
timers:
  learn_throughput: 458.185
  learn_time_ms: 36011.622
  load_throughput: 4908611.026
  load_time_ms: 3.361
  training_iteration_time_ms: 47162.19
  update_time_ms: 2.654
timesteps_total: 3036000
training_iteration: 184

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.896551724137931
  reward for individual goal_min: 0.5
episode_len_mean: 69.62711864406779
episode_reward_max: 2.0
episode_reward_mean: 1.8983050847457628
episode_reward_min: 1.0
episodes_this_iter: 236
episodes_total: 25112
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9915254237288136
  agent_1: 0.9067796610169492
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 64.35148048400879
time_total_s: 9329.662017345428
timers:
  learn_throughput: 336.7
  learn_time_ms: 49005.027
  load_throughput: 3446772.219
  load_time_ms: 4.787
  training_iteration_time_ms: 64045.58
  update_time_ms: 3.146
timesteps_total: 2557500
training_iteration: 155

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9654088050314465
  reward for individual goal_min: 0.5
episode_len_mean: 54.31023102310231
episode_reward_max: 2.0
episode_reward_mean: 1.9636963696369636
episode_reward_min: 1.0
episodes_this_iter: 303
episodes_total: 36653
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9636963696369637
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 54.300076723098755
time_total_s: 9301.810340881348
timers:
  learn_throughput: 410.986
  learn_time_ms: 40147.357
  load_throughput: 4354469.298
  load_time_ms: 3.789
  training_iteration_time_ms: 52281.762
  update_time_ms: 2.779
timesteps_total: 3036000
training_iteration: 184

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2922077922077922
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9929577464788732
  reward for individual goal_min: 0.5
episode_len_mean: 173.12
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 15166
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.93877124786377
time_total_s: 9331.851129055023
timers:
  learn_throughput: 436.613
  learn_time_ms: 37790.862
  load_throughput: 4674376.646
  load_time_ms: 3.53
  training_iteration_time_ms: 49686.209
  update_time_ms: 2.634
timesteps_total: 2805000
training_iteration: 170

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974619289340102
  reward for individual goal_min: 0.5
episode_len_mean: 43.32808398950131
episode_reward_max: 2.0
episode_reward_mean: 1.9973753280839894
episode_reward_min: 1.0
episodes_this_iter: 381
episodes_total: 55314
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973753280839895
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.757648229599
time_total_s: 9252.299843788147
timers:
  learn_throughput: 455.059
  learn_time_ms: 36259.065
  load_throughput: 4176207.103
  load_time_ms: 3.951
  training_iteration_time_ms: 47875.531
  update_time_ms: 2.955
timesteps_total: 3283500
training_iteration: 199

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 184.3
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 15095
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.16650342941284
time_total_s: 9342.429861068726
timers:
  learn_throughput: 458.551
  learn_time_ms: 35982.89
  load_throughput: 4741924.424
  load_time_ms: 3.48
  training_iteration_time_ms: 47431.972
  update_time_ms: 2.607
timesteps_total: 2937000
training_iteration: 178

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25882352941176473
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8
  reward for individual goal_min: 0.0
episode_len_mean: 209.16
episode_reward_max: 2.0
episode_reward_mean: 1.06
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 16734
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.4
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.641584634780884
time_total_s: 9369.056321382523
timers:
  learn_throughput: 503.373
  learn_time_ms: 32778.904
  load_throughput: 4689453.442
  load_time_ms: 3.519
  training_iteration_time_ms: 43782.191
  update_time_ms: 2.582
timesteps_total: 3366000
training_iteration: 204

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2565789473684211
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.96
  reward for individual goal_min: 0.0
episode_len_mean: 178.45
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 16413
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.89707660675049
time_total_s: 9339.79662847519
timers:
  learn_throughput: 454.052
  learn_time_ms: 36339.438
  load_throughput: 4776715.948
  load_time_ms: 3.454
  training_iteration_time_ms: 47842.477
  update_time_ms: 2.613
timesteps_total: 3019500
training_iteration: 183

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27631578947368424
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.0
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 15218
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.64305305480957
time_total_s: 9352.353130340576
timers:
  learn_throughput: 416.195
  learn_time_ms: 39644.916
  load_throughput: 4363364.543
  load_time_ms: 3.781
  training_iteration_time_ms: 52087.705
  update_time_ms: 2.601
timesteps_total: 2805000
training_iteration: 170

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18493150684931506
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 182.1
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 18946
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.97618651390076
time_total_s: 9350.315955162048
timers:
  learn_throughput: 522.359
  learn_time_ms: 31587.448
  load_throughput: 5122539.137
  load_time_ms: 3.221
  training_iteration_time_ms: 41921.235
  update_time_ms: 2.548
timesteps_total: 3465000
training_iteration: 210

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.85
  reward for individual goal_min: 0.5
episode_len_mean: 113.46666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.8166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9333333333333333
  agent_1: 0.8833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8289473684210527
  reward for individual goal_min: 0.0
episode_len_mean: 205.74
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 14820
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.00489521026611
time_total_s: 9363.824410676956
timers:
  learn_throughput: 437.875
  learn_time_ms: 37681.955
  load_throughput: 4576784.493
  load_time_ms: 3.605
  training_iteration_time_ms: 49544.548
  update_time_ms: 2.644
timesteps_total: 2970000
training_iteration: 180

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19mcvnks1j/checkpoint_000180/checkpoint-180
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28378378378378377
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9121621621621622
  reward for individual goal_min: 0.0
episode_len_mean: 189.6
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 16027
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.29371762275696
time_total_s: 9361.099220514297
timers:
  learn_throughput: 486.414
  learn_time_ms: 33921.747
  load_throughput: 5071078.023
  load_time_ms: 3.254
  training_iteration_time_ms: 45176.804
  update_time_ms: 2.467
timesteps_total: 3250500
training_iteration: 197

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.39946380697051
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 373
episodes_total: 40383
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.54002261161804
time_total_s: 9316.800008296967
timers:
  learn_throughput: 366.648
  learn_time_ms: 45002.285
  load_throughput: 3539931.56
  load_time_ms: 4.661
  training_iteration_time_ms: 57997.198
  update_time_ms: 2.627
timesteps_total: 2755500
training_iteration: 167

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2777777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 166.5
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 15950
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.86573505401611
time_total_s: 9369.774070501328
timers:
  learn_throughput: 426.996
  learn_time_ms: 38642.055
  load_throughput: 4807509.065
  load_time_ms: 3.432
  training_iteration_time_ms: 50561.27
  update_time_ms: 2.689
timesteps_total: 2920500
training_iteration: 177

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2911392405063291
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 182.16
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 16341
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.562183141708374
time_total_s: 9372.855999708176
timers:
  learn_throughput: 460.918
  learn_time_ms: 35798.143
  load_throughput: 4927308.3
  load_time_ms: 3.349
  training_iteration_time_ms: 46952.623
  update_time_ms: 2.672
timesteps_total: 3052500
training_iteration: 185

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9733333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 52.438709677419354
episode_reward_max: 2.0
episode_reward_mean: 1.9741935483870967
episode_reward_min: 1.0
episodes_this_iter: 310
episodes_total: 36963
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9774193548387097
  agent_1: 0.9967741935483871
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.48920035362244
time_total_s: 9354.29954123497
timers:
  learn_throughput: 409.063
  learn_time_ms: 40336.119
  load_throughput: 4370087.457
  load_time_ms: 3.776
  training_iteration_time_ms: 52461.244
  update_time_ms: 2.757
timesteps_total: 3052500
training_iteration: 185

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.28
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 15257
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.4811806678772
time_total_s: 9380.3323097229
timers:
  learn_throughput: 439.343
  learn_time_ms: 37556.055
  load_throughput: 4673682.163
  load_time_ms: 3.53
  training_iteration_time_ms: 49310.587
  update_time_ms: 2.63
timesteps_total: 2821500
training_iteration: 171

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8794642857142857
  reward for individual goal_min: 0.5
episode_len_mean: 72.31277533039648
episode_reward_max: 2.0
episode_reward_mean: 1.881057268722467
episode_reward_min: 1.0
episodes_this_iter: 227
episodes_total: 25339
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8810572687224669
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 65.23480224609375
time_total_s: 9394.896819591522
timers:
  learn_throughput: 336.266
  learn_time_ms: 49068.258
  load_throughput: 3392252.22
  load_time_ms: 4.864
  training_iteration_time_ms: 64141.498
  update_time_ms: 3.168
timesteps_total: 2574000
training_iteration: 156

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2926829268292683
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.782051282051282
  reward for individual goal_min: 0.0
episode_len_mean: 209.29
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 16811
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.42
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.22120904922485
time_total_s: 9412.277530431747
timers:
  learn_throughput: 502.592
  learn_time_ms: 32829.833
  load_throughput: 4661970.252
  load_time_ms: 3.539
  training_iteration_time_ms: 43760.384
  update_time_ms: 2.577
timesteps_total: 3382500
training_iteration: 205

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22297297297297297
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9940476190476191
  reward for individual goal_min: 0.5
episode_len_mean: 175.14
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 16508
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.43455386161804
time_total_s: 9384.231182336807
timers:
  learn_throughput: 457.777
  learn_time_ms: 36043.719
  load_throughput: 4756231.083
  load_time_ms: 3.469
  training_iteration_time_ms: 47464.015
  update_time_ms: 2.641
timesteps_total: 3036000
training_iteration: 184

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.5
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9928571428571429
  reward for individual goal_min: 0.5
episode_len_mean: 43.795212765957444
episode_reward_max: 2.0
episode_reward_mean: 1.9920212765957446
episode_reward_min: 1.0
episodes_this_iter: 376
episodes_total: 55690
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973404255319149
  agent_1: 0.9946808510638298
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.47340154647827
time_total_s: 9303.773245334625
timers:
  learn_throughput: 456.069
  learn_time_ms: 36178.737
  load_throughput: 4179713.002
  load_time_ms: 3.948
  training_iteration_time_ms: 47694.638
  update_time_ms: 2.966
timesteps_total: 3300000
training_iteration: 200

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-172kuwc_58/checkpoint_000200/checkpoint-200
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27564102564102566
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.971830985915493
  reward for individual goal_min: 0.0
episode_len_mean: 180.98
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 19036
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.97923302650452
time_total_s: 9393.295188188553
timers:
  learn_throughput: 520.619
  learn_time_ms: 31693.025
  load_throughput: 5115836.721
  load_time_ms: 3.225
  training_iteration_time_ms: 42053.235
  update_time_ms: 2.537
timesteps_total: 3481500
training_iteration: 211

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2328767123287671
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.87
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 15187
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.6992347240448
time_total_s: 9390.12909579277
timers:
  learn_throughput: 455.887
  learn_time_ms: 36193.208
  load_throughput: 4709750.514
  load_time_ms: 3.503
  training_iteration_time_ms: 47727.026
  update_time_ms: 2.597
timesteps_total: 2953500
training_iteration: 179

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29605263157894735
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9246575342465754
  reward for individual goal_min: 0.0
episode_len_mean: 184.03
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 14911
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.97188448905945
time_total_s: 9409.796295166016
timers:
  learn_throughput: 441.31
  learn_time_ms: 37388.657
  load_throughput: 4558905.958
  load_time_ms: 3.619
  training_iteration_time_ms: 49159.776
  update_time_ms: 2.621
timesteps_total: 2986500
training_iteration: 181

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2708333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 159.44660194174756
episode_reward_max: 2.0
episode_reward_mean: 1.3786407766990292
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 15321
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7281553398058253
  agent_1: 0.6504854368932039
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.24696183204651
time_total_s: 9402.600092172623
timers:
  learn_throughput: 420.411
  learn_time_ms: 39247.313
  load_throughput: 4363914.823
  load_time_ms: 3.781
  training_iteration_time_ms: 51676.671
  update_time_ms: 2.6
timesteps_total: 2821500
training_iteration: 171

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19117647058823528
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8227848101265823
  reward for individual goal_min: 0.0
episode_len_mean: 190.53
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 16112
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.83916258811951
time_total_s: 9407.938383102417
timers:
  learn_throughput: 484.113
  learn_time_ms: 34082.946
  load_throughput: 5072527.614
  load_time_ms: 3.253
  training_iteration_time_ms: 45381.465
  update_time_ms: 2.474
timesteps_total: 3267000
training_iteration: 198

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1891891891891892
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.38
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 16041
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.418105363845825
time_total_s: 9421.192175865173
timers:
  learn_throughput: 426.175
  learn_time_ms: 38716.5
  load_throughput: 4842459.924
  load_time_ms: 3.407
  training_iteration_time_ms: 50600.187
  update_time_ms: 2.695
timesteps_total: 2937000
training_iteration: 178

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 162.54
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 16440
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.39718794822693
time_total_s: 9420.253187656403
timers:
  learn_throughput: 459.883
  learn_time_ms: 35878.686
  load_throughput: 4945158.953
  load_time_ms: 3.337
  training_iteration_time_ms: 47000.311
  update_time_ms: 2.704
timesteps_total: 3069000
training_iteration: 186

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.949333333333335
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 375
episodes_total: 40758
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.39668393135071
time_total_s: 9376.196692228317
timers:
  learn_throughput: 365.476
  learn_time_ms: 45146.664
  load_throughput: 3541543.815
  load_time_ms: 4.659
  training_iteration_time_ms: 58200.248
  update_time_ms: 2.624
timesteps_total: 2772000
training_iteration: 168

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2631578947368421
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.16
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 15353
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.75084686279297
time_total_s: 9428.083156585693
timers:
  learn_throughput: 439.705
  learn_time_ms: 37525.191
  load_throughput: 4693587.977
  load_time_ms: 3.515
  training_iteration_time_ms: 49288.083
  update_time_ms: 2.657
timesteps_total: 2838000
training_iteration: 172

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.98
  reward for individual goal_min: 0.5
episode_len_mean: 50.76993865030675
episode_reward_max: 2.0
episode_reward_mean: 1.98159509202454
episode_reward_min: 1.0
episodes_this_iter: 326
episodes_total: 37289
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9815950920245399
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 54.824787855148315
time_total_s: 9409.124329090118
timers:
  learn_throughput: 406.699
  learn_time_ms: 40570.556
  load_throughput: 4357512.656
  load_time_ms: 3.787
  training_iteration_time_ms: 52752.203
  update_time_ms: 2.723
timesteps_total: 3069000
training_iteration: 186

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27848101265822783
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8607594936708861
  reward for individual goal_min: 0.0
episode_len_mean: 189.15
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 16898
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.43
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.5856032371521
time_total_s: 9455.8631336689
timers:
  learn_throughput: 503.198
  learn_time_ms: 32790.26
  load_throughput: 4691869.67
  load_time_ms: 3.517
  training_iteration_time_ms: 43738.767
  update_time_ms: 2.596
timesteps_total: 3399000
training_iteration: 206

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25675675675675674
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 181.86
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 19129
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.86266589164734
time_total_s: 9434.1578540802
timers:
  learn_throughput: 518.828
  learn_time_ms: 31802.446
  load_throughput: 5107115.837
  load_time_ms: 3.231
  training_iteration_time_ms: 42105.772
  update_time_ms: 2.524
timesteps_total: 3498000
training_iteration: 212

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22602739726027396
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.0
episode_len_mean: 170.27
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 16598
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.9948456287384
time_total_s: 9431.226027965546
timers:
  learn_throughput: 459.328
  learn_time_ms: 35922.033
  load_throughput: 4790935.12
  load_time_ms: 3.444
  training_iteration_time_ms: 47317.986
  update_time_ms: 2.631
timesteps_total: 3052500
training_iteration: 185

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.052219321148826
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 383
episodes_total: 56073
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.00274395942688
time_total_s: 9351.775989294052
timers:
  learn_throughput: 456.967
  learn_time_ms: 36107.614
  load_throughput: 4191001.998
  load_time_ms: 3.937
  training_iteration_time_ms: 47602.225
  update_time_ms: 2.96
timesteps_total: 3316500
training_iteration: 201

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20512820512820512
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8428571428571429
  reward for individual goal_min: 0.0
episode_len_mean: 198.88
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 16196
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.919121980667114
time_total_s: 9453.857505083084
timers:
  learn_throughput: 483.05
  learn_time_ms: 34157.983
  load_throughput: 5050649.229
  load_time_ms: 3.267
  training_iteration_time_ms: 45532.099
  update_time_ms: 2.465
timesteps_total: 3283500
training_iteration: 199

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8940677966101694
  reward for individual goal_min: 0.5
episode_len_mean: 73.45814977973568
episode_reward_max: 2.0
episode_reward_mean: 1.8898678414096917
episode_reward_min: 1.0
episodes_this_iter: 227
episodes_total: 25566
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8898678414096917
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 63.93134832382202
time_total_s: 9458.828167915344
timers:
  learn_throughput: 335.696
  learn_time_ms: 49151.642
  load_throughput: 3370428.329
  load_time_ms: 4.896
  training_iteration_time_ms: 64141.304
  update_time_ms: 3.137
timesteps_total: 2590500
training_iteration: 157

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3356164383561644
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9626865671641791
  reward for individual goal_min: 0.0
episode_len_mean: 185.09
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 15000
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.69824004173279
time_total_s: 9459.494535207748
timers:
  learn_throughput: 440.332
  learn_time_ms: 37471.752
  load_throughput: 4631581.427
  load_time_ms: 3.562
  training_iteration_time_ms: 49203.904
  update_time_ms: 2.627
timesteps_total: 3003000
training_iteration: 182

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9807692307692307
  reward for individual goal_min: 0.5
episode_len_mean: 179.07
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 15412
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.89424133300781
time_total_s: 9453.49433350563
timers:
  learn_throughput: 421.455
  learn_time_ms: 39150.09
  load_throughput: 4360093.242
  load_time_ms: 3.784
  training_iteration_time_ms: 51548.134
  update_time_ms: 2.617
timesteps_total: 2838000
training_iteration: 172

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 92.61666666666666
episode_reward_max: 2.0
episode_reward_mean: 1.8333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9333333333333333
  agent_1: 0.9
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.5
episode_len_mean: 176.93
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 15282
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.08042860031128
time_total_s: 9450.209524393082
timers:
  learn_throughput: 452.286
  learn_time_ms: 36481.305
  load_throughput: 4690756.624
  load_time_ms: 3.518
  training_iteration_time_ms: 48025.337
  update_time_ms: 2.599
timesteps_total: 2970000
training_iteration: 180

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3684210526315789
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 162.93069306930693
episode_reward_max: 2.0
episode_reward_mean: 1.4059405940594059
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 16541
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6732673267326733
  agent_1: 0.7326732673267327
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.39049816131592
time_total_s: 9469.643685817719
timers:
  learn_throughput: 454.788
  learn_time_ms: 36280.601
  load_throughput: 4928641.75
  load_time_ms: 3.348
  training_iteration_time_ms: 47432.36
  update_time_ms: 2.704
timesteps_total: 3085500
training_iteration: 187

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2642857142857143
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 166.33663366336634
episode_reward_max: 2.0
episode_reward_mean: 1.3762376237623761
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 16142
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6039603960396039
  agent_1: 0.7722772277227723
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.25842475891113
time_total_s: 9471.450600624084
timers:
  learn_throughput: 425.608
  learn_time_ms: 38768.049
  load_throughput: 4860621.572
  load_time_ms: 3.395
  training_iteration_time_ms: 50616.996
  update_time_ms: 2.692
timesteps_total: 2953500
training_iteration: 179

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8417721518987342
  reward for individual goal_min: 0.0
episode_len_mean: 190.46
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 16985
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.81
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.34216547012329
time_total_s: 9500.205299139023
timers:
  learn_throughput: 500.847
  learn_time_ms: 32944.218
  load_throughput: 4722992.971
  load_time_ms: 3.494
  training_iteration_time_ms: 43864.061
  update_time_ms: 2.591
timesteps_total: 3415500
training_iteration: 207

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 155.11214953271028
episode_reward_max: 2.0
episode_reward_mean: 1.4485981308411215
episode_reward_min: 0.0
episodes_this_iter: 107
episodes_total: 19236
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7289719626168224
  agent_1: 0.719626168224299
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.192872047424316
time_total_s: 9477.350726127625
timers:
  learn_throughput: 517.68
  learn_time_ms: 31872.994
  load_throughput: 5140383.861
  load_time_ms: 3.21
  training_iteration_time_ms: 42166.038
  update_time_ms: 2.491
timesteps_total: 3514500
training_iteration: 213

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974489795918368
  reward for individual goal_min: 0.5
episode_len_mean: 42.681347150259064
episode_reward_max: 2.0
episode_reward_mean: 1.9974093264248705
episode_reward_min: 1.0
episodes_this_iter: 386
episodes_total: 41144
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974093264248705
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 59.58583903312683
time_total_s: 9435.782531261444
timers:
  learn_throughput: 365.218
  learn_time_ms: 45178.532
  load_throughput: 3530234.75
  load_time_ms: 4.674
  training_iteration_time_ms: 58247.466
  update_time_ms: 2.631
timesteps_total: 2788500
training_iteration: 169

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3141025641025641
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.21
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 15450
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.86820578575134
time_total_s: 9478.951362371445
timers:
  learn_throughput: 436.729
  learn_time_ms: 37780.853
  load_throughput: 4619894.259
  load_time_ms: 3.572
  training_iteration_time_ms: 49634.361
  update_time_ms: 2.654
timesteps_total: 2854500
training_iteration: 173

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9783950617283951
  reward for individual goal_min: 0.5
episode_len_mean: 49.28529411764706
episode_reward_max: 2.0
episode_reward_mean: 1.9794117647058824
episode_reward_min: 1.0
episodes_this_iter: 340
episodes_total: 37629
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9823529411764705
  agent_1: 0.9970588235294118
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.38190007209778
time_total_s: 9463.506229162216
timers:
  learn_throughput: 406.723
  learn_time_ms: 40568.188
  load_throughput: 4339316.053
  load_time_ms: 3.802
  training_iteration_time_ms: 52855.729
  update_time_ms: 2.734
timesteps_total: 3085500
training_iteration: 187

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 154.32110091743118
episode_reward_max: 2.0
episode_reward_mean: 1.3761467889908257
episode_reward_min: 0.0
episodes_this_iter: 109
episodes_total: 16707
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6880733944954128
  agent_1: 0.6880733944954128
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.94219207763672
time_total_s: 9480.168220043182
timers:
  learn_throughput: 456.879
  learn_time_ms: 36114.614
  load_throughput: 4838600.284
  load_time_ms: 3.41
  training_iteration_time_ms: 47576.38
  update_time_ms: 2.621
timesteps_total: 3069000
training_iteration: 186

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.74074074074074
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 378
episodes_total: 56451
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 49.13568687438965
time_total_s: 9400.911676168442
timers:
  learn_throughput: 455.554
  learn_time_ms: 36219.62
  load_throughput: 4198095.007
  load_time_ms: 3.93
  training_iteration_time_ms: 47721.399
  update_time_ms: 2.945
timesteps_total: 3333000
training_iteration: 202

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9025974025974026
  reward for individual goal_min: 0.0
episode_len_mean: 181.67
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 15093
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.23302721977234
time_total_s: 9508.72756242752
timers:
  learn_throughput: 442.361
  learn_time_ms: 37299.891
  load_throughput: 4637043.271
  load_time_ms: 3.558
  training_iteration_time_ms: 49040.895
  update_time_ms: 2.619
timesteps_total: 3019500
training_iteration: 183

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20833333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.65
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 15372
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.72758507728577
time_total_s: 9496.937109470367
timers:
  learn_throughput: 453.12
  learn_time_ms: 36414.212
  load_throughput: 4685484.79
  load_time_ms: 3.522
  training_iteration_time_ms: 47969.167
  update_time_ms: 2.588
timesteps_total: 2986500
training_iteration: 181

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29012345679012347
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 174.11
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 15508
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.96786379814148
time_total_s: 9505.462197303772
timers:
  learn_throughput: 424.302
  learn_time_ms: 38887.354
  load_throughput: 4366888.736
  load_time_ms: 3.778
  training_iteration_time_ms: 51224.578
  update_time_ms: 2.612
timesteps_total: 2854500
training_iteration: 173

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 107.33333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.8166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9333333333333333
  agent_1: 0.8833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25735294117647056
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.88125
  reward for individual goal_min: 0.0
episode_len_mean: 191.28
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 16281
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.43083310127258
time_total_s: 9509.288338184357
timers:
  learn_throughput: 485.188
  learn_time_ms: 34007.435
  load_throughput: 5100378.515
  load_time_ms: 3.235
  training_iteration_time_ms: 45245.109
  update_time_ms: 2.471
timesteps_total: 3300000
training_iteration: 200

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19istq_qwi/checkpoint_000200/checkpoint-200
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24390243902439024
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 194.56
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 16624
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.84667992591858
time_total_s: 9516.490365743637
timers:
  learn_throughput: 455.535
  learn_time_ms: 36221.127
  load_throughput: 4914257.635
  load_time_ms: 3.358
  training_iteration_time_ms: 47355.552
  update_time_ms: 2.709
timesteps_total: 3102000
training_iteration: 188

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8865546218487395
  reward for individual goal_min: 0.5
episode_len_mean: 70.65652173913044
episode_reward_max: 2.0
episode_reward_mean: 1.882608695652174
episode_reward_min: 1.0
episodes_this_iter: 230
episodes_total: 25796
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9956521739130435
  agent_1: 0.8869565217391304
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.28034138679504
time_total_s: 9524.10850930214
timers:
  learn_throughput: 334.932
  learn_time_ms: 49263.731
  load_throughput: 3269261.504
  load_time_ms: 5.047
  training_iteration_time_ms: 64318.01
  update_time_ms: 3.132
timesteps_total: 2607000
training_iteration: 158

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25316455696202533
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 184.56
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 19322
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.58803963661194
time_total_s: 9518.938765764236
timers:
  learn_throughput: 519.265
  learn_time_ms: 31775.681
  load_throughput: 5131084.552
  load_time_ms: 3.216
  training_iteration_time_ms: 42049.311
  update_time_ms: 2.492
timesteps_total: 3531000
training_iteration: 214

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2808219178082192
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 183.71
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 17074
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.96746802330017
time_total_s: 9544.172767162323
timers:
  learn_throughput: 501.103
  learn_time_ms: 32927.363
  load_throughput: 4673240.327
  load_time_ms: 3.531
  training_iteration_time_ms: 43851.073
  update_time_ms: 2.568
timesteps_total: 3432000
training_iteration: 208

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 71.13333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.9166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9666666666666667
  agent_1: 0.95
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3142857142857143
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9705882352941176
  reward for individual goal_min: 0.0
episode_len_mean: 168.44
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 16240
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.90810012817383
time_total_s: 9534.358700752258
timers:
  learn_throughput: 422.338
  learn_time_ms: 39068.193
  load_throughput: 4869753.578
  load_time_ms: 3.388
  training_iteration_time_ms: 50870.488
  update_time_ms: 2.708
timesteps_total: 2970000
training_iteration: 180

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19igysrqn6/checkpoint_000180/checkpoint-180
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22784810126582278
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.0
episode_len_mean: 183.72
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 15540
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.1849479675293
time_total_s: 9529.136310338974
timers:
  learn_throughput: 433.971
  learn_time_ms: 38020.973
  load_throughput: 4576845.029
  load_time_ms: 3.605
  training_iteration_time_ms: 49882.511
  update_time_ms: 2.65
timesteps_total: 2871000
training_iteration: 174

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2631578947368421
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.44
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 16805
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.992947816848755
time_total_s: 9527.161167860031
timers:
  learn_throughput: 457.357
  learn_time_ms: 36076.843
  load_throughput: 4831000.6
  load_time_ms: 3.415
  training_iteration_time_ms: 47495.632
  update_time_ms: 2.611
timesteps_total: 3085500
training_iteration: 187

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.986666666666665
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 375
episodes_total: 41519
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.90381097793579
time_total_s: 9495.68634223938
timers:
  learn_throughput: 363.984
  learn_time_ms: 45331.645
  load_throughput: 3532144.622
  load_time_ms: 4.671
  training_iteration_time_ms: 58448.456
  update_time_ms: 2.639
timesteps_total: 2805000
training_iteration: 170

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9855491329479769
  reward for individual goal_min: 0.5
episode_len_mean: 49.66767371601208
episode_reward_max: 2.0
episode_reward_mean: 1.9848942598187311
episode_reward_min: 1.0
episodes_this_iter: 331
episodes_total: 37960
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9879154078549849
  agent_1: 0.9969788519637462
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.53562927246094
time_total_s: 9516.041858434677
timers:
  learn_throughput: 405.515
  learn_time_ms: 40688.967
  load_throughput: 4286370.033
  load_time_ms: 3.849
  training_iteration_time_ms: 53054.78
  update_time_ms: 2.703
timesteps_total: 3102000
training_iteration: 188

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.93882978723404
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 376
episodes_total: 56827
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.403525590896606
time_total_s: 9449.315201759338
timers:
  learn_throughput: 455.004
  learn_time_ms: 36263.431
  load_throughput: 4202939.129
  load_time_ms: 3.926
  training_iteration_time_ms: 47757.065
  update_time_ms: 2.67
timesteps_total: 3349500
training_iteration: 203

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2708333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9671052631578947
  reward for individual goal_min: 0.0
episode_len_mean: 176.57
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 15187
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.77504539489746
time_total_s: 9556.502607822418
timers:
  learn_throughput: 444.988
  learn_time_ms: 37079.692
  load_throughput: 4688659.174
  load_time_ms: 3.519
  training_iteration_time_ms: 48769.009
  update_time_ms: 2.619
timesteps_total: 3036000
training_iteration: 184

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.55
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 15466
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.28425359725952
time_total_s: 9544.221363067627
timers:
  learn_throughput: 450.701
  learn_time_ms: 36609.673
  load_throughput: 4579722.329
  load_time_ms: 3.603
  training_iteration_time_ms: 48169.925
  update_time_ms: 2.585
timesteps_total: 3003000
training_iteration: 182

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23972602739726026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9090909090909091
  reward for individual goal_min: 0.0
episode_len_mean: 185.12
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 16371
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.24169564247131
time_total_s: 9556.530033826828
timers:
  learn_throughput: 482.818
  learn_time_ms: 34174.356
  load_throughput: 5144893.171
  load_time_ms: 3.207
  training_iteration_time_ms: 45428.473
  update_time_ms: 2.473
timesteps_total: 3316500
training_iteration: 201

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.95
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 15599
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.626662492752075
time_total_s: 9555.088859796524
timers:
  learn_throughput: 427.143
  learn_time_ms: 38628.756
  load_throughput: 4427711.482
  load_time_ms: 3.727
  training_iteration_time_ms: 50819.349
  update_time_ms: 2.647
timesteps_total: 2871000
training_iteration: 174

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2948717948717949
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 179.79
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 16718
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.12560319900513
time_total_s: 9565.615968942642
timers:
  learn_throughput: 455.482
  learn_time_ms: 36225.353
  load_throughput: 4962961.454
  load_time_ms: 3.325
  training_iteration_time_ms: 47454.93
  update_time_ms: 2.69
timesteps_total: 3118500
training_iteration: 189

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.92
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 19413
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.49035859107971
time_total_s: 9561.429124355316
timers:
  learn_throughput: 520.782
  learn_time_ms: 31683.14
  load_throughput: 5120985.038
  load_time_ms: 3.222
  training_iteration_time_ms: 41925.358
  update_time_ms: 2.497
timesteps_total: 3547500
training_iteration: 215

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8246753246753247
  reward for individual goal_min: 0.0
episode_len_mean: 201.05
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 17156
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.68726372718811
time_total_s: 9585.860030889511
timers:
  learn_throughput: 503.59
  learn_time_ms: 32764.755
  load_throughput: 4647350.233
  load_time_ms: 3.55
  training_iteration_time_ms: 43592.703
  update_time_ms: 2.589
timesteps_total: 3448500
training_iteration: 209

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2535211267605634
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.24
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 16333
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.12318706512451
time_total_s: 9586.481887817383
timers:
  learn_throughput: 420.146
  learn_time_ms: 39272.095
  load_throughput: 4854143.97
  load_time_ms: 3.399
  training_iteration_time_ms: 51163.482
  update_time_ms: 2.675
timesteps_total: 2986500
training_iteration: 181

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 200.93
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 15622
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.63176512718201
time_total_s: 9576.768075466156
timers:
  learn_throughput: 436.405
  learn_time_ms: 37808.91
  load_throughput: 4560438.08
  load_time_ms: 3.618
  training_iteration_time_ms: 49610.647
  update_time_ms: 2.691
timesteps_total: 2887500
training_iteration: 175

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8869565217391304
  reward for individual goal_min: 0.5
episode_len_mean: 72.63478260869566
episode_reward_max: 2.0
episode_reward_mean: 1.8869565217391304
episode_reward_min: 1.0
episodes_this_iter: 230
episodes_total: 26026
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8869565217391304
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 65.97463369369507
time_total_s: 9590.083142995834
timers:
  learn_throughput: 332.483
  learn_time_ms: 49626.554
  load_throughput: 3382767.763
  load_time_ms: 4.878
  training_iteration_time_ms: 64704.766
  update_time_ms: 3.26
timesteps_total: 2623500
training_iteration: 159

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32098765432098764
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 165.53465346534654
episode_reward_max: 2.0
episode_reward_mean: 1.3366336633663367
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 16906
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6237623762376238
  agent_1: 0.7128712871287128
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.84428882598877
time_total_s: 9574.00545668602
timers:
  learn_throughput: 459.525
  learn_time_ms: 35906.645
  load_throughput: 4855676.578
  load_time_ms: 3.398
  training_iteration_time_ms: 47304.946
  update_time_ms: 2.583
timesteps_total: 3102000
training_iteration: 188

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974489795918368
  reward for individual goal_min: 0.5
episode_len_mean: 43.82933333333333
episode_reward_max: 2.0
episode_reward_mean: 1.9973333333333334
episode_reward_min: 1.0
episodes_this_iter: 375
episodes_total: 57202
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973333333333333
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 47.70198607444763
time_total_s: 9497.017187833786
timers:
  learn_throughput: 455.398
  learn_time_ms: 36232.02
  load_throughput: 4184995.555
  load_time_ms: 3.943
  training_iteration_time_ms: 47721.874
  update_time_ms: 2.672
timesteps_total: 3366000
training_iteration: 204

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2565789473684211
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8857142857142857
  reward for individual goal_min: 0.0
episode_len_mean: 198.07
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 15270
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.045819997787476
time_total_s: 9604.548427820206
timers:
  learn_throughput: 447.794
  learn_time_ms: 36847.314
  load_throughput: 4703093.17
  load_time_ms: 3.508
  training_iteration_time_ms: 48521.496
  update_time_ms: 2.612
timesteps_total: 3052500
training_iteration: 185

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22560975609756098
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 207.41
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 16450
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.7767333984375
time_total_s: 9602.306767225266
timers:
  learn_throughput: 484.147
  learn_time_ms: 34080.559
  load_throughput: 5133177.769
  load_time_ms: 3.214
  training_iteration_time_ms: 45278.263
  update_time_ms: 2.494
timesteps_total: 3333000
training_iteration: 202

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9846625766871165
  reward for individual goal_min: 0.5
episode_len_mean: 49.442424242424245
episode_reward_max: 2.0
episode_reward_mean: 1.9848484848484849
episode_reward_min: 1.0
episodes_this_iter: 330
episodes_total: 38290
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9848484848484849
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 54.23003578186035
time_total_s: 9570.271894216537
timers:
  learn_throughput: 404.336
  learn_time_ms: 40807.673
  load_throughput: 4292724.465
  load_time_ms: 3.844
  training_iteration_time_ms: 53178.753
  update_time_ms: 2.706
timesteps_total: 3118500
training_iteration: 189

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28378378378378377
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.31
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 15563
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.29632639884949
time_total_s: 9591.517689466476
timers:
  learn_throughput: 452.598
  learn_time_ms: 36456.212
  load_throughput: 4579722.329
  load_time_ms: 3.603
  training_iteration_time_ms: 48016.008
  update_time_ms: 2.572
timesteps_total: 3019500
training_iteration: 183

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.47978436657682
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 371
episodes_total: 41890
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.51685881614685
time_total_s: 9554.203201055527
timers:
  learn_throughput: 362.798
  learn_time_ms: 45479.839
  load_throughput: 3536187.381
  load_time_ms: 4.666
  training_iteration_time_ms: 58612.626
  update_time_ms: 2.665
timesteps_total: 2821500
training_iteration: 171

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2671232876712329
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 159.25
episode_reward_max: 2.0
episode_reward_mean: 1.3365384615384615
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 15703
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6538461538461539
  agent_1: 0.6826923076923077
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.77957320213318
time_total_s: 9605.868432998657
timers:
  learn_throughput: 429.33
  learn_time_ms: 38431.938
  load_throughput: 4451921.879
  load_time_ms: 3.706
  training_iteration_time_ms: 50557.167
  update_time_ms: 2.635
timesteps_total: 2887500
training_iteration: 175

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19480519480519481
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 190.28
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 16802
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.120502948760986
time_total_s: 9612.736471891403
timers:
  learn_throughput: 454.891
  learn_time_ms: 36272.4
  load_throughput: 4905549.168
  load_time_ms: 3.364
  training_iteration_time_ms: 47527.865
  update_time_ms: 2.675
timesteps_total: 3135000
training_iteration: 190

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8481012658227848
  reward for individual goal_min: 0.0
episode_len_mean: 186.72
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 17242
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.76
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.40207862854004
time_total_s: 9626.262109518051
timers:
  learn_throughput: 505.538
  learn_time_ms: 32638.474
  load_throughput: 4634807.324
  load_time_ms: 3.56
  training_iteration_time_ms: 43430.187
  update_time_ms: 2.586
timesteps_total: 3465000
training_iteration: 210

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3987341772151899
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 144.94827586206895
episode_reward_max: 2.0
episode_reward_mean: 1.4827586206896552
episode_reward_min: 0.0
episodes_this_iter: 116
episodes_total: 19529
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7241379310344828
  agent_1: 0.7586206896551724
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.71755576133728
time_total_s: 9605.146680116653
timers:
  learn_throughput: 518.942
  learn_time_ms: 31795.487
  load_throughput: 5123942.427
  load_time_ms: 3.22
  training_iteration_time_ms: 42108.304
  update_time_ms: 2.475
timesteps_total: 3564000
training_iteration: 216

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28205128205128205
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 180.17
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 16428
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.16261911392212
time_total_s: 9634.644506931305
timers:
  learn_throughput: 421.472
  learn_time_ms: 39148.549
  load_throughput: 4788548.338
  load_time_ms: 3.446
  training_iteration_time_ms: 50954.018
  update_time_ms: 2.675
timesteps_total: 3003000
training_iteration: 182

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36486486486486486
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9807692307692307
  reward for individual goal_min: 0.0
episode_len_mean: 152.01923076923077
episode_reward_max: 2.0
episode_reward_mean: 1.4326923076923077
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 17010
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6923076923076923
  agent_1: 0.7403846153846154
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.392651319503784
time_total_s: 9619.398108005524
timers:
  learn_throughput: 462.067
  learn_time_ms: 35709.115
  load_throughput: 4862978.245
  load_time_ms: 3.393
  training_iteration_time_ms: 47058.945
  update_time_ms: 2.568
timesteps_total: 3118500
training_iteration: 189

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3150684931506849
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9788732394366197
  reward for individual goal_min: 0.0
episode_len_mean: 168.46
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 15722
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.01222825050354
time_total_s: 9627.78030371666
timers:
  learn_throughput: 434.439
  learn_time_ms: 37980.046
  load_throughput: 4592547.448
  load_time_ms: 3.593
  training_iteration_time_ms: 49809.109
  update_time_ms: 2.735
timesteps_total: 2904000
training_iteration: 176

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24285714285714285
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9695121951219512
  reward for individual goal_min: 0.5
episode_len_mean: 179.7
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 16543
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.052379846572876
time_total_s: 9645.359147071838
timers:
  learn_throughput: 486.745
  learn_time_ms: 33898.643
  load_throughput: 5061842.438
  load_time_ms: 3.26
  training_iteration_time_ms: 44999.885
  update_time_ms: 2.493
timesteps_total: 3349500
training_iteration: 203

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9943820224719101
  reward for individual goal_min: 0.5
episode_len_mean: 44.55135135135135
episode_reward_max: 2.0
episode_reward_mean: 1.9945945945945946
episode_reward_min: 1.0
episodes_this_iter: 370
episodes_total: 57572
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972972972972973
  agent_1: 0.9972972972972973
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.42819929122925
time_total_s: 9544.445387125015
timers:
  learn_throughput: 455.175
  learn_time_ms: 36249.828
  load_throughput: 4213686.96
  load_time_ms: 3.916
  training_iteration_time_ms: 47733.127
  update_time_ms: 2.673
timesteps_total: 3382500
training_iteration: 205

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9791666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 178.59
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 15656
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.44486594200134
time_total_s: 9638.962555408478
timers:
  learn_throughput: 454.496
  learn_time_ms: 36303.984
  load_throughput: 4628638.616
  load_time_ms: 3.565
  training_iteration_time_ms: 47833.175
  update_time_ms: 2.569
timesteps_total: 3036000
training_iteration: 184

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3904109589041096
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9420289855072463
  reward for individual goal_min: 0.0
episode_len_mean: 171.09
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 15364
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.20196890830994
time_total_s: 9654.750396728516
timers:
  learn_throughput: 447.49
  learn_time_ms: 36872.373
  load_throughput: 4743517.026
  load_time_ms: 3.478
  training_iteration_time_ms: 48546.892
  update_time_ms: 2.616
timesteps_total: 3069000
training_iteration: 186

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3048780487804878
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8611111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 198.27
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 17324
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.793933153152466
time_total_s: 9668.056042671204
timers:
  learn_throughput: 508.171
  learn_time_ms: 32469.401
  load_throughput: 4659867.084
  load_time_ms: 3.541
  training_iteration_time_ms: 43174.632
  update_time_ms: 2.589
timesteps_total: 3481500
training_iteration: 211

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3026315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.79
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 19625
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.43969511985779
time_total_s: 9645.586375236511
timers:
  learn_throughput: 521.365
  learn_time_ms: 31647.673
  load_throughput: 5110849.716
  load_time_ms: 3.228
  training_iteration_time_ms: 41957.002
  update_time_ms: 2.483
timesteps_total: 3580500
training_iteration: 217

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9821428571428571
  reward for individual goal_min: 0.5
episode_len_mean: 50.89473684210526
episode_reward_max: 2.0
episode_reward_mean: 1.9814241486068112
episode_reward_min: 1.0
episodes_this_iter: 323
episodes_total: 38613
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9876160990712074
  agent_1: 0.9938080495356038
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.801324129104614
time_total_s: 9624.073218345642
timers:
  learn_throughput: 402.018
  learn_time_ms: 41042.977
  load_throughput: 4292458.211
  load_time_ms: 3.844
  training_iteration_time_ms: 53462.63
  update_time_ms: 2.566
timesteps_total: 3135000
training_iteration: 190

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9
  reward for individual goal_min: 0.5
episode_len_mean: 68.21666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.9
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9833333333333333
  agent_1: 0.9166666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2876712328767123
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 172.89
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 16899
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.213350772857666
time_total_s: 9661.94982266426
timers:
  learn_throughput: 451.614
  learn_time_ms: 36535.635
  load_throughput: 4896040.07
  load_time_ms: 3.37
  training_iteration_time_ms: 47828.176
  update_time_ms: 2.693
timesteps_total: 3151500
training_iteration: 191

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2054794520547945
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.27
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 15795
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.50071573257446
time_total_s: 9656.369148731232
timers:
  learn_throughput: 428.755
  learn_time_ms: 38483.527
  load_throughput: 4445573.185
  load_time_ms: 3.712
  training_iteration_time_ms: 50666.598
  update_time_ms: 2.632
timesteps_total: 2904000
training_iteration: 176

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8605769230769231
  reward for individual goal_min: 0.5
episode_len_mean: 74.32727272727273
episode_reward_max: 2.0
episode_reward_mean: 1.8681818181818182
episode_reward_min: 1.0
episodes_this_iter: 220
episodes_total: 26246
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8681818181818182
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 74.2074248790741
time_total_s: 9664.290567874908
timers:
  learn_throughput: 332.446
  learn_time_ms: 49632.098
  load_throughput: 3237011.731
  load_time_ms: 5.097
  training_iteration_time_ms: 64708.903
  update_time_ms: 3.258
timesteps_total: 2640000
training_iteration: 160

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-19wecnkw4d/checkpoint_000160/checkpoint-160
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.24866310160428
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 374
episodes_total: 42264
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 60.0101420879364
time_total_s: 9614.213343143463
timers:
  learn_throughput: 361.949
  learn_time_ms: 45586.555
  load_throughput: 3516905.392
  load_time_ms: 4.692
  training_iteration_time_ms: 58771.401
  update_time_ms: 2.669
timesteps_total: 2838000
training_iteration: 172

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24050632911392406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 186.95
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 16517
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.03198218345642
time_total_s: 9683.676489114761
timers:
  learn_throughput: 420.625
  learn_time_ms: 39227.326
  load_throughput: 4796447.057
  load_time_ms: 3.44
  training_iteration_time_ms: 51014.842
  update_time_ms: 2.665
timesteps_total: 3019500
training_iteration: 183

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25903614457831325
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 193.54
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 17097
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.96558213233948
time_total_s: 9665.363690137863
timers:
  learn_throughput: 462.867
  learn_time_ms: 35647.384
  load_throughput: 4849585.929
  load_time_ms: 3.402
  training_iteration_time_ms: 46989.96
  update_time_ms: 2.553
timesteps_total: 3135000
training_iteration: 190

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24675324675324675
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9794520547945206
  reward for individual goal_min: 0.0
episode_len_mean: 180.73
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 15813
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.09442186355591
time_total_s: 9677.874725580215
timers:
  learn_throughput: 435.08
  learn_time_ms: 37924.059
  load_throughput: 4558425.504
  load_time_ms: 3.62
  training_iteration_time_ms: 49689.917
  update_time_ms: 2.754
timesteps_total: 2920500
training_iteration: 177

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18831168831168832
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9324324324324325
  reward for individual goal_min: 0.0
episode_len_mean: 193.24
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 16626
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.53116488456726
time_total_s: 9689.890311956406
timers:
  learn_throughput: 488.19
  learn_time_ms: 33798.326
  load_throughput: 5109076.386
  load_time_ms: 3.23
  training_iteration_time_ms: 44929.119
  update_time_ms: 2.521
timesteps_total: 3366000
training_iteration: 204

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.863013698630137
  reward for individual goal_min: 0.0
episode_len_mean: 198.57
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 17408
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.128828287124634
time_total_s: 9709.184870958328
timers:
  learn_throughput: 511.084
  learn_time_ms: 32284.291
  load_throughput: 4673619.039
  load_time_ms: 3.53
  training_iteration_time_ms: 42948.245
  update_time_ms: 2.6
timesteps_total: 3498000
training_iteration: 212

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15753424657534246
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.29
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 15749
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.236594438552856
time_total_s: 9684.19914984703
timers:
  learn_throughput: 459.835
  learn_time_ms: 35882.403
  load_throughput: 4641615.034
  load_time_ms: 3.555
  training_iteration_time_ms: 47377.064
  update_time_ms: 2.568
timesteps_total: 3052500
training_iteration: 185

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31756756756756754
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 159.93269230769232
episode_reward_max: 2.0
episode_reward_mean: 1.4038461538461537
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 19729
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7692307692307693
  agent_1: 0.6346153846153846
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.718605518341064
time_total_s: 9687.304980754852
timers:
  learn_throughput: 521.697
  learn_time_ms: 31627.56
  load_throughput: 5097898.846
  load_time_ms: 3.237
  training_iteration_time_ms: 41997.188
  update_time_ms: 2.45
timesteps_total: 3597000
training_iteration: 218

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.775193798449614
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 387
episodes_total: 57959
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.71404981613159
time_total_s: 9593.159436941147
timers:
  learn_throughput: 453.867
  learn_time_ms: 36354.238
  load_throughput: 4354578.895
  load_time_ms: 3.789
  training_iteration_time_ms: 47866.719
  update_time_ms: 2.699
timesteps_total: 3399000
training_iteration: 206

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3835616438356164
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9202898550724637
  reward for individual goal_min: 0.0
episode_len_mean: 179.03
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 15459
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.21501708030701
time_total_s: 9702.965413808823
timers:
  learn_throughput: 448.478
  learn_time_ms: 36791.111
  load_throughput: 4740267.953
  load_time_ms: 3.481
  training_iteration_time_ms: 48447.054
  update_time_ms: 2.627
timesteps_total: 3085500
training_iteration: 187

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1736111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9691358024691358
  reward for individual goal_min: 0.0
episode_len_mean: 180.36
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 16993
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.65158414840698
time_total_s: 9711.601406812668
timers:
  learn_throughput: 449.437
  learn_time_ms: 36712.633
  load_throughput: 4905201.471
  load_time_ms: 3.364
  training_iteration_time_ms: 48137.93
  update_time_ms: 2.687
timesteps_total: 3168000
training_iteration: 192

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935897435897436
  reward for individual goal_min: 0.5
episode_len_mean: 181.22
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 15884
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.2562518119812
time_total_s: 9706.625400543213
timers:
  learn_throughput: 431.904
  learn_time_ms: 38202.905
  load_throughput: 4415000.51
  load_time_ms: 3.737
  training_iteration_time_ms: 50351.118
  update_time_ms: 2.623
timesteps_total: 2920500
training_iteration: 177

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9854651162790697
  reward for individual goal_min: 0.5
episode_len_mean: 53.91318327974277
episode_reward_max: 2.0
episode_reward_mean: 1.9839228295819935
episode_reward_min: 1.0
episodes_this_iter: 311
episodes_total: 38924
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9871382636655949
  agent_1: 0.9967845659163987
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.711419343948364
time_total_s: 9676.78463768959
timers:
  learn_throughput: 401.846
  learn_time_ms: 41060.501
  load_throughput: 4309082.283
  load_time_ms: 3.829
  training_iteration_time_ms: 53521.002
  update_time_ms: 2.57
timesteps_total: 3151500
training_iteration: 191

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9067796610169492
  reward for individual goal_min: 0.5
episode_len_mean: 67.22357723577235
episode_reward_max: 2.0
episode_reward_mean: 1.910569105691057
episode_reward_min: 1.0
episodes_this_iter: 246
episodes_total: 26492
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9105691056910569
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 65.6525194644928
time_total_s: 9729.943087339401
timers:
  learn_throughput: 332.131
  learn_time_ms: 49679.243
  load_throughput: 3254425.822
  load_time_ms: 5.07
  training_iteration_time_ms: 64805.899
  update_time_ms: 3.309
timesteps_total: 2656500
training_iteration: 161

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.273972602739726
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.5
episode_len_mean: 172.1
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 17193
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.42179322242737
time_total_s: 9710.78548336029
timers:
  learn_throughput: 466.17
  learn_time_ms: 35394.815
  load_throughput: 4696359.01
  load_time_ms: 3.513
  training_iteration_time_ms: 46651.995
  update_time_ms: 2.53
timesteps_total: 3151500
training_iteration: 191

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.07068062827225
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 382
episodes_total: 42646
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 60.76404047012329
time_total_s: 9674.977383613586
timers:
  learn_throughput: 360.072
  learn_time_ms: 45824.159
  load_throughput: 3517084.123
  load_time_ms: 4.691
  training_iteration_time_ms: 59075.614
  update_time_ms: 2.672
timesteps_total: 2854500
training_iteration: 173

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28169014084507044
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9873417721518988
  reward for individual goal_min: 0.5
episode_len_mean: 168.26
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 16616
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.07973074913025
time_total_s: 9735.756219863892
timers:
  learn_throughput: 419.092
  learn_time_ms: 39370.82
  load_throughput: 4787190.26
  load_time_ms: 3.447
  training_iteration_time_ms: 51193.802
  update_time_ms: 2.686
timesteps_total: 3036000
training_iteration: 184

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1619718309859155
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9294871794871795
  reward for individual goal_min: 0.0
episode_len_mean: 186.83
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 16714
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.28888988494873
time_total_s: 9734.179201841354
timers:
  learn_throughput: 485.139
  learn_time_ms: 34010.885
  load_throughput: 5059141.19
  load_time_ms: 3.261
  training_iteration_time_ms: 45164.382
  update_time_ms: 2.516
timesteps_total: 3382500
training_iteration: 205

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2191780821917808
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 181.12
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 19822
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 39.90307569503784
time_total_s: 9727.20805644989
timers:
  learn_throughput: 524.807
  learn_time_ms: 31440.114
  load_throughput: 5099326.24
  load_time_ms: 3.236
  training_iteration_time_ms: 41737.115
  update_time_ms: 2.433
timesteps_total: 3613500
training_iteration: 219

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24025974025974026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 180.34
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 15901
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.16326093673706
time_total_s: 9729.037986516953
timers:
  learn_throughput: 432.866
  learn_time_ms: 38118.034
  load_throughput: 4555155.105
  load_time_ms: 3.622
  training_iteration_time_ms: 49936.4
  update_time_ms: 2.747
timesteps_total: 2937000
training_iteration: 178

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28205128205128205
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9076923076923077
  reward for individual goal_min: 0.0
episode_len_mean: 193.87
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 17489
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.22907376289368
time_total_s: 9751.413944721222
timers:
  learn_throughput: 515.71
  learn_time_ms: 31994.712
  load_throughput: 4707507.959
  load_time_ms: 3.505
  training_iteration_time_ms: 42663.831
  update_time_ms: 2.581
timesteps_total: 3514500
training_iteration: 213

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19078947368421054
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.87
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 15840
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.89011859893799
time_total_s: 9730.089268445969
timers:
  learn_throughput: 460.158
  learn_time_ms: 35857.251
  load_throughput: 4658047.963
  load_time_ms: 3.542
  training_iteration_time_ms: 47379.233
  update_time_ms: 2.534
timesteps_total: 3069000
training_iteration: 186

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.11794871794872
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 390
episodes_total: 58349
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.81461215019226
time_total_s: 9641.97404909134
timers:
  learn_throughput: 452.852
  learn_time_ms: 36435.782
  load_throughput: 4368652.968
  load_time_ms: 3.777
  training_iteration_time_ms: 47967.1
  update_time_ms: 2.717
timesteps_total: 3415500
training_iteration: 207

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3712121212121212
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8717948717948718
  reward for individual goal_min: 0.0
episode_len_mean: 165.2
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 15555
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.790547609329224
time_total_s: 9752.755961418152
timers:
  learn_throughput: 447.622
  learn_time_ms: 36861.494
  load_throughput: 4712765.308
  load_time_ms: 3.501
  training_iteration_time_ms: 48642.795
  update_time_ms: 2.589
timesteps_total: 3102000
training_iteration: 188

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.63
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 17077
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.45839858055115
time_total_s: 9759.059805393219
timers:
  learn_throughput: 448.999
  learn_time_ms: 36748.388
  load_throughput: 4936481.565
  load_time_ms: 3.342
  training_iteration_time_ms: 48173.909
  update_time_ms: 2.662
timesteps_total: 3184500
training_iteration: 193

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27564102564102566
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.01
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 15978
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.944739818573
time_total_s: 9759.570140361786
timers:
  learn_throughput: 428.391
  learn_time_ms: 38516.192
  load_throughput: 4468363.636
  load_time_ms: 3.693
  training_iteration_time_ms: 50792.376
  update_time_ms: 2.629
timesteps_total: 2937000
training_iteration: 178

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974226804123711
  reward for individual goal_min: 0.5
episode_len_mean: 45.90027700831025
episode_reward_max: 2.0
episode_reward_mean: 1.997229916897507
episode_reward_min: 1.0
episodes_this_iter: 361
episodes_total: 39285
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.997229916897507
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 53.856842041015625
time_total_s: 9730.641479730606
timers:
  learn_throughput: 401.97
  learn_time_ms: 41047.834
  load_throughput: 4318897.654
  load_time_ms: 3.82
  training_iteration_time_ms: 53471.477
  update_time_ms: 2.555
timesteps_total: 3168000
training_iteration: 192

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.84
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 17284
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.24230480194092
time_total_s: 9754.027788162231
timers:
  learn_throughput: 470.461
  learn_time_ms: 35071.975
  load_throughput: 4721253.07
  load_time_ms: 3.495
  training_iteration_time_ms: 46275.395
  update_time_ms: 2.538
timesteps_total: 3168000
training_iteration: 192

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23076923076923078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9214285714285714
  reward for individual goal_min: 0.0
episode_len_mean: 200.65
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 16797
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.65688753128052
time_total_s: 9775.836089372635
timers:
  learn_throughput: 487.844
  learn_time_ms: 33822.255
  load_throughput: 4990842.456
  load_time_ms: 3.306
  training_iteration_time_ms: 44879.007
  update_time_ms: 2.488
timesteps_total: 3399000
training_iteration: 206

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15151515151515152
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8452380952380952
  reward for individual goal_min: 0.0
episode_len_mean: 203.15
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 17572
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 39.57456731796265
time_total_s: 9790.988512039185
timers:
  learn_throughput: 522.424
  learn_time_ms: 31583.544
  load_throughput: 4717744.95
  load_time_ms: 3.497
  training_iteration_time_ms: 42156.839
  update_time_ms: 2.537
timesteps_total: 3531000
training_iteration: 214

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27702702702702703
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.99375
  reward for individual goal_min: 0.5
episode_len_mean: 162.5
episode_reward_max: 2.0
episode_reward_mean: 1.3725490196078431
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 16718
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6764705882352942
  agent_1: 0.696078431372549
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.45628023147583
time_total_s: 9784.212500095367
timers:
  learn_throughput: 423.164
  learn_time_ms: 38991.949
  load_throughput: 4750419.813
  load_time_ms: 3.473
  training_iteration_time_ms: 50854.647
  update_time_ms: 2.687
timesteps_total: 3052500
training_iteration: 185

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 81.83333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.8666666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9333333333333333
  agent_1: 0.9333333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 170.91
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 15937
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.53234386444092
time_total_s: 9775.62161231041
timers:
  learn_throughput: 463.975
  learn_time_ms: 35562.285
  load_throughput: 4688500.352
  load_time_ms: 3.519
  training_iteration_time_ms: 47051.481
  update_time_ms: 2.541
timesteps_total: 3085500
training_iteration: 187

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24324324324324326
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9726027397260274
  reward for individual goal_min: 0.0
episode_len_mean: 178.29
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 19915
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.35613489151001
time_total_s: 9778.5641913414
timers:
  learn_throughput: 524.802
  learn_time_ms: 31440.404
  load_throughput: 5144893.171
  load_time_ms: 3.207
  training_iteration_time_ms: 41830.79
  update_time_ms: 2.426
timesteps_total: 3630000
training_iteration: 220

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18518518518518517
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9929577464788732
  reward for individual goal_min: 0.5
episode_len_mean: 192.08
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 15991
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.014474868774414
time_total_s: 9780.052461385727
timers:
  learn_throughput: 434.845
  learn_time_ms: 37944.587
  load_throughput: 4513151.301
  load_time_ms: 3.656
  training_iteration_time_ms: 49776.583
  update_time_ms: 2.752
timesteps_total: 2953500
training_iteration: 179

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.2962962962963
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 378
episodes_total: 43024
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 60.231791496276855
time_total_s: 9735.209175109863
timers:
  learn_throughput: 358.257
  learn_time_ms: 46056.317
  load_throughput: 3489983.106
  load_time_ms: 4.728
  training_iteration_time_ms: 59389.952
  update_time_ms: 2.603
timesteps_total: 2871000
training_iteration: 174

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8983050847457628
  reward for individual goal_min: 0.5
episode_len_mean: 70.91774891774892
episode_reward_max: 2.0
episode_reward_mean: 1.896103896103896
episode_reward_min: 1.0
episodes_this_iter: 231
episodes_total: 26723
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.8961038961038961
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 65.68308711051941
time_total_s: 9795.62617444992
timers:
  learn_throughput: 330.286
  learn_time_ms: 49956.692
  load_throughput: 3186339.343
  load_time_ms: 5.178
  training_iteration_time_ms: 65219.095
  update_time_ms: 3.226
timesteps_total: 2673000
training_iteration: 162

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.61340206185567
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 388
episodes_total: 58737
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.707762002944946
time_total_s: 9690.681811094284
timers:
  learn_throughput: 451.835
  learn_time_ms: 36517.752
  load_throughput: 4336189.372
  load_time_ms: 3.805
  training_iteration_time_ms: 48064.779
  update_time_ms: 2.385
timesteps_total: 3432000
training_iteration: 208

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28169014084507044
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8719512195121951
  reward for individual goal_min: 0.0
episode_len_mean: 187.11
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 15641
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.311208724975586
time_total_s: 9802.067170143127
timers:
  learn_throughput: 447.256
  learn_time_ms: 36891.656
  load_throughput: 4703700.512
  load_time_ms: 3.508
  training_iteration_time_ms: 48603.57
  update_time_ms: 2.595
timesteps_total: 3118500
training_iteration: 189

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25675675675675674
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.89
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 17174
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.71134877204895
time_total_s: 9807.771154165268
timers:
  learn_throughput: 449.403
  learn_time_ms: 36715.385
  load_throughput: 4864619.018
  load_time_ms: 3.392
  training_iteration_time_ms: 48109.706
  update_time_ms: 2.645
timesteps_total: 3201000
training_iteration: 194

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2328767123287671
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.48
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 16075
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.3743360042572
time_total_s: 9810.944476366043
timers:
  learn_throughput: 427.821
  learn_time_ms: 38567.537
  load_throughput: 4475848.429
  load_time_ms: 3.686
  training_iteration_time_ms: 50785.898
  update_time_ms: 2.671
timesteps_total: 2953500
training_iteration: 179

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26811594202898553
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 159.55882352941177
episode_reward_max: 2.0
episode_reward_mean: 1.3725490196078431
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 17386
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6666666666666666
  agent_1: 0.7058823529411765
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.46590614318848
time_total_s: 9800.49369430542
timers:
  learn_throughput: 472.536
  learn_time_ms: 34918.0
  load_throughput: 4719128.265
  load_time_ms: 3.496
  training_iteration_time_ms: 46032.281
  update_time_ms: 2.537
timesteps_total: 3184500
training_iteration: 193

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.88
  reward for individual goal_min: 0.0
episode_len_mean: 194.88
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 17655
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.06818413734436
time_total_s: 9834.056696176529
timers:
  learn_throughput: 523.489
  learn_time_ms: 31519.259
  load_throughput: 4745273.378
  load_time_ms: 3.477
  training_iteration_time_ms: 42141.454
  update_time_ms: 2.551
timesteps_total: 3547500
training_iteration: 215

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3181818181818182
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8958333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 190.42
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 16885
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.96077561378479
time_total_s: 9819.79686498642
timers:
  learn_throughput: 489.105
  learn_time_ms: 33735.111
  load_throughput: 4986096.053
  load_time_ms: 3.309
  training_iteration_time_ms: 44746.016
  update_time_ms: 2.496
timesteps_total: 3415500
training_iteration: 207

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9869281045751634
  reward for individual goal_min: 0.5
episode_len_mean: 47.00289017341041
episode_reward_max: 2.0
episode_reward_mean: 1.9884393063583814
episode_reward_min: 1.0
episodes_this_iter: 346
episodes_total: 39631
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9942196531791907
  agent_1: 0.9942196531791907
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.473105669021606
time_total_s: 9786.114585399628
timers:
  learn_throughput: 399.318
  learn_time_ms: 41320.463
  load_throughput: 4296482.157
  load_time_ms: 3.84
  training_iteration_time_ms: 53757.608
  update_time_ms: 2.548
timesteps_total: 3184500
training_iteration: 193

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2635135135135135
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 167.87
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 16817
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.14994812011719
time_total_s: 9831.362448215485
timers:
  learn_throughput: 427.131
  learn_time_ms: 38629.842
  load_throughput: 4795150.944
  load_time_ms: 3.441
  training_iteration_time_ms: 50479.137
  update_time_ms: 2.692
timesteps_total: 3069000
training_iteration: 186

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2971014492753623
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.5
episode_len_mean: 159.1359223300971
episode_reward_max: 2.0
episode_reward_mean: 1.4368932038834952
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 20018
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7475728155339806
  agent_1: 0.6893203883495146
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.488471031188965
time_total_s: 9819.05266237259
timers:
  learn_throughput: 527.632
  learn_time_ms: 31271.817
  load_throughput: 5179935.93
  load_time_ms: 3.185
  training_iteration_time_ms: 41581.106
  update_time_ms: 2.448
timesteps_total: 3646500
training_iteration: 221

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2808219178082192
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 171.52
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 16035
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.00665068626404
time_total_s: 9821.628262996674
timers:
  learn_throughput: 466.68
  learn_time_ms: 35356.147
  load_throughput: 4755969.597
  load_time_ms: 3.469
  training_iteration_time_ms: 46735.61
  update_time_ms: 2.546
timesteps_total: 3102000
training_iteration: 188

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.40157480314961
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 381
episodes_total: 59118
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.09134793281555
time_total_s: 9738.7731590271
timers:
  learn_throughput: 451.26
  learn_time_ms: 36564.263
  load_throughput: 4342311.013
  load_time_ms: 3.8
  training_iteration_time_ms: 48098.411
  update_time_ms: 2.4
timesteps_total: 3448500
training_iteration: 209

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.93125
  reward for individual goal_min: 0.0
episode_len_mean: 162.52941176470588
episode_reward_max: 2.0
episode_reward_mean: 1.392156862745098
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 15743
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7156862745098039
  agent_1: 0.6764705882352942
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.44052982330322
time_total_s: 9850.50769996643
timers:
  learn_throughput: 447.43
  learn_time_ms: 36877.308
  load_throughput: 4746933.35
  load_time_ms: 3.476
  training_iteration_time_ms: 48619.405
  update_time_ms: 2.606
timesteps_total: 3135000
training_iteration: 190

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 85.38333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.8333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9166666666666666
  agent_1: 0.9166666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9928571428571429
  reward for individual goal_min: 0.5
episode_len_mean: 169.61
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 16087
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.05956292152405
time_total_s: 9840.112024307251
timers:
  learn_throughput: 434.623
  learn_time_ms: 37963.929
  load_throughput: 4486032.022
  load_time_ms: 3.678
  training_iteration_time_ms: 49724.046
  update_time_ms: 2.784
timesteps_total: 2970000
training_iteration: 180

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.30133333333333
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 375
episodes_total: 43399
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.72558283805847
time_total_s: 9794.934757947922
timers:
  learn_throughput: 357.589
  learn_time_ms: 46142.386
  load_throughput: 3464631.589
  load_time_ms: 4.762
  training_iteration_time_ms: 59492.554
  update_time_ms: 2.617
timesteps_total: 2887500
training_iteration: 175

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23076923076923078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 188.09
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 17260
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.384769439697266
time_total_s: 9857.155923604965
timers:
  learn_throughput: 446.49
  learn_time_ms: 36954.941
  load_throughput: 4873800.389
  load_time_ms: 3.385
  training_iteration_time_ms: 48392.363
  update_time_ms: 2.64
timesteps_total: 3217500
training_iteration: 195

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8902439024390244
  reward for individual goal_min: 0.5
episode_len_mean: 70.95278969957081
episode_reward_max: 2.0
episode_reward_mean: 1.8841201716738198
episode_reward_min: 1.0
episodes_this_iter: 233
episodes_total: 26956
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9957081545064378
  agent_1: 0.8884120171673819
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.63698434829712
time_total_s: 9859.263158798218
timers:
  learn_throughput: 331.551
  learn_time_ms: 49766.068
  load_throughput: 3403998.662
  load_time_ms: 4.847
  training_iteration_time_ms: 64931.882
  update_time_ms: 3.252
timesteps_total: 2689500
training_iteration: 163

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3670886075949367
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 163.03960396039605
episode_reward_max: 2.0
episode_reward_mean: 1.396039603960396
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 17487
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6534653465346535
  agent_1: 0.7425742574257426
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.948570013046265
time_total_s: 9845.442264318466
timers:
  learn_throughput: 471.856
  learn_time_ms: 34968.262
  load_throughput: 4763498.802
  load_time_ms: 3.464
  training_iteration_time_ms: 46072.267
  update_time_ms: 2.808
timesteps_total: 3201000
training_iteration: 194

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.345679012345679
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8125
  reward for individual goal_min: 0.0
episode_len_mean: 193.5
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 17736
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.3858699798584
time_total_s: 9877.442566156387
timers:
  learn_throughput: 523.651
  learn_time_ms: 31509.563
  load_throughput: 4753617.518
  load_time_ms: 3.471
  training_iteration_time_ms: 42121.483
  update_time_ms: 2.518
timesteps_total: 3564000
training_iteration: 216

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2635135135135135
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9178082191780822
  reward for individual goal_min: 0.0
episode_len_mean: 182.76
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 16977
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.68472599983215
time_total_s: 9864.481590986252
timers:
  learn_throughput: 491.837
  learn_time_ms: 33547.708
  load_throughput: 4976845.012
  load_time_ms: 3.315
  training_iteration_time_ms: 44530.469
  update_time_ms: 2.502
timesteps_total: 3432000
training_iteration: 208

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32142857142857145
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9857142857142858
  reward for individual goal_min: 0.0
episode_len_mean: 181.59
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 20106
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.21368741989136
time_total_s: 9859.26634979248
timers:
  learn_throughput: 528.826
  learn_time_ms: 31201.184
  load_throughput: 5228106.638
  load_time_ms: 3.156
  training_iteration_time_ms: 41516.151
  update_time_ms: 2.462
timesteps_total: 3663000
training_iteration: 222

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 80.38333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.9
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9666666666666667
  agent_1: 0.9333333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18493150684931506
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.18
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 16169
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.09145951271057
time_total_s: 9869.035935878754
timers:
  learn_throughput: 429.581
  learn_time_ms: 38409.556
  load_throughput: 4481732.441
  load_time_ms: 3.682
  training_iteration_time_ms: 50595.135
  update_time_ms: 2.694
timesteps_total: 2970000
training_iteration: 180

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-29ws591pkw/checkpoint_000180/checkpoint-180
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9913793103448276
  reward for individual goal_min: 0.5
episode_len_mean: 46.96348314606742
episode_reward_max: 2.0
episode_reward_mean: 1.9915730337078652
episode_reward_min: 1.0
episodes_this_iter: 356
episodes_total: 39987
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9915730337078652
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 55.93739891052246
time_total_s: 9842.05198431015
timers:
  learn_throughput: 397.852
  learn_time_ms: 41472.735
  load_throughput: 4313567.609
  load_time_ms: 3.825
  training_iteration_time_ms: 53919.822
  update_time_ms: 2.536
timesteps_total: 3201000
training_iteration: 194

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23076923076923078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 157.20388349514562
episode_reward_max: 2.0
episode_reward_mean: 1.4077669902912622
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 16920
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7378640776699029
  agent_1: 0.6699029126213593
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.52731370925903
time_total_s: 9882.889761924744
timers:
  learn_throughput: 429.085
  learn_time_ms: 38453.916
  load_throughput: 4786627.381
  load_time_ms: 3.447
  training_iteration_time_ms: 50345.539
  update_time_ms: 2.657
timesteps_total: 3085500
training_iteration: 187

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1891891891891892
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.78
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 16126
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.54179549217224
time_total_s: 9869.170058488846
timers:
  learn_throughput: 466.38
  learn_time_ms: 35378.849
  load_throughput: 4747584.636
  load_time_ms: 3.475
  training_iteration_time_ms: 46719.919
  update_time_ms: 2.557
timesteps_total: 3118500
training_iteration: 189

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.096692111959285
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 393
episodes_total: 59511
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.164222717285156
time_total_s: 9786.937381744385
timers:
  learn_throughput: 450.522
  learn_time_ms: 36624.172
  load_throughput: 4309618.956
  load_time_ms: 3.829
  training_iteration_time_ms: 48197.182
  update_time_ms: 2.406
timesteps_total: 3465000
training_iteration: 210

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21794871794871795
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.5
episode_len_mean: 187.77
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 16175
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.1781587600708
time_total_s: 9889.290183067322
timers:
  learn_throughput: 434.704
  learn_time_ms: 37956.834
  load_throughput: 4501731.325
  load_time_ms: 3.665
  training_iteration_time_ms: 49793.768
  update_time_ms: 2.8
timesteps_total: 2986500
training_iteration: 181

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30434782608695654
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8986486486486487
  reward for individual goal_min: 0.0
episode_len_mean: 177.47
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 15835
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.731162309646606
time_total_s: 9902.238862276077
timers:
  learn_throughput: 442.101
  learn_time_ms: 37321.788
  load_throughput: 4811185.451
  load_time_ms: 3.43
  training_iteration_time_ms: 49195.199
  update_time_ms: 2.618
timesteps_total: 3151500
training_iteration: 191

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2808219178082192
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 168.43
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 17359
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.160902976989746
time_total_s: 9905.316826581955
timers:
  learn_throughput: 445.989
  learn_time_ms: 36996.389
  load_throughput: 4843747.839
  load_time_ms: 3.406
  training_iteration_time_ms: 48468.522
  update_time_ms: 2.618
timesteps_total: 3234000
training_iteration: 196

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2753623188405797
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9133333333333333
  reward for individual goal_min: 0.0
episode_len_mean: 182.24
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 17067
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.6872341632843
time_total_s: 9907.168825149536
timers:
  learn_throughput: 495.446
  learn_time_ms: 33303.318
  load_throughput: 4972089.461
  load_time_ms: 3.319
  training_iteration_time_ms: 44207.199
  update_time_ms: 2.51
timesteps_total: 3448500
training_iteration: 209

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2916666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8174603174603174
  reward for individual goal_min: 0.0
episode_len_mean: 200.38
episode_reward_max: 2.0
episode_reward_mean: 1.07
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 17822
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.4
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.269421339035034
time_total_s: 9921.711987495422
timers:
  learn_throughput: 524.864
  learn_time_ms: 31436.733
  load_throughput: 4760942.75
  load_time_ms: 3.466
  training_iteration_time_ms: 42114.287
  update_time_ms: 2.509
timesteps_total: 3580500
training_iteration: 217

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.07
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 17577
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.883283615112305
time_total_s: 9892.325547933578
timers:
  learn_throughput: 472.17
  learn_time_ms: 34945.021
  load_throughput: 4751235.145
  load_time_ms: 3.473
  training_iteration_time_ms: 46060.955
  update_time_ms: 2.796
timesteps_total: 3217500
training_iteration: 195

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973684210526316
  reward for individual goal_min: 0.5
episode_len_mean: 44.10752688172043
episode_reward_max: 2.0
episode_reward_mean: 1.9973118279569892
episode_reward_min: 1.0
episodes_this_iter: 372
episodes_total: 43771
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973118279569892
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 59.26143550872803
time_total_s: 9854.19619345665
timers:
  learn_throughput: 357.717
  learn_time_ms: 46125.82
  load_throughput: 3451585.546
  load_time_ms: 4.78
  training_iteration_time_ms: 59484.121
  update_time_ms: 2.627
timesteps_total: 2904000
training_iteration: 176

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2328767123287671
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.63
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 20201
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.697733640670776
time_total_s: 9900.964083433151
timers:
  learn_throughput: 532.237
  learn_time_ms: 31001.247
  load_throughput: 5191788.085
  load_time_ms: 3.178
  training_iteration_time_ms: 41367.134
  update_time_ms: 2.471
timesteps_total: 3679500
training_iteration: 223

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9094827586206896
  reward for individual goal_min: 0.5
episode_len_mean: 70.41379310344827
episode_reward_max: 2.0
episode_reward_mean: 1.9094827586206897
episode_reward_min: 1.0
episodes_this_iter: 232
episodes_total: 27188
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9913793103448276
  agent_1: 0.9181034482758621
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.56129193305969
time_total_s: 9924.824450731277
timers:
  learn_throughput: 331.525
  learn_time_ms: 49770.001
  load_throughput: 3343156.593
  load_time_ms: 4.935
  training_iteration_time_ms: 64935.166
  update_time_ms: 3.217
timesteps_total: 2706000
training_iteration: 164

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2357142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 163.36633663366337
episode_reward_max: 2.0
episode_reward_mean: 1.3564356435643565
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 16270
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6237623762376238
  agent_1: 0.7326732673267327
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.19149398803711
time_total_s: 9920.22742986679
timers:
  learn_throughput: 428.427
  learn_time_ms: 38513.005
  load_throughput: 4494276.53
  load_time_ms: 3.671
  training_iteration_time_ms: 50689.488
  update_time_ms: 2.682
timesteps_total: 2986500
training_iteration: 181

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 192.11
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 16211
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.94172716140747
time_total_s: 9916.111785650253
timers:
  learn_throughput: 469.066
  learn_time_ms: 35176.294
  load_throughput: 4767666.458
  load_time_ms: 3.461
  training_iteration_time_ms: 46553.738
  update_time_ms: 2.54
timesteps_total: 3135000
training_iteration: 190

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 184.59
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 17010
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.52415323257446
time_total_s: 9933.413915157318
timers:
  learn_throughput: 431.489
  learn_time_ms: 38239.684
  load_throughput: 4755283.334
  load_time_ms: 3.47
  training_iteration_time_ms: 50256.449
  update_time_ms: 2.649
timesteps_total: 3102000
training_iteration: 188

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9912280701754386
  reward for individual goal_min: 0.5
episode_len_mean: 47.781976744186046
episode_reward_max: 2.0
episode_reward_mean: 1.9912790697674418
episode_reward_min: 1.0
episodes_this_iter: 344
episodes_total: 40331
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9912790697674418
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 55.22187376022339
time_total_s: 9897.273858070374
timers:
  learn_throughput: 395.427
  learn_time_ms: 41727.034
  load_throughput: 4314670.225
  load_time_ms: 3.824
  training_iteration_time_ms: 54192.481
  update_time_ms: 2.548
timesteps_total: 3217500
training_iteration: 195

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973262032085561
  reward for individual goal_min: 0.5
episode_len_mean: 44.390243902439025
episode_reward_max: 2.0
episode_reward_mean: 1.997289972899729
episode_reward_min: 1.0
episodes_this_iter: 369
episodes_total: 59880
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.997289972899729
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 48.000378131866455
time_total_s: 9834.937759876251
timers:
  learn_throughput: 450.518
  learn_time_ms: 36624.529
  load_throughput: 4288202.643
  load_time_ms: 3.848
  training_iteration_time_ms: 48197.384
  update_time_ms: 2.423
timesteps_total: 3481500
training_iteration: 211

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2876712328767123
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.918918918918919
  reward for individual goal_min: 0.0
episode_len_mean: 188.14
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 15923
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.32060241699219
time_total_s: 9950.55946469307
timers:
  learn_throughput: 443.862
  learn_time_ms: 37173.752
  load_throughput: 4727316.047
  load_time_ms: 3.49
  training_iteration_time_ms: 49057.119
  update_time_ms: 2.614
timesteps_total: 3168000
training_iteration: 192

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2077922077922078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9802631578947368
  reward for individual goal_min: 0.0
episode_len_mean: 183.74
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 16266
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.91956043243408
time_total_s: 9939.209743499756
timers:
  learn_throughput: 433.153
  learn_time_ms: 38092.744
  load_throughput: 4496992.475
  load_time_ms: 3.669
  training_iteration_time_ms: 50010.567
  update_time_ms: 2.794
timesteps_total: 3003000
training_iteration: 182

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22435897435897437
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.881578947368421
  reward for individual goal_min: 0.0
episode_len_mean: 199.14
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 17905
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.798463582992554
time_total_s: 9963.510451078415
timers:
  learn_throughput: 527.462
  learn_time_ms: 31281.847
  load_throughput: 4826485.898
  load_time_ms: 3.419
  training_iteration_time_ms: 41897.594
  update_time_ms: 2.497
timesteps_total: 3597000
training_iteration: 218

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28378378378378377
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.99
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 20301
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.26925849914551
time_total_s: 9942.233341932297
timers:
  learn_throughput: 533.104
  learn_time_ms: 30950.795
  load_throughput: 5177881.892
  load_time_ms: 3.187
  training_iteration_time_ms: 41335.564
  update_time_ms: 2.469
timesteps_total: 3696000
training_iteration: 224

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28846153846153844
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9142857142857143
  reward for individual goal_min: 0.0
episode_len_mean: 187.23
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 17157
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.688674449920654
time_total_s: 9950.857499599457
timers:
  learn_throughput: 496.891
  learn_time_ms: 33206.451
  load_throughput: 4931205.404
  load_time_ms: 3.346
  training_iteration_time_ms: 44121.588
  update_time_ms: 2.496
timesteps_total: 3465000
training_iteration: 210

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28205128205128205
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9929577464788732
  reward for individual goal_min: 0.5
episode_len_mean: 182.25
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 17448
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.358025789260864
time_total_s: 9954.674852371216
timers:
  learn_throughput: 446.958
  learn_time_ms: 36916.23
  load_throughput: 4860450.887
  load_time_ms: 3.395
  training_iteration_time_ms: 48465.792
  update_time_ms: 2.625
timesteps_total: 3250500
training_iteration: 197

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18181818181818182
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 184.78
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 17669
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.90643882751465
time_total_s: 9939.231986761093
timers:
  learn_throughput: 474.578
  learn_time_ms: 34767.71
  load_throughput: 4695211.979
  load_time_ms: 3.514
  training_iteration_time_ms: 45857.606
  update_time_ms: 2.795
timesteps_total: 3234000
training_iteration: 196

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9940119760479041
  reward for individual goal_min: 0.5
episode_len_mean: 43.7467018469657
episode_reward_max: 2.0
episode_reward_mean: 1.9947229551451187
episode_reward_min: 1.0
episodes_this_iter: 379
episodes_total: 44150
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973614775725593
  agent_1: 0.9973614775725593
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.53034329414368
time_total_s: 9914.726536750793
timers:
  learn_throughput: 356.557
  learn_time_ms: 46275.964
  load_throughput: 3438876.599
  load_time_ms: 4.798
  training_iteration_time_ms: 59683.188
  update_time_ms: 2.64
timesteps_total: 2920500
training_iteration: 177

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2808219178082192
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9938271604938271
  reward for individual goal_min: 0.5
episode_len_mean: 163.375
episode_reward_max: 2.0
episode_reward_mean: 1.375
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 16315
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6730769230769231
  agent_1: 0.7019230769230769
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.207488775253296
time_total_s: 9963.319274425507
timers:
  learn_throughput: 469.31
  learn_time_ms: 35158.024
  load_throughput: 4823256.67
  load_time_ms: 3.421
  training_iteration_time_ms: 46601.511
  update_time_ms: 2.56
timesteps_total: 3151500
training_iteration: 191

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1987179487179487
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 184.11
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 16357
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.508397340774536
time_total_s: 9972.735827207565
timers:
  learn_throughput: 427.333
  learn_time_ms: 38611.616
  load_throughput: 4516685.876
  load_time_ms: 3.653
  training_iteration_time_ms: 50850.973
  update_time_ms: 2.673
timesteps_total: 3003000
training_iteration: 182

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2621951219512195
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 188.89
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 17099
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.079896450042725
time_total_s: 9980.49381160736
timers:
  learn_throughput: 434.235
  learn_time_ms: 37997.865
  load_throughput: 4743582.053
  load_time_ms: 3.478
  training_iteration_time_ms: 49938.627
  update_time_ms: 2.653
timesteps_total: 3118500
training_iteration: 189

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9475524475524476
  reward for individual goal_min: 0.5
episode_len_mean: 60.193548387096776
episode_reward_max: 2.0
episode_reward_mean: 1.946236559139785
episode_reward_min: 1.0
episodes_this_iter: 279
episodes_total: 27467
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.946236559139785
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 67.6052131652832
time_total_s: 9992.42966389656
timers:
  learn_throughput: 329.562
  learn_time_ms: 50066.441
  load_throughput: 3437339.373
  load_time_ms: 4.8
  training_iteration_time_ms: 65251.142
  update_time_ms: 3.224
timesteps_total: 2722500
training_iteration: 165

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9943181818181818
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 47.7514450867052
episode_reward_max: 2.0
episode_reward_mean: 1.9942196531791907
episode_reward_min: 0.0
episodes_this_iter: 346
episodes_total: 40677
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9971098265895953
  agent_1: 0.9971098265895953
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.94250988960266
time_total_s: 9952.216367959976
timers:
  learn_throughput: 395.145
  learn_time_ms: 41756.859
  load_throughput: 4313325.646
  load_time_ms: 3.825
  training_iteration_time_ms: 54203.359
  update_time_ms: 2.565
timesteps_total: 3234000
training_iteration: 196

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.70360824742268
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 388
episodes_total: 60268
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.84148144721985
time_total_s: 9882.779241323471
timers:
  learn_throughput: 452.04
  learn_time_ms: 36501.224
  load_throughput: 4330464.296
  load_time_ms: 3.81
  training_iteration_time_ms: 48067.52
  update_time_ms: 2.412
timesteps_total: 3498000
training_iteration: 212

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3493150684931507
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 164.9
episode_reward_max: 2.0
episode_reward_mean: 1.41
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 20399
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.76
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.52262258529663
time_total_s: 9983.755964517593
timers:
  learn_throughput: 535.766
  learn_time_ms: 30797.011
  load_throughput: 5198769.231
  load_time_ms: 3.174
  training_iteration_time_ms: 41238.763
  update_time_ms: 2.462
timesteps_total: 3712500
training_iteration: 225

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3072289156626506
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8014705882352942
  reward for individual goal_min: 0.0
episode_len_mean: 205.24
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 17985
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.297738552093506
time_total_s: 10006.808189630508
timers:
  learn_throughput: 526.904
  learn_time_ms: 31315.008
  load_throughput: 4849348.058
  load_time_ms: 3.403
  training_iteration_time_ms: 42058.581
  update_time_ms: 2.472
timesteps_total: 3613500
training_iteration: 219

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2826086956521739
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8904109589041096
  reward for individual goal_min: 0.0
episode_len_mean: 178.53
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 17245
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.41999864578247
time_total_s: 9993.27749824524
timers:
  learn_throughput: 503.023
  learn_time_ms: 32801.678
  load_throughput: 4898188.536
  load_time_ms: 3.369
  training_iteration_time_ms: 43639.316
  update_time_ms: 2.507
timesteps_total: 3481500
training_iteration: 211

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3475609756097561
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95
  reward for individual goal_min: 0.0
episode_len_mean: 177.3
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 16017
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.009729862213135
time_total_s: 10000.569194555283
timers:
  learn_throughput: 442.453
  learn_time_ms: 37292.086
  load_throughput: 4739910.826
  load_time_ms: 3.481
  training_iteration_time_ms: 49134.633
  update_time_ms: 2.628
timesteps_total: 3184500
training_iteration: 193

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2785714285714286
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9647887323943662
  reward for individual goal_min: 0.0
episode_len_mean: 169.31
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 16364
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.943849086761475
time_total_s: 9990.153592586517
timers:
  learn_throughput: 433.051
  learn_time_ms: 38101.73
  load_throughput: 4572037.419
  load_time_ms: 3.609
  training_iteration_time_ms: 50017.969
  update_time_ms: 2.805
timesteps_total: 3019500
training_iteration: 183

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24691358024691357
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9927536231884058
  reward for individual goal_min: 0.5
episode_len_mean: 182.46
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 17757
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.34528374671936
time_total_s: 9983.577270507812
timers:
  learn_throughput: 477.153
  learn_time_ms: 34580.113
  load_throughput: 4697474.716
  load_time_ms: 3.513
  training_iteration_time_ms: 45593.018
  update_time_ms: 2.814
timesteps_total: 3250500
training_iteration: 197

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.36619718309859156
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 156.28971962616822
episode_reward_max: 2.0
episode_reward_mean: 1.4672897196261683
episode_reward_min: 0.0
episodes_this_iter: 107
episodes_total: 17555
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7476635514018691
  agent_1: 0.719626168224299
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.716951847076416
time_total_s: 10005.391804218292
timers:
  learn_throughput: 443.352
  learn_time_ms: 37216.442
  load_throughput: 4811051.666
  load_time_ms: 3.43
  training_iteration_time_ms: 48852.329
  update_time_ms: 2.645
timesteps_total: 3267000
training_iteration: 198

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1917808219178082
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9938271604938271
  reward for individual goal_min: 0.5
episode_len_mean: 175.75
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 16406
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.2390239238739
time_total_s: 10009.55829834938
timers:
  learn_throughput: 470.703
  learn_time_ms: 35053.934
  load_throughput: 4955427.655
  load_time_ms: 3.33
  training_iteration_time_ms: 46496.69
  update_time_ms: 2.56
timesteps_total: 3168000
training_iteration: 192

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20253164556962025
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 195.09
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 17184
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.4145028591156
time_total_s: 10027.908314466476
timers:
  learn_throughput: 440.563
  learn_time_ms: 37452.116
  load_throughput: 4724508.373
  load_time_ms: 3.492
  training_iteration_time_ms: 49315.115
  update_time_ms: 2.649
timesteps_total: 3135000
training_iteration: 190

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17058823529411765
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 200.88
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 16442
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.2373411655426
time_total_s: 10023.973168373108
timers:
  learn_throughput: 427.853
  learn_time_ms: 38564.666
  load_throughput: 4515418.684
  load_time_ms: 3.654
  training_iteration_time_ms: 50777.858
  update_time_ms: 2.675
timesteps_total: 3019500
training_iteration: 183

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24358974358974358
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.33
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 20490
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 37.753859519958496
time_total_s: 10021.509824037552
timers:
  learn_throughput: 544.17
  learn_time_ms: 30321.397
  load_throughput: 5216127.588
  load_time_ms: 3.163
  training_iteration_time_ms: 40654.488
  update_time_ms: 2.452
timesteps_total: 3729000
training_iteration: 226

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 45.590529247910865
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 359
episodes_total: 44509
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.69265794754028
time_total_s: 9974.419194698334
timers:
  learn_throughput: 355.989
  learn_time_ms: 46349.708
  load_throughput: 3460837.23
  load_time_ms: 4.768
  training_iteration_time_ms: 59712.652
  update_time_ms: 2.644
timesteps_total: 2937000
training_iteration: 178

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9044117647058824
  reward for individual goal_min: 0.0
episode_len_mean: 206.57
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 17323
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.65449643135071
time_total_s: 10035.93199467659
timers:
  learn_throughput: 507.164
  learn_time_ms: 32533.833
  load_throughput: 4920441.948
  load_time_ms: 3.353
  training_iteration_time_ms: 43326.979
  update_time_ms: 2.49
timesteps_total: 3498000
training_iteration: 212

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.77925531914894
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 376
episodes_total: 60644
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 49.129985332489014
time_total_s: 9931.90922665596
timers:
  learn_throughput: 451.245
  learn_time_ms: 36565.513
  load_throughput: 4327648.015
  load_time_ms: 3.813
  training_iteration_time_ms: 48139.863
  update_time_ms: 2.414
timesteps_total: 3514500
training_iteration: 213

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.85
  reward for individual goal_min: 0.5
episode_len_mean: 110.4
episode_reward_max: 2.0
episode_reward_mean: 1.8166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9666666666666667
  agent_1: 0.85
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8481012658227848
  reward for individual goal_min: 0.0
episode_len_mean: 192.47
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 18071
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.636850357055664
time_total_s: 10059.445039987564
timers:
  learn_throughput: 525.54
  learn_time_ms: 31396.253
  load_throughput: 4881053.426
  load_time_ms: 3.38
  training_iteration_time_ms: 42174.085
  update_time_ms: 2.454
timesteps_total: 3630000
training_iteration: 220

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3271604938271605
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 172.38
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 17853
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.46534466743469
time_total_s: 10030.042615175247
timers:
  learn_throughput: 477.4
  learn_time_ms: 34562.206
  load_throughput: 4678864.19
  load_time_ms: 3.526
  training_iteration_time_ms: 45555.015
  update_time_ms: 2.853
timesteps_total: 3267000
training_iteration: 198

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936305732484076
  reward for individual goal_min: 0.5
episode_len_mean: 47.7906976744186
episode_reward_max: 2.0
episode_reward_mean: 1.994186046511628
episode_reward_min: 1.0
episodes_this_iter: 344
episodes_total: 41021
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9941860465116279
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 55.502912759780884
time_total_s: 10007.719280719757
timers:
  learn_throughput: 393.864
  learn_time_ms: 41892.593
  load_throughput: 4299658.667
  load_time_ms: 3.838
  training_iteration_time_ms: 54315.084
  update_time_ms: 2.557
timesteps_total: 3250500
training_iteration: 197

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9166666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 187.06
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 16103
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.917731046676636
time_total_s: 10051.48692560196
timers:
  learn_throughput: 440.187
  learn_time_ms: 37484.05
  load_throughput: 4701240.15
  load_time_ms: 3.51
  training_iteration_time_ms: 49449.02
  update_time_ms: 2.604
timesteps_total: 3201000
training_iteration: 194

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24285714285714285
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 171.02
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 16460
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.68368315696716
time_total_s: 10039.837275743484
timers:
  learn_throughput: 433.82
  learn_time_ms: 38034.165
  load_throughput: 4604923.646
  load_time_ms: 3.583
  training_iteration_time_ms: 49967.551
  update_time_ms: 2.79
timesteps_total: 3036000
training_iteration: 184

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 192.99
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 17639
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.18448495864868
time_total_s: 10052.576289176941
timers:
  learn_throughput: 444.738
  learn_time_ms: 37100.497
  load_throughput: 4767370.872
  load_time_ms: 3.461
  training_iteration_time_ms: 48658.371
  update_time_ms: 2.661
timesteps_total: 3283500
training_iteration: 199

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9349593495934959
  reward for individual goal_min: 0.5
episode_len_mean: 61.84386617100372
episode_reward_max: 2.0
episode_reward_mean: 1.9405204460966543
episode_reward_min: 1.0
episodes_this_iter: 269
episodes_total: 27736
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9405204460966543
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 67.22693085670471
time_total_s: 10059.656594753265
timers:
  learn_throughput: 328.642
  learn_time_ms: 50206.622
  load_throughput: 3509842.223
  load_time_ms: 4.701
  training_iteration_time_ms: 65449.221
  update_time_ms: 3.228
timesteps_total: 2739000
training_iteration: 166

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28313253012048195
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 179.03
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 16501
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.844066858291626
time_total_s: 10057.402365207672
timers:
  learn_throughput: 469.354
  learn_time_ms: 35154.721
  load_throughput: 4975413.815
  load_time_ms: 3.316
  training_iteration_time_ms: 46551.537
  update_time_ms: 2.544
timesteps_total: 3184500
training_iteration: 193

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34615384615384615
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9805194805194806
  reward for individual goal_min: 0.0
episode_len_mean: 162.44117647058823
episode_reward_max: 2.0
episode_reward_mean: 1.3823529411764706
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 20592
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6666666666666666
  agent_1: 0.7156862745098039
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.17611861228943
time_total_s: 10063.685942649841
timers:
  learn_throughput: 541.05
  learn_time_ms: 30496.245
  load_throughput: 5247730.175
  load_time_ms: 3.144
  training_iteration_time_ms: 40828.167
  update_time_ms: 2.433
timesteps_total: 3745500
training_iteration: 227

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22058823529411764
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.36
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 17282
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.9532368183136
time_total_s: 10078.86155128479
timers:
  learn_throughput: 442.079
  learn_time_ms: 37323.626
  load_throughput: 4595932.82
  load_time_ms: 3.59
  training_iteration_time_ms: 49197.821
  update_time_ms: 2.676
timesteps_total: 3151500
training_iteration: 191

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2328767123287671
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.5
episode_len_mean: 181.31
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 16533
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.76
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.496187925338745
time_total_s: 10073.469356298447
timers:
  learn_throughput: 428.19
  learn_time_ms: 38534.275
  load_throughput: 4524333.569
  load_time_ms: 3.647
  training_iteration_time_ms: 50764.889
  update_time_ms: 2.642
timesteps_total: 3036000
training_iteration: 184

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9121621621621622
  reward for individual goal_min: 0.0
episode_len_mean: 197.14
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 17407
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.99560070037842
time_total_s: 10079.927595376968
timers:
  learn_throughput: 506.495
  learn_time_ms: 32576.795
  load_throughput: 5005642.865
  load_time_ms: 3.296
  training_iteration_time_ms: 43421.253
  update_time_ms: 2.502
timesteps_total: 3514500
training_iteration: 213

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2235294117647059
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8943661971830986
  reward for individual goal_min: 0.0
episode_len_mean: 206.26
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 18152
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.63457489013672
time_total_s: 10102.0796148777
timers:
  learn_throughput: 525.503
  learn_time_ms: 31398.488
  load_throughput: 4846291.789
  load_time_ms: 3.405
  training_iteration_time_ms: 42258.084
  update_time_ms: 2.437
timesteps_total: 3646500
training_iteration: 221

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.764550264550266
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 378
episodes_total: 61022
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.723522901535034
time_total_s: 9980.632749557495
timers:
  learn_throughput: 449.876
  learn_time_ms: 36676.795
  load_throughput: 4362594.383
  load_time_ms: 3.782
  training_iteration_time_ms: 48242.16
  update_time_ms: 2.419
timesteps_total: 3531000
training_iteration: 214

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973404255319149
  reward for individual goal_min: 0.5
episode_len_mean: 43.59055118110236
episode_reward_max: 2.0
episode_reward_mean: 1.9973753280839894
episode_reward_min: 1.0
episodes_this_iter: 381
episodes_total: 44890
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973753280839895
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 59.542365074157715
time_total_s: 10033.961559772491
timers:
  learn_throughput: 355.922
  learn_time_ms: 46358.483
  load_throughput: 3484483.113
  load_time_ms: 4.735
  training_iteration_time_ms: 59708.601
  update_time_ms: 2.652
timesteps_total: 2953500
training_iteration: 179

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3051948051948052
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 168.04950495049505
episode_reward_max: 2.0
episode_reward_mean: 1.3465346534653466
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 17954
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7029702970297029
  agent_1: 0.6435643564356436
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.223797082901
time_total_s: 10076.266412258148
timers:
  learn_throughput: 476.124
  learn_time_ms: 34654.84
  load_throughput: 4653913.184
  load_time_ms: 3.545
  training_iteration_time_ms: 45638.175
  update_time_ms: 2.841
timesteps_total: 3283500
training_iteration: 199

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19863013698630136
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9698795180722891
  reward for individual goal_min: 0.0
episode_len_mean: 179.89
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 16197
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.879467248916626
time_total_s: 10100.366392850876
timers:
  learn_throughput: 438.842
  learn_time_ms: 37598.941
  load_throughput: 4669078.544
  load_time_ms: 3.534
  training_iteration_time_ms: 49532.571
  update_time_ms: 2.586
timesteps_total: 3217500
training_iteration: 195

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18421052631578946
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9879518072289156
  reward for individual goal_min: 0.0
episode_len_mean: 181.82
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 16548
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.532575845718384
time_total_s: 10090.369851589203
timers:
  learn_throughput: 431.28
  learn_time_ms: 38258.184
  load_throughput: 4586703.428
  load_time_ms: 3.597
  training_iteration_time_ms: 50257.222
  update_time_ms: 2.77
timesteps_total: 3052500
training_iteration: 185

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9942857142857143
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 47.57183908045977
episode_reward_max: 2.0
episode_reward_mean: 1.9942528735632183
episode_reward_min: 0.0
episodes_this_iter: 348
episodes_total: 41369
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9971264367816092
  agent_1: 0.9971264367816092
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.021549701690674
time_total_s: 10063.740830421448
timers:
  learn_throughput: 391.4
  learn_time_ms: 42156.35
  load_throughput: 4319598.537
  load_time_ms: 3.82
  training_iteration_time_ms: 54663.117
  update_time_ms: 2.562
timesteps_total: 3267000
training_iteration: 198

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 77.9
episode_reward_max: 2.0
episode_reward_mean: 1.9
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.95
  agent_1: 0.95
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9791666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 185.85
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 17729
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.31757712364197
time_total_s: 10110.893866300583
timers:
  learn_throughput: 444.115
  learn_time_ms: 37152.571
  load_throughput: 4797777.131
  load_time_ms: 3.439
  training_iteration_time_ms: 48811.74
  update_time_ms: 2.665
timesteps_total: 3300000
training_iteration: 200

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2708333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939024390243902
  reward for individual goal_min: 0.5
episode_len_mean: 169.74
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 20688
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 38.82071328163147
time_total_s: 10102.506655931473
timers:
  learn_throughput: 544.645
  learn_time_ms: 30294.97
  load_throughput: 5163972.929
  load_time_ms: 3.195
  training_iteration_time_ms: 40538.214
  update_time_ms: 2.428
timesteps_total: 3762000
training_iteration: 228

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 176.25
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 16592
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.51060724258423
time_total_s: 10106.912972450256
timers:
  learn_throughput: 468.106
  learn_time_ms: 35248.419
  load_throughput: 4925239.373
  load_time_ms: 3.35
  training_iteration_time_ms: 46757.953
  update_time_ms: 2.546
timesteps_total: 3201000
training_iteration: 194

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 162.62
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 17381
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.45520830154419
time_total_s: 10128.316759586334
timers:
  learn_throughput: 441.284
  learn_time_ms: 37390.906
  load_throughput: 4611889.644
  load_time_ms: 3.578
  training_iteration_time_ms: 49326.664
  update_time_ms: 2.712
timesteps_total: 3168000
training_iteration: 192

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15822784810126583
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9551282051282052
  reward for individual goal_min: 0.0
episode_len_mean: 196.04
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 17491
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 39.944380044937134
time_total_s: 10119.871975421906
timers:
  learn_throughput: 511.693
  learn_time_ms: 32245.926
  load_throughput: 5018929.291
  load_time_ms: 3.288
  training_iteration_time_ms: 42962.692
  update_time_ms: 2.458
timesteps_total: 3531000
training_iteration: 214

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.930327868852459
  reward for individual goal_min: 0.5
episode_len_mean: 61.60820895522388
episode_reward_max: 2.0
episode_reward_mean: 1.9365671641791045
episode_reward_min: 1.0
episodes_this_iter: 268
episodes_total: 28004
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.996268656716418
  agent_1: 0.9402985074626866
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 69.07924818992615
time_total_s: 10128.735842943192
timers:
  learn_throughput: 326.894
  learn_time_ms: 50475.134
  load_throughput: 3403396.019
  load_time_ms: 4.848
  training_iteration_time_ms: 65963.254
  update_time_ms: 3.137
timesteps_total: 2755500
training_iteration: 167

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.81
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 16631
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.916303634643555
time_total_s: 10127.38565993309
timers:
  learn_throughput: 426.166
  learn_time_ms: 38717.35
  load_throughput: 4539230.497
  load_time_ms: 3.635
  training_iteration_time_ms: 51078.693
  update_time_ms: 2.647
timesteps_total: 3052500
training_iteration: 185

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30246913580246915
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8731343283582089
  reward for individual goal_min: 0.0
episode_len_mean: 191.46
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 18237
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.01392960548401
time_total_s: 10144.093544483185
timers:
  learn_throughput: 524.939
  learn_time_ms: 31432.222
  load_throughput: 4843510.54
  load_time_ms: 3.407
  training_iteration_time_ms: 42346.368
  update_time_ms: 2.443
timesteps_total: 3663000
training_iteration: 222

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.37467700258398
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 387
episodes_total: 61409
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.38344883918762
time_total_s: 10029.016198396683
timers:
  learn_throughput: 449.088
  learn_time_ms: 36741.097
  load_throughput: 4386040.422
  load_time_ms: 3.762
  training_iteration_time_ms: 48337.689
  update_time_ms: 2.43
timesteps_total: 3547500
training_iteration: 215

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2013888888888889
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9285714285714286
  reward for individual goal_min: 0.0
episode_len_mean: 182.74
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 16286
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.414926528930664
time_total_s: 10149.781319379807
timers:
  learn_throughput: 439.333
  learn_time_ms: 37556.961
  load_throughput: 4637198.625
  load_time_ms: 3.558
  training_iteration_time_ms: 49454.024
  update_time_ms: 2.58
timesteps_total: 3234000
training_iteration: 196

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 61.38333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.9333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9833333333333333
  agent_1: 0.95
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2887323943661972
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.5
episode_len_mean: 167.69
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 18052
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.351184368133545
time_total_s: 10131.617596626282
timers:
  learn_throughput: 473.887
  learn_time_ms: 34818.453
  load_throughput: 4624215.956
  load_time_ms: 3.568
  training_iteration_time_ms: 45863.512
  update_time_ms: 2.851
timesteps_total: 3300000
training_iteration: 200

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24647887323943662
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.98
  reward for individual goal_min: 0.0
episode_len_mean: 167.86
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 16648
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.82502603530884
time_total_s: 10142.194877624512
timers:
  learn_throughput: 430.649
  learn_time_ms: 38314.301
  load_throughput: 4561430.003
  load_time_ms: 3.617
  training_iteration_time_ms: 50338.502
  update_time_ms: 2.742
timesteps_total: 3069000
training_iteration: 186

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.37012987012987014
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 157.8190476190476
episode_reward_max: 2.0
episode_reward_mean: 1.4476190476190476
episode_reward_min: 0.0
episodes_this_iter: 105
episodes_total: 20793
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7714285714285715
  agent_1: 0.6761904761904762
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.84043216705322
time_total_s: 10143.347088098526
timers:
  learn_throughput: 543.77
  learn_time_ms: 30343.694
  load_throughput: 5188207.3
  load_time_ms: 3.18
  training_iteration_time_ms: 40631.9
  update_time_ms: 2.434
timesteps_total: 3778500
training_iteration: 229

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.4
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972677595628415
  reward for individual goal_min: 0.5
episode_len_mean: 46.34173669467787
episode_reward_max: 2.0
episode_reward_mean: 1.9971988795518207
episode_reward_min: 1.0
episodes_this_iter: 357
episodes_total: 45247
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9971988795518207
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 63.238844871520996
time_total_s: 10097.200404644012
timers:
  learn_throughput: 356.634
  learn_time_ms: 46265.955
  load_throughput: 3469373.211
  load_time_ms: 4.756
  training_iteration_time_ms: 59562.168
  update_time_ms: 2.633
timesteps_total: 2970000
training_iteration: 180

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-184s1jdk42/checkpoint_000180/checkpoint-180
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27702702702702703
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 175.76
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 17824
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.5880331993103
time_total_s: 10160.481899499893
timers:
  learn_throughput: 444.003
  learn_time_ms: 37161.923
  load_throughput: 4747812.629
  load_time_ms: 3.475
  training_iteration_time_ms: 48849.109
  update_time_ms: 2.676
timesteps_total: 3316500
training_iteration: 201

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9907407407407407
  reward for individual goal_min: 0.5
episode_len_mean: 48.166666666666664
episode_reward_max: 2.0
episode_reward_mean: 1.9912280701754386
episode_reward_min: 1.0
episodes_this_iter: 342
episodes_total: 41711
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9912280701754386
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 54.050413846969604
time_total_s: 10117.791244268417
timers:
  learn_throughput: 391.13
  learn_time_ms: 42185.5
  load_throughput: 4252420.412
  load_time_ms: 3.88
  training_iteration_time_ms: 54644.703
  update_time_ms: 2.57
timesteps_total: 3283500
training_iteration: 199

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 191.62
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 16679
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.55519247055054
time_total_s: 10155.468164920807
timers:
  learn_throughput: 464.788
  learn_time_ms: 35500.019
  load_throughput: 4873319.907
  load_time_ms: 3.386
  training_iteration_time_ms: 47089.717
  update_time_ms: 2.562
timesteps_total: 3217500
training_iteration: 195

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18571428571428572
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9090909090909091
  reward for individual goal_min: 0.0
episode_len_mean: 193.09
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 17578
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.154212474823
time_total_s: 10164.026187896729
timers:
  learn_throughput: 511.729
  learn_time_ms: 32243.616
  load_throughput: 5059104.207
  load_time_ms: 3.261
  training_iteration_time_ms: 42948.928
  update_time_ms: 2.444
timesteps_total: 3547500
training_iteration: 215

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.795774647887324
  reward for individual goal_min: 0.0
episode_len_mean: 208.27
episode_reward_max: 2.0
episode_reward_mean: 1.04
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 18315
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.4
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.4308135509491
time_total_s: 10187.524358034134
timers:
  learn_throughput: 523.461
  learn_time_ms: 31520.961
  load_throughput: 4822315.625
  load_time_ms: 3.422
  training_iteration_time_ms: 42466.627
  update_time_ms: 2.453
timesteps_total: 3679500
training_iteration: 223

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3055555555555556
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9797297297297297
  reward for individual goal_min: 0.0
episode_len_mean: 171.36
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 17477
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.94751262664795
time_total_s: 10181.264272212982
timers:
  learn_throughput: 437.591
  learn_time_ms: 37706.428
  load_throughput: 4643795.234
  load_time_ms: 3.553
  training_iteration_time_ms: 49718.363
  update_time_ms: 2.734
timesteps_total: 3184500
training_iteration: 193

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20987654320987653
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 185.47
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 16720
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.002646684646606
time_total_s: 10177.388306617737
timers:
  learn_throughput: 426.722
  learn_time_ms: 38666.824
  load_throughput: 4548807.751
  load_time_ms: 3.627
  training_iteration_time_ms: 51028.681
  update_time_ms: 2.671
timesteps_total: 3069000
training_iteration: 186

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.514066496163686
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 391
episodes_total: 61800
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.98837161064148
time_total_s: 10078.004570007324
timers:
  learn_throughput: 448.814
  learn_time_ms: 36763.568
  load_throughput: 4401942.284
  load_time_ms: 3.748
  training_iteration_time_ms: 48365.099
  update_time_ms: 2.413
timesteps_total: 3564000
training_iteration: 216

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9126984126984127
  reward for individual goal_min: 0.5
episode_len_mean: 67.9875
episode_reward_max: 2.0
episode_reward_mean: 1.9083333333333334
episode_reward_min: 1.0
episodes_this_iter: 240
episodes_total: 28244
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9958333333333333
  agent_1: 0.9125
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.77732276916504
time_total_s: 10194.513165712357
timers:
  learn_throughput: 326.375
  learn_time_ms: 50555.291
  load_throughput: 3521934.656
  load_time_ms: 4.685
  training_iteration_time_ms: 66012.244
  update_time_ms: 3.108
timesteps_total: 2772000
training_iteration: 168

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3194444444444444
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9848484848484849
  reward for individual goal_min: 0.5
episode_len_mean: 174.46
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 18145
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.06395983695984
time_total_s: 10176.681556463242
timers:
  learn_throughput: 474.763
  learn_time_ms: 34754.194
  load_throughput: 4761663.41
  load_time_ms: 3.465
  training_iteration_time_ms: 45827.793
  update_time_ms: 2.899
timesteps_total: 3316500
training_iteration: 201

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21604938271604937
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 189.9
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 20882
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.150819540023804
time_total_s: 10183.49790763855
timers:
  learn_throughput: 545.42
  learn_time_ms: 30251.895
  load_throughput: 5150445.118
  load_time_ms: 3.204
  training_iteration_time_ms: 40455.458
  update_time_ms: 2.444
timesteps_total: 3795000
training_iteration: 230

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2013888888888889
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9551282051282052
  reward for individual goal_min: 0.0
episode_len_mean: 193.62
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 16373
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.88313269615173
time_total_s: 10197.664452075958
timers:
  learn_throughput: 439.773
  learn_time_ms: 37519.365
  load_throughput: 4612012.582
  load_time_ms: 3.578
  training_iteration_time_ms: 49420.645
  update_time_ms: 2.561
timesteps_total: 3250500
training_iteration: 197

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33783783783783783
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9652777777777778
  reward for individual goal_min: 0.0
episode_len_mean: 163.79
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 16746
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.98297333717346
time_total_s: 10194.177850961685
timers:
  learn_throughput: 430.196
  learn_time_ms: 38354.647
  load_throughput: 4520255.516
  load_time_ms: 3.65
  training_iteration_time_ms: 50527.418
  update_time_ms: 2.725
timesteps_total: 3085500
training_iteration: 187

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3092105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.82
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 17922
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.269986391067505
time_total_s: 10210.75188589096
timers:
  learn_throughput: 442.995
  learn_time_ms: 37246.492
  load_throughput: 4700761.158
  load_time_ms: 3.51
  training_iteration_time_ms: 48910.663
  update_time_ms: 2.673
timesteps_total: 3333000
training_iteration: 202

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18243243243243243
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 185.17
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 16768
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.458359241485596
time_total_s: 10198.926524162292
timers:
  learn_throughput: 467.427
  learn_time_ms: 35299.622
  load_throughput: 4852272.096
  load_time_ms: 3.4
  training_iteration_time_ms: 46846.624
  update_time_ms: 2.571
timesteps_total: 3234000
training_iteration: 196

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2662337662337662
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9121621621621622
  reward for individual goal_min: 0.0
episode_len_mean: 200.87
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 17661
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.99980616569519
time_total_s: 10207.025994062424
timers:
  learn_throughput: 509.54
  learn_time_ms: 32382.172
  load_throughput: 5089388.665
  load_time_ms: 3.242
  training_iteration_time_ms: 43083.182
  update_time_ms: 2.457
timesteps_total: 3564000
training_iteration: 216

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.93882978723404
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 376
episodes_total: 45623
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.0740122795105
time_total_s: 10155.274416923523
timers:
  learn_throughput: 356.929
  learn_time_ms: 46227.732
  load_throughput: 3477514.497
  load_time_ms: 4.745
  training_iteration_time_ms: 59517.399
  update_time_ms: 2.618
timesteps_total: 2986500
training_iteration: 181

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8402777777777778
  reward for individual goal_min: 0.0
episode_len_mean: 198.8
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 18398
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.43
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.85274934768677
time_total_s: 10229.37710738182
timers:
  learn_throughput: 520.498
  learn_time_ms: 31700.402
  load_throughput: 4804738.782
  load_time_ms: 3.434
  training_iteration_time_ms: 42694.479
  update_time_ms: 2.471
timesteps_total: 3696000
training_iteration: 224

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.916666666666664
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.280748663101605
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 374
episodes_total: 42085
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 60.566701889038086
time_total_s: 10178.357946157455
timers:
  learn_throughput: 389.839
  learn_time_ms: 42325.135
  load_throughput: 4186970.458
  load_time_ms: 3.941
  training_iteration_time_ms: 54802.58
  update_time_ms: 2.586
timesteps_total: 3300000
training_iteration: 200

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-18g5xov65h/checkpoint_000200/checkpoint-200
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25675675675675674
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 179.26
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 17569
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.02745223045349
time_total_s: 10231.291724443436
timers:
  learn_throughput: 439.257
  learn_time_ms: 37563.414
  load_throughput: 4616935.475
  load_time_ms: 3.574
  training_iteration_time_ms: 49513.287
  update_time_ms: 2.75
timesteps_total: 3201000
training_iteration: 194

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20833333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.97
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 16815
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.29556703567505
time_total_s: 10226.683873653412
timers:
  learn_throughput: 427.395
  learn_time_ms: 38605.971
  load_throughput: 4562662.992
  load_time_ms: 3.616
  training_iteration_time_ms: 50932.348
  update_time_ms: 2.698
timesteps_total: 3085500
training_iteration: 187

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1780821917808219
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.63
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 20974
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 39.602120876312256
time_total_s: 10223.100028514862
timers:
  learn_throughput: 546.151
  learn_time_ms: 30211.438
  load_throughput: 5169643.385
  load_time_ms: 3.192
  training_iteration_time_ms: 40367.479
  update_time_ms: 2.425
timesteps_total: 3811500
training_iteration: 231

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24025974025974026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.31
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 18241
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.70600366592407
time_total_s: 10221.387560129166
timers:
  learn_throughput: 472.671
  learn_time_ms: 34907.98
  load_throughput: 4727574.391
  load_time_ms: 3.49
  training_iteration_time_ms: 45974.014
  update_time_ms: 2.917
timesteps_total: 3333000
training_iteration: 202

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.21739130434783
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 391
episodes_total: 62191
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.38475942611694
time_total_s: 10126.389329433441
timers:
  learn_throughput: 449.271
  learn_time_ms: 36726.2
  load_throughput: 4378049.407
  load_time_ms: 3.769
  training_iteration_time_ms: 48321.898
  update_time_ms: 2.398
timesteps_total: 3580500
training_iteration: 217

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2565789473684211
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8716216216216216
  reward for individual goal_min: 0.0
episode_len_mean: 190.44
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 16456
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.82100987434387
time_total_s: 10247.485461950302
timers:
  learn_throughput: 439.514
  learn_time_ms: 37541.451
  load_throughput: 4642144.323
  load_time_ms: 3.554
  training_iteration_time_ms: 49423.664
  update_time_ms: 2.573
timesteps_total: 3267000
training_iteration: 198

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24358974358974358
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9797297297297297
  reward for individual goal_min: 0.0
episode_len_mean: 177.67
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 16840
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.91391062736511
time_total_s: 10247.09176158905
timers:
  learn_throughput: 428.821
  learn_time_ms: 38477.618
  load_throughput: 4539647.355
  load_time_ms: 3.635
  training_iteration_time_ms: 50702.471
  update_time_ms: 2.729
timesteps_total: 3102000
training_iteration: 188

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2662337662337662
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8888888888888888
  reward for individual goal_min: 0.0
episode_len_mean: 196.43
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 17742
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.12111735343933
time_total_s: 10250.147111415863
timers:
  learn_throughput: 510.073
  learn_time_ms: 32348.325
  load_throughput: 5105985.436
  load_time_ms: 3.232
  training_iteration_time_ms: 42999.166
  update_time_ms: 2.457
timesteps_total: 3580500
training_iteration: 217

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2602739726027397
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.88
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 18016
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.77
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.08814573287964
time_total_s: 10258.84003162384
timers:
  learn_throughput: 442.743
  learn_time_ms: 37267.689
  load_throughput: 4668574.59
  load_time_ms: 3.534
  training_iteration_time_ms: 48973.782
  update_time_ms: 2.686
timesteps_total: 3349500
training_iteration: 203

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.875
  reward for individual goal_min: 0.0
episode_len_mean: 206.26
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 18479
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.44
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.04297494888306
time_total_s: 10270.420082330704
timers:
  learn_throughput: 522.836
  learn_time_ms: 31558.639
  load_throughput: 4766681.314
  load_time_ms: 3.462
  training_iteration_time_ms: 42492.095
  update_time_ms: 2.45
timesteps_total: 3712500
training_iteration: 225

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.13291139240506328
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9726027397260274
  reward for individual goal_min: 0.0
episode_len_mean: 197.48
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 16852
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.97991347312927
time_total_s: 10247.906437635422
timers:
  learn_throughput: 462.712
  learn_time_ms: 35659.358
  load_throughput: 4859938.905
  load_time_ms: 3.395
  training_iteration_time_ms: 47191.198
  update_time_ms: 2.565
timesteps_total: 3250500
training_iteration: 197

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9429530201342282
  reward for individual goal_min: 0.5
episode_len_mean: 60.3584229390681
episode_reward_max: 2.0
episode_reward_mean: 1.939068100358423
episode_reward_min: 1.0
episodes_this_iter: 279
episodes_total: 28523
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.996415770609319
  agent_1: 0.942652329749104
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 70.0614984035492
time_total_s: 10264.574664115906
timers:
  learn_throughput: 324.732
  learn_time_ms: 50811.128
  load_throughput: 3381296.8
  load_time_ms: 4.88
  training_iteration_time_ms: 66420.012
  update_time_ms: 2.986
timesteps_total: 2788500
training_iteration: 169

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2945205479452055
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 166.94
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 21072
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.20583248138428
time_total_s: 10263.305860996246
timers:
  learn_throughput: 546.316
  learn_time_ms: 30202.3
  load_throughput: 5183971.236
  load_time_ms: 3.183
  training_iteration_time_ms: 40366.836
  update_time_ms: 2.424
timesteps_total: 3828000
training_iteration: 232

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974226804123711
  reward for individual goal_min: 0.5
episode_len_mean: 43.72074468085106
episode_reward_max: 2.0
episode_reward_mean: 1.997340425531915
episode_reward_min: 1.0
episodes_this_iter: 376
episodes_total: 45999
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973404255319149
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 58.694313287734985
time_total_s: 10213.968730211258
timers:
  learn_throughput: 357.968
  learn_time_ms: 46093.452
  load_throughput: 3501070.769
  load_time_ms: 4.713
  training_iteration_time_ms: 59386.451
  update_time_ms: 2.636
timesteps_total: 3003000
training_iteration: 182

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936305732484076
  reward for individual goal_min: 0.5
episode_len_mean: 47.61849710982659
episode_reward_max: 2.0
episode_reward_mean: 1.9942196531791907
episode_reward_min: 1.0
episodes_this_iter: 346
episodes_total: 42431
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9971098265895953
  agent_1: 0.9971098265895953
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.69670486450195
time_total_s: 10234.054651021957
timers:
  learn_throughput: 387.387
  learn_time_ms: 42593.066
  load_throughput: 4152701.482
  load_time_ms: 3.973
  training_iteration_time_ms: 55099.608
  update_time_ms: 2.596
timesteps_total: 3316500
training_iteration: 201

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25675675675675674
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.5
episode_len_mean: 177.22
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 16911
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.51172852516174
time_total_s: 10277.195602178574
timers:
  learn_throughput: 429.166
  learn_time_ms: 38446.655
  load_throughput: 4532689.905
  load_time_ms: 3.64
  training_iteration_time_ms: 50688.829
  update_time_ms: 2.698
timesteps_total: 3102000
training_iteration: 188

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17567567567567569
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9807692307692307
  reward for individual goal_min: 0.0
episode_len_mean: 189.29
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 17657
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.66587162017822
time_total_s: 10285.957596063614
timers:
  learn_throughput: 431.816
  learn_time_ms: 38210.753
  load_throughput: 4621066.492
  load_time_ms: 3.571
  training_iteration_time_ms: 50134.281
  update_time_ms: 2.748
timesteps_total: 3217500
training_iteration: 195

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3076923076923077
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.69
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 18336
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.81290030479431
time_total_s: 10268.20046043396
timers:
  learn_throughput: 471.821
  learn_time_ms: 34970.903
  load_throughput: 4713471.456
  load_time_ms: 3.501
  training_iteration_time_ms: 46008.78
  update_time_ms: 2.929
timesteps_total: 3349500
training_iteration: 203

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.10966057441253
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 383
episodes_total: 62574
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.487531661987305
time_total_s: 10174.876861095428
timers:
  learn_throughput: 449.205
  learn_time_ms: 36731.572
  load_throughput: 4400682.682
  load_time_ms: 3.749
  training_iteration_time_ms: 48299.511
  update_time_ms: 2.395
timesteps_total: 3597000
training_iteration: 218

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9457831325301205
  reward for individual goal_min: 0.0
episode_len_mean: 163.26470588235293
episode_reward_max: 2.0
episode_reward_mean: 1.4019607843137254
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 16558
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7254901960784313
  agent_1: 0.6764705882352942
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.582658767700195
time_total_s: 10295.068120718002
timers:
  learn_throughput: 441.341
  learn_time_ms: 37386.064
  load_throughput: 4636608.334
  load_time_ms: 3.559
  training_iteration_time_ms: 49251.038
  update_time_ms: 2.575
timesteps_total: 3283500
training_iteration: 199

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21518987341772153
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.958904109589041
  reward for individual goal_min: 0.0
episode_len_mean: 195.82
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 17823
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.562169551849365
time_total_s: 10293.709280967712
timers:
  learn_throughput: 511.154
  learn_time_ms: 32279.904
  load_throughput: 5109415.864
  load_time_ms: 3.229
  training_iteration_time_ms: 42887.155
  update_time_ms: 2.462
timesteps_total: 3597000
training_iteration: 218

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33783783783783783
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8591549295774648
  reward for individual goal_min: 0.0
episode_len_mean: 182.75
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 18571
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.577420711517334
time_total_s: 10315.997503042221
timers:
  learn_throughput: 519.866
  learn_time_ms: 31738.948
  load_throughput: 4776287.381
  load_time_ms: 3.455
  training_iteration_time_ms: 42711.379
  update_time_ms: 2.47
timesteps_total: 3729000
training_iteration: 226

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25595238095238093
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9782608695652174
  reward for individual goal_min: 0.0
episode_len_mean: 188.33
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 16929
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.680084228515625
time_total_s: 10297.771845817566
timers:
  learn_throughput: 428.595
  learn_time_ms: 38497.887
  load_throughput: 4550422.85
  load_time_ms: 3.626
  training_iteration_time_ms: 50669.137
  update_time_ms: 2.737
timesteps_total: 3118500
training_iteration: 189

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.189873417721519
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 189.98
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 16939
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.244264125823975
time_total_s: 10294.150701761246
timers:
  learn_throughput: 462.926
  learn_time_ms: 35642.819
  load_throughput: 4807208.519
  load_time_ms: 3.432
  training_iteration_time_ms: 47214.975
  update_time_ms: 2.559
timesteps_total: 3267000
training_iteration: 198

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3051948051948052
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 179.33
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 18108
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.992899656295776
time_total_s: 10309.832931280136
timers:
  learn_throughput: 440.837
  learn_time_ms: 37428.829
  load_throughput: 4656386.904
  load_time_ms: 3.544
  training_iteration_time_ms: 49202.168
  update_time_ms: 2.713
timesteps_total: 3366000
training_iteration: 204

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18666666666666668
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 189.61
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 21160
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.94336915016174
time_total_s: 10305.249230146408
timers:
  learn_throughput: 544.621
  learn_time_ms: 30296.27
  load_throughput: 5193541.357
  load_time_ms: 3.177
  training_iteration_time_ms: 40391.508
  update_time_ms: 2.443
timesteps_total: 3844500
training_iteration: 233

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9078947368421053
  reward for individual goal_min: 0.5
episode_len_mean: 65.82786885245902
episode_reward_max: 2.0
episode_reward_mean: 1.9139344262295082
episode_reward_min: 1.0
episodes_this_iter: 244
episodes_total: 28767
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9959016393442623
  agent_1: 0.9180327868852459
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.78592872619629
time_total_s: 10326.360592842102
timers:
  learn_throughput: 326.489
  learn_time_ms: 50537.698
  load_throughput: 3514601.391
  load_time_ms: 4.695
  training_iteration_time_ms: 66117.766
  update_time_ms: 2.967
timesteps_total: 2805000
training_iteration: 170

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971428571428571
  reward for individual goal_min: 0.5
episode_len_mean: 47.12571428571429
episode_reward_max: 2.0
episode_reward_mean: 1.997142857142857
episode_reward_min: 1.0
episodes_this_iter: 350
episodes_total: 42781
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9971428571428571
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 53.5516300201416
time_total_s: 10287.606281042099
timers:
  learn_throughput: 388.118
  learn_time_ms: 42512.84
  load_throughput: 4135554.068
  load_time_ms: 3.99
  training_iteration_time_ms: 55073.942
  update_time_ms: 2.632
timesteps_total: 3333000
training_iteration: 202

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19718309859154928
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 173.38
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 17006
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.44874858856201
time_total_s: 10327.644350767136
timers:
  learn_throughput: 430.342
  learn_time_ms: 38341.597
  load_throughput: 4557945.151
  load_time_ms: 3.62
  training_iteration_time_ms: 50596.043
  update_time_ms: 2.677
timesteps_total: 3118500
training_iteration: 189

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30405405405405406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.55
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 17756
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.68451380729675
time_total_s: 10334.64210987091
timers:
  learn_throughput: 429.773
  learn_time_ms: 38392.344
  load_throughput: 4626750.993
  load_time_ms: 3.566
  training_iteration_time_ms: 50288.108
  update_time_ms: 2.777
timesteps_total: 3234000
training_iteration: 196

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2826086956521739
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9556962025316456
  reward for individual goal_min: 0.0
episode_len_mean: 160.32380952380953
episode_reward_max: 2.0
episode_reward_mean: 1.3904761904761904
episode_reward_min: 0.0
episodes_this_iter: 105
episodes_total: 18441
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7238095238095238
  agent_1: 0.6666666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.65405488014221
time_total_s: 10315.854515314102
timers:
  learn_throughput: 469.05
  learn_time_ms: 35177.456
  load_throughput: 4702837.494
  load_time_ms: 3.509
  training_iteration_time_ms: 46290.658
  update_time_ms: 2.666
timesteps_total: 3366000
training_iteration: 204

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.31578947368421
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 380
episodes_total: 46379
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.45730757713318
time_total_s: 10273.426037788391
timers:
  learn_throughput: 358.692
  learn_time_ms: 46000.475
  load_throughput: 3495906.609
  load_time_ms: 4.72
  training_iteration_time_ms: 59255.315
  update_time_ms: 2.642
timesteps_total: 3019500
training_iteration: 183

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972826086956522
  reward for individual goal_min: 0.5
episode_len_mean: 41.79746835443038
episode_reward_max: 2.0
episode_reward_mean: 1.9974683544303797
episode_reward_min: 1.0
episodes_this_iter: 395
episodes_total: 62969
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9974683544303797
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 48.92464995384216
time_total_s: 10223.80151104927
timers:
  learn_throughput: 448.766
  learn_time_ms: 36767.504
  load_throughput: 4379822.671
  load_time_ms: 3.767
  training_iteration_time_ms: 48382.379
  update_time_ms: 2.395
timesteps_total: 3613500
training_iteration: 219

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3013698630136986
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8581081081081081
  reward for individual goal_min: 0.0
episode_len_mean: 191.78
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 18657
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.11426043510437
time_total_s: 10359.111763477325
timers:
  learn_throughput: 520.898
  learn_time_ms: 31676.045
  load_throughput: 4734851.912
  load_time_ms: 3.485
  training_iteration_time_ms: 42595.436
  update_time_ms: 2.489
timesteps_total: 3745500
training_iteration: 227

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9236111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 199.74
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 17906
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.90975475311279
time_total_s: 10339.619035720825
timers:
  learn_throughput: 506.404
  learn_time_ms: 32582.712
  load_throughput: 5096847.594
  load_time_ms: 3.237
  training_iteration_time_ms: 43209.483
  update_time_ms: 2.464
timesteps_total: 3613500
training_iteration: 219

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9333333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 98.98333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.9
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9833333333333333
  agent_1: 0.9166666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.42
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.875
  reward for individual goal_min: 0.0
episode_len_mean: 174.45
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 16651
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.11508846282959
time_total_s: 10357.183209180832
timers:
  learn_throughput: 437.831
  learn_time_ms: 37685.775
  load_throughput: 4637509.365
  load_time_ms: 3.558
  training_iteration_time_ms: 49558.712
  update_time_ms: 2.57
timesteps_total: 3300000
training_iteration: 200

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19mcvnks1j/checkpoint_000200/checkpoint-200
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3082191780821918
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.12
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 17037
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.81785011291504
time_total_s: 10343.96855187416
timers:
  learn_throughput: 460.358
  learn_time_ms: 35841.699
  load_throughput: 4841917.848
  load_time_ms: 3.408
  training_iteration_time_ms: 47442.726
  update_time_ms: 2.56
timesteps_total: 3283500
training_iteration: 199

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22916666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.88
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 18201
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.74505686759949
time_total_s: 10358.577988147736
timers:
  learn_throughput: 442.048
  learn_time_ms: 37326.253
  load_throughput: 4513828.333
  load_time_ms: 3.655
  training_iteration_time_ms: 49138.129
  update_time_ms: 2.698
timesteps_total: 3382500
training_iteration: 205

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1951219512195122
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.972972972972973
  reward for individual goal_min: 0.0
episode_len_mean: 194.64
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 17010
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.712727069854736
time_total_s: 10348.48457288742
timers:
  learn_throughput: 428.07
  learn_time_ms: 38545.135
  load_throughput: 4556834.724
  load_time_ms: 3.621
  training_iteration_time_ms: 50799.083
  update_time_ms: 2.74
timesteps_total: 3135000
training_iteration: 190

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3051948051948052
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.0
episode_len_mean: 164.74257425742573
episode_reward_max: 2.0
episode_reward_mean: 1.3366336633663367
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 21261
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6336633663366337
  agent_1: 0.7029702970297029
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.39239859580994
time_total_s: 10349.641628742218
timers:
  learn_throughput: 539.681
  learn_time_ms: 30573.598
  load_throughput: 5205808.335
  load_time_ms: 3.17
  training_iteration_time_ms: 40703.497
  update_time_ms: 2.445
timesteps_total: 3861000
training_iteration: 234

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2112676056338028
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 179.34
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 17842
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.37683892250061
time_total_s: 10382.018948793411
timers:
  learn_throughput: 433.561
  learn_time_ms: 38056.969
  load_throughput: 4630961.577
  load_time_ms: 3.563
  training_iteration_time_ms: 49872.907
  update_time_ms: 2.783
timesteps_total: 3250500
training_iteration: 197

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2898550724637681
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 161.44
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 18539
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.543278217315674
time_total_s: 10363.397793531418
timers:
  learn_throughput: 468.117
  learn_time_ms: 35247.616
  load_throughput: 4698080.607
  load_time_ms: 3.512
  training_iteration_time_ms: 46356.608
  update_time_ms: 2.672
timesteps_total: 3382500
training_iteration: 205

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2535211267605634
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 167.45
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 17103
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.97001576423645
time_total_s: 10378.614366531372
timers:
  learn_throughput: 427.543
  learn_time_ms: 38592.571
  load_throughput: 4549405.802
  load_time_ms: 3.627
  training_iteration_time_ms: 50919.318
  update_time_ms: 2.693
timesteps_total: 3135000
training_iteration: 190

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 45.73130193905817
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 361
episodes_total: 43142
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 55.149701833724976
time_total_s: 10342.755982875824
timers:
  learn_throughput: 388.342
  learn_time_ms: 42488.311
  load_throughput: 4109010.895
  load_time_ms: 4.016
  training_iteration_time_ms: 55041.492
  update_time_ms: 2.847
timesteps_total: 3349500
training_iteration: 203

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9307692307692308
  reward for individual goal_min: 0.5
episode_len_mean: 60.25448028673835
episode_reward_max: 2.0
episode_reward_mean: 1.935483870967742
episode_reward_min: 1.0
episodes_this_iter: 279
episodes_total: 29046
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.996415770609319
  agent_1: 0.9390681003584229
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.405723333358765
time_total_s: 10389.76631617546
timers:
  learn_throughput: 327.295
  learn_time_ms: 50413.188
  load_throughput: 3508756.73
  load_time_ms: 4.703
  training_iteration_time_ms: 65892.571
  update_time_ms: 2.913
timesteps_total: 2821500
training_iteration: 171

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32098765432098764
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8928571428571429
  reward for individual goal_min: 0.0
episode_len_mean: 189.32
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 18742
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.95259928703308
time_total_s: 10403.064362764359
timers:
  learn_throughput: 518.011
  learn_time_ms: 31852.595
  load_throughput: 4741112.283
  load_time_ms: 3.48
  training_iteration_time_ms: 42810.497
  update_time_ms: 2.516
timesteps_total: 3762000
training_iteration: 228

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 44.13333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.9833333333333334
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9833333333333333
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.345360824742265
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 388
episodes_total: 63357
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 52.83531212806702
time_total_s: 10276.636823177338
timers:
  learn_throughput: 448.709
  learn_time_ms: 36772.146
  load_throughput: 4414240.172
  load_time_ms: 3.738
  training_iteration_time_ms: 48389.157
  update_time_ms: 2.38
timesteps_total: 3630000
training_iteration: 220

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.40052356020942
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 382
episodes_total: 46761
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 60.190794706344604
time_total_s: 10333.616832494736
timers:
  learn_throughput: 358.704
  learn_time_ms: 45998.982
  load_throughput: 3496630.794
  load_time_ms: 4.719
  training_iteration_time_ms: 59251.211
  update_time_ms: 2.659
timesteps_total: 3036000
training_iteration: 184

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1987179487179487
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9156626506024096
  reward for individual goal_min: 0.0
episode_len_mean: 202.85
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 16734
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.0173614025116
time_total_s: 10403.200570583344
timers:
  learn_throughput: 442.858
  learn_time_ms: 37257.994
  load_throughput: 4560137.583
  load_time_ms: 3.618
  training_iteration_time_ms: 48987.446
  update_time_ms: 2.566
timesteps_total: 3316500
training_iteration: 201

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 111.91666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.8166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9
  agent_1: 0.9166666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8896103896103896
  reward for individual goal_min: 0.0
episode_len_mean: 200.49
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 17987
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.84472680091858
time_total_s: 10394.463762521744
timers:
  learn_throughput: 505.923
  learn_time_ms: 32613.681
  load_throughput: 5147150.794
  load_time_ms: 3.206
  training_iteration_time_ms: 43261.512
  update_time_ms: 2.461
timesteps_total: 3630000
training_iteration: 220

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19istq_qwi/checkpoint_000220/checkpoint-220
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9930555555555556
  reward for individual goal_min: 0.5
episode_len_mean: 189.31
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 21344
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.89815020561218
time_total_s: 10391.53977894783
timers:
  learn_throughput: 537.716
  learn_time_ms: 30685.327
  load_throughput: 5171884.136
  load_time_ms: 3.19
  training_iteration_time_ms: 40741.093
  update_time_ms: 2.464
timesteps_total: 3877500
training_iteration: 235

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34177215189873417
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9848484848484849
  reward for individual goal_min: 0.5
episode_len_mean: 178.79
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 18292
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.14747333526611
time_total_s: 10406.725461483002
timers:
  learn_throughput: 441.895
  learn_time_ms: 37339.206
  load_throughput: 4536136.23
  load_time_ms: 3.637
  training_iteration_time_ms: 49136.635
  update_time_ms: 2.696
timesteps_total: 3399000
training_iteration: 206

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1794871794871795
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9802631578947368
  reward for individual goal_min: 0.0
episode_len_mean: 186.72
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 17097
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.98750948905945
time_total_s: 10400.47208237648
timers:
  learn_throughput: 424.99
  learn_time_ms: 38824.419
  load_throughput: 4523535.11
  load_time_ms: 3.648
  training_iteration_time_ms: 51080.017
  update_time_ms: 2.729
timesteps_total: 3151500
training_iteration: 191

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 92.05
episode_reward_max: 2.0
episode_reward_mean: 1.6333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8166666666666667
  agent_1: 0.8166666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2152777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.7
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 17133
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.95706367492676
time_total_s: 10401.925615549088
timers:
  learn_throughput: 459.021
  learn_time_ms: 35946.047
  load_throughput: 4795981.705
  load_time_ms: 3.44
  training_iteration_time_ms: 47556.041
  update_time_ms: 2.575
timesteps_total: 3300000
training_iteration: 200

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1597222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9647058823529412
  reward for individual goal_min: 0.0
episode_len_mean: 184.65
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 17933
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.564199447631836
time_total_s: 10429.583148241043
timers:
  learn_throughput: 436.393
  learn_time_ms: 37809.996
  load_throughput: 4638877.114
  load_time_ms: 3.557
  training_iteration_time_ms: 49576.639
  update_time_ms: 2.795
timesteps_total: 3267000
training_iteration: 198

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3424657534246575
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 157.34285714285716
episode_reward_max: 2.0
episode_reward_mean: 1.4095238095238096
episode_reward_min: 0.0
episodes_this_iter: 105
episodes_total: 18644
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6952380952380952
  agent_1: 0.7142857142857143
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.04835891723633
time_total_s: 10411.446152448654
timers:
  learn_throughput: 466.624
  learn_time_ms: 35360.39
  load_throughput: 4743777.144
  load_time_ms: 3.478
  training_iteration_time_ms: 46470.902
  update_time_ms: 2.69
timesteps_total: 3399000
training_iteration: 206

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18181818181818182
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9733333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 189.94
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 17191
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.794737339019775
time_total_s: 10429.409103870392
timers:
  learn_throughput: 427.978
  learn_time_ms: 38553.381
  load_throughput: 4537682.836
  load_time_ms: 3.636
  training_iteration_time_ms: 50879.865
  update_time_ms: 2.687
timesteps_total: 3151500
training_iteration: 191

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24647887323943662
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8972602739726028
  reward for individual goal_min: 0.0
episode_len_mean: 188.63
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 18830
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.83978080749512
time_total_s: 10445.904143571854
timers:
  learn_throughput: 517.721
  learn_time_ms: 31870.436
  load_throughput: 4765663.761
  load_time_ms: 3.462
  training_iteration_time_ms: 42764.544
  update_time_ms: 2.52
timesteps_total: 3778500
training_iteration: 229

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.415
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 400
episodes_total: 63757
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.77357316017151
time_total_s: 10325.41039633751
timers:
  learn_throughput: 447.863
  learn_time_ms: 36841.619
  load_throughput: 4402334.307
  load_time_ms: 3.748
  training_iteration_time_ms: 48466.023
  update_time_ms: 2.373
timesteps_total: 3646500
training_iteration: 221

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971428571428571
  reward for individual goal_min: 0.5
episode_len_mean: 44.59349593495935
episode_reward_max: 2.0
episode_reward_mean: 1.997289972899729
episode_reward_min: 1.0
episodes_this_iter: 369
episodes_total: 43511
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.997289972899729
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 57.01749658584595
time_total_s: 10399.77347946167
timers:
  learn_throughput: 387.752
  learn_time_ms: 42552.987
  load_throughput: 4071924.17
  load_time_ms: 4.052
  training_iteration_time_ms: 55149.125
  update_time_ms: 2.865
timesteps_total: 3366000
training_iteration: 204

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28169014084507044
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 164.4851485148515
episode_reward_max: 2.0
episode_reward_mean: 1.3366336633663367
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 21445
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6633663366336634
  agent_1: 0.6732673267326733
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.85608792304993
time_total_s: 10435.39586687088
timers:
  learn_throughput: 529.544
  learn_time_ms: 31158.883
  load_throughput: 5193658.284
  load_time_ms: 3.177
  training_iteration_time_ms: 41351.115
  update_time_ms: 2.5
timesteps_total: 3894000
training_iteration: 236

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27631578947368424
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8783783783783784
  reward for individual goal_min: 0.0
episode_len_mean: 198.85
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 18065
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.641215324401855
time_total_s: 10440.104977846146
timers:
  learn_throughput: 501.403
  learn_time_ms: 32907.643
  load_throughput: 5162278.067
  load_time_ms: 3.196
  training_iteration_time_ms: 43583.547
  update_time_ms: 2.454
timesteps_total: 3646500
training_iteration: 221

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2710843373493976
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9264705882352942
  reward for individual goal_min: 0.0
episode_len_mean: 207.55
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 16813
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.93672037124634
time_total_s: 10452.13729095459
timers:
  learn_throughput: 442.453
  learn_time_ms: 37292.093
  load_throughput: 4583878.074
  load_time_ms: 3.6
  training_iteration_time_ms: 49049.207
  update_time_ms: 2.598
timesteps_total: 3333000
training_iteration: 202

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9321428571428572
  reward for individual goal_min: 0.5
episode_len_mean: 61.100746268656714
episode_reward_max: 2.0
episode_reward_mean: 1.9291044776119404
episode_reward_min: 1.0
episodes_this_iter: 268
episodes_total: 29314
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9291044776119403
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 62.96461796760559
time_total_s: 10452.730934143066
timers:
  learn_throughput: 328.403
  learn_time_ms: 50243.181
  load_throughput: 3647489.986
  load_time_ms: 4.524
  training_iteration_time_ms: 65619.947
  update_time_ms: 2.926
timesteps_total: 2838000
training_iteration: 172

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2971014492753623
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9797297297297297
  reward for individual goal_min: 0.5
episode_len_mean: 169.29
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 18390
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.05795621871948
time_total_s: 10455.783417701721
timers:
  learn_throughput: 442.033
  learn_time_ms: 37327.507
  load_throughput: 4547462.71
  load_time_ms: 3.628
  training_iteration_time_ms: 49106.382
  update_time_ms: 2.692
timesteps_total: 3415500
training_iteration: 207

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9942857142857143
  reward for individual goal_min: 0.5
episode_len_mean: 45.28767123287671
episode_reward_max: 2.0
episode_reward_mean: 1.9945205479452055
episode_reward_min: 1.0
episodes_this_iter: 365
episodes_total: 47126
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9945205479452055
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 59.46302509307861
time_total_s: 10393.079857587814
timers:
  learn_throughput: 358.848
  learn_time_ms: 45980.461
  load_throughput: 3497479.002
  load_time_ms: 4.718
  training_iteration_time_ms: 59225.362
  update_time_ms: 2.642
timesteps_total: 3052500
training_iteration: 185

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16233766233766234
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.975
  reward for individual goal_min: 0.0
episode_len_mean: 190.03
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 17220
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.02907180786133
time_total_s: 10447.954687356949
timers:
  learn_throughput: 459.802
  learn_time_ms: 35885.007
  load_throughput: 4743191.918
  load_time_ms: 3.479
  training_iteration_time_ms: 47438.383
  update_time_ms: 2.583
timesteps_total: 3316500
training_iteration: 201

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17901234567901234
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9930555555555556
  reward for individual goal_min: 0.5
episode_len_mean: 195.58
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 17181
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.53877830505371
time_total_s: 10452.010860681534
timers:
  learn_throughput: 423.449
  learn_time_ms: 38965.745
  load_throughput: 4516862.75
  load_time_ms: 3.653
  training_iteration_time_ms: 51241.794
  update_time_ms: 2.702
timesteps_total: 3168000
training_iteration: 192

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25735294117647056
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.5
episode_len_mean: 173.34
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 18028
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.52429986000061
time_total_s: 10479.107448101044
timers:
  learn_throughput: 434.541
  learn_time_ms: 37971.08
  load_throughput: 4679750.074
  load_time_ms: 3.526
  training_iteration_time_ms: 49820.919
  update_time_ms: 2.782
timesteps_total: 3283500
training_iteration: 199

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 169.91
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 18742
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.16269564628601
time_total_s: 10459.60884809494
timers:
  learn_throughput: 462.894
  learn_time_ms: 35645.304
  load_throughput: 4788448.94
  load_time_ms: 3.446
  training_iteration_time_ms: 46852.678
  update_time_ms: 2.705
timesteps_total: 3415500
training_iteration: 207

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16265060240963855
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8943661971830986
  reward for individual goal_min: 0.0
episode_len_mean: 217.1
episode_reward_max: 2.0
episode_reward_mean: 1.01
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 18907
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.21639013290405
time_total_s: 10490.120533704758
timers:
  learn_throughput: 514.808
  learn_time_ms: 32050.798
  load_throughput: 4768159.182
  load_time_ms: 3.46
  training_iteration_time_ms: 43030.152
  update_time_ms: 2.537
timesteps_total: 3795000
training_iteration: 230

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18493150684931506
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 180.67
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 17285
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.88379526138306
time_total_s: 10481.292899131775
timers:
  learn_throughput: 428.114
  learn_time_ms: 38541.101
  load_throughput: 4528537.514
  load_time_ms: 3.644
  training_iteration_time_ms: 50817.096
  update_time_ms: 2.7
timesteps_total: 3168000
training_iteration: 192

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19230769230769232
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 193.18
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 21527
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.440162897109985
time_total_s: 10476.83602976799
timers:
  learn_throughput: 531.145
  learn_time_ms: 31064.991
  load_throughput: 5135234.59
  load_time_ms: 3.213
  training_iteration_time_ms: 41277.299
  update_time_ms: 2.503
timesteps_total: 3910500
training_iteration: 237

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.51679586563308
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 387
episodes_total: 64144
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.51288938522339
time_total_s: 10373.923285722733
timers:
  learn_throughput: 447.158
  learn_time_ms: 36899.712
  load_throughput: 4370335.83
  load_time_ms: 3.775
  training_iteration_time_ms: 48533.474
  update_time_ms: 2.406
timesteps_total: 3663000
training_iteration: 222

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2605633802816901
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8650793650793651
  reward for individual goal_min: 0.0
episode_len_mean: 189.24
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 18156
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.3499653339386
time_total_s: 10487.454943180084
timers:
  learn_throughput: 496.305
  learn_time_ms: 33245.7
  load_throughput: 5124245.942
  load_time_ms: 3.22
  training_iteration_time_ms: 44053.009
  update_time_ms: 2.474
timesteps_total: 3663000
training_iteration: 222

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2235294117647059
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9375
  reward for individual goal_min: 0.0
episode_len_mean: 211.25
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 75
episodes_total: 16888
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.09984636306763
time_total_s: 10500.237137317657
timers:
  learn_throughput: 444.073
  learn_time_ms: 37156.091
  load_throughput: 4580086.035
  load_time_ms: 3.603
  training_iteration_time_ms: 48858.424
  update_time_ms: 2.596
timesteps_total: 3349500
training_iteration: 203

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9915730337078652
  reward for individual goal_min: 0.5
episode_len_mean: 47.668587896253605
episode_reward_max: 2.0
episode_reward_mean: 1.9913544668587897
episode_reward_min: 1.0
episodes_this_iter: 347
episodes_total: 43858
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9942363112391931
  agent_1: 0.9971181556195965
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.897693395614624
time_total_s: 10455.671172857285
timers:
  learn_throughput: 387.671
  learn_time_ms: 42561.895
  load_throughput: 3993814.512
  load_time_ms: 4.131
  training_iteration_time_ms: 55216.273
  update_time_ms: 2.875
timesteps_total: 3382500
training_iteration: 205

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22058823529411764
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.27
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 18489
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.249596118927
time_total_s: 10503.033013820648
timers:
  learn_throughput: 445.617
  learn_time_ms: 37027.297
  load_throughput: 4560227.728
  load_time_ms: 3.618
  training_iteration_time_ms: 48760.131
  update_time_ms: 2.722
timesteps_total: 3432000
training_iteration: 208

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1736111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.55
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 17311
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.87147498130798
time_total_s: 10497.826162338257
timers:
  learn_throughput: 456.353
  learn_time_ms: 36156.226
  load_throughput: 4688913.31
  load_time_ms: 3.519
  training_iteration_time_ms: 47802.045
  update_time_ms: 2.584
timesteps_total: 3333000
training_iteration: 202

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1724137931034483
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 204.53
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 17263
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.441487073898315
time_total_s: 10503.452347755432
timers:
  learn_throughput: 423.075
  learn_time_ms: 39000.161
  load_throughput: 4513622.258
  load_time_ms: 3.656
  training_iteration_time_ms: 51291.543
  update_time_ms: 2.687
timesteps_total: 3184500
training_iteration: 193

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9236641221374046
  reward for individual goal_min: 0.5
episode_len_mean: 62.84905660377358
episode_reward_max: 2.0
episode_reward_mean: 1.9245283018867925
episode_reward_min: 1.0
episodes_this_iter: 265
episodes_total: 29579
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9245283018867925
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 62.189765214920044
time_total_s: 10514.920699357986
timers:
  learn_throughput: 329.313
  learn_time_ms: 50104.327
  load_throughput: 3592262.565
  load_time_ms: 4.593
  training_iteration_time_ms: 65475.006
  update_time_ms: 2.906
timesteps_total: 2854500
training_iteration: 173

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.276881720430104
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 372
episodes_total: 47498
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.97843956947327
time_total_s: 10451.058297157288
timers:
  learn_throughput: 359.604
  learn_time_ms: 45883.866
  load_throughput: 3497850.222
  load_time_ms: 4.717
  training_iteration_time_ms: 59097.161
  update_time_ms: 2.628
timesteps_total: 3069000
training_iteration: 186

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2746478873239437
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.835820895522388
  reward for individual goal_min: 0.0
episode_len_mean: 196.01
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 18992
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.41
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.137913942337036
time_total_s: 10534.258447647095
timers:
  learn_throughput: 511.909
  learn_time_ms: 32232.304
  load_throughput: 4684787.003
  load_time_ms: 3.522
  training_iteration_time_ms: 43180.671
  update_time_ms: 2.554
timesteps_total: 3811500
training_iteration: 231

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.9
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 18838
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.49549651145935
time_total_s: 10507.1043446064
timers:
  learn_throughput: 461.465
  learn_time_ms: 35755.68
  load_throughput: 4786925.36
  load_time_ms: 3.447
  training_iteration_time_ms: 46955.893
  update_time_ms: 2.683
timesteps_total: 3432000
training_iteration: 208

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9873417721518988
  reward for individual goal_min: 0.0
episode_len_mean: 180.95
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 21617
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.24181842803955
time_total_s: 10520.07784819603
timers:
  learn_throughput: 525.727
  learn_time_ms: 31385.118
  load_throughput: 5217346.622
  load_time_ms: 3.163
  training_iteration_time_ms: 41719.46
  update_time_ms: 2.527
timesteps_total: 3927000
training_iteration: 238

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 86.26666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.8166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9
  agent_1: 0.9166666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2777777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9794520547945206
  reward for individual goal_min: 0.0
episode_len_mean: 185.08
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 18116
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.48486804962158
time_total_s: 10536.592316150665
timers:
  learn_throughput: 435.87
  learn_time_ms: 37855.342
  load_throughput: 4701176.279
  load_time_ms: 3.51
  training_iteration_time_ms: 49699.445
  update_time_ms: 2.752
timesteps_total: 3300000
training_iteration: 200

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2564102564102564
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 174.52
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 17379
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.232404947280884
time_total_s: 10533.525304079056
timers:
  learn_throughput: 427.905
  learn_time_ms: 38559.973
  load_throughput: 4513916.656
  load_time_ms: 3.655
  training_iteration_time_ms: 50916.952
  update_time_ms: 2.696
timesteps_total: 3184500
training_iteration: 193

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.13815789473684212
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9556962025316456
  reward for individual goal_min: 0.0
episode_len_mean: 195.61
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 18237
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.55195069313049
time_total_s: 10532.006893873215
timers:
  learn_throughput: 495.04
  learn_time_ms: 33330.661
  load_throughput: 5084079.546
  load_time_ms: 3.245
  training_iteration_time_ms: 44108.539
  update_time_ms: 2.453
timesteps_total: 3679500
training_iteration: 223

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.55778894472362
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 398
episodes_total: 64542
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.19725847244263
time_total_s: 10422.120544195175
timers:
  learn_throughput: 447.915
  learn_time_ms: 36837.359
  load_throughput: 4347166.17
  load_time_ms: 3.796
  training_iteration_time_ms: 48440.03
  update_time_ms: 2.4
timesteps_total: 3679500
training_iteration: 223

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22560975609756098
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.918918918918919
  reward for individual goal_min: 0.0
episode_len_mean: 205.17
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 16972
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.05712676048279
time_total_s: 10547.29426407814
timers:
  learn_throughput: 447.269
  learn_time_ms: 36890.541
  load_throughput: 4596665.449
  load_time_ms: 3.59
  training_iteration_time_ms: 48472.183
  update_time_ms: 2.593
timesteps_total: 3366000
training_iteration: 204

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2534246575342466
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.17
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 18584
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.98058319091797
time_total_s: 10551.013597011566
timers:
  learn_throughput: 444.598
  learn_time_ms: 37112.154
  load_throughput: 4571977.01
  load_time_ms: 3.609
  training_iteration_time_ms: 48839.718
  update_time_ms: 2.713
timesteps_total: 3448500
training_iteration: 209

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972527472527473
  reward for individual goal_min: 0.5
episode_len_mean: 46.157303370786515
episode_reward_max: 2.0
episode_reward_mean: 1.997191011235955
episode_reward_min: 1.0
episodes_this_iter: 356
episodes_total: 44214
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9971910112359551
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 55.63244962692261
time_total_s: 10511.303622484207
timers:
  learn_throughput: 387.307
  learn_time_ms: 42601.869
  load_throughput: 4006902.389
  load_time_ms: 4.118
  training_iteration_time_ms: 55284.812
  update_time_ms: 2.879
timesteps_total: 3399000
training_iteration: 206

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1554054054054054
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.36
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 17400
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.2540009021759
time_total_s: 10546.080163240433
timers:
  learn_throughput: 456.471
  learn_time_ms: 36146.912
  load_throughput: 4662315.73
  load_time_ms: 3.539
  training_iteration_time_ms: 47843.09
  update_time_ms: 2.62
timesteps_total: 3349500
training_iteration: 203

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2710843373493976
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9779411764705882
  reward for individual goal_min: 0.0
episode_len_mean: 184.84
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 17354
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.242934703826904
time_total_s: 10556.695282459259
timers:
  learn_throughput: 420.059
  learn_time_ms: 39280.194
  load_throughput: 4524422.304
  load_time_ms: 3.647
  training_iteration_time_ms: 51647.425
  update_time_ms: 2.678
timesteps_total: 3201000
training_iteration: 194

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2898550724637681
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.831081081081081
  reward for individual goal_min: 0.0
episode_len_mean: 180.75
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 19084
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.16001892089844
time_total_s: 10579.418466567993
timers:
  learn_throughput: 507.474
  learn_time_ms: 32513.967
  load_throughput: 4656167.607
  load_time_ms: 3.544
  training_iteration_time_ms: 43495.044
  update_time_ms: 2.568
timesteps_total: 3828000
training_iteration: 232

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2905405405405405
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 164.77227722772278
episode_reward_max: 2.0
episode_reward_mean: 1.3663366336633664
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 18939
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6831683168316832
  agent_1: 0.6831683168316832
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.17927098274231
time_total_s: 10555.283615589142
timers:
  learn_throughput: 459.323
  learn_time_ms: 35922.434
  load_throughput: 4828001.088
  load_time_ms: 3.418
  training_iteration_time_ms: 47151.233
  update_time_ms: 2.708
timesteps_total: 3448500
training_iteration: 209

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2948717948717949
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 176.02
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 21713
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.028446435928345
time_total_s: 10564.106294631958
timers:
  learn_throughput: 521.311
  learn_time_ms: 31650.989
  load_throughput: 5169295.857
  load_time_ms: 3.192
  training_iteration_time_ms: 42038.344
  update_time_ms: 2.552
timesteps_total: 3943500
training_iteration: 239

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.61477572559367
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 379
episodes_total: 47877
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.534202575683594
time_total_s: 10510.592499732971
timers:
  learn_throughput: 360.204
  learn_time_ms: 45807.359
  load_throughput: 3521038.718
  load_time_ms: 4.686
  training_iteration_time_ms: 58996.997
  update_time_ms: 2.615
timesteps_total: 3085500
training_iteration: 187

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9105691056910569
  reward for individual goal_min: 0.5
episode_len_mean: 66.66532258064517
episode_reward_max: 2.0
episode_reward_mean: 1.9112903225806452
episode_reward_min: 1.0
episodes_this_iter: 248
episodes_total: 29827
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9919354838709677
  agent_1: 0.9193548387096774
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.38228106498718
time_total_s: 10577.302980422974
timers:
  learn_throughput: 331.013
  learn_time_ms: 49846.91
  load_throughput: 3725941.822
  load_time_ms: 4.428
  training_iteration_time_ms: 65157.394
  update_time_ms: 3.007
timesteps_total: 2871000
training_iteration: 174

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 186.94
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 18204
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.61578416824341
time_total_s: 10584.208100318909
timers:
  learn_throughput: 438.273
  learn_time_ms: 37647.786
  load_throughput: 4855404.044
  load_time_ms: 3.398
  training_iteration_time_ms: 49365.401
  update_time_ms: 2.75
timesteps_total: 3316500
training_iteration: 201

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21621621621621623
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8987341772151899
  reward for individual goal_min: 0.0
episode_len_mean: 194.03
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 18325
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.12152433395386
time_total_s: 10578.128418207169
timers:
  learn_throughput: 487.939
  learn_time_ms: 33815.682
  load_throughput: 5070409.261
  load_time_ms: 3.254
  training_iteration_time_ms: 44725.97
  update_time_ms: 2.473
timesteps_total: 3696000
training_iteration: 224

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2236842105263158
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.87
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 17476
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.51634383201599
time_total_s: 10584.041647911072
timers:
  learn_throughput: 427.04
  learn_time_ms: 38638.037
  load_throughput: 4500589.578
  load_time_ms: 3.666
  training_iteration_time_ms: 51018.688
  update_time_ms: 2.723
timesteps_total: 3201000
training_iteration: 194

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.303664921465966
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 382
episodes_total: 64924
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.45763945579529
time_total_s: 10470.57818365097
timers:
  learn_throughput: 448.584
  learn_time_ms: 36782.379
  load_throughput: 4338309.585
  load_time_ms: 3.803
  training_iteration_time_ms: 48413.84
  update_time_ms: 2.379
timesteps_total: 3696000
training_iteration: 224

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2654320987654321
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9256756756756757
  reward for individual goal_min: 0.0
episode_len_mean: 201.92
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 17053
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.24693989753723
time_total_s: 10594.541203975677
timers:
  learn_throughput: 449.766
  learn_time_ms: 36685.71
  load_throughput: 4627802.921
  load_time_ms: 3.565
  training_iteration_time_ms: 48308.931
  update_time_ms: 2.609
timesteps_total: 3382500
training_iteration: 205

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2898550724637681
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 165.06
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 18677
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.12991118431091
time_total_s: 10600.143508195877
timers:
  learn_throughput: 443.441
  learn_time_ms: 37208.989
  load_throughput: 4568385.559
  load_time_ms: 3.612
  training_iteration_time_ms: 48887.179
  update_time_ms: 2.704
timesteps_total: 3465000
training_iteration: 210

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.13513513513513514
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 191.0
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 17487
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.53761005401611
time_total_s: 10593.617773294449
timers:
  learn_throughput: 457.783
  learn_time_ms: 36043.249
  load_throughput: 4681332.846
  load_time_ms: 3.525
  training_iteration_time_ms: 47645.722
  update_time_ms: 2.628
timesteps_total: 3366000
training_iteration: 204

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1518987341772152
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8618421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 211.89
episode_reward_max: 2.0
episode_reward_mean: 1.05
episode_reward_min: 0.0
episodes_this_iter: 75
episodes_total: 19159
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.3076114654541
time_total_s: 10622.726078033447
timers:
  learn_throughput: 506.964
  learn_time_ms: 32546.704
  load_throughput: 4641677.297
  load_time_ms: 3.555
  training_iteration_time_ms: 43482.586
  update_time_ms: 2.565
timesteps_total: 3844500
training_iteration: 233

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9945652173913043
  reward for individual goal_min: 0.5
episode_len_mean: 45.8941504178273
episode_reward_max: 2.0
episode_reward_mean: 1.9944289693593316
episode_reward_min: 1.0
episodes_this_iter: 359
episodes_total: 44573
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972144846796658
  agent_1: 0.9972144846796658
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.55384683609009
time_total_s: 10565.857469320297
timers:
  learn_throughput: 388.253
  learn_time_ms: 42498.116
  load_throughput: 4019119.123
  load_time_ms: 4.105
  training_iteration_time_ms: 55189.542
  update_time_ms: 2.864
timesteps_total: 3415500
training_iteration: 207

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17692307692307693
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9761904761904762
  reward for individual goal_min: 0.0
episode_len_mean: 173.72
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 19032
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.59095859527588
time_total_s: 10599.874574184418
timers:
  learn_throughput: 463.34
  learn_time_ms: 35611.019
  load_throughput: 4825341.719
  load_time_ms: 3.419
  training_iteration_time_ms: 46788.305
  update_time_ms: 2.704
timesteps_total: 3465000
training_iteration: 210

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21428571428571427
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.967948717948718
  reward for individual goal_min: 0.0
episode_len_mean: 171.63
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 17448
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.7568724155426
time_total_s: 10609.452154874802
timers:
  learn_throughput: 418.027
  learn_time_ms: 39471.126
  load_throughput: 4473736.279
  load_time_ms: 3.688
  training_iteration_time_ms: 51869.983
  update_time_ms: 2.659
timesteps_total: 3217500
training_iteration: 195

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9166666666666666
  reward for individual goal_min: 0.5
episode_len_mean: 84.3
episode_reward_max: 2.0
episode_reward_mean: 1.8833333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9333333333333333
  agent_1: 0.95
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2462686567164179
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 160.80769230769232
episode_reward_max: 2.0
episode_reward_mean: 1.4038461538461537
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 21817
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7211538461538461
  agent_1: 0.6826923076923077
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.90185189247131
time_total_s: 10615.00814652443
timers:
  learn_throughput: 519.44
  learn_time_ms: 31764.967
  load_throughput: 5164859.322
  load_time_ms: 3.195
  training_iteration_time_ms: 42153.177
  update_time_ms: 2.539
timesteps_total: 3960000
training_iteration: 240

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21428571428571427
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 174.18
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 18301
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.01252627372742
time_total_s: 10631.220626592636
timers:
  learn_throughput: 440.103
  learn_time_ms: 37491.225
  load_throughput: 4885601.858
  load_time_ms: 3.377
  training_iteration_time_ms: 49121.196
  update_time_ms: 2.73
timesteps_total: 3333000
training_iteration: 202

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2222222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.881578947368421
  reward for individual goal_min: 0.0
episode_len_mean: 195.66
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 18407
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.050721168518066
time_total_s: 10623.179139375687
timers:
  learn_throughput: 486.328
  learn_time_ms: 33927.71
  load_throughput: 5059511.054
  load_time_ms: 3.261
  training_iteration_time_ms: 44815.841
  update_time_ms: 2.504
timesteps_total: 3712500
training_iteration: 225

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972826086956522
  reward for individual goal_min: 0.5
episode_len_mean: 44.306451612903224
episode_reward_max: 2.0
episode_reward_mean: 1.9973118279569892
episode_reward_min: 1.0
episodes_this_iter: 372
episodes_total: 48249
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973118279569892
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 58.935656785964966
time_total_s: 10569.528156518936
timers:
  learn_throughput: 360.908
  learn_time_ms: 45718.046
  load_throughput: 3505806.164
  load_time_ms: 4.706
  training_iteration_time_ms: 58921.27
  update_time_ms: 2.613
timesteps_total: 3102000
training_iteration: 188

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24691358024691357
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8698630136986302
  reward for individual goal_min: 0.0
episode_len_mean: 204.77
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 17133
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.720476150512695
time_total_s: 10643.26168012619
timers:
  learn_throughput: 451.659
  learn_time_ms: 36532.008
  load_throughput: 4639685.709
  load_time_ms: 3.556
  training_iteration_time_ms: 48239.51
  update_time_ms: 2.597
timesteps_total: 3399000
training_iteration: 206

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22535211267605634
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 168.43
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 17573
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.70781421661377
time_total_s: 10636.749462127686
timers:
  learn_throughput: 427.827
  learn_time_ms: 38567.026
  load_throughput: 4481239.097
  load_time_ms: 3.682
  training_iteration_time_ms: 50897.743
  update_time_ms: 2.724
timesteps_total: 3217500
training_iteration: 195

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972972972972973
  reward for individual goal_min: 0.5
episode_len_mean: 41.67258883248731
episode_reward_max: 2.0
episode_reward_mean: 1.99746192893401
episode_reward_min: 1.0
episodes_this_iter: 394
episodes_total: 65318
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974619289340102
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 49.11526131629944
time_total_s: 10519.69344496727
timers:
  learn_throughput: 447.774
  learn_time_ms: 36848.941
  load_throughput: 4306213.351
  load_time_ms: 3.832
  training_iteration_time_ms: 48486.327
  update_time_ms: 2.408
timesteps_total: 3712500
training_iteration: 225

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9471830985915493
  reward for individual goal_min: 0.5
episode_len_mean: 58.024647887323944
episode_reward_max: 2.0
episode_reward_mean: 1.9471830985915493
episode_reward_min: 1.0
episodes_this_iter: 284
episodes_total: 30111
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9471830985915493
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 63.843637466430664
time_total_s: 10641.146617889404
timers:
  learn_throughput: 332.994
  learn_time_ms: 49550.448
  load_throughput: 3627663.034
  load_time_ms: 4.548
  training_iteration_time_ms: 64789.83
  update_time_ms: 2.893
timesteps_total: 2887500
training_iteration: 175

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 185.42
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 18769
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.76
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.37410879135132
time_total_s: 10647.517616987228
timers:
  learn_throughput: 445.45
  learn_time_ms: 37041.228
  load_throughput: 4598131.407
  load_time_ms: 3.588
  training_iteration_time_ms: 48665.824
  update_time_ms: 2.683
timesteps_total: 3481500
training_iteration: 211

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16901408450704225
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.64
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 17579
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.14758324623108
time_total_s: 10640.76535654068
timers:
  learn_throughput: 458.488
  learn_time_ms: 35987.863
  load_throughput: 4688246.261
  load_time_ms: 3.519
  training_iteration_time_ms: 47504.949
  update_time_ms: 2.621
timesteps_total: 3382500
training_iteration: 205

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19078947368421054
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9178082191780822
  reward for individual goal_min: 0.0
episode_len_mean: 205.34
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 19242
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.50210475921631
time_total_s: 10667.228182792664
timers:
  learn_throughput: 503.722
  learn_time_ms: 32756.14
  load_throughput: 4662535.606
  load_time_ms: 3.539
  training_iteration_time_ms: 43747.474
  update_time_ms: 2.588
timesteps_total: 3861000
training_iteration: 234

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3291139240506329
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.85
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 19128
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.346336126327515
time_total_s: 10649.220910310745
timers:
  learn_throughput: 459.009
  learn_time_ms: 35947.045
  load_throughput: 4858233.077
  load_time_ms: 3.396
  training_iteration_time_ms: 47216.411
  update_time_ms: 2.649
timesteps_total: 3481500
training_iteration: 211

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33544303797468356
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.61
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 21912
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.50822329521179
time_total_s: 10656.516369819641
timers:
  learn_throughput: 517.48
  learn_time_ms: 31885.304
  load_throughput: 5123449.292
  load_time_ms: 3.22
  training_iteration_time_ms: 42343.884
  update_time_ms: 2.543
timesteps_total: 3976500
training_iteration: 241

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9916201117318436
  reward for individual goal_min: 0.5
episode_len_mean: 48.47953216374269
episode_reward_max: 2.0
episode_reward_mean: 1.9912280701754386
episode_reward_min: 1.0
episodes_this_iter: 342
episodes_total: 44915
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9912280701754386
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 55.08386301994324
time_total_s: 10620.94133234024
timers:
  learn_throughput: 388.188
  learn_time_ms: 42505.157
  load_throughput: 4013385.449
  load_time_ms: 4.111
  training_iteration_time_ms: 55096.063
  update_time_ms: 2.865
timesteps_total: 3432000
training_iteration: 208

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2391304347826087
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 167.2
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 17548
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.87054133415222
time_total_s: 10662.322696208954
timers:
  learn_throughput: 417.024
  learn_time_ms: 39566.073
  load_throughput: 4431482.304
  load_time_ms: 3.723
  training_iteration_time_ms: 51974.198
  update_time_ms: 2.65
timesteps_total: 3234000
training_iteration: 196

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9875
  reward for individual goal_min: 0.0
episode_len_mean: 171.26
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 18396
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.97724676132202
time_total_s: 10678.197873353958
timers:
  learn_throughput: 445.634
  learn_time_ms: 37025.924
  load_throughput: 4867972.37
  load_time_ms: 3.39
  training_iteration_time_ms: 48524.071
  update_time_ms: 2.712
timesteps_total: 3349500
training_iteration: 203

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2714285714285714
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9013157894736842
  reward for individual goal_min: 0.0
episode_len_mean: 173.68
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 18501
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.45491909980774
time_total_s: 10669.634058475494
timers:
  learn_throughput: 482.423
  learn_time_ms: 34202.325
  load_throughput: 5052013.403
  load_time_ms: 3.266
  training_iteration_time_ms: 45161.316
  update_time_ms: 2.502
timesteps_total: 3729000
training_iteration: 226

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3716216216216216
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9214285714285714
  reward for individual goal_min: 0.0
episode_len_mean: 168.72
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 17231
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.94906687736511
time_total_s: 10690.210747003555
timers:
  learn_throughput: 452.661
  learn_time_ms: 36451.117
  load_throughput: 4653819.297
  load_time_ms: 3.545
  training_iteration_time_ms: 48146.296
  update_time_ms: 2.593
timesteps_total: 3415500
training_iteration: 207

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.3
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 390
episodes_total: 65708
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.22226285934448
time_total_s: 10567.915707826614
timers:
  learn_throughput: 448.561
  learn_time_ms: 36784.266
  load_throughput: 4263606.994
  load_time_ms: 3.87
  training_iteration_time_ms: 48409.902
  update_time_ms: 2.411
timesteps_total: 3729000
training_iteration: 226

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19285714285714287
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.51
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 17668
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.48407983779907
time_total_s: 10689.233541965485
timers:
  learn_throughput: 425.125
  learn_time_ms: 38812.106
  load_throughput: 4481732.441
  load_time_ms: 3.682
  training_iteration_time_ms: 51146.126
  update_time_ms: 2.714
timesteps_total: 3234000
training_iteration: 196

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2054794520547945
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 180.3
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 18859
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.873265743255615
time_total_s: 10696.390882730484
timers:
  learn_throughput: 447.118
  learn_time_ms: 36903.016
  load_throughput: 4630279.934
  load_time_ms: 3.563
  training_iteration_time_ms: 48526.349
  update_time_ms: 2.672
timesteps_total: 3498000
training_iteration: 212

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.80569948186528
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 386
episodes_total: 48635
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.235135555267334
time_total_s: 10628.763292074203
timers:
  learn_throughput: 360.908
  learn_time_ms: 45717.968
  load_throughput: 3484658.563
  load_time_ms: 4.735
  training_iteration_time_ms: 58890.23
  update_time_ms: 2.649
timesteps_total: 3118500
training_iteration: 189

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22916666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8701298701298701
  reward for individual goal_min: 0.0
episode_len_mean: 198.38
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 19326
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.63704252243042
time_total_s: 10709.865225315094
timers:
  learn_throughput: 501.285
  learn_time_ms: 32915.409
  load_throughput: 4663729.581
  load_time_ms: 3.538
  training_iteration_time_ms: 43906.699
  update_time_ms: 2.602
timesteps_total: 3877500
training_iteration: 235

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1619718309859155
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.01
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 17669
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.339112520217896
time_total_s: 10687.104469060898
timers:
  learn_throughput: 456.507
  learn_time_ms: 36144.022
  load_throughput: 4697602.259
  load_time_ms: 3.512
  training_iteration_time_ms: 47793.028
  update_time_ms: 2.604
timesteps_total: 3399000
training_iteration: 206

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9598540145985401
  reward for individual goal_min: 0.5
episode_len_mean: 54.478547854785475
episode_reward_max: 2.0
episode_reward_mean: 1.9636963696369636
episode_reward_min: 1.0
episodes_this_iter: 303
episodes_total: 30414
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9636963696369637
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 65.12335014343262
time_total_s: 10706.269968032837
timers:
  learn_throughput: 334.443
  learn_time_ms: 49335.74
  load_throughput: 3590920.534
  load_time_ms: 4.595
  training_iteration_time_ms: 64579.402
  update_time_ms: 2.821
timesteps_total: 2904000
training_iteration: 176

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939024390243902
  reward for individual goal_min: 0.5
episode_len_mean: 192.02
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 21996
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.16434597969055
time_total_s: 10697.680715799332
timers:
  learn_throughput: 516.291
  learn_time_ms: 31958.702
  load_throughput: 5091073.446
  load_time_ms: 3.241
  training_iteration_time_ms: 42439.691
  update_time_ms: 2.55
timesteps_total: 3993000
training_iteration: 242

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2631578947368421
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 176.62
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 19223
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.57354211807251
time_total_s: 10694.794452428818
timers:
  learn_throughput: 458.375
  learn_time_ms: 35996.735
  load_throughput: 4859154.075
  load_time_ms: 3.396
  training_iteration_time_ms: 47303.085
  update_time_ms: 2.661
timesteps_total: 3498000
training_iteration: 212

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19014084507042253
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9329268292682927
  reward for individual goal_min: 0.0
episode_len_mean: 188.47
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 18587
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.829813957214355
time_total_s: 10713.463872432709
timers:
  learn_throughput: 481.933
  learn_time_ms: 34237.134
  load_throughput: 5014747.002
  load_time_ms: 3.29
  training_iteration_time_ms: 45231.888
  update_time_ms: 2.505
timesteps_total: 3745500
training_iteration: 227

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18452380952380953
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9726027397260274
  reward for individual goal_min: 0.0
episode_len_mean: 199.65
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 18479
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.099955558776855
time_total_s: 10725.297828912735
timers:
  learn_throughput: 448.896
  learn_time_ms: 36756.858
  load_throughput: 4948624.302
  load_time_ms: 3.334
  training_iteration_time_ms: 48231.16
  update_time_ms: 2.683
timesteps_total: 3366000
training_iteration: 204

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9969325153374233
  reward for individual goal_min: 0.5
episode_len_mean: 45.986072423398326
episode_reward_max: 2.0
episode_reward_mean: 1.9972144846796658
episode_reward_min: 1.0
episodes_this_iter: 359
episodes_total: 45274
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972144846796658
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 53.76753067970276
time_total_s: 10674.708863019943
timers:
  learn_throughput: 388.445
  learn_time_ms: 42477.079
  load_throughput: 4031246.199
  load_time_ms: 4.093
  training_iteration_time_ms: 55067.415
  update_time_ms: 2.857
timesteps_total: 3448500
training_iteration: 209

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.85
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 17641
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.01100468635559
time_total_s: 10718.33370089531
timers:
  learn_throughput: 413.428
  learn_time_ms: 39910.243
  load_throughput: 4445601.742
  load_time_ms: 3.712
  training_iteration_time_ms: 52376.732
  update_time_ms: 2.649
timesteps_total: 3250500
training_iteration: 197

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3088235294117647
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9178082191780822
  reward for individual goal_min: 0.0
episode_len_mean: 172.62
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 17324
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.38724422454834
time_total_s: 10736.597991228104
timers:
  learn_throughput: 456.215
  learn_time_ms: 36167.126
  load_throughput: 4658518.289
  load_time_ms: 3.542
  training_iteration_time_ms: 47803.059
  update_time_ms: 2.59
timesteps_total: 3432000
training_iteration: 208

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9941176470588236
  reward for individual goal_min: 0.5
episode_len_mean: 43.386842105263156
episode_reward_max: 2.0
episode_reward_mean: 1.9947368421052631
episode_reward_min: 1.0
episodes_this_iter: 380
episodes_total: 66088
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973684210526316
  agent_1: 0.9973684210526316
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.29936718940735
time_total_s: 10616.215075016022
timers:
  learn_throughput: 448.759
  learn_time_ms: 36768.055
  load_throughput: 4267524.373
  load_time_ms: 3.866
  training_iteration_time_ms: 48401.365
  update_time_ms: 2.421
timesteps_total: 3745500
training_iteration: 227

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2328767123287671
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 178.87
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 18952
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.55314922332764
time_total_s: 10742.944031953812
timers:
  learn_throughput: 449.237
  learn_time_ms: 36728.963
  load_throughput: 4621621.968
  load_time_ms: 3.57
  training_iteration_time_ms: 48372.991
  update_time_ms: 2.655
timesteps_total: 3514500
training_iteration: 213

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26153846153846155
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.85625
  reward for individual goal_min: 0.0
episode_len_mean: 188.86
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 19414
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.76
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.24862003326416
time_total_s: 10754.113845348358
timers:
  learn_throughput: 502.486
  learn_time_ms: 32836.744
  load_throughput: 4675545.106
  load_time_ms: 3.529
  training_iteration_time_ms: 43773.582
  update_time_ms: 2.604
timesteps_total: 3894000
training_iteration: 236

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20833333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.0
episode_len_mean: 171.03
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 17763
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.65438365936279
time_total_s: 10739.887925624847
timers:
  learn_throughput: 424.029
  learn_time_ms: 38912.472
  load_throughput: 4472030.655
  load_time_ms: 3.69
  training_iteration_time_ms: 51281.872
  update_time_ms: 2.7
timesteps_total: 3250500
training_iteration: 197

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23076923076923078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 184.73
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 17758
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.197551012039185
time_total_s: 10733.302020072937
timers:
  learn_throughput: 460.173
  learn_time_ms: 35856.086
  load_throughput: 4688849.773
  load_time_ms: 3.519
  training_iteration_time_ms: 47514.882
  update_time_ms: 2.597
timesteps_total: 3415500
training_iteration: 207

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2974683544303797
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.54
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 22090
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.76
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.26019263267517
time_total_s: 10739.940908432007
timers:
  learn_throughput: 516.051
  learn_time_ms: 31973.612
  load_throughput: 5075950.448
  load_time_ms: 3.251
  training_iteration_time_ms: 42471.379
  update_time_ms: 2.855
timesteps_total: 4009500
training_iteration: 243

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.887978142076506
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 366
episodes_total: 49001
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.56044411659241
time_total_s: 10686.323736190796
timers:
  learn_throughput: 361.739
  learn_time_ms: 45613.037
  load_throughput: 3512532.153
  load_time_ms: 4.697
  training_iteration_time_ms: 58801.957
  update_time_ms: 2.651
timesteps_total: 3135000
training_iteration: 190

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3271604938271605
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.38
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 19317
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.31668782234192
time_total_s: 10741.11114025116
timers:
  learn_throughput: 459.112
  learn_time_ms: 35938.96
  load_throughput: 4863114.934
  load_time_ms: 3.393
  training_iteration_time_ms: 47253.767
  update_time_ms: 2.643
timesteps_total: 3514500
training_iteration: 213

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20714285714285716
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9391891891891891
  reward for individual goal_min: 0.0
episode_len_mean: 194.67
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 18670
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.56416940689087
time_total_s: 10759.0280418396
timers:
  learn_throughput: 479.158
  learn_time_ms: 34435.394
  load_throughput: 5037597.886
  load_time_ms: 3.275
  training_iteration_time_ms: 45431.822
  update_time_ms: 2.505
timesteps_total: 3762000
training_iteration: 228

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19230769230769232
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935897435897436
  reward for individual goal_min: 0.5
episode_len_mean: 187.67
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 18567
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.18473196029663
time_total_s: 10773.482560873032
timers:
  learn_throughput: 456.071
  learn_time_ms: 36178.593
  load_throughput: 4961609.372
  load_time_ms: 3.326
  training_iteration_time_ms: 47583.337
  update_time_ms: 2.68
timesteps_total: 3382500
training_iteration: 205

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9444444444444444
  reward for individual goal_min: 0.5
episode_len_mean: 56.45360824742268
episode_reward_max: 2.0
episode_reward_mean: 1.9518900343642611
episode_reward_min: 1.0
episodes_this_iter: 291
episodes_total: 30705
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9518900343642611
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 64.04617428779602
time_total_s: 10770.316142320633
timers:
  learn_throughput: 336.409
  learn_time_ms: 49047.497
  load_throughput: 3648201.414
  load_time_ms: 4.523
  training_iteration_time_ms: 64075.061
  update_time_ms: 2.808
timesteps_total: 2920500
training_iteration: 177

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2647058823529412
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.975609756097561
  reward for individual goal_min: 0.5
episode_len_mean: 174.52
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 17420
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.05193567276001
time_total_s: 10781.649926900864
timers:
  learn_throughput: 458.78
  learn_time_ms: 35964.93
  load_throughput: 4666182.289
  load_time_ms: 3.536
  training_iteration_time_ms: 47549.927
  update_time_ms: 2.576
timesteps_total: 3448500
training_iteration: 209

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9884393063583815
  reward for individual goal_min: 0.5
episode_len_mean: 46.98860398860399
episode_reward_max: 2.0
episode_reward_mean: 1.9886039886039886
episode_reward_min: 1.0
episodes_this_iter: 351
episodes_total: 45625
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9914529914529915
  agent_1: 0.9971509971509972
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.426995038986206
time_total_s: 10729.13585805893
timers:
  learn_throughput: 389.229
  learn_time_ms: 42391.487
  load_throughput: 4084926.985
  load_time_ms: 4.039
  training_iteration_time_ms: 54971.374
  update_time_ms: 2.83
timesteps_total: 3465000
training_iteration: 210

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2361111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 168.51
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 17734
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.11030316352844
time_total_s: 10772.444004058838
timers:
  learn_throughput: 411.942
  learn_time_ms: 40054.169
  load_throughput: 4365346.201
  load_time_ms: 3.78
  training_iteration_time_ms: 52496.269
  update_time_ms: 2.65
timesteps_total: 3267000
training_iteration: 198

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30158730158730157
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8819444444444444
  reward for individual goal_min: 0.0
episode_len_mean: 181.74
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 19505
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.484561920166016
time_total_s: 10795.598407268524
timers:
  learn_throughput: 504.059
  learn_time_ms: 32734.237
  load_throughput: 4680604.638
  load_time_ms: 3.525
  training_iteration_time_ms: 43610.953
  update_time_ms: 2.594
timesteps_total: 3910500
training_iteration: 237

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971751412429378
  reward for individual goal_min: 0.5
episode_len_mean: 44.700272479564035
episode_reward_max: 2.0
episode_reward_mean: 1.997275204359673
episode_reward_min: 1.0
episodes_this_iter: 367
episodes_total: 66455
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.997275204359673
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 48.18929886817932
time_total_s: 10664.404373884201
timers:
  learn_throughput: 449.305
  learn_time_ms: 36723.4
  load_throughput: 4275856.24
  load_time_ms: 3.859
  training_iteration_time_ms: 48372.401
  update_time_ms: 2.427
timesteps_total: 3762000
training_iteration: 228

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.323943661971831
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 163.67961165048544
episode_reward_max: 2.0
episode_reward_mean: 1.4466019417475728
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 19055
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7184466019417476
  agent_1: 0.7281553398058253
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.766966819763184
time_total_s: 10790.710998773575
timers:
  learn_throughput: 451.584
  learn_time_ms: 36538.02
  load_throughput: 4633876.316
  load_time_ms: 3.561
  training_iteration_time_ms: 48050.31
  update_time_ms: 2.643
timesteps_total: 3531000
training_iteration: 214

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2012987012987013
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.99375
  reward for individual goal_min: 0.5
episode_len_mean: 185.75
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 22179
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.94606971740723
time_total_s: 10781.886978149414
timers:
  learn_throughput: 519.132
  learn_time_ms: 31783.846
  load_throughput: 5075615.402
  load_time_ms: 3.251
  training_iteration_time_ms: 42227.038
  update_time_ms: 2.852
timesteps_total: 4026000
training_iteration: 244

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 173.39
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 17852
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.54766392707825
time_total_s: 10780.849684000015
timers:
  learn_throughput: 459.103
  learn_time_ms: 35939.678
  load_throughput: 4725185.782
  load_time_ms: 3.492
  training_iteration_time_ms: 47645.255
  update_time_ms: 2.591
timesteps_total: 3432000
training_iteration: 208

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2012987012987013
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9928571428571429
  reward for individual goal_min: 0.5
episode_len_mean: 184.58
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 17852
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.11534333229065
time_total_s: 10796.003268957138
timers:
  learn_throughput: 419.512
  learn_time_ms: 39331.434
  load_throughput: 4484491.359
  load_time_ms: 3.679
  training_iteration_time_ms: 51842.345
  update_time_ms: 2.695
timesteps_total: 3267000
training_iteration: 198

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 162.12871287128712
episode_reward_max: 2.0
episode_reward_mean: 1.4059405940594059
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 19418
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7029702970297029
  agent_1: 0.7029702970297029
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.05102324485779
time_total_s: 10786.162163496017
timers:
  learn_throughput: 462.074
  learn_time_ms: 35708.55
  load_throughput: 4854007.785
  load_time_ms: 3.399
  training_iteration_time_ms: 46993.595
  update_time_ms: 2.611
timesteps_total: 3531000
training_iteration: 214

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2077922077922078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9357142857142857
  reward for individual goal_min: 0.0
episode_len_mean: 198.97
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 18752
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.05430555343628
time_total_s: 10803.082347393036
timers:
  learn_throughput: 482.015
  learn_time_ms: 34231.299
  load_throughput: 5063990.692
  load_time_ms: 3.258
  training_iteration_time_ms: 45246.06
  update_time_ms: 2.521
timesteps_total: 3778500
training_iteration: 229

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.597938144329895
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 388
episodes_total: 49389
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.39251947402954
time_total_s: 10744.716255664825
timers:
  learn_throughput: 361.494
  learn_time_ms: 45643.94
  load_throughput: 3502505.997
  load_time_ms: 4.711
  training_iteration_time_ms: 58833.262
  update_time_ms: 2.663
timesteps_total: 3151500
training_iteration: 191

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18666666666666668
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.5
episode_len_mean: 189.7
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 18655
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.65981721878052
time_total_s: 10822.142378091812
timers:
  learn_throughput: 456.404
  learn_time_ms: 36152.146
  load_throughput: 4968163.16
  load_time_ms: 3.321
  training_iteration_time_ms: 47580.846
  update_time_ms: 2.675
timesteps_total: 3399000
training_iteration: 206

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2867647058823529
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9473684210526315
  reward for individual goal_min: 0.0
episode_len_mean: 185.77
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 17508
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.628145933151245
time_total_s: 10827.278072834015
timers:
  learn_throughput: 464.427
  learn_time_ms: 35527.66
  load_throughput: 4692601.387
  load_time_ms: 3.516
  training_iteration_time_ms: 46972.727
  update_time_ms: 2.569
timesteps_total: 3465000
training_iteration: 210

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8589743589743589
  reward for individual goal_min: 0.0
episode_len_mean: 198.68
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 19586
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.62153434753418
time_total_s: 10840.219941616058
timers:
  learn_throughput: 503.615
  learn_time_ms: 32763.104
  load_throughput: 4664075.32
  load_time_ms: 3.538
  training_iteration_time_ms: 43678.093
  update_time_ms: 2.587
timesteps_total: 3927000
training_iteration: 238

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9883040935672515
  reward for individual goal_min: 0.5
episode_len_mean: 47.62390670553936
episode_reward_max: 2.0
episode_reward_mean: 1.9883381924198251
episode_reward_min: 1.0
episodes_this_iter: 343
episodes_total: 45968
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9912536443148688
  agent_1: 0.9970845481049563
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.451544761657715
time_total_s: 10782.587402820587
timers:
  learn_throughput: 390.971
  learn_time_ms: 42202.593
  load_throughput: 4092536.265
  load_time_ms: 4.032
  training_iteration_time_ms: 54747.255
  update_time_ms: 2.825
timesteps_total: 3481500
training_iteration: 211

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 183.49
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 17824
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.78822708129883
time_total_s: 10825.232231140137
timers:
  learn_throughput: 410.392
  learn_time_ms: 40205.509
  load_throughput: 4340622.436
  load_time_ms: 3.801
  training_iteration_time_ms: 52706.977
  update_time_ms: 2.639
timesteps_total: 3283500
training_iteration: 199

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2222222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 177.55
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 19146
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.57429313659668
time_total_s: 10836.285291910172
timers:
  learn_throughput: 454.478
  learn_time_ms: 36305.385
  load_throughput: 4760877.246
  load_time_ms: 3.466
  training_iteration_time_ms: 47733.359
  update_time_ms: 2.649
timesteps_total: 3547500
training_iteration: 215

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.325
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.49
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 22275
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.48827886581421
time_total_s: 10824.375257015228
timers:
  learn_throughput: 518.455
  learn_time_ms: 31825.316
  load_throughput: 5111793.478
  load_time_ms: 3.228
  training_iteration_time_ms: 42285.948
  update_time_ms: 2.838
timesteps_total: 4042500
training_iteration: 245

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973118279569892
  reward for individual goal_min: 0.5
episode_len_mean: 44.36266666666667
episode_reward_max: 2.0
episode_reward_mean: 1.9973333333333334
episode_reward_min: 1.0
episodes_this_iter: 375
episodes_total: 66830
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973333333333333
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 47.64453053474426
time_total_s: 10712.048904418945
timers:
  learn_throughput: 450.519
  learn_time_ms: 36624.39
  load_throughput: 4280643.278
  load_time_ms: 3.855
  training_iteration_time_ms: 48245.377
  update_time_ms: 2.432
timesteps_total: 3778500
training_iteration: 229

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9606299212598425
  reward for individual goal_min: 0.5
episode_len_mean: 57.69122807017544
episode_reward_max: 2.0
episode_reward_mean: 1.9649122807017543
episode_reward_min: 1.0
episodes_this_iter: 285
episodes_total: 30990
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9929824561403509
  agent_1: 0.9719298245614035
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 64.33619737625122
time_total_s: 10834.652339696884
timers:
  learn_throughput: 336.921
  learn_time_ms: 48972.857
  load_throughput: 3611759.954
  load_time_ms: 4.568
  training_iteration_time_ms: 63930.191
  update_time_ms: 2.815
timesteps_total: 2937000
training_iteration: 178

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19594594594594594
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 180.25
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 17945
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.939899921417236
time_total_s: 10827.789583921432
timers:
  learn_throughput: 462.271
  learn_time_ms: 35693.377
  load_throughput: 4706547.517
  load_time_ms: 3.506
  training_iteration_time_ms: 47357.307
  update_time_ms: 2.576
timesteps_total: 3448500
training_iteration: 209

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2631578947368421
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 178.34
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 19513
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.805455446243286
time_total_s: 10830.96761894226
timers:
  learn_throughput: 465.441
  learn_time_ms: 35450.272
  load_throughput: 4860655.71
  load_time_ms: 3.395
  training_iteration_time_ms: 46720.026
  update_time_ms: 2.599
timesteps_total: 3547500
training_iteration: 215

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17261904761904762
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9857142857142858
  reward for individual goal_min: 0.0
episode_len_mean: 199.57
episode_reward_max: 2.0
episode_reward_mean: 1.1
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 17932
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.86941623687744
time_total_s: 10849.872685194016
timers:
  learn_throughput: 416.514
  learn_time_ms: 39614.502
  load_throughput: 4428958.261
  load_time_ms: 3.725
  training_iteration_time_ms: 52184.557
  update_time_ms: 2.693
timesteps_total: 3283500
training_iteration: 199

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2702702702702703
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9493670886075949
  reward for individual goal_min: 0.0
episode_len_mean: 178.19
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 18845
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.754032373428345
time_total_s: 10847.836379766464
timers:
  learn_throughput: 481.137
  learn_time_ms: 34293.764
  load_throughput: 5054596.288
  load_time_ms: 3.264
  training_iteration_time_ms: 45300.305
  update_time_ms: 2.539
timesteps_total: 3795000
training_iteration: 230

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2535211267605634
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9873417721518988
  reward for individual goal_min: 0.0
episode_len_mean: 166.76
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 18754
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.79
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.2670738697052
time_total_s: 10869.409451961517
timers:
  learn_throughput: 456.87
  learn_time_ms: 36115.282
  load_throughput: 4947810.57
  load_time_ms: 3.335
  training_iteration_time_ms: 47570.045
  update_time_ms: 2.676
timesteps_total: 3415500
training_iteration: 207

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.446236559139784
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 372
episodes_total: 49761
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.00924468040466
time_total_s: 10801.72550034523
timers:
  learn_throughput: 362.423
  learn_time_ms: 45526.881
  load_throughput: 3481485.64
  load_time_ms: 4.739
  training_iteration_time_ms: 58664.783
  update_time_ms: 2.631
timesteps_total: 3168000
training_iteration: 192

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28378378378378377
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9807692307692307
  reward for individual goal_min: 0.0
episode_len_mean: 174.46
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 17599
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.87338900566101
time_total_s: 10874.151461839676
timers:
  learn_throughput: 464.017
  learn_time_ms: 35559.066
  load_throughput: 4774771.528
  load_time_ms: 3.456
  training_iteration_time_ms: 47058.214
  update_time_ms: 2.576
timesteps_total: 3481500
training_iteration: 211

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3176470588235294
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8783783783783784
  reward for individual goal_min: 0.0
episode_len_mean: 185.36
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 19675
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.80907678604126
time_total_s: 10887.0290184021
timers:
  learn_throughput: 499.038
  learn_time_ms: 33063.583
  load_throughput: 4652161.252
  load_time_ms: 3.547
  training_iteration_time_ms: 44075.11
  update_time_ms: 2.581
timesteps_total: 3943500
training_iteration: 239

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9938271604938271
  reward for individual goal_min: 0.5
episode_len_mean: 187.89
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 22366
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.2509331703186
time_total_s: 10866.626190185547
timers:
  learn_throughput: 519.958
  learn_time_ms: 31733.346
  load_throughput: 5102032.969
  load_time_ms: 3.234
  training_iteration_time_ms: 42125.524
  update_time_ms: 2.821
timesteps_total: 4059000
training_iteration: 246

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2571428571428571
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 168.76
episode_reward_max: 2.0
episode_reward_mean: 1.41
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 19244
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.59395670890808
time_total_s: 10882.87924861908
timers:
  learn_throughput: 456.239
  learn_time_ms: 36165.251
  load_throughput: 4746217.141
  load_time_ms: 3.476
  training_iteration_time_ms: 47578.169
  update_time_ms: 2.637
timesteps_total: 3564000
training_iteration: 216

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.98969072164948
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 388
episodes_total: 67218
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.734073877334595
time_total_s: 10759.78297829628
timers:
  learn_throughput: 450.605
  learn_time_ms: 36617.409
  load_throughput: 4229886.316
  load_time_ms: 3.901
  training_iteration_time_ms: 48206.135
  update_time_ms: 2.443
timesteps_total: 3795000
training_iteration: 230

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.09
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 18037
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.90958523750305
time_total_s: 10873.699169158936
timers:
  learn_throughput: 465.008
  learn_time_ms: 35483.262
  load_throughput: 4727025.443
  load_time_ms: 3.491
  training_iteration_time_ms: 47140.596
  update_time_ms: 2.551
timesteps_total: 3465000
training_iteration: 210

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9778481012658228
  reward for individual goal_min: 0.5
episode_len_mean: 51.67289719626168
episode_reward_max: 2.0
episode_reward_mean: 1.9781931464174456
episode_reward_min: 1.0
episodes_this_iter: 321
episodes_total: 46289
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9781931464174455
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 53.3160924911499
time_total_s: 10835.903495311737
timers:
  learn_throughput: 390.981
  learn_time_ms: 42201.59
  load_throughput: 4102361.38
  load_time_ms: 4.022
  training_iteration_time_ms: 54724.313
  update_time_ms: 2.791
timesteps_total: 3498000
training_iteration: 212

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2602739726027397
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.974025974025974
  reward for individual goal_min: 0.0
episode_len_mean: 176.07
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 19607
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.38910627365112
time_total_s: 10874.356725215912
timers:
  learn_throughput: 470.942
  learn_time_ms: 35036.128
  load_throughput: 4822886.93
  load_time_ms: 3.421
  training_iteration_time_ms: 46254.445
  update_time_ms: 2.593
timesteps_total: 3564000
training_iteration: 216

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2916666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9133333333333333
  reward for individual goal_min: 0.0
episode_len_mean: 176.3
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 18938
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.951781034469604
time_total_s: 10892.788160800934
timers:
  learn_throughput: 482.238
  learn_time_ms: 34215.442
  load_throughput: 5056886.193
  load_time_ms: 3.263
  training_iteration_time_ms: 45231.365
  update_time_ms: 2.527
timesteps_total: 3811500
training_iteration: 231

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9622641509433962
  reward for individual goal_min: 0.5
episode_len_mean: 58.8576512455516
episode_reward_max: 2.0
episode_reward_mean: 1.9572953736654803
episode_reward_min: 1.0
episodes_this_iter: 281
episodes_total: 31271
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9572953736654805
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 63.82855582237244
time_total_s: 10898.480895519257
timers:
  learn_throughput: 339.6
  learn_time_ms: 48586.538
  load_throughput: 3827406.494
  load_time_ms: 4.311
  training_iteration_time_ms: 63307.276
  update_time_ms: 2.815
timesteps_total: 2953500
training_iteration: 179

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 103.91666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.6
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7833333333333333
  agent_1: 0.8166666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17088607594936708
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9733333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 191.42
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 17906
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 67.9216046333313
time_total_s: 10893.153835773468
timers:
  learn_throughput: 407.934
  learn_time_ms: 40447.766
  load_throughput: 4364823.089
  load_time_ms: 3.78
  training_iteration_time_ms: 52994.917
  update_time_ms: 2.623
timesteps_total: 3300000
training_iteration: 200

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95
  reward for individual goal_min: 0.5
episode_len_mean: 90.43333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.8833333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.95
  agent_1: 0.9333333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2777777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 164.3
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 18031
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.9379506111145
time_total_s: 10913.81063580513
timers:
  learn_throughput: 415.304
  learn_time_ms: 39729.934
  load_throughput: 4417226.708
  load_time_ms: 3.735
  training_iteration_time_ms: 52359.578
  update_time_ms: 2.693
timesteps_total: 3300000
training_iteration: 200

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2905405405405405
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.78
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 18849
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.01269292831421
time_total_s: 10921.422144889832
timers:
  learn_throughput: 450.969
  learn_time_ms: 36587.912
  load_throughput: 4929308.746
  load_time_ms: 3.347
  training_iteration_time_ms: 48014.947
  update_time_ms: 2.675
timesteps_total: 3432000
training_iteration: 208

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3933333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9461538461538461
  reward for individual goal_min: 0.0
episode_len_mean: 180.06
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 17692
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.43946194648743
time_total_s: 10922.590923786163
timers:
  learn_throughput: 464.428
  learn_time_ms: 35527.565
  load_throughput: 4778002.113
  load_time_ms: 3.453
  training_iteration_time_ms: 47008.384
  update_time_ms: 2.559
timesteps_total: 3498000
training_iteration: 212

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9671052631578947
  reward for individual goal_min: 0.0
episode_len_mean: 188.48
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 22454
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.718279123306274
time_total_s: 10910.344469308853
timers:
  learn_throughput: 516.469
  learn_time_ms: 31947.699
  load_throughput: 5149755.259
  load_time_ms: 3.204
  training_iteration_time_ms: 42353.618
  update_time_ms: 2.852
timesteps_total: 4075500
training_iteration: 247

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27848101265822783
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.14
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 19331
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.611714363098145
time_total_s: 10928.490962982178
timers:
  learn_throughput: 459.891
  learn_time_ms: 35878.049
  load_throughput: 4774178.63
  load_time_ms: 3.456
  training_iteration_time_ms: 47233.608
  update_time_ms: 2.617
timesteps_total: 3580500
training_iteration: 217

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.72020725388601
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 386
episodes_total: 50147
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.4684112071991
time_total_s: 10859.19391155243
timers:
  learn_throughput: 363.936
  learn_time_ms: 45337.7
  load_throughput: 3494900.313
  load_time_ms: 4.721
  training_iteration_time_ms: 58466.257
  update_time_ms: 2.622
timesteps_total: 3184500
training_iteration: 193

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9166666666666666
  reward for individual goal_min: 0.5
episode_len_mean: 107.36666666666666
episode_reward_max: 2.0
episode_reward_mean: 1.8166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.95
  agent_1: 0.8666666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18493150684931506
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.85
  reward for individual goal_min: 0.0
episode_len_mean: 201.57
episode_reward_max: 2.0
episode_reward_mean: 1.12
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 19761
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.24255037307739
time_total_s: 10942.271568775177
timers:
  learn_throughput: 498.547
  learn_time_ms: 33096.176
  load_throughput: 4603239.015
  load_time_ms: 3.584
  training_iteration_time_ms: 44057.269
  update_time_ms: 2.579
timesteps_total: 3960000
training_iteration: 240

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16071428571428573
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.0
episode_len_mean: 200.12
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 18121
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.97862720489502
time_total_s: 10917.67779636383
timers:
  learn_throughput: 467.163
  learn_time_ms: 35319.613
  load_throughput: 4730030.073
  load_time_ms: 3.488
  training_iteration_time_ms: 46935.195
  update_time_ms: 2.532
timesteps_total: 3481500
training_iteration: 211

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20270270270270271
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9940476190476191
  reward for individual goal_min: 0.5
episode_len_mean: 172.36
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 19700
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.37694215774536
time_total_s: 10916.733667373657
timers:
  learn_throughput: 477.29
  learn_time_ms: 34570.182
  load_throughput: 4814599.491
  load_time_ms: 3.427
  training_iteration_time_ms: 45675.692
  update_time_ms: 2.569
timesteps_total: 3580500
training_iteration: 217

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.398477157360404
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 394
episodes_total: 67612
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.947532653808594
time_total_s: 10807.730510950089
timers:
  learn_throughput: 451.353
  learn_time_ms: 36556.731
  load_throughput: 4249104.573
  load_time_ms: 3.883
  training_iteration_time_ms: 48112.785
  update_time_ms: 2.44
timesteps_total: 3811500
training_iteration: 231

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9941176470588236
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9970414201183432
  reward for individual goal_min: 0.5
episode_len_mean: 48.634218289085545
episode_reward_max: 2.0
episode_reward_mean: 1.991150442477876
episode_reward_min: 0.0
episodes_this_iter: 339
episodes_total: 46628
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9941002949852508
  agent_1: 0.9970501474926253
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.2297797203064
time_total_s: 10889.133275032043
timers:
  learn_throughput: 392.888
  learn_time_ms: 41996.743
  load_throughput: 4104380.749
  load_time_ms: 4.02
  training_iteration_time_ms: 54532.396
  update_time_ms: 2.601
timesteps_total: 3514500
training_iteration: 213

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27631578947368424
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9166666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 183.03
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 19026
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.22144913673401
time_total_s: 10938.009609937668
timers:
  learn_throughput: 483.895
  learn_time_ms: 34098.315
  load_throughput: 5060287.943
  load_time_ms: 3.261
  training_iteration_time_ms: 45018.492
  update_time_ms: 2.51
timesteps_total: 3828000
training_iteration: 232

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24025974025974026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.0
episode_len_mean: 179.42
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 18000
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.174084424972534
time_total_s: 10946.32792019844
timers:
  learn_throughput: 407.127
  learn_time_ms: 40527.906
  load_throughput: 4379296.083
  load_time_ms: 3.768
  training_iteration_time_ms: 53113.572
  update_time_ms: 2.617
timesteps_total: 3316500
training_iteration: 201

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2465753424657534
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935897435897436
  reward for individual goal_min: 0.5
episode_len_mean: 175.29
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 22548
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.02836227416992
time_total_s: 10952.372831583023
timers:
  learn_throughput: 517.314
  learn_time_ms: 31895.536
  load_throughput: 5148146.308
  load_time_ms: 3.205
  training_iteration_time_ms: 42232.402
  update_time_ms: 2.857
timesteps_total: 4092000
training_iteration: 248

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23648648648648649
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9266666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 186.29
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 17780
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.8087203502655
time_total_s: 10969.399644136429
timers:
  learn_throughput: 466.113
  learn_time_ms: 35399.147
  load_throughput: 4763826.7
  load_time_ms: 3.464
  training_iteration_time_ms: 46879.165
  update_time_ms: 2.563
timesteps_total: 3514500
training_iteration: 213

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19594594594594594
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 181.3
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 18942
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.589964628219604
time_total_s: 10969.012109518051
timers:
  learn_throughput: 453.021
  learn_time_ms: 36422.177
  load_throughput: 4926501.563
  load_time_ms: 3.349
  training_iteration_time_ms: 47821.482
  update_time_ms: 2.684
timesteps_total: 3448500
training_iteration: 209

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 53.95
episode_reward_max: 2.0
episode_reward_mean: 1.9666666666666666
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9666666666666667
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9239130434782609
  reward for individual goal_min: 0.5
episode_len_mean: 68.15573770491804
episode_reward_max: 2.0
episode_reward_mean: 1.9139344262295082
episode_reward_min: 1.0
episodes_this_iter: 244
episodes_total: 31515
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9139344262295082
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 68.57350850105286
time_total_s: 10967.05440402031
timers:
  learn_throughput: 340.002
  learn_time_ms: 48529.068
  load_throughput: 3720033.542
  load_time_ms: 4.435
  training_iteration_time_ms: 63233.679
  update_time_ms: 2.77
timesteps_total: 2970000
training_iteration: 180

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-19wecnkw4d/checkpoint_000180/checkpoint-180
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.189873417721519
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 184.43
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 18118
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.37670421600342
time_total_s: 10969.187340021133
timers:
  learn_throughput: 411.781
  learn_time_ms: 40069.848
  load_throughput: 4423381.547
  load_time_ms: 3.73
  training_iteration_time_ms: 52817.436
  update_time_ms: 2.71
timesteps_total: 3316500
training_iteration: 201

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24025974025974026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 182.67
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 19420
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.735273122787476
time_total_s: 10975.226236104965
timers:
  learn_throughput: 460.081
  learn_time_ms: 35863.213
  load_throughput: 4788084.518
  load_time_ms: 3.446
  training_iteration_time_ms: 47182.015
  update_time_ms: 2.565
timesteps_total: 3597000
training_iteration: 218

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.84375
  reward for individual goal_min: 0.0
episode_len_mean: 192.22
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 19843
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.701945066452026
time_total_s: 10986.973513841629
timers:
  learn_throughput: 497.896
  learn_time_ms: 33139.473
  load_throughput: 4680288.097
  load_time_ms: 3.525
  training_iteration_time_ms: 44113.534
  update_time_ms: 2.582
timesteps_total: 3976500
training_iteration: 241

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20394736842105263
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.59
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 18212
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.30135226249695
time_total_s: 10962.979148626328
timers:
  learn_throughput: 472.398
  learn_time_ms: 34928.154
  load_throughput: 4772038.835
  load_time_ms: 3.458
  training_iteration_time_ms: 46478.196
  update_time_ms: 2.541
timesteps_total: 3498000
training_iteration: 212

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34415584415584416
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9879518072289156
  reward for individual goal_min: 0.5
episode_len_mean: 159.22115384615384
episode_reward_max: 2.0
episode_reward_mean: 1.3942307692307692
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 19804
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6826923076923077
  agent_1: 0.7115384615384616
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.69595909118652
time_total_s: 10961.429626464844
timers:
  learn_throughput: 480.989
  learn_time_ms: 34304.35
  load_throughput: 4810015.082
  load_time_ms: 3.43
  training_iteration_time_ms: 45395.441
  update_time_ms: 2.564
timesteps_total: 3597000
training_iteration: 218

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.64070351758794
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 398
episodes_total: 68010
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.08314871788025
time_total_s: 10855.813659667969
timers:
  learn_throughput: 451.499
  learn_time_ms: 36544.95
  load_throughput: 4259670.581
  load_time_ms: 3.874
  training_iteration_time_ms: 48069.453
  update_time_ms: 2.4
timesteps_total: 3828000
training_iteration: 232

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.09090909090909
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 374
episodes_total: 50521
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.73131585121155
time_total_s: 10916.92522740364
timers:
  learn_throughput: 365.682
  learn_time_ms: 45121.136
  load_throughput: 3502718.724
  load_time_ms: 4.711
  training_iteration_time_ms: 58208.025
  update_time_ms: 2.61
timesteps_total: 3201000
training_iteration: 194

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9366197183098591
  reward for individual goal_min: 0.0
episode_len_mean: 193.92
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 19110
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.97564077377319
time_total_s: 10981.985250711441
timers:
  learn_throughput: 484.234
  learn_time_ms: 34074.416
  load_throughput: 5059806.984
  load_time_ms: 3.261
  training_iteration_time_ms: 44961.001
  update_time_ms: 2.505
timesteps_total: 3844500
training_iteration: 233

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9884393063583815
  reward for individual goal_min: 0.5
episode_len_mean: 47.31428571428572
episode_reward_max: 2.0
episode_reward_mean: 1.9885714285714287
episode_reward_min: 1.0
episodes_this_iter: 350
episodes_total: 46978
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9885714285714285
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 53.94545269012451
time_total_s: 10943.078727722168
timers:
  learn_throughput: 395.04
  learn_time_ms: 41767.954
  load_throughput: 4123701.236
  load_time_ms: 4.001
  training_iteration_time_ms: 54226.113
  update_time_ms: 2.59
timesteps_total: 3531000
training_iteration: 214

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23972602739726026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 181.04
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 22638
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.54544448852539
time_total_s: 10994.918276071548
timers:
  learn_throughput: 519.084
  learn_time_ms: 31786.763
  load_throughput: 5206199.955
  load_time_ms: 3.169
  training_iteration_time_ms: 42084.032
  update_time_ms: 2.838
timesteps_total: 4108500
training_iteration: 249

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22058823529411764
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9736842105263158
  reward for individual goal_min: 0.0
episode_len_mean: 172.4
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 18093
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.59457802772522
time_total_s: 10998.922498226166
timers:
  learn_throughput: 406.545
  learn_time_ms: 40585.943
  load_throughput: 4380876.226
  load_time_ms: 3.766
  training_iteration_time_ms: 53219.007
  update_time_ms: 2.631
timesteps_total: 3333000
training_iteration: 202

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 180.03
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 19031
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.44057321548462
time_total_s: 11014.452682733536
timers:
  learn_throughput: 453.474
  learn_time_ms: 36385.766
  load_throughput: 4916841.275
  load_time_ms: 3.356
  training_iteration_time_ms: 47745.688
  update_time_ms: 2.701
timesteps_total: 3465000
training_iteration: 210

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22794117647058823
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9671052631578947
  reward for individual goal_min: 0.0
episode_len_mean: 177.16
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 17876
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.41263723373413
time_total_s: 11017.812281370163
timers:
  learn_throughput: 464.111
  learn_time_ms: 35551.868
  load_throughput: 4750126.362
  load_time_ms: 3.474
  training_iteration_time_ms: 47014.509
  update_time_ms: 2.567
timesteps_total: 3531000
training_iteration: 214

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3157894736842105
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.61
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 19519
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.524691343307495
time_total_s: 11020.750927448273
timers:
  learn_throughput: 463.283
  learn_time_ms: 35615.354
  load_throughput: 4808143.676
  load_time_ms: 3.432
  training_iteration_time_ms: 46936.324
  update_time_ms: 2.556
timesteps_total: 3613500
training_iteration: 219

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2236842105263158
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8716216216216216
  reward for individual goal_min: 0.0
episode_len_mean: 192.63
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 19933
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.050379514694214
time_total_s: 11033.023893356323
timers:
  learn_throughput: 497.212
  learn_time_ms: 33185.073
  load_throughput: 4677125.035
  load_time_ms: 3.528
  training_iteration_time_ms: 44202.973
  update_time_ms: 2.595
timesteps_total: 3993000
training_iteration: 242

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21604938271604937
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.5
episode_len_mean: 189.63
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 19892
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.65110182762146
time_total_s: 11007.080728292465
timers:
  learn_throughput: 484.136
  learn_time_ms: 34081.342
  load_throughput: 4818186.097
  load_time_ms: 3.425
  training_iteration_time_ms: 45142.652
  update_time_ms: 2.535
timesteps_total: 3613500
training_iteration: 219

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2835820895522388
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 157.17307692307693
episode_reward_max: 2.0
episode_reward_mean: 1.3653846153846154
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 18316
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6538461538461539
  agent_1: 0.7115384615384616
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.298250675201416
time_total_s: 11012.277399301529
timers:
  learn_throughput: 471.136
  learn_time_ms: 35021.706
  load_throughput: 4833463.658
  load_time_ms: 3.414
  training_iteration_time_ms: 46582.316
  update_time_ms: 2.506
timesteps_total: 3514500
training_iteration: 213

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23376623376623376
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 168.97
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 18214
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.37270665168762
time_total_s: 11021.560046672821
timers:
  learn_throughput: 411.771
  learn_time_ms: 40070.849
  load_throughput: 4402810.428
  load_time_ms: 3.748
  training_iteration_time_ms: 52866.212
  update_time_ms: 2.724
timesteps_total: 3333000
training_iteration: 202

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9224806201550387
  reward for individual goal_min: 0.5
episode_len_mean: 68.8135593220339
episode_reward_max: 2.0
episode_reward_mean: 1.9152542372881356
episode_reward_min: 1.0
episodes_this_iter: 236
episodes_total: 31751
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9152542372881356
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 59.88951230049133
time_total_s: 11026.9439163208
timers:
  learn_throughput: 341.949
  learn_time_ms: 48252.799
  load_throughput: 3713526.148
  load_time_ms: 4.443
  training_iteration_time_ms: 62883.326
  update_time_ms: 2.8
timesteps_total: 2986500
training_iteration: 181

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.95641025641026
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 390
episodes_total: 68400
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.778717279434204
time_total_s: 10903.592376947403
timers:
  learn_throughput: 452.103
  learn_time_ms: 36496.087
  load_throughput: 4280987.511
  load_time_ms: 3.854
  training_iteration_time_ms: 48027.456
  update_time_ms: 2.405
timesteps_total: 3844500
training_iteration: 233

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2012987012987013
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9605263157894737
  reward for individual goal_min: 0.0
episode_len_mean: 191.93
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 19196
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.93100619316101
time_total_s: 11027.916256904602
timers:
  learn_throughput: 484.174
  learn_time_ms: 34078.676
  load_throughput: 5070000.659
  load_time_ms: 3.254
  training_iteration_time_ms: 44942.095
  update_time_ms: 2.504
timesteps_total: 3861000
training_iteration: 234

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.49226804123711
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 388
episodes_total: 50909
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 56.07934093475342
time_total_s: 10973.004568338394
timers:
  learn_throughput: 368.16
  learn_time_ms: 44817.521
  load_throughput: 3500929.082
  load_time_ms: 4.713
  training_iteration_time_ms: 57868.976
  update_time_ms: 2.621
timesteps_total: 3217500
training_iteration: 195

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9846153846153847
  reward for individual goal_min: 0.5
episode_len_mean: 48.45882352941177
episode_reward_max: 2.0
episode_reward_mean: 1.9823529411764707
episode_reward_min: 1.0
episodes_this_iter: 340
episodes_total: 47318
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9823529411764705
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 52.23428797721863
time_total_s: 10995.313015699387
timers:
  learn_throughput: 397.767
  learn_time_ms: 41481.559
  load_throughput: 4205416.494
  load_time_ms: 3.924
  training_iteration_time_ms: 53860.21
  update_time_ms: 2.569
timesteps_total: 3547500
training_iteration: 215

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.358974358974359
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9662162162162162
  reward for individual goal_min: 0.0
episode_len_mean: 171.05
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 22735
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.232526540756226
time_total_s: 11038.150802612305
timers:
  learn_throughput: 516.834
  learn_time_ms: 31925.124
  load_throughput: 5163549.109
  load_time_ms: 3.195
  training_iteration_time_ms: 42277.901
  update_time_ms: 3.126
timesteps_total: 4125000
training_iteration: 250

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2597402597402597
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.88
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 19125
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.27193474769592
time_total_s: 11063.724617481232
timers:
  learn_throughput: 450.427
  learn_time_ms: 36631.885
  load_throughput: 4946537.439
  load_time_ms: 3.336
  training_iteration_time_ms: 47911.676
  update_time_ms: 2.695
timesteps_total: 3481500
training_iteration: 211

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23026315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.01
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 18186
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.758493423461914
time_total_s: 11053.680991649628
timers:
  learn_throughput: 404.144
  learn_time_ms: 40826.999
  load_throughput: 4373898.941
  load_time_ms: 3.772
  training_iteration_time_ms: 53550.541
  update_time_ms: 2.64
timesteps_total: 3349500
training_iteration: 203

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25316455696202533
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9178082191780822
  reward for individual goal_min: 0.0
episode_len_mean: 185.82
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 20020
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.518354177474976
time_total_s: 11074.542247533798
timers:
  learn_throughput: 499.522
  learn_time_ms: 33031.602
  load_throughput: 4681871.233
  load_time_ms: 3.524
  training_iteration_time_ms: 44024.112
  update_time_ms: 2.632
timesteps_total: 4009500
training_iteration: 243

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24647887323943662
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9305555555555556
  reward for individual goal_min: 0.0
episode_len_mean: 187.76
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 17964
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.04815721511841
time_total_s: 11065.860438585281
timers:
  learn_throughput: 462.988
  learn_time_ms: 35638.056
  load_throughput: 4700920.811
  load_time_ms: 3.51
  training_iteration_time_ms: 47094.689
  update_time_ms: 2.543
timesteps_total: 3547500
training_iteration: 215

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22142857142857142
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.48
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 18411
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.74626326560974
time_total_s: 11059.023662567139
timers:
  learn_throughput: 472.181
  learn_time_ms: 34944.261
  load_throughput: 4789409.957
  load_time_ms: 3.445
  training_iteration_time_ms: 46503.412
  update_time_ms: 2.48
timesteps_total: 3531000
training_iteration: 214

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9333333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 112.66666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.5333333333333334
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.7833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 58.483333333333334
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.275
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 180.28
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 19607
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.67793846130371
time_total_s: 11079.428865909576
timers:
  learn_throughput: 467.744
  learn_time_ms: 35275.702
  load_throughput: 4781402.238
  load_time_ms: 3.451
  training_iteration_time_ms: 46547.721
  update_time_ms: 2.568
timesteps_total: 3630000
training_iteration: 220

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2465753424657534
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 167.96
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 19989
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.77676033973694
time_total_s: 11061.857488632202
timers:
  learn_throughput: 480.66
  learn_time_ms: 34327.794
  load_throughput: 4861236.136
  load_time_ms: 3.394
  training_iteration_time_ms: 45473.312
  update_time_ms: 2.531
timesteps_total: 3630000
training_iteration: 220

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-28met_xp4k/checkpoint_000220/checkpoint-220
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2962962962962963
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 173.43
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 18308
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.36219263076782
time_total_s: 11075.922239303589
timers:
  learn_throughput: 410.002
  learn_time_ms: 40243.692
  load_throughput: 4377855.543
  load_time_ms: 3.769
  training_iteration_time_ms: 53078.825
  update_time_ms: 2.743
timesteps_total: 3349500
training_iteration: 203

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.35411471321696
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 401
episodes_total: 68801
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.819839000701904
time_total_s: 10952.412215948105
timers:
  learn_throughput: 451.667
  learn_time_ms: 36531.354
  load_throughput: 4267419.114
  load_time_ms: 3.867
  training_iteration_time_ms: 48062.846
  update_time_ms: 2.408
timesteps_total: 3861000
training_iteration: 234

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27205882352941174
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9246575342465754
  reward for individual goal_min: 0.0
episode_len_mean: 177.63
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 19284
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.43453574180603
time_total_s: 11073.350792646408
timers:
  learn_throughput: 484.492
  learn_time_ms: 34056.324
  load_throughput: 5048033.553
  load_time_ms: 3.269
  training_iteration_time_ms: 44980.461
  update_time_ms: 2.481
timesteps_total: 3877500
training_iteration: 235

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2602739726027397
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 170.19
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 22832
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.91921639442444
time_total_s: 11080.07001900673
timers:
  learn_throughput: 515.645
  learn_time_ms: 31998.743
  load_throughput: 5162855.736
  load_time_ms: 3.196
  training_iteration_time_ms: 42318.984
  update_time_ms: 3.132
timesteps_total: 4141500
training_iteration: 251

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9586206896551724
  reward for individual goal_min: 0.5
episode_len_mean: 57.255102040816325
episode_reward_max: 2.0
episode_reward_mean: 1.9591836734693877
episode_reward_min: 1.0
episodes_this_iter: 294
episodes_total: 32045
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9591836734693877
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 64.71244287490845
time_total_s: 11091.65635919571
timers:
  learn_throughput: 340.845
  learn_time_ms: 48409.106
  load_throughput: 3647778.369
  load_time_ms: 4.523
  training_iteration_time_ms: 63057.503
  update_time_ms: 2.823
timesteps_total: 3003000
training_iteration: 182

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9943181818181818
  reward for individual goal_min: 0.5
episode_len_mean: 43.67195767195767
episode_reward_max: 2.0
episode_reward_mean: 1.9947089947089947
episode_reward_min: 1.0
episodes_this_iter: 378
episodes_total: 51287
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9947089947089947
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 55.6598904132843
time_total_s: 11028.664458751678
timers:
  learn_throughput: 369.525
  learn_time_ms: 44651.954
  load_throughput: 3519140.835
  load_time_ms: 4.689
  training_iteration_time_ms: 57636.984
  update_time_ms: 2.621
timesteps_total: 3234000
training_iteration: 196

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9915254237288136
  reward for individual goal_min: 0.5
episode_len_mean: 46.15642458100559
episode_reward_max: 2.0
episode_reward_mean: 1.9916201117318435
episode_reward_min: 1.0
episodes_this_iter: 358
episodes_total: 47676
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9916201117318436
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 51.56966519355774
time_total_s: 11046.882680892944
timers:
  learn_throughput: 401.129
  learn_time_ms: 41133.885
  load_throughput: 4176408.721
  load_time_ms: 3.951
  training_iteration_time_ms: 53454.435
  update_time_ms: 2.537
timesteps_total: 3564000
training_iteration: 216

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2534246575342466
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8766233766233766
  reward for individual goal_min: 0.0
episode_len_mean: 191.88
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 20106
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.437095165252686
time_total_s: 11117.97934269905
timers:
  learn_throughput: 500.795
  learn_time_ms: 32947.602
  load_throughput: 4710968.796
  load_time_ms: 3.502
  training_iteration_time_ms: 43917.775
  update_time_ms: 2.621
timesteps_total: 4026000
training_iteration: 244

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2948717948717949
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9671052631578947
  reward for individual goal_min: 0.0
episode_len_mean: 175.23
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 18056
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.27467226982117
time_total_s: 11115.135110855103
timers:
  learn_throughput: 461.889
  learn_time_ms: 35722.85
  load_throughput: 4719128.265
  load_time_ms: 3.496
  training_iteration_time_ms: 47150.148
  update_time_ms: 2.551
timesteps_total: 3564000
training_iteration: 216

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30666666666666664
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9852941176470589
  reward for individual goal_min: 0.0
episode_len_mean: 171.63
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 19219
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.52956581115723
time_total_s: 11117.254183292389
timers:
  learn_throughput: 444.411
  learn_time_ms: 37127.828
  load_throughput: 4954718.1
  load_time_ms: 3.33
  training_iteration_time_ms: 48563.577
  update_time_ms: 2.688
timesteps_total: 3498000
training_iteration: 212

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2647058823529412
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9929577464788732
  reward for individual goal_min: 0.5
episode_len_mean: 164.18
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 18283
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.69131684303284
time_total_s: 11108.37230849266
timers:
  learn_throughput: 403.349
  learn_time_ms: 40907.542
  load_throughput: 4323457.466
  load_time_ms: 3.816
  training_iteration_time_ms: 53695.389
  update_time_ms: 2.646
timesteps_total: 3366000
training_iteration: 204

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1619718309859155
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.92
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 18507
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.337565183639526
time_total_s: 11104.361227750778
timers:
  learn_throughput: 474.489
  learn_time_ms: 34774.27
  load_throughput: 4828506.363
  load_time_ms: 3.417
  training_iteration_time_ms: 46322.442
  update_time_ms: 2.456
timesteps_total: 3547500
training_iteration: 215

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34146341463414637
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.67
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 20082
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.29577589035034
time_total_s: 11107.153264522552
timers:
  learn_throughput: 486.206
  learn_time_ms: 33936.21
  load_throughput: 4881569.867
  load_time_ms: 3.38
  training_iteration_time_ms: 45067.968
  update_time_ms: 2.532
timesteps_total: 3646500
training_iteration: 221

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18243243243243243
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.07
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 19698
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.8884961605072
time_total_s: 11126.317362070084
timers:
  learn_throughput: 468.457
  learn_time_ms: 35221.982
  load_throughput: 4800306.305
  load_time_ms: 3.437
  training_iteration_time_ms: 46499.347
  update_time_ms: 2.549
timesteps_total: 3646500
training_iteration: 221

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1736111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9551282051282052
  reward for individual goal_min: 0.0
episode_len_mean: 188.14
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 19366
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.48245906829834
time_total_s: 11118.833251714706
timers:
  learn_throughput: 485.272
  learn_time_ms: 34001.532
  load_throughput: 5036168.188
  load_time_ms: 3.276
  training_iteration_time_ms: 44883.034
  update_time_ms: 2.501
timesteps_total: 3894000
training_iteration: 236

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.38345864661654
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 399
episodes_total: 69200
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.97985553741455
time_total_s: 11000.39207148552
timers:
  learn_throughput: 453.004
  learn_time_ms: 36423.54
  load_throughput: 4282338.498
  load_time_ms: 3.853
  training_iteration_time_ms: 47949.33
  update_time_ms: 2.367
timesteps_total: 3877500
training_iteration: 235

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24324324324324326
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 175.85
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 22927
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.72093749046326
time_total_s: 11120.790956497192
timers:
  learn_throughput: 516.224
  learn_time_ms: 31962.864
  load_throughput: 5141796.946
  load_time_ms: 3.209
  training_iteration_time_ms: 42274.679
  update_time_ms: 3.143
timesteps_total: 4158000
training_iteration: 252

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21604938271604937
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 184.65
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 18393
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.68272852897644
time_total_s: 11130.604967832565
timers:
  learn_throughput: 407.117
  learn_time_ms: 40528.91
  load_throughput: 4370777.451
  load_time_ms: 3.775
  training_iteration_time_ms: 53495.55
  update_time_ms: 2.728
timesteps_total: 3366000
training_iteration: 204

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2676056338028169
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.85625
  reward for individual goal_min: 0.0
episode_len_mean: 187.84
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 20193
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.781028032302856
time_total_s: 11160.760370731354
timers:
  learn_throughput: 500.81
  learn_time_ms: 32946.597
  load_throughput: 4726799.443
  load_time_ms: 3.491
  training_iteration_time_ms: 43931.955
  update_time_ms: 2.612
timesteps_total: 4042500
training_iteration: 245

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972826086956522
  reward for individual goal_min: 0.5
episode_len_mean: 43.794195250659634
episode_reward_max: 2.0
episode_reward_mean: 1.9973614775725594
episode_reward_min: 1.0
episodes_this_iter: 379
episodes_total: 48055
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973614775725593
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 53.33250617980957
time_total_s: 11100.215187072754
timers:
  learn_throughput: 401.688
  learn_time_ms: 41076.615
  load_throughput: 4172606.446
  load_time_ms: 3.954
  training_iteration_time_ms: 53331.904
  update_time_ms: 2.56
timesteps_total: 3580500
training_iteration: 217

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972826086956522
  reward for individual goal_min: 0.5
episode_len_mean: 43.792
episode_reward_max: 2.0
episode_reward_mean: 1.9973333333333334
episode_reward_min: 1.0
episodes_this_iter: 375
episodes_total: 51662
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973333333333333
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 55.5383574962616
time_total_s: 11084.20281624794
timers:
  learn_throughput: 372.655
  learn_time_ms: 44276.882
  load_throughput: 3494423.849
  load_time_ms: 4.722
  training_iteration_time_ms: 57225.774
  update_time_ms: 2.613
timesteps_total: 3250500
training_iteration: 197

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9633333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 57.87544483985765
episode_reward_max: 2.0
episode_reward_mean: 1.9608540925266904
episode_reward_min: 1.0
episodes_this_iter: 281
episodes_total: 32326
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.99644128113879
  agent_1: 0.9644128113879004
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.22869825363159
time_total_s: 11154.88505744934
timers:
  learn_throughput: 340.027
  learn_time_ms: 48525.547
  load_throughput: 3730681.976
  load_time_ms: 4.423
  training_iteration_time_ms: 63161.139
  update_time_ms: 2.82
timesteps_total: 3019500
training_iteration: 183

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2721518987341772
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.94
  reward for individual goal_min: 0.0
episode_len_mean: 185.95
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 18147
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.051758766174316
time_total_s: 11163.186869621277
timers:
  learn_throughput: 460.932
  learn_time_ms: 35797.056
  load_throughput: 4724218.114
  load_time_ms: 3.493
  training_iteration_time_ms: 47260.364
  update_time_ms: 2.564
timesteps_total: 3580500
training_iteration: 217

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.48
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 19310
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.59616756439209
time_total_s: 11163.850350856781
timers:
  learn_throughput: 444.841
  learn_time_ms: 37091.89
  load_throughput: 4944734.958
  load_time_ms: 3.337
  training_iteration_time_ms: 48525.061
  update_time_ms: 2.713
timesteps_total: 3514500
training_iteration: 213

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2357142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 163.27
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 18607
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.73609662055969
time_total_s: 11150.097324371338
timers:
  learn_throughput: 472.922
  learn_time_ms: 34889.45
  load_throughput: 4838329.663
  load_time_ms: 3.41
  training_iteration_time_ms: 46261.956
  update_time_ms: 2.468
timesteps_total: 3564000
training_iteration: 216

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.71
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 20180
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.91834902763367
time_total_s: 11153.071613550186
timers:
  learn_throughput: 485.037
  learn_time_ms: 34018.053
  load_throughput: 4887844.733
  load_time_ms: 3.376
  training_iteration_time_ms: 45102.37
  update_time_ms: 2.492
timesteps_total: 3663000
training_iteration: 222

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17857142857142858
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 171.69
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 18381
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.417768478393555
time_total_s: 11161.790076971054
timers:
  learn_throughput: 403.127
  learn_time_ms: 40930.071
  load_throughput: 4355428.456
  load_time_ms: 3.788
  training_iteration_time_ms: 53761.433
  update_time_ms: 2.672
timesteps_total: 3382500
training_iteration: 205

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18421052631578946
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 184.36
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 19790
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.41662931442261
time_total_s: 11172.733991384506
timers:
  learn_throughput: 470.818
  learn_time_ms: 35045.405
  load_throughput: 4850027.752
  load_time_ms: 3.402
  training_iteration_time_ms: 46253.637
  update_time_ms: 2.56
timesteps_total: 3663000
training_iteration: 222

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23943661971830985
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9430379746835443
  reward for individual goal_min: 0.0
episode_len_mean: 179.11
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 19460
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.21920084953308
time_total_s: 11165.05245256424
timers:
  learn_throughput: 482.547
  learn_time_ms: 34193.542
  load_throughput: 5063286.753
  load_time_ms: 3.259
  training_iteration_time_ms: 45122.121
  update_time_ms: 2.496
timesteps_total: 3910500
training_iteration: 237

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3082191780821918
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9857142857142858
  reward for individual goal_min: 0.0
episode_len_mean: 171.64
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 23022
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.468870878219604
time_total_s: 11163.259827375412
timers:
  learn_throughput: 516.002
  learn_time_ms: 31976.594
  load_throughput: 5160391.917
  load_time_ms: 3.197
  training_iteration_time_ms: 42295.506
  update_time_ms: 2.825
timesteps_total: 4174500
training_iteration: 253

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.82741116751269
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 394
episodes_total: 69594
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.97819805145264
time_total_s: 11048.370269536972
timers:
  learn_throughput: 453.244
  learn_time_ms: 36404.197
  load_throughput: 4310209.45
  load_time_ms: 3.828
  training_iteration_time_ms: 47924.462
  update_time_ms: 2.382
timesteps_total: 3894000
training_iteration: 236

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16025641025641027
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935897435897436
  reward for individual goal_min: 0.5
episode_len_mean: 188.58
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 18483
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.9564414024353
time_total_s: 11184.561409235
timers:
  learn_throughput: 406.008
  learn_time_ms: 40639.637
  load_throughput: 4330545.589
  load_time_ms: 3.81
  training_iteration_time_ms: 53620.135
  update_time_ms: 2.742
timesteps_total: 3382500
training_iteration: 205

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3287671232876712
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8680555555555556
  reward for individual goal_min: 0.0
episode_len_mean: 185.24
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 20284
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.30406594276428
time_total_s: 11206.064436674118
timers:
  learn_throughput: 499.595
  learn_time_ms: 33026.735
  load_throughput: 4703668.543
  load_time_ms: 3.508
  training_iteration_time_ms: 44037.434
  update_time_ms: 2.617
timesteps_total: 4059000
training_iteration: 246

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24305555555555555
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9880952380952381
  reward for individual goal_min: 0.5
episode_len_mean: 172.36
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 18242
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.62504863739014
time_total_s: 11209.811918258667
timers:
  learn_throughput: 460.341
  learn_time_ms: 35843.031
  load_throughput: 4706899.633
  load_time_ms: 3.505
  training_iteration_time_ms: 47284.077
  update_time_ms: 2.576
timesteps_total: 3597000
training_iteration: 218

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9912790697674418
  reward for individual goal_min: 0.5
episode_len_mean: 47.44057971014493
episode_reward_max: 2.0
episode_reward_mean: 1.991304347826087
episode_reward_min: 1.0
episodes_this_iter: 345
episodes_total: 48400
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.991304347826087
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 53.18145155906677
time_total_s: 11153.39663863182
timers:
  learn_throughput: 403.193
  learn_time_ms: 40923.351
  load_throughput: 4194965.025
  load_time_ms: 3.933
  training_iteration_time_ms: 53141.41
  update_time_ms: 2.575
timesteps_total: 3597000
training_iteration: 218

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2905405405405405
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 166.6764705882353
episode_reward_max: 2.0
episode_reward_mean: 1.3725490196078431
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 19412
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6470588235294118
  agent_1: 0.7254901960784313
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.340282678604126
time_total_s: 11214.190633535385
timers:
  learn_throughput: 442.783
  learn_time_ms: 37264.295
  load_throughput: 4893063.06
  load_time_ms: 3.372
  training_iteration_time_ms: 48848.913
  update_time_ms: 2.722
timesteps_total: 3531000
training_iteration: 214

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.87
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 18700
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.18727684020996
time_total_s: 11200.284601211548
timers:
  learn_throughput: 469.101
  learn_time_ms: 35173.692
  load_throughput: 4840360.058
  load_time_ms: 3.409
  training_iteration_time_ms: 46660.763
  update_time_ms: 2.481
timesteps_total: 3580500
training_iteration: 217

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9969512195121951
  reward for individual goal_min: 0.5
episode_len_mean: 44.99184782608695
episode_reward_max: 2.0
episode_reward_mean: 1.997282608695652
episode_reward_min: 1.0
episodes_this_iter: 368
episodes_total: 52030
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972826086956522
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 56.363553524017334
time_total_s: 11140.566369771957
timers:
  learn_throughput: 374.344
  learn_time_ms: 44077.05
  load_throughput: 3500911.372
  load_time_ms: 4.713
  training_iteration_time_ms: 56969.093
  update_time_ms: 2.59
timesteps_total: 3267000
training_iteration: 198

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2721518987341772
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.49
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 19882
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.94974207878113
time_total_s: 11218.683733463287
timers:
  learn_throughput: 470.912
  learn_time_ms: 35038.417
  load_throughput: 4895208.912
  load_time_ms: 3.371
  training_iteration_time_ms: 46181.265
  update_time_ms: 2.681
timesteps_total: 3679500
training_iteration: 223

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2792207792207792
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 175.92
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 23115
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.92354106903076
time_total_s: 11207.183368444443
timers:
  learn_throughput: 514.009
  learn_time_ms: 32100.601
  load_throughput: 5163972.929
  load_time_ms: 3.195
  training_iteration_time_ms: 42493.114
  update_time_ms: 2.821
timesteps_total: 4191000
training_iteration: 254

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.04
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 20273
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.67741513252258
time_total_s: 11201.749028682709
timers:
  learn_throughput: 482.838
  learn_time_ms: 34172.948
  load_throughput: 4897391.304
  load_time_ms: 3.369
  training_iteration_time_ms: 45338.314
  update_time_ms: 2.482
timesteps_total: 3679500
training_iteration: 223

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2972972972972973
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9466666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 175.5
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 19550
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.83149194717407
time_total_s: 11210.883944511414
timers:
  learn_throughput: 483.041
  learn_time_ms: 34158.612
  load_throughput: 5020895.557
  load_time_ms: 3.286
  training_iteration_time_ms: 45148.774
  update_time_ms: 2.488
timesteps_total: 3927000
training_iteration: 238

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.952755905511811
  reward for individual goal_min: 0.5
episode_len_mean: 56.30795847750865
episode_reward_max: 2.0
episode_reward_mean: 1.958477508650519
episode_reward_min: 1.0
episodes_this_iter: 289
episodes_total: 32615
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9584775086505191
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 60.9166145324707
time_total_s: 11215.801671981812
timers:
  learn_throughput: 341.29
  learn_time_ms: 48345.977
  load_throughput: 3683483.037
  load_time_ms: 4.479
  training_iteration_time_ms: 63013.919
  update_time_ms: 2.73
timesteps_total: 3036000
training_iteration: 184

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23648648648648649
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9794520547945206
  reward for individual goal_min: 0.0
episode_len_mean: 171.73
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 18477
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.510244369506836
time_total_s: 11217.300321340561
timers:
  learn_throughput: 401.096
  learn_time_ms: 41137.269
  load_throughput: 4364905.677
  load_time_ms: 3.78
  training_iteration_time_ms: 54025.274
  update_time_ms: 2.7
timesteps_total: 3399000
training_iteration: 206

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.91644908616188
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 383
episodes_total: 69977
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.034862756729126
time_total_s: 11095.405132293701
timers:
  learn_throughput: 454.395
  learn_time_ms: 36312.013
  load_throughput: 4332307.692
  load_time_ms: 3.809
  training_iteration_time_ms: 47798.474
  update_time_ms: 2.37
timesteps_total: 3910500
training_iteration: 237

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22151898734177214
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9166666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 196.97
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 20364
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.77375674247742
time_total_s: 11250.838193416595
timers:
  learn_throughput: 495.461
  learn_time_ms: 33302.344
  load_throughput: 4705427.497
  load_time_ms: 3.507
  training_iteration_time_ms: 44366.289
  update_time_ms: 2.638
timesteps_total: 4075500
training_iteration: 247

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24675324675324675
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.5
episode_len_mean: 173.83
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 18580
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.59112191200256
time_total_s: 11238.152531147003
timers:
  learn_throughput: 405.247
  learn_time_ms: 40715.876
  load_throughput: 4322593.331
  load_time_ms: 3.817
  training_iteration_time_ms: 53730.775
  update_time_ms: 2.738
timesteps_total: 3399000
training_iteration: 206

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2537313432835821
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95625
  reward for individual goal_min: 0.0
episode_len_mean: 171.45
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 18341
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.63724994659424
time_total_s: 11258.449168205261
timers:
  learn_throughput: 456.426
  learn_time_ms: 36150.431
  load_throughput: 4726799.443
  load_time_ms: 3.491
  training_iteration_time_ms: 47642.67
  update_time_ms: 2.695
timesteps_total: 3613500
training_iteration: 219

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20588235294117646
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9807692307692307
  reward for individual goal_min: 0.0
episode_len_mean: 174.17
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 19505
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.35171055793762
time_total_s: 11262.542344093323
timers:
  learn_throughput: 443.191
  learn_time_ms: 37230.027
  load_throughput: 4866979.57
  load_time_ms: 3.39
  training_iteration_time_ms: 48865.285
  update_time_ms: 2.769
timesteps_total: 3547500
training_iteration: 215

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30714285714285716
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 162.44554455445544
episode_reward_max: 2.0
episode_reward_mean: 1.4059405940594059
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 23216
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7326732673267327
  agent_1: 0.6732673267326733
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.441133975982666
time_total_s: 11250.624502420425
timers:
  learn_throughput: 512.599
  learn_time_ms: 32188.884
  load_throughput: 5109679.934
  load_time_ms: 3.229
  training_iteration_time_ms: 42588.455
  update_time_ms: 2.813
timesteps_total: 4207500
training_iteration: 255

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2536231884057971
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 165.20792079207922
episode_reward_max: 2.0
episode_reward_mean: 1.3564356435643565
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 19983
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6336633663366337
  agent_1: 0.7227722772277227
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.657289266586304
time_total_s: 11264.341022729874
timers:
  learn_throughput: 473.734
  learn_time_ms: 34829.693
  load_throughput: 4957522.028
  load_time_ms: 3.328
  training_iteration_time_ms: 45970.432
  update_time_ms: 2.66
timesteps_total: 3696000
training_iteration: 224

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.225
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 185.02
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 18791
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.33260440826416
time_total_s: 11250.617205619812
timers:
  learn_throughput: 466.261
  learn_time_ms: 35387.94
  load_throughput: 4815537.526
  load_time_ms: 3.426
  training_iteration_time_ms: 46939.177
  update_time_ms: 2.501
timesteps_total: 3597000
training_iteration: 218

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20987654320987653
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9305555555555556
  reward for individual goal_min: 0.0
episode_len_mean: 200.66
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 19634
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.47
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.10226511955261
time_total_s: 11256.986209630966
timers:
  learn_throughput: 480.221
  learn_time_ms: 34359.146
  load_throughput: 4976952.385
  load_time_ms: 3.315
  training_iteration_time_ms: 45353.656
  update_time_ms: 2.469
timesteps_total: 3943500
training_iteration: 239

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.1624649859944
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 357
episodes_total: 48757
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 55.05718636512756
time_total_s: 11208.453824996948
timers:
  learn_throughput: 402.348
  learn_time_ms: 41009.323
  load_throughput: 4199496.104
  load_time_ms: 3.929
  training_iteration_time_ms: 53269.997
  update_time_ms: 2.573
timesteps_total: 3613500
training_iteration: 219

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3082191780821918
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9533333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 170.3
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 20369
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.75
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.79175591468811
time_total_s: 11249.540784597397
timers:
  learn_throughput: 478.726
  learn_time_ms: 34466.511
  load_throughput: 4901623.061
  load_time_ms: 3.366
  training_iteration_time_ms: 45612.479
  update_time_ms: 2.502
timesteps_total: 3696000
training_iteration: 224

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974226804123711
  reward for individual goal_min: 0.5
episode_len_mean: 43.816
episode_reward_max: 2.0
episode_reward_mean: 1.9973333333333334
episode_reward_min: 1.0
episodes_this_iter: 375
episodes_total: 52405
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973333333333333
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 55.57573676109314
time_total_s: 11196.14210653305
timers:
  learn_throughput: 377.116
  learn_time_ms: 43753.093
  load_throughput: 3527733.424
  load_time_ms: 4.677
  training_iteration_time_ms: 56604.022
  update_time_ms: 2.535
timesteps_total: 3283500
training_iteration: 199

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972972972972973
  reward for individual goal_min: 0.5
episode_len_mean: 42.096692111959285
episode_reward_max: 2.0
episode_reward_mean: 1.9974554707379135
episode_reward_min: 1.0
episodes_this_iter: 393
episodes_total: 70370
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974554707379135
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.68186974525452
time_total_s: 11144.087002038956
timers:
  learn_throughput: 453.842
  learn_time_ms: 36356.256
  load_throughput: 4343210.307
  load_time_ms: 3.799
  training_iteration_time_ms: 47846.778
  update_time_ms: 2.39
timesteps_total: 3927000
training_iteration: 238

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3269230769230769
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9782608695652174
  reward for individual goal_min: 0.0
episode_len_mean: 169.51
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 18576
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.873128175735474
time_total_s: 11269.173449516296
timers:
  learn_throughput: 404.277
  learn_time_ms: 40813.612
  load_throughput: 4357759.601
  load_time_ms: 3.786
  training_iteration_time_ms: 53611.255
  update_time_ms: 2.702
timesteps_total: 3415500
training_iteration: 207

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9711538461538461
  reward for individual goal_min: 0.5
episode_len_mean: 55.88333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.97
episode_reward_min: 1.0
episodes_this_iter: 300
episodes_total: 32915
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.97
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 59.771376609802246
time_total_s: 11275.573048591614
timers:
  learn_throughput: 342.826
  learn_time_ms: 48129.374
  load_throughput: 3766231.81
  load_time_ms: 4.381
  training_iteration_time_ms: 62606.702
  update_time_ms: 2.707
timesteps_total: 3052500
training_iteration: 185

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2571428571428571
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9024390243902439
  reward for individual goal_min: 0.0
episode_len_mean: 181.51
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 20457
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.7920880317688
time_total_s: 11293.630281448364
timers:
  learn_throughput: 497.559
  learn_time_ms: 33161.882
  load_throughput: 4680921.223
  load_time_ms: 3.525
  training_iteration_time_ms: 44183.329
  update_time_ms: 2.631
timesteps_total: 4092000
training_iteration: 248

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.29
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 18675
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.95192241668701
time_total_s: 11292.10445356369
timers:
  learn_throughput: 402.429
  learn_time_ms: 41001.011
  load_throughput: 4311766.985
  load_time_ms: 3.827
  training_iteration_time_ms: 54060.661
  update_time_ms: 2.729
timesteps_total: 3415500
training_iteration: 207

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21052631578947367
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 182.13
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 23303
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.74590754508972
time_total_s: 11292.370409965515
timers:
  learn_throughput: 514.393
  learn_time_ms: 32076.627
  load_throughput: 5105307.435
  load_time_ms: 3.232
  training_iteration_time_ms: 42537.962
  update_time_ms: 2.797
timesteps_total: 4224000
training_iteration: 256

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1780821917808219
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9879518072289156
  reward for individual goal_min: 0.0
episode_len_mean: 177.3
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 20074
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.97395181655884
time_total_s: 11309.314974546432
timers:
  learn_throughput: 473.907
  learn_time_ms: 34816.979
  load_throughput: 4992318.557
  load_time_ms: 3.305
  training_iteration_time_ms: 45910.193
  update_time_ms: 2.659
timesteps_total: 3712500
training_iteration: 225

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9930555555555556
  reward for individual goal_min: 0.5
episode_len_mean: 176.71
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 19599
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.98580861091614
time_total_s: 11311.528152704239
timers:
  learn_throughput: 443.346
  learn_time_ms: 37216.965
  load_throughput: 4890089.667
  load_time_ms: 3.374
  training_iteration_time_ms: 48897.629
  update_time_ms: 2.748
timesteps_total: 3564000
training_iteration: 216

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.7666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 122.5
episode_reward_max: 2.0
episode_reward_mean: 1.7333333333333334
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9833333333333333
  agent_1: 0.75
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939024390243902
  reward for individual goal_min: 0.5
episode_len_mean: 182.73
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 18431
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.44364380836487
time_total_s: 11316.892812013626
timers:
  learn_throughput: 456.322
  learn_time_ms: 36158.669
  load_throughput: 4692601.387
  load_time_ms: 3.516
  training_iteration_time_ms: 47639.975
  update_time_ms: 2.697
timesteps_total: 3630000
training_iteration: 220

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2642857142857143
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 165.93
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 20468
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.46219348907471
time_total_s: 11297.002978086472
timers:
  learn_throughput: 475.007
  learn_time_ms: 34736.295
  load_throughput: 4897772.556
  load_time_ms: 3.369
  training_iteration_time_ms: 45878.061
  update_time_ms: 2.518
timesteps_total: 3712500
training_iteration: 225

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3141025641025641
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.88
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 18886
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.875792264938354
time_total_s: 11302.49299788475
timers:
  learn_throughput: 461.469
  learn_time_ms: 35755.374
  load_throughput: 4779288.97
  load_time_ms: 3.452
  training_iteration_time_ms: 47432.501
  update_time_ms: 2.507
timesteps_total: 3613500
training_iteration: 219

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9
  reward for individual goal_min: 0.5
episode_len_mean: 108.3
episode_reward_max: 2.0
episode_reward_mean: 1.8666666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.95
  agent_1: 0.9166666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16455696202531644
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8851351351351351
  reward for individual goal_min: 0.0
episode_len_mean: 214.98
episode_reward_max: 2.0
episode_reward_mean: 1.08
episode_reward_min: 0.0
episodes_this_iter: 76
episodes_total: 19710
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.15976643562317
time_total_s: 11312.14597606659
timers:
  learn_throughput: 481.452
  learn_time_ms: 34271.299
  load_throughput: 4964349.885
  load_time_ms: 3.324
  training_iteration_time_ms: 45236.665
  update_time_ms: 2.471
timesteps_total: 3960000
training_iteration: 240

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19istq_qwi/checkpoint_000240/checkpoint-240
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.55
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974874371859297
  reward for individual goal_min: 0.5
episode_len_mean: 42.568062827225134
episode_reward_max: 2.0
episode_reward_mean: 1.9973821989528795
episode_reward_min: 1.0
episodes_this_iter: 382
episodes_total: 70752
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973821989528796
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 47.94763374328613
time_total_s: 11192.034635782242
timers:
  learn_throughput: 453.54
  learn_time_ms: 36380.485
  load_throughput: 4293310.338
  load_time_ms: 3.843
  training_iteration_time_ms: 47876.565
  update_time_ms: 2.398
timesteps_total: 3943500
training_iteration: 239

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9887005649717514
  reward for individual goal_min: 0.5
episode_len_mean: 46.67134831460674
episode_reward_max: 2.0
episode_reward_mean: 1.9887640449438202
episode_reward_min: 1.0
episodes_this_iter: 356
episodes_total: 49113
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9887640449438202
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 59.325047969818115
time_total_s: 11267.778872966766
timers:
  learn_throughput: 401.959
  learn_time_ms: 41048.944
  load_throughput: 4211276.721
  load_time_ms: 3.918
  training_iteration_time_ms: 53300.927
  update_time_ms: 2.612
timesteps_total: 3630000
training_iteration: 220

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-18g5xov65h/checkpoint_000220/checkpoint-220
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2246376811594203
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8618421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 184.93
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 20545
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.843804121017456
time_total_s: 11337.474085569382
timers:
  learn_throughput: 500.929
  learn_time_ms: 32938.77
  load_throughput: 4708020.354
  load_time_ms: 3.505
  training_iteration_time_ms: 43886.81
  update_time_ms: 2.642
timesteps_total: 4108500
training_iteration: 249

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.81666666666667
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.609947643979055
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 382
episodes_total: 52787
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 61.2650465965271
time_total_s: 11257.407153129578
timers:
  learn_throughput: 377.48
  learn_time_ms: 43710.959
  load_throughput: 3529190.601
  load_time_ms: 4.675
  training_iteration_time_ms: 56532.562
  update_time_ms: 2.857
timesteps_total: 3300000
training_iteration: 200

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-184s1jdk42/checkpoint_000200/checkpoint-200
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26282051282051283
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9492753623188406
  reward for individual goal_min: 0.0
episode_len_mean: 171.82
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 18668
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.355785846710205
time_total_s: 11324.529235363007
timers:
  learn_throughput: 403.292
  learn_time_ms: 40913.242
  load_throughput: 4430546.088
  load_time_ms: 3.724
  training_iteration_time_ms: 53735.972
  update_time_ms: 2.697
timesteps_total: 3432000
training_iteration: 208

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 180.83
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 23394
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.457459449768066
time_total_s: 11335.827869415283
timers:
  learn_throughput: 514.891
  learn_time_ms: 32045.642
  load_throughput: 5139276.851
  load_time_ms: 3.211
  training_iteration_time_ms: 42511.456
  update_time_ms: 2.787
timesteps_total: 4240500
training_iteration: 257

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9935064935064936
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9545454545454546
  reward for individual goal_min: 0.5
episode_len_mean: 56.94405594405595
episode_reward_max: 2.0
episode_reward_mean: 1.951048951048951
episode_reward_min: 0.0
episodes_this_iter: 286
episodes_total: 33201
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9965034965034965
  agent_1: 0.9545454545454546
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 67.01946091651917
time_total_s: 11342.592509508133
timers:
  learn_throughput: 341.094
  learn_time_ms: 48373.795
  load_throughput: 3776342.941
  load_time_ms: 4.369
  training_iteration_time_ms: 62795.942
  update_time_ms: 2.796
timesteps_total: 3069000
training_iteration: 186

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14864864864864866
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.46
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 18766
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.53794074058533
time_total_s: 11344.642394304276
timers:
  learn_throughput: 405.517
  learn_time_ms: 40688.762
  load_throughput: 4305195.397
  load_time_ms: 3.833
  training_iteration_time_ms: 53702.711
  update_time_ms: 2.737
timesteps_total: 3432000
training_iteration: 208

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2532467532467532
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9938271604938271
  reward for individual goal_min: 0.5
episode_len_mean: 176.87
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 20167
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.38429093360901
time_total_s: 11353.699265480042
timers:
  learn_throughput: 475.715
  learn_time_ms: 34684.601
  load_throughput: 4990554.538
  load_time_ms: 3.306
  training_iteration_time_ms: 45689.22
  update_time_ms: 2.668
timesteps_total: 3729000
training_iteration: 226

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2465753424657534
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935897435897436
  reward for individual goal_min: 0.5
episode_len_mean: 172.35
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 19690
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.202916860580444
time_total_s: 11359.73106956482
timers:
  learn_throughput: 442.551
  learn_time_ms: 37283.801
  load_throughput: 4869787.845
  load_time_ms: 3.388
  training_iteration_time_ms: 48990.997
  update_time_ms: 2.741
timesteps_total: 3580500
training_iteration: 217

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24324324324324326
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95
  reward for individual goal_min: 0.0
episode_len_mean: 183.16
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 18516
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.57100558280945
time_total_s: 11365.463817596436
timers:
  learn_throughput: 454.584
  learn_time_ms: 36296.886
  load_throughput: 4696327.14
  load_time_ms: 3.513
  training_iteration_time_ms: 47809.615
  update_time_ms: 2.704
timesteps_total: 3646500
training_iteration: 221

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2152777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.97
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 20559
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.38263392448425
time_total_s: 11345.385612010956
timers:
  learn_throughput: 469.164
  learn_time_ms: 35168.913
  load_throughput: 4919952.227
  load_time_ms: 3.354
  training_iteration_time_ms: 46377.095
  update_time_ms: 2.503
timesteps_total: 3729000
training_iteration: 226

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17567567567567569
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8974358974358975
  reward for individual goal_min: 0.0
episode_len_mean: 194.99
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 19794
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.36965775489807
time_total_s: 11357.515633821487
timers:
  learn_throughput: 480.762
  learn_time_ms: 34320.517
  load_throughput: 4962392.067
  load_time_ms: 3.325
  training_iteration_time_ms: 45278.542
  update_time_ms: 2.47
timesteps_total: 3976500
training_iteration: 241

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2887323943661972
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8846153846153846
  reward for individual goal_min: 0.0
episode_len_mean: 184.94
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 20631
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.08761978149414
time_total_s: 11380.561705350876
timers:
  learn_throughput: 501.949
  learn_time_ms: 32871.889
  load_throughput: 4741664.508
  load_time_ms: 3.48
  training_iteration_time_ms: 43791.875
  update_time_ms: 2.652
timesteps_total: 4125000
training_iteration: 250

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.6666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 93.95
episode_reward_max: 2.0
episode_reward_mean: 1.6333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8
  agent_1: 0.8333333333333334
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18055555555555555
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 179.83
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 18977
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.638816595077515
time_total_s: 11363.131814479828
timers:
  learn_throughput: 458.155
  learn_time_ms: 36014.017
  load_throughput: 4730030.073
  load_time_ms: 3.488
  training_iteration_time_ms: 47808.661
  update_time_ms: 2.503
timesteps_total: 3630000
training_iteration: 220

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.766666666666666
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9950248756218906
  reward for individual goal_min: 0.5
episode_len_mean: 43.66579634464752
episode_reward_max: 2.0
episode_reward_mean: 1.9947780678851175
episode_reward_min: 1.0
episodes_this_iter: 383
episodes_total: 71135
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973890339425587
  agent_1: 0.9973890339425587
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.50531458854675
time_total_s: 11244.539950370789
timers:
  learn_throughput: 453.236
  learn_time_ms: 36404.909
  load_throughput: 4320137.833
  load_time_ms: 3.819
  training_iteration_time_ms: 47937.42
  update_time_ms: 2.415
timesteps_total: 3960000
training_iteration: 240

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-172kuwc_58/checkpoint_000240/checkpoint-240
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9885057471264368
  reward for individual goal_min: 0.5
episode_len_mean: 48.26979472140763
episode_reward_max: 2.0
episode_reward_mean: 1.9882697947214076
episode_reward_min: 1.0
episodes_this_iter: 341
episodes_total: 49454
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9882697947214076
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 53.41493606567383
time_total_s: 11321.19380903244
timers:
  learn_throughput: 402.017
  learn_time_ms: 41043.04
  load_throughput: 4216844.953
  load_time_ms: 3.913
  training_iteration_time_ms: 53297.695
  update_time_ms: 2.614
timesteps_total: 3646500
training_iteration: 221

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2391304347826087
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9533333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 171.94
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 18765
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.79850625991821
time_total_s: 11377.327741622925
timers:
  learn_throughput: 403.555
  learn_time_ms: 40886.656
  load_throughput: 4392470.994
  load_time_ms: 3.756
  training_iteration_time_ms: 53736.936
  update_time_ms: 2.72
timesteps_total: 3448500
training_iteration: 209

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21153846153846154
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 187.66
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 23481
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.70291185379028
time_total_s: 11379.530781269073
timers:
  learn_throughput: 513.436
  learn_time_ms: 32136.408
  load_throughput: 5105232.113
  load_time_ms: 3.232
  training_iteration_time_ms: 42678.982
  update_time_ms: 2.774
timesteps_total: 4257000
training_iteration: 258

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.12468827930174
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 401
episodes_total: 53188
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.677120208740234
time_total_s: 11315.084273338318
timers:
  learn_throughput: 377.922
  learn_time_ms: 43659.756
  load_throughput: 3529082.62
  load_time_ms: 4.675
  training_iteration_time_ms: 56460.589
  update_time_ms: 2.845
timesteps_total: 3316500
training_iteration: 201

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 195.34
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 20251
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.3576979637146
time_total_s: 11401.056963443756
timers:
  learn_throughput: 473.874
  learn_time_ms: 34819.399
  load_throughput: 4985736.845
  load_time_ms: 3.309
  training_iteration_time_ms: 45863.982
  update_time_ms: 2.67
timesteps_total: 3745500
training_iteration: 227

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1956521739130435
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 167.71
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 18865
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.99340295791626
time_total_s: 11398.635797262192
timers:
  learn_throughput: 405.753
  learn_time_ms: 40665.154
  load_throughput: 4347958.208
  load_time_ms: 3.795
  training_iteration_time_ms: 53715.049
  update_time_ms: 2.728
timesteps_total: 3448500
training_iteration: 209

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1780821917808219
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9927536231884058
  reward for individual goal_min: 0.5
episode_len_mean: 181.24
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 19782
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.210787534713745
time_total_s: 11410.941857099533
timers:
  learn_throughput: 445.424
  learn_time_ms: 37043.39
  load_throughput: 4883774.576
  load_time_ms: 3.379
  training_iteration_time_ms: 48910.853
  update_time_ms: 2.725
timesteps_total: 3597000
training_iteration: 218

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22839506172839505
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 189.76
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 20646
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.825960636138916
time_total_s: 11394.211572647095
timers:
  learn_throughput: 462.019
  learn_time_ms: 35712.832
  load_throughput: 4912548.34
  load_time_ms: 3.359
  training_iteration_time_ms: 47021.731
  update_time_ms: 2.512
timesteps_total: 3745500
training_iteration: 227

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2692307692307692
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9333333333333333
  reward for individual goal_min: 0.0
episode_len_mean: 182.65
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 19885
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.86893391609192
time_total_s: 11403.38456773758
timers:
  learn_throughput: 479.919
  learn_time_ms: 34380.78
  load_throughput: 4972518.16
  load_time_ms: 3.318
  training_iteration_time_ms: 45343.351
  update_time_ms: 2.477
timesteps_total: 3993000
training_iteration: 242

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2808219178082192
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9166666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 175.83
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 18612
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.43134784698486
time_total_s: 11414.89516544342
timers:
  learn_throughput: 453.595
  learn_time_ms: 36376.054
  load_throughput: 4704883.68
  load_time_ms: 3.507
  training_iteration_time_ms: 47908.717
  update_time_ms: 2.701
timesteps_total: 3663000
training_iteration: 222

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3618421052631579
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.875
  reward for individual goal_min: 0.0
episode_len_mean: 179.62
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 20726
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.76
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.50871253013611
time_total_s: 11425.070417881012
timers:
  learn_throughput: 502.103
  learn_time_ms: 32861.804
  load_throughput: 4728349.594
  load_time_ms: 3.49
  training_iteration_time_ms: 43772.312
  update_time_ms: 2.647
timesteps_total: 4141500
training_iteration: 251

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9761904761904762
  reward for individual goal_min: 0.5
episode_len_mean: 53.198083067092654
episode_reward_max: 2.0
episode_reward_mean: 1.97444089456869
episode_reward_min: 1.0
episodes_this_iter: 313
episodes_total: 33514
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9744408945686901
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 68.3727662563324
time_total_s: 11410.965275764465
timers:
  learn_throughput: 338.95
  learn_time_ms: 48679.794
  load_throughput: 3810211.58
  load_time_ms: 4.33
  training_iteration_time_ms: 63227.593
  update_time_ms: 2.804
timesteps_total: 3085500
training_iteration: 187

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 166.1
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 19075
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.59874081611633
time_total_s: 11412.730555295944
timers:
  learn_throughput: 452.113
  learn_time_ms: 36495.302
  load_throughput: 4675387.172
  load_time_ms: 3.529
  training_iteration_time_ms: 48371.028
  update_time_ms: 2.503
timesteps_total: 3646500
training_iteration: 221

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972222222222222
  reward for individual goal_min: 0.5
episode_len_mean: 42.33762886597938
episode_reward_max: 2.0
episode_reward_mean: 1.9974226804123711
episode_reward_min: 1.0
episodes_this_iter: 388
episodes_total: 71523
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974226804123711
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.649635314941406
time_total_s: 11293.18958568573
timers:
  learn_throughput: 452.738
  learn_time_ms: 36444.953
  load_throughput: 4334478.405
  load_time_ms: 3.807
  training_iteration_time_ms: 48018.273
  update_time_ms: 2.399
timesteps_total: 3976500
training_iteration: 241

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.67663043478261
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 368
episodes_total: 49822
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 53.722901821136475
time_total_s: 11374.916710853577
timers:
  learn_throughput: 401.692
  learn_time_ms: 41076.205
  load_throughput: 4216202.701
  load_time_ms: 3.913
  training_iteration_time_ms: 53337.648
  update_time_ms: 2.614
timesteps_total: 3663000
training_iteration: 222

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2905405405405405
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.12
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 23579
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.268497943878174
time_total_s: 11422.799279212952
timers:
  learn_throughput: 513.002
  learn_time_ms: 32163.617
  load_throughput: 5075354.841
  load_time_ms: 3.251
  training_iteration_time_ms: 42751.285
  update_time_ms: 2.763
timesteps_total: 4273500
training_iteration: 259

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23780487804878048
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9652777777777778
  reward for individual goal_min: 0.0
episode_len_mean: 192.54
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 18851
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.37137579917908
time_total_s: 11428.699117422104
timers:
  learn_throughput: 405.155
  learn_time_ms: 40725.145
  load_throughput: 4386235.011
  load_time_ms: 3.762
  training_iteration_time_ms: 53501.733
  update_time_ms: 2.734
timesteps_total: 3465000
training_iteration: 210

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2733333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9926470588235294
  reward for individual goal_min: 0.5
episode_len_mean: 181.7
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 20343
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.45272636413574
time_total_s: 11448.509689807892
timers:
  learn_throughput: 472.813
  learn_time_ms: 34897.545
  load_throughput: 5005425.641
  load_time_ms: 3.296
  training_iteration_time_ms: 45935.772
  update_time_ms: 2.681
timesteps_total: 3762000
training_iteration: 228

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.23684210526316
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 380
episodes_total: 53568
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.604578495025635
time_total_s: 11372.688851833344
timers:
  learn_throughput: 377.496
  learn_time_ms: 43709.115
  load_throughput: 3520752.114
  load_time_ms: 4.686
  training_iteration_time_ms: 56519.795
  update_time_ms: 2.841
timesteps_total: 3333000
training_iteration: 202

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2246376811594203
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939759036144579
  reward for individual goal_min: 0.5
episode_len_mean: 162.37864077669903
episode_reward_max: 2.0
episode_reward_mean: 1.3495145631067962
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 18968
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6213592233009708
  agent_1: 0.7281553398058253
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.992151975631714
time_total_s: 11450.627949237823
timers:
  learn_throughput: 405.977
  learn_time_ms: 40642.67
  load_throughput: 4330924.998
  load_time_ms: 3.81
  training_iteration_time_ms: 53641.9
  update_time_ms: 2.71
timesteps_total: 3465000
training_iteration: 210

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18493150684931506
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9567901234567902
  reward for individual goal_min: 0.0
episode_len_mean: 186.8
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 19973
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.87230205535889
time_total_s: 11447.256869792938
timers:
  learn_throughput: 480.07
  learn_time_ms: 34369.988
  load_throughput: 5028154.929
  load_time_ms: 3.282
  training_iteration_time_ms: 45332.843
  update_time_ms: 2.489
timesteps_total: 4009500
training_iteration: 243

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8846153846153846
  reward for individual goal_min: 0.0
episode_len_mean: 191.44
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 20810
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.07375359535217
time_total_s: 11468.144171476364
timers:
  learn_throughput: 505.319
  learn_time_ms: 32652.62
  load_throughput: 4748008.068
  load_time_ms: 3.475
  training_iteration_time_ms: 43474.698
  update_time_ms: 2.617
timesteps_total: 4158000
training_iteration: 252

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2876712328767123
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935897435897436
  reward for individual goal_min: 0.5
episode_len_mean: 166.8
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 20742
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.17543888092041
time_total_s: 11441.387011528015
timers:
  learn_throughput: 459.013
  learn_time_ms: 35946.714
  load_throughput: 4899506.269
  load_time_ms: 3.368
  training_iteration_time_ms: 47269.639
  update_time_ms: 2.512
timesteps_total: 3762000
training_iteration: 228

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3108108108108108
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9054054054054054
  reward for individual goal_min: 0.0
episode_len_mean: 184.3
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 18700
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.201178550720215
time_total_s: 11464.09634399414
timers:
  learn_throughput: 450.579
  learn_time_ms: 36619.588
  load_throughput: 4723573.223
  load_time_ms: 3.493
  training_iteration_time_ms: 48147.827
  update_time_ms: 2.699
timesteps_total: 3679500
training_iteration: 223

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24324324324324326
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 176.09
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 19874
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.09854578971863
time_total_s: 11464.040402889252
timers:
  learn_throughput: 439.223
  learn_time_ms: 37566.345
  load_throughput: 4841714.602
  load_time_ms: 3.408
  training_iteration_time_ms: 49461.764
  update_time_ms: 2.71
timesteps_total: 3613500
training_iteration: 219

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14084507042253522
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 181.86
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 19167
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.56362700462341
time_total_s: 11464.294182300568
timers:
  learn_throughput: 445.809
  learn_time_ms: 37011.373
  load_throughput: 4661813.234
  load_time_ms: 3.539
  training_iteration_time_ms: 48997.009
  update_time_ms: 2.502
timesteps_total: 3663000
training_iteration: 222

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.07106598984772
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 394
episodes_total: 71917
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.97743248939514
time_total_s: 11341.167018175125
timers:
  learn_throughput: 453.043
  learn_time_ms: 36420.418
  load_throughput: 4332606.036
  load_time_ms: 3.808
  training_iteration_time_ms: 48008.002
  update_time_ms: 2.412
timesteps_total: 3993000
training_iteration: 242

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9496644295302014
  reward for individual goal_min: 0.5
episode_len_mean: 57.20136518771331
episode_reward_max: 2.0
episode_reward_mean: 1.9488054607508531
episode_reward_min: 1.0
episodes_this_iter: 293
episodes_total: 33807
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9931740614334471
  agent_1: 0.9556313993174061
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 66.63760280609131
time_total_s: 11477.602878570557
timers:
  learn_throughput: 337.827
  learn_time_ms: 48841.497
  load_throughput: 3843177.343
  load_time_ms: 4.293
  training_iteration_time_ms: 63457.653
  update_time_ms: 2.775
timesteps_total: 3102000
training_iteration: 188

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 78.66666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.9833333333333334
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9833333333333333
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 186.26
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 23667
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.77456855773926
time_total_s: 11472.573847770691
timers:
  learn_throughput: 514.473
  learn_time_ms: 32071.67
  load_throughput: 5101205.608
  load_time_ms: 3.235
  training_iteration_time_ms: 42576.23
  update_time_ms: 2.483
timesteps_total: 4290000
training_iteration: 260

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971264367816092
  reward for individual goal_min: 0.5
episode_len_mean: 45.782967032967036
episode_reward_max: 2.0
episode_reward_mean: 1.9972527472527473
episode_reward_min: 1.0
episodes_this_iter: 364
episodes_total: 50186
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972527472527473
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 53.6922721862793
time_total_s: 11428.608983039856
timers:
  learn_throughput: 400.836
  learn_time_ms: 41164.014
  load_throughput: 4243554.956
  load_time_ms: 3.888
  training_iteration_time_ms: 53383.786
  update_time_ms: 2.587
timesteps_total: 3679500
training_iteration: 223

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2597402597402597
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 178.93
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 18946
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.10803747177124
time_total_s: 11480.807154893875
timers:
  learn_throughput: 405.875
  learn_time_ms: 40652.915
  load_throughput: 4370667.037
  load_time_ms: 3.775
  training_iteration_time_ms: 53395.094
  update_time_ms: 2.745
timesteps_total: 3481500
training_iteration: 211

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2894736842105263
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.9
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 20433
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.491660356521606
time_total_s: 11496.001350164413
timers:
  learn_throughput: 470.381
  learn_time_ms: 35077.951
  load_throughput: 4973089.874
  load_time_ms: 3.318
  training_iteration_time_ms: 46132.526
  update_time_ms: 2.752
timesteps_total: 3778500
training_iteration: 229

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14383561643835616
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9415584415584416
  reward for individual goal_min: 0.0
episode_len_mean: 197.57
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 20057
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.717395067214966
time_total_s: 11491.974264860153
timers:
  learn_throughput: 481.607
  learn_time_ms: 34260.281
  load_throughput: 5021733.508
  load_time_ms: 3.286
  training_iteration_time_ms: 45211.409
  update_time_ms: 2.493
timesteps_total: 4026000
training_iteration: 244

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27941176470588236
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8782051282051282
  reward for individual goal_min: 0.0
episode_len_mean: 178.14
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 20903
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.920539140701294
time_total_s: 11512.064710617065
timers:
  learn_throughput: 502.028
  learn_time_ms: 32866.662
  load_throughput: 4753323.672
  load_time_ms: 3.471
  training_iteration_time_ms: 43714.674
  update_time_ms: 2.607
timesteps_total: 4174500
training_iteration: 253

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3150684931506849
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9821428571428571
  reward for individual goal_min: 0.0
episode_len_mean: 154.4392523364486
episode_reward_max: 2.0
episode_reward_mean: 1.4299065420560748
episode_reward_min: 0.0
episodes_this_iter: 107
episodes_total: 20849
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6728971962616822
  agent_1: 0.7570093457943925
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.94153571128845
time_total_s: 11490.328547239304
timers:
  learn_throughput: 455.805
  learn_time_ms: 36199.705
  load_throughput: 4913071.468
  load_time_ms: 3.358
  training_iteration_time_ms: 47598.395
  update_time_ms: 2.514
timesteps_total: 3778500
training_iteration: 229

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24324324324324326
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.5
episode_len_mean: 175.03
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 19062
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.81825113296509
time_total_s: 11504.446200370789
timers:
  learn_throughput: 406.789
  learn_time_ms: 40561.61
  load_throughput: 4320623.314
  load_time_ms: 3.819
  training_iteration_time_ms: 53486.37
  update_time_ms: 2.727
timesteps_total: 3481500
training_iteration: 211

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.62015503875969
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 387
episodes_total: 53955
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.778759479522705
time_total_s: 11430.467611312866
timers:
  learn_throughput: 377.251
  learn_time_ms: 43737.414
  load_throughput: 3533659.573
  load_time_ms: 4.669
  training_iteration_time_ms: 56551.019
  update_time_ms: 2.849
timesteps_total: 3349500
training_iteration: 203

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9333333333333333
  reward for individual goal_min: 0.0
episode_len_mean: 196.3
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 18785
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.4311683177948
time_total_s: 11514.527512311935
timers:
  learn_throughput: 448.816
  learn_time_ms: 36763.4
  load_throughput: 4732617.758
  load_time_ms: 3.486
  training_iteration_time_ms: 48349.681
  update_time_ms: 2.76
timesteps_total: 3696000
training_iteration: 224

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 81.03333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.8
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9166666666666666
  agent_1: 0.8833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.345679012345679
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9930555555555556
  reward for individual goal_min: 0.5
episode_len_mean: 170.38
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 19973
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.71068453788757
time_total_s: 11526.75108742714
timers:
  learn_throughput: 433.269
  learn_time_ms: 38082.596
  load_throughput: 4808845.283
  load_time_ms: 3.431
  training_iteration_time_ms: 50250.188
  update_time_ms: 2.722
timesteps_total: 3630000
training_iteration: 220

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23376623376623376
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 179.41
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 23756
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.83035230636597
time_total_s: 11514.404200077057
timers:
  learn_throughput: 514.992
  learn_time_ms: 32039.319
  load_throughput: 5119659.114
  load_time_ms: 3.223
  training_iteration_time_ms: 42567.368
  update_time_ms: 2.481
timesteps_total: 4306500
training_iteration: 261

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974226804123711
  reward for individual goal_min: 0.5
episode_len_mean: 41.651399491094146
episode_reward_max: 2.0
episode_reward_mean: 1.9974554707379135
episode_reward_min: 1.0
episodes_this_iter: 393
episodes_total: 72310
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9974554707379135
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 48.04511475563049
time_total_s: 11389.212132930756
timers:
  learn_throughput: 452.607
  learn_time_ms: 36455.447
  load_throughput: 4346183.353
  load_time_ms: 3.796
  training_iteration_time_ms: 48034.247
  update_time_ms: 2.387
timesteps_total: 4009500
training_iteration: 243

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2911392405063291
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.78
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 19264
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.83911347389221
time_total_s: 11517.13329577446
timers:
  learn_throughput: 442.865
  learn_time_ms: 37257.407
  load_throughput: 4616627.487
  load_time_ms: 3.574
  training_iteration_time_ms: 49351.204
  update_time_ms: 2.521
timesteps_total: 3679500
training_iteration: 223

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19078947368421054
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9743589743589743
  reward for individual goal_min: 0.0
episode_len_mean: 182.52
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 19035
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.31417679786682
time_total_s: 11532.121331691742
timers:
  learn_throughput: 406.957
  learn_time_ms: 40544.795
  load_throughput: 4367136.745
  load_time_ms: 3.778
  training_iteration_time_ms: 53267.224
  update_time_ms: 2.771
timesteps_total: 3498000
training_iteration: 212

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20512820512820512
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9746835443037974
  reward for individual goal_min: 0.0
episode_len_mean: 187.94
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 20521
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.834564447402954
time_total_s: 11542.835914611816
timers:
  learn_throughput: 469.097
  learn_time_ms: 35173.998
  load_throughput: 5028264.528
  load_time_ms: 3.281
  training_iteration_time_ms: 46291.705
  update_time_ms: 2.747
timesteps_total: 3795000
training_iteration: 230

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21232876712328766
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8734177215189873
  reward for individual goal_min: 0.0
episode_len_mean: 202.72
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 77
episodes_total: 20980
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.76
  agent_1: 0.42
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.54616928100586
time_total_s: 11554.610879898071
timers:
  learn_throughput: 502.836
  learn_time_ms: 32813.879
  load_throughput: 4719192.625
  load_time_ms: 3.496
  training_iteration_time_ms: 43625.383
  update_time_ms: 2.646
timesteps_total: 4191000
training_iteration: 254

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.996969696969697
  reward for individual goal_min: 0.5
episode_len_mean: 45.184573002754824
episode_reward_max: 2.0
episode_reward_mean: 1.997245179063361
episode_reward_min: 1.0
episodes_this_iter: 363
episodes_total: 50549
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972451790633609
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 53.84338665008545
time_total_s: 11482.452369689941
timers:
  learn_throughput: 400.701
  learn_time_ms: 41177.815
  load_throughput: 4238149.584
  load_time_ms: 3.893
  training_iteration_time_ms: 53373.133
  update_time_ms: 2.598
timesteps_total: 3696000
training_iteration: 224

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.12857142857142856
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9578313253012049
  reward for individual goal_min: 0.0
episode_len_mean: 190.31
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 20144
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.33273124694824
time_total_s: 11535.306996107101
timers:
  learn_throughput: 483.75
  learn_time_ms: 34108.523
  load_throughput: 5055777.916
  load_time_ms: 3.264
  training_iteration_time_ms: 45001.156
  update_time_ms: 2.523
timesteps_total: 4042500
training_iteration: 245

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9675324675324676
  reward for individual goal_min: 0.5
episode_len_mean: 56.095238095238095
episode_reward_max: 2.0
episode_reward_mean: 1.965986394557823
episode_reward_min: 1.0
episodes_this_iter: 294
episodes_total: 34101
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9659863945578231
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 68.13342547416687
time_total_s: 11545.736304044724
timers:
  learn_throughput: 335.699
  learn_time_ms: 49151.11
  load_throughput: 3791632.616
  load_time_ms: 4.352
  training_iteration_time_ms: 63887.548
  update_time_ms: 2.815
timesteps_total: 3118500
training_iteration: 189

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2945205479452055
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 166.37
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 20949
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.13327360153198
time_total_s: 11542.461820840836
timers:
  learn_throughput: 451.309
  learn_time_ms: 36560.346
  load_throughput: 4862294.916
  load_time_ms: 3.393
  training_iteration_time_ms: 48021.952
  update_time_ms: 2.524
timesteps_total: 3795000
training_iteration: 230

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9671052631578947
  reward for individual goal_min: 0.0
episode_len_mean: 198.91
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 18867
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.125492572784424
time_total_s: 11562.65300488472
timers:
  learn_throughput: 447.902
  learn_time_ms: 36838.428
  load_throughput: 4760811.744
  load_time_ms: 3.466
  training_iteration_time_ms: 48357.313
  update_time_ms: 2.786
timesteps_total: 3712500
training_iteration: 225

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2529411764705882
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.34
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 19155
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.87627649307251
time_total_s: 11558.322476863861
timers:
  learn_throughput: 405.725
  learn_time_ms: 40667.924
  load_throughput: 4325132.711
  load_time_ms: 3.815
  training_iteration_time_ms: 53637.118
  update_time_ms: 2.978
timesteps_total: 3498000
training_iteration: 212

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9948979591836735
  reward for individual goal_min: 0.5
episode_len_mean: 44.280748663101605
episode_reward_max: 2.0
episode_reward_mean: 1.9946524064171123
episode_reward_min: 1.0
episodes_this_iter: 374
episodes_total: 54329
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973262032085561
  agent_1: 0.9973262032085561
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.09020805358887
time_total_s: 11486.557819366455
timers:
  learn_throughput: 378.045
  learn_time_ms: 43645.56
  load_throughput: 3553418.122
  load_time_ms: 4.643
  training_iteration_time_ms: 56399.063
  update_time_ms: 2.856
timesteps_total: 3366000
training_iteration: 204

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23717948717948717
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 185.63
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 23847
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.11807942390442
time_total_s: 11557.522279500961
timers:
  learn_throughput: 511.911
  learn_time_ms: 32232.145
  load_throughput: 5197910.201
  load_time_ms: 3.174
  training_iteration_time_ms: 42807.151
  update_time_ms: 2.487
timesteps_total: 4323000
training_iteration: 262

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974489795918368
  reward for individual goal_min: 0.5
episode_len_mean: 43.431937172774866
episode_reward_max: 2.0
episode_reward_mean: 1.9973821989528795
episode_reward_min: 1.0
episodes_this_iter: 382
episodes_total: 72692
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973821989528796
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.89098238945007
time_total_s: 11437.103115320206
timers:
  learn_throughput: 453.552
  learn_time_ms: 36379.518
  load_throughput: 4366585.652
  load_time_ms: 3.779
  training_iteration_time_ms: 47942.265
  update_time_ms: 2.385
timesteps_total: 4026000
training_iteration: 244

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23972602739726026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 169.45
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 20070
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.92898511886597
time_total_s: 11578.680072546005
timers:
  learn_throughput: 431.797
  learn_time_ms: 38212.364
  load_throughput: 4756819.532
  load_time_ms: 3.469
  training_iteration_time_ms: 50515.964
  update_time_ms: 2.707
timesteps_total: 3646500
training_iteration: 221

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23134328358208955
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.989010989010989
  reward for individual goal_min: 0.0
episode_len_mean: 148.71559633027522
episode_reward_max: 2.0
episode_reward_mean: 1.4128440366972477
episode_reward_min: 0.0
episodes_this_iter: 109
episodes_total: 19373
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6972477064220184
  agent_1: 0.7155963302752294
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.47537326812744
time_total_s: 11569.608669042587
timers:
  learn_throughput: 437.666
  learn_time_ms: 37700.008
  load_throughput: 4625854.138
  load_time_ms: 3.567
  training_iteration_time_ms: 49923.799
  update_time_ms: 2.551
timesteps_total: 3696000
training_iteration: 224

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25301204819277107
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9025974025974026
  reward for individual goal_min: 0.0
episode_len_mean: 208.32
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 21061
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.179991483688354
time_total_s: 11595.79087138176
timers:
  learn_throughput: 504.986
  learn_time_ms: 32674.187
  load_throughput: 4751169.908
  load_time_ms: 3.473
  training_iteration_time_ms: 43465.36
  update_time_ms: 2.669
timesteps_total: 4207500
training_iteration: 255

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1232876712328767
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9651162790697675
  reward for individual goal_min: 0.0
episode_len_mean: 191.36
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 20231
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.0793092250824
time_total_s: 11580.386305332184
timers:
  learn_throughput: 484.363
  learn_time_ms: 34065.359
  load_throughput: 5084042.197
  load_time_ms: 3.245
  training_iteration_time_ms: 44961.052
  update_time_ms: 2.519
timesteps_total: 4059000
training_iteration: 246

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23417721518987342
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 186.08
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 20611
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.18110513687134
time_total_s: 11591.017019748688
timers:
  learn_throughput: 467.639
  learn_time_ms: 35283.657
  load_throughput: 4986060.13
  load_time_ms: 3.309
  training_iteration_time_ms: 46421.052
  update_time_ms: 2.761
timesteps_total: 3811500
training_iteration: 231

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2328767123287671
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9857142857142858
  reward for individual goal_min: 0.0
episode_len_mean: 175.06
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 19128
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.933212995529175
time_total_s: 11588.054544687271
timers:
  learn_throughput: 406.233
  learn_time_ms: 40617.051
  load_throughput: 4374839.024
  load_time_ms: 3.772
  training_iteration_time_ms: 53384.665
  update_time_ms: 2.781
timesteps_total: 3514500
training_iteration: 213

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9948186528497409
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.04735376044568
episode_reward_max: 2.0
episode_reward_mean: 1.9944289693593316
episode_reward_min: 0.0
episodes_this_iter: 359
episodes_total: 50908
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972144846796658
  agent_1: 0.9972144846796658
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.5913200378418
time_total_s: 11537.043689727783
timers:
  learn_throughput: 398.836
  learn_time_ms: 41370.38
  load_throughput: 4228154.864
  load_time_ms: 3.902
  training_iteration_time_ms: 53608.452
  update_time_ms: 2.612
timesteps_total: 3712500
training_iteration: 225

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.16
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 21044
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.30605435371399
time_total_s: 11591.76787519455
timers:
  learn_throughput: 446.701
  learn_time_ms: 36937.446
  load_throughput: 4817247.03
  load_time_ms: 3.425
  training_iteration_time_ms: 48423.037
  update_time_ms: 2.532
timesteps_total: 3811500
training_iteration: 231

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29605263157894735
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.922077922077922
  reward for individual goal_min: 0.0
episode_len_mean: 182.49
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 18959
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.11678123474121
time_total_s: 11613.769786119461
timers:
  learn_throughput: 445.626
  learn_time_ms: 37026.605
  load_throughput: 4754107.343
  load_time_ms: 3.471
  training_iteration_time_ms: 48541.545
  update_time_ms: 2.783
timesteps_total: 3729000
training_iteration: 226

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2535211267605634
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.54
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 23942
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.75121283531189
time_total_s: 11600.273492336273
timers:
  learn_throughput: 511.677
  learn_time_ms: 32246.879
  load_throughput: 5138780.759
  load_time_ms: 3.211
  training_iteration_time_ms: 42835.282
  update_time_ms: 2.482
timesteps_total: 4339500
training_iteration: 263

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.75
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 19253
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.85220146179199
time_total_s: 11612.174678325653
timers:
  learn_throughput: 405.849
  learn_time_ms: 40655.481
  load_throughput: 4328811.995
  load_time_ms: 3.812
  training_iteration_time_ms: 53586.143
  update_time_ms: 2.985
timesteps_total: 3514500
training_iteration: 213

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9668874172185431
  reward for individual goal_min: 0.5
episode_len_mean: 52.76602564102564
episode_reward_max: 2.0
episode_reward_mean: 1.9679487179487178
episode_reward_min: 1.0
episodes_this_iter: 312
episodes_total: 34413
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.967948717948718
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 68.94640684127808
time_total_s: 11614.682710886002
timers:
  learn_throughput: 331.041
  learn_time_ms: 49842.78
  load_throughput: 3783134.59
  load_time_ms: 4.361
  training_iteration_time_ms: 64675.748
  update_time_ms: 2.839
timesteps_total: 3135000
training_iteration: 190

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973684210526316
  reward for individual goal_min: 0.5
episode_len_mean: 43.90909090909091
episode_reward_max: 2.0
episode_reward_mean: 1.9973262032085561
episode_reward_min: 1.0
episodes_this_iter: 374
episodes_total: 54703
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973262032085561
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 56.326268672943115
time_total_s: 11542.884088039398
timers:
  learn_throughput: 377.863
  learn_time_ms: 43666.624
  load_throughput: 3564160.435
  load_time_ms: 4.629
  training_iteration_time_ms: 56424.42
  update_time_ms: 2.845
timesteps_total: 3382500
training_iteration: 205

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9943502824858758
  reward for individual goal_min: 0.5
episode_len_mean: 43.33681462140992
episode_reward_max: 2.0
episode_reward_mean: 1.9947780678851175
episode_reward_min: 1.0
episodes_this_iter: 383
episodes_total: 73075
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9947780678851175
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.02136588096619
time_total_s: 11485.124481201172
timers:
  learn_throughput: 453.453
  learn_time_ms: 36387.461
  load_throughput: 4366806.073
  load_time_ms: 3.779
  training_iteration_time_ms: 47946.621
  update_time_ms: 2.38
timesteps_total: 4042500
training_iteration: 245

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2987012987012987
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9791666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 177.18
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 20165
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.34780311584473
time_total_s: 11629.02787566185
timers:
  learn_throughput: 434.383
  learn_time_ms: 37984.873
  load_throughput: 4775035.085
  load_time_ms: 3.455
  training_iteration_time_ms: 50197.322
  update_time_ms: 2.706
timesteps_total: 3663000
training_iteration: 222

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33974358974358976
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9066666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 185.52
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 21146
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.38332796096802
time_total_s: 11638.174199342728
timers:
  learn_throughput: 508.912
  learn_time_ms: 32422.096
  load_throughput: 4780444.432
  load_time_ms: 3.452
  training_iteration_time_ms: 43173.298
  update_time_ms: 2.666
timesteps_total: 4224000
training_iteration: 256

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14864864864864866
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.26
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 19462
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.74329662322998
time_total_s: 11620.351965665817
timers:
  learn_throughput: 432.455
  learn_time_ms: 38154.286
  load_throughput: 4580752.979
  load_time_ms: 3.602
  training_iteration_time_ms: 50464.191
  update_time_ms: 2.569
timesteps_total: 3712500
training_iteration: 225

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23717948717948717
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9513888888888888
  reward for individual goal_min: 0.0
episode_len_mean: 187.86
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 20317
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.055665731430054
time_total_s: 11628.441971063614
timers:
  learn_throughput: 482.343
  learn_time_ms: 34208.024
  load_throughput: 5052345.34
  load_time_ms: 3.266
  training_iteration_time_ms: 45144.859
  update_time_ms: 2.514
timesteps_total: 4075500
training_iteration: 247

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30128205128205127
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.91
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 20707
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.163575887680054
time_total_s: 11638.180595636368
timers:
  learn_throughput: 466.98
  learn_time_ms: 35333.447
  load_throughput: 4944876.282
  load_time_ms: 3.337
  training_iteration_time_ms: 46495.422
  update_time_ms: 2.752
timesteps_total: 3828000
training_iteration: 232

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20270270270270271
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9873417721518988
  reward for individual goal_min: 0.0
episode_len_mean: 183.52
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 24032
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.73425793647766
time_total_s: 11642.00775027275
timers:
  learn_throughput: 513.725
  learn_time_ms: 32118.342
  load_throughput: 5124928.983
  load_time_ms: 3.22
  training_iteration_time_ms: 42616.255
  update_time_ms: 2.491
timesteps_total: 4356000
training_iteration: 264

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26973684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.79
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 19225
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.80641031265259
time_total_s: 11645.860954999924
timers:
  learn_throughput: 403.126
  learn_time_ms: 40930.096
  load_throughput: 4335890.535
  load_time_ms: 3.805
  training_iteration_time_ms: 53696.289
  update_time_ms: 2.785
timesteps_total: 3531000
training_iteration: 214

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.485175202156334
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 371
episodes_total: 51279
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 56.01121115684509
time_total_s: 11593.054900884628
timers:
  learn_throughput: 395.15
  learn_time_ms: 41756.313
  load_throughput: 4213302.162
  load_time_ms: 3.916
  training_iteration_time_ms: 54051.897
  update_time_ms: 2.635
timesteps_total: 3729000
training_iteration: 226

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24675324675324675
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.32
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 21138
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.34393620491028
time_total_s: 11641.11181139946
timers:
  learn_throughput: 444.166
  learn_time_ms: 37148.241
  load_throughput: 4801605.206
  load_time_ms: 3.436
  training_iteration_time_ms: 48765.783
  update_time_ms: 2.55
timesteps_total: 3828000
training_iteration: 232

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2662337662337662
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.961038961038961
  reward for individual goal_min: 0.0
episode_len_mean: 184.95
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 19049
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.45317196846008
time_total_s: 11662.222958087921
timers:
  learn_throughput: 443.882
  learn_time_ms: 37172.045
  load_throughput: 4756983.015
  load_time_ms: 3.469
  training_iteration_time_ms: 48581.432
  update_time_ms: 2.788
timesteps_total: 3745500
training_iteration: 227

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18666666666666668
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.93
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 19345
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.64441394805908
time_total_s: 11663.819092273712
timers:
  learn_throughput: 407.863
  learn_time_ms: 40454.77
  load_throughput: 4309323.769
  load_time_ms: 3.829
  training_iteration_time_ms: 53282.383
  update_time_ms: 3.003
timesteps_total: 3531000
training_iteration: 214

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3141025641025641
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.918918918918919
  reward for individual goal_min: 0.0
episode_len_mean: 183.62
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 21240
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.22753882408142
time_total_s: 11682.401738166809
timers:
  learn_throughput: 510.163
  learn_time_ms: 32342.628
  load_throughput: 4822046.823
  load_time_ms: 3.422
  training_iteration_time_ms: 43118.647
  update_time_ms: 2.664
timesteps_total: 4240500
training_iteration: 257

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9947368421052631
  reward for individual goal_min: 0.5
episode_len_mean: 44.1769436997319
episode_reward_max: 2.0
episode_reward_mean: 1.9946380697050938
episode_reward_min: 1.0
episodes_this_iter: 373
episodes_total: 73448
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973190348525469
  agent_1: 0.9973190348525469
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.325727224349976
time_total_s: 11532.450208425522
timers:
  learn_throughput: 454.204
  learn_time_ms: 36327.297
  load_throughput: 4335890.535
  load_time_ms: 3.805
  training_iteration_time_ms: 47881.863
  update_time_ms: 2.367
timesteps_total: 4059000
training_iteration: 246

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2602739726027397
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.5
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 20257
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.6117684841156
time_total_s: 11682.639644145966
timers:
  learn_throughput: 427.948
  learn_time_ms: 38556.064
  load_throughput: 4768980.616
  load_time_ms: 3.46
  training_iteration_time_ms: 50899.162
  update_time_ms: 2.697
timesteps_total: 3679500
training_iteration: 223

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.17602040816327
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 392
episodes_total: 55095
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 56.625898122787476
time_total_s: 11599.509986162186
timers:
  learn_throughput: 376.941
  learn_time_ms: 43773.393
  load_throughput: 3568736.87
  load_time_ms: 4.623
  training_iteration_time_ms: 56520.278
  update_time_ms: 2.848
timesteps_total: 3399000
training_iteration: 206

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16025641025641027
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9506172839506173
  reward for individual goal_min: 0.0
episode_len_mean: 197.91
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 20398
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.945781230926514
time_total_s: 11673.38775229454
timers:
  learn_throughput: 483.337
  learn_time_ms: 34137.696
  load_throughput: 5070669.314
  load_time_ms: 3.254
  training_iteration_time_ms: 45056.391
  update_time_ms: 2.524
timesteps_total: 4092000
training_iteration: 248

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2792207792207792
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.76
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 20798
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.812541007995605
time_total_s: 11683.993136644363
timers:
  learn_throughput: 466.753
  learn_time_ms: 35350.613
  load_throughput: 4888846.064
  load_time_ms: 3.375
  training_iteration_time_ms: 46493.375
  update_time_ms: 2.64
timesteps_total: 3844500
training_iteration: 233

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22077922077922077
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 179.06
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 19557
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.6954505443573
time_total_s: 11672.047416210175
timers:
  learn_throughput: 427.805
  learn_time_ms: 38568.931
  load_throughput: 4512562.743
  load_time_ms: 3.656
  training_iteration_time_ms: 51060.135
  update_time_ms: 2.581
timesteps_total: 3729000
training_iteration: 226

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9566666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 56.19795221843003
episode_reward_max: 2.0
episode_reward_mean: 1.9556313993174061
episode_reward_min: 1.0
episodes_this_iter: 293
episodes_total: 34706
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9965870307167235
  agent_1: 0.9590443686006825
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 68.41242718696594
time_total_s: 11683.095138072968
timers:
  learn_throughput: 326.804
  learn_time_ms: 50489.008
  load_throughput: 3706942.199
  load_time_ms: 4.451
  training_iteration_time_ms: 65526.726
  update_time_ms: 2.827
timesteps_total: 3151500
training_iteration: 191

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17105263157894737
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9873417721518988
  reward for individual goal_min: 0.0
episode_len_mean: 190.97
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 24119
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.96926665306091
time_total_s: 11682.977016925812
timers:
  learn_throughput: 517.381
  learn_time_ms: 31891.376
  load_throughput: 5212081.338
  load_time_ms: 3.166
  training_iteration_time_ms: 42369.139
  update_time_ms: 2.526
timesteps_total: 4372500
training_iteration: 265

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2345679012345679
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 186.16
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 19311
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.96911311149597
time_total_s: 11697.83006811142
timers:
  learn_throughput: 403.846
  learn_time_ms: 40857.137
  load_throughput: 4286608.98
  load_time_ms: 3.849
  training_iteration_time_ms: 53551.292
  update_time_ms: 2.772
timesteps_total: 3547500
training_iteration: 215

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.922077922077922
  reward for individual goal_min: 0.0
episode_len_mean: 201.01
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 19130
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.15655779838562
time_total_s: 11710.379515886307
timers:
  learn_throughput: 442.427
  learn_time_ms: 37294.264
  load_throughput: 4768586.292
  load_time_ms: 3.46
  training_iteration_time_ms: 48734.572
  update_time_ms: 2.79
timesteps_total: 3762000
training_iteration: 228

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32432432432432434
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.99375
  reward for individual goal_min: 0.5
episode_len_mean: 154.66037735849056
episode_reward_max: 2.0
episode_reward_mean: 1.3962264150943395
episode_reward_min: 0.0
episodes_this_iter: 106
episodes_total: 21244
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6886792452830188
  agent_1: 0.7075471698113207
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.85294699668884
time_total_s: 11691.964758396149
timers:
  learn_throughput: 441.527
  learn_time_ms: 37370.304
  load_throughput: 4771644.006
  load_time_ms: 3.458
  training_iteration_time_ms: 48982.781
  update_time_ms: 2.558
timesteps_total: 3844500
training_iteration: 233

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974358974358974
  reward for individual goal_min: 0.5
episode_len_mean: 44.77445652173913
episode_reward_max: 2.0
episode_reward_mean: 1.997282608695652
episode_reward_min: 1.0
episodes_this_iter: 368
episodes_total: 51647
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972826086956522
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 54.77314901351929
time_total_s: 11647.828049898148
timers:
  learn_throughput: 394.008
  learn_time_ms: 41877.31
  load_throughput: 4185020.863
  load_time_ms: 3.943
  training_iteration_time_ms: 54196.343
  update_time_ms: 2.644
timesteps_total: 3745500
training_iteration: 227

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2721518987341772
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9078947368421053
  reward for individual goal_min: 0.0
episode_len_mean: 191.26
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 21322
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.02870965003967
time_total_s: 11725.430447816849
timers:
  learn_throughput: 509.643
  learn_time_ms: 32375.611
  load_throughput: 4872187.718
  load_time_ms: 3.387
  training_iteration_time_ms: 43142.373
  update_time_ms: 2.659
timesteps_total: 4257000
training_iteration: 258

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18493150684931506
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.99375
  reward for individual goal_min: 0.5
episode_len_mean: 175.21
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 19438
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.096052408218384
time_total_s: 11716.91514468193
timers:
  learn_throughput: 408.637
  learn_time_ms: 40378.12
  load_throughput: 4340595.212
  load_time_ms: 3.801
  training_iteration_time_ms: 53196.585
  update_time_ms: 2.989
timesteps_total: 3547500
training_iteration: 215

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973262032085561
  reward for individual goal_min: 0.5
episode_len_mean: 43.334210526315786
episode_reward_max: 2.0
episode_reward_mean: 1.9973684210526317
episode_reward_min: 1.0
episodes_this_iter: 380
episodes_total: 73828
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973684210526316
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.21722865104675
time_total_s: 11580.667437076569
timers:
  learn_throughput: 452.815
  learn_time_ms: 36438.755
  load_throughput: 4344628.133
  load_time_ms: 3.798
  training_iteration_time_ms: 47999.838
  update_time_ms: 2.355
timesteps_total: 4075500
training_iteration: 247

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.189873417721519
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9430379746835443
  reward for individual goal_min: 0.0
episode_len_mean: 212.84
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 20476
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.25268840789795
time_total_s: 11715.640440702438
timers:
  learn_throughput: 487.674
  learn_time_ms: 33834.051
  load_throughput: 5047444.479
  load_time_ms: 3.269
  training_iteration_time_ms: 44671.58
  update_time_ms: 2.53
timesteps_total: 4108500
training_iteration: 249

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1796875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9810126582278481
  reward for individual goal_min: 0.5
episode_len_mean: 169.33
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 20895
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.6083288192749
time_total_s: 11729.601465463638
timers:
  learn_throughput: 466.612
  learn_time_ms: 35361.268
  load_throughput: 4846766.955
  load_time_ms: 3.404
  training_iteration_time_ms: 46488.289
  update_time_ms: 2.695
timesteps_total: 3861000
training_iteration: 234

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3108108108108108
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 166.25
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 20357
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.37069916725159
time_total_s: 11733.010343313217
timers:
  learn_throughput: 427.486
  learn_time_ms: 38597.721
  load_throughput: 4745403.53
  load_time_ms: 3.477
  training_iteration_time_ms: 50902.426
  update_time_ms: 2.684
timesteps_total: 3696000
training_iteration: 224

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 177.25
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 19649
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.06657671928406
time_total_s: 11723.113992929459
timers:
  learn_throughput: 427.179
  learn_time_ms: 38625.485
  load_throughput: 4492001.168
  load_time_ms: 3.673
  training_iteration_time_ms: 51148.054
  update_time_ms: 2.584
timesteps_total: 3745500
training_iteration: 227

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.208955223880597
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9807692307692307
  reward for individual goal_min: 0.0
episode_len_mean: 170.02
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 24218
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.00210380554199
time_total_s: 11724.979120731354
timers:
  learn_throughput: 516.252
  learn_time_ms: 31961.108
  load_throughput: 5189102.033
  load_time_ms: 3.18
  training_iteration_time_ms: 42394.991
  update_time_ms: 2.546
timesteps_total: 4389000
training_iteration: 266

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.714646464646464
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 396
episodes_total: 55491
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 56.816625118255615
time_total_s: 11656.326611280441
timers:
  learn_throughput: 375.639
  learn_time_ms: 43925.145
  load_throughput: 3570817.605
  load_time_ms: 4.621
  training_iteration_time_ms: 56659.471
  update_time_ms: 2.845
timesteps_total: 3415500
training_iteration: 207

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2463768115942029
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.91875
  reward for individual goal_min: 0.0
episode_len_mean: 181.53
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 21413
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.63019013404846
time_total_s: 11768.060637950897
timers:
  learn_throughput: 510.343
  learn_time_ms: 32331.169
  load_throughput: 4855506.241
  load_time_ms: 3.398
  training_iteration_time_ms: 43020.968
  update_time_ms: 2.656
timesteps_total: 4273500
training_iteration: 259

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2972972972972973
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9805194805194806
  reward for individual goal_min: 0.0
episode_len_mean: 168.91
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 19410
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.628512144088745
time_total_s: 11748.458580255508
timers:
  learn_throughput: 408.047
  learn_time_ms: 40436.49
  load_throughput: 4287883.816
  load_time_ms: 3.848
  training_iteration_time_ms: 53063.198
  update_time_ms: 2.772
timesteps_total: 3564000
training_iteration: 216

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32432432432432434
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.96
  reward for individual goal_min: 0.0
episode_len_mean: 167.75
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 19227
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.67518639564514
time_total_s: 11760.054702281952
timers:
  learn_throughput: 441.554
  learn_time_ms: 37368.035
  load_throughput: 4771512.41
  load_time_ms: 3.458
  training_iteration_time_ms: 48838.043
  update_time_ms: 2.686
timesteps_total: 3778500
training_iteration: 229

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3108108108108108
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 159.31132075471697
episode_reward_max: 2.0
episode_reward_mean: 1.4150943396226414
episode_reward_min: 0.0
episodes_this_iter: 106
episodes_total: 21350
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7264150943396226
  agent_1: 0.6886792452830188
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.54450869560242
time_total_s: 11740.509267091751
timers:
  learn_throughput: 441.232
  learn_time_ms: 37395.254
  load_throughput: 4703988.255
  load_time_ms: 3.508
  training_iteration_time_ms: 49057.642
  update_time_ms: 2.579
timesteps_total: 3861000
training_iteration: 234

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9867549668874173
  reward for individual goal_min: 0.5
episode_len_mean: 46.668539325842694
episode_reward_max: 2.0
episode_reward_mean: 1.9887640449438202
episode_reward_min: 1.0
episodes_this_iter: 356
episodes_total: 35062
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9887640449438202
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 72.64350199699402
time_total_s: 11755.738640069962
timers:
  learn_throughput: 322.883
  learn_time_ms: 51102.154
  load_throughput: 3730883.097
  load_time_ms: 4.423
  training_iteration_time_ms: 66312.025
  update_time_ms: 2.882
timesteps_total: 3168000
training_iteration: 192

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9946524064171123
  reward for individual goal_min: 0.5
episode_len_mean: 46.260504201680675
episode_reward_max: 2.0
episode_reward_mean: 1.9943977591036415
episode_reward_min: 1.0
episodes_this_iter: 357
episodes_total: 52004
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9943977591036415
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 54.7788770198822
time_total_s: 11702.60692691803
timers:
  learn_throughput: 392.672
  learn_time_ms: 42019.825
  load_throughput: 4199725.464
  load_time_ms: 3.929
  training_iteration_time_ms: 54356.004
  update_time_ms: 2.691
timesteps_total: 3762000
training_iteration: 228

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26153846153846155
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9210526315789473
  reward for individual goal_min: 0.0
episode_len_mean: 171.67
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 20573
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.25007891654968
time_total_s: 11760.890519618988
timers:
  learn_throughput: 484.568
  learn_time_ms: 34050.941
  load_throughput: 5044868.896
  load_time_ms: 3.271
  training_iteration_time_ms: 44838.308
  update_time_ms: 2.512
timesteps_total: 4125000
training_iteration: 250

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972222222222222
  reward for individual goal_min: 0.5
episode_len_mean: 44.36290322580645
episode_reward_max: 2.0
episode_reward_mean: 1.9973118279569892
episode_reward_min: 1.0
episodes_this_iter: 372
episodes_total: 74200
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973118279569892
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.94698214530945
time_total_s: 11628.614419221878
timers:
  learn_throughput: 453.605
  learn_time_ms: 36375.232
  load_throughput: 4276648.931
  load_time_ms: 3.858
  training_iteration_time_ms: 47927.472
  update_time_ms: 2.341
timesteps_total: 4092000
training_iteration: 248

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14492753623188406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.19
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 19533
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.90727949142456
time_total_s: 11769.822424173355
timers:
  learn_throughput: 409.282
  learn_time_ms: 40314.528
  load_throughput: 4343237.564
  load_time_ms: 3.799
  training_iteration_time_ms: 53128.105
  update_time_ms: 3.006
timesteps_total: 3564000
training_iteration: 216

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 186.01
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 20981
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.07916831970215
time_total_s: 11775.68063378334
timers:
  learn_throughput: 465.793
  learn_time_ms: 35423.428
  load_throughput: 4871193.198
  load_time_ms: 3.387
  training_iteration_time_ms: 46598.549
  update_time_ms: 2.691
timesteps_total: 3877500
training_iteration: 235

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2602739726027397
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9810126582278481
  reward for individual goal_min: 0.0
episode_len_mean: 168.76
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 20453
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.90084195137024
time_total_s: 11781.911185264587
timers:
  learn_throughput: 427.062
  learn_time_ms: 38636.099
  load_throughput: 4752116.018
  load_time_ms: 3.472
  training_iteration_time_ms: 50957.309
  update_time_ms: 2.629
timesteps_total: 3712500
training_iteration: 225

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3287671232876712
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.99375
  reward for individual goal_min: 0.5
episode_len_mean: 159.11650485436894
episode_reward_max: 2.0
episode_reward_mean: 1.4466019417475728
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 24321
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7864077669902912
  agent_1: 0.6601941747572816
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.897395849227905
time_total_s: 11767.876516580582
timers:
  learn_throughput: 516.962
  learn_time_ms: 31917.247
  load_throughput: 5109038.669
  load_time_ms: 3.23
  training_iteration_time_ms: 42339.3
  update_time_ms: 2.533
timesteps_total: 4405500
training_iteration: 267

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19480519480519481
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 187.32
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 19739
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.123080253601074
time_total_s: 11774.23707318306
timers:
  learn_throughput: 426.048
  learn_time_ms: 38727.99
  load_throughput: 4486351.915
  load_time_ms: 3.678
  training_iteration_time_ms: 51227.1
  update_time_ms: 2.588
timesteps_total: 3762000
training_iteration: 228

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.271794871794874
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 390
episodes_total: 55881
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.37789463996887
time_total_s: 11713.70450592041
timers:
  learn_throughput: 374.845
  learn_time_ms: 44018.194
  load_throughput: 3589970.536
  load_time_ms: 4.596
  training_iteration_time_ms: 56759.661
  update_time_ms: 2.849
timesteps_total: 3432000
training_iteration: 208

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27564102564102566
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.961038961038961
  reward for individual goal_min: 0.0
episode_len_mean: 183.79
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 19316
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.906065702438354
time_total_s: 11807.96076798439
timers:
  learn_throughput: 440.266
  learn_time_ms: 37477.368
  load_throughput: 4777078.642
  load_time_ms: 3.454
  training_iteration_time_ms: 49068.672
  update_time_ms: 2.67
timesteps_total: 3795000
training_iteration: 230

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28169014084507044
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.5
episode_len_mean: 170.94
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 21442
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.46479892730713
time_total_s: 11788.974066019058
timers:
  learn_throughput: 440.751
  learn_time_ms: 37436.09
  load_throughput: 4688722.705
  load_time_ms: 3.519
  training_iteration_time_ms: 49157.969
  update_time_ms: 2.57
timesteps_total: 3877500
training_iteration: 235

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 81.9
episode_reward_max: 2.0
episode_reward_mean: 1.8833333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9333333333333333
  agent_1: 0.95
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8851351351351351
  reward for individual goal_min: 0.0
episode_len_mean: 200.22
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 21495
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.01516914367676
time_total_s: 11820.075807094574
timers:
  learn_throughput: 509.476
  learn_time_ms: 32386.208
  load_throughput: 4901067.66
  load_time_ms: 3.367
  training_iteration_time_ms: 43115.5
  update_time_ms: 2.638
timesteps_total: 4290000
training_iteration: 260

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-091_srff7m/checkpoint_000260/checkpoint-260
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9794520547945206
  reward for individual goal_min: 0.0
episode_len_mean: 173.47
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 19504
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.2764048576355
time_total_s: 11801.734985113144
timers:
  learn_throughput: 407.029
  learn_time_ms: 40537.628
  load_throughput: 4309672.63
  load_time_ms: 3.829
  training_iteration_time_ms: 53203.959
  update_time_ms: 2.761
timesteps_total: 3580500
training_iteration: 217

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21951219512195122
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.952054794520548
  reward for individual goal_min: 0.0
episode_len_mean: 196.65
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 20654
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.73053860664368
time_total_s: 11805.621058225632
timers:
  learn_throughput: 484.876
  learn_time_ms: 34029.334
  load_throughput: 4991526.394
  load_time_ms: 3.306
  training_iteration_time_ms: 44774.261
  update_time_ms: 2.549
timesteps_total: 4141500
training_iteration: 251

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971910112359551
  reward for individual goal_min: 0.5
episode_len_mean: 44.13101604278075
episode_reward_max: 2.0
episode_reward_mean: 1.9973262032085561
episode_reward_min: 1.0
episodes_this_iter: 374
episodes_total: 74574
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973262032085561
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.901262044906616
time_total_s: 11676.515681266785
timers:
  learn_throughput: 453.682
  learn_time_ms: 36369.074
  load_throughput: 4331060.517
  load_time_ms: 3.81
  training_iteration_time_ms: 47923.177
  update_time_ms: 2.341
timesteps_total: 4108500
training_iteration: 249

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3026315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9938271604938271
  reward for individual goal_min: 0.5
episode_len_mean: 167.78
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 21079
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.38020443916321
time_total_s: 11822.060838222504
timers:
  learn_throughput: 463.714
  learn_time_ms: 35582.26
  load_throughput: 4865952.962
  load_time_ms: 3.391
  training_iteration_time_ms: 46798.323
  update_time_ms: 2.692
timesteps_total: 3894000
training_iteration: 236

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2109375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9941176470588236
  reward for individual goal_min: 0.5
episode_len_mean: 159.80582524271844
episode_reward_max: 2.0
episode_reward_mean: 1.4077669902912622
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 24424
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7281553398058253
  agent_1: 0.6796116504854369
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.18491291999817
time_total_s: 11810.06142950058
timers:
  learn_throughput: 518.432
  learn_time_ms: 31826.718
  load_throughput: 5124966.935
  load_time_ms: 3.22
  training_iteration_time_ms: 42187.362
  update_time_ms: 2.535
timesteps_total: 4422000
training_iteration: 268

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973262032085561
  reward for individual goal_min: 0.5
episode_len_mean: 46.52394366197183
episode_reward_max: 2.0
episode_reward_mean: 1.9971830985915493
episode_reward_min: 1.0
episodes_this_iter: 355
episodes_total: 52359
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9971830985915493
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 56.31604814529419
time_total_s: 11758.922975063324
timers:
  learn_throughput: 391.468
  learn_time_ms: 42149.089
  load_throughput: 4217410.296
  load_time_ms: 3.912
  training_iteration_time_ms: 54482.581
  update_time_ms: 2.702
timesteps_total: 3778500
training_iteration: 229

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.96
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 19622
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.99859070777893
time_total_s: 11821.821014881134
timers:
  learn_throughput: 410.915
  learn_time_ms: 40154.289
  load_throughput: 4329489.015
  load_time_ms: 3.811
  training_iteration_time_ms: 52932.412
  update_time_ms: 3.038
timesteps_total: 3580500
training_iteration: 217

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2465753424657534
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.52
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 20552
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.522844076156616
time_total_s: 11831.434029340744
timers:
  learn_throughput: 426.502
  learn_time_ms: 38686.816
  load_throughput: 4719096.085
  load_time_ms: 3.496
  training_iteration_time_ms: 51011.303
  update_time_ms: 2.659
timesteps_total: 3729000
training_iteration: 226

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9664429530201343
  reward for individual goal_min: 0.5
episode_len_mean: 55.39393939393939
episode_reward_max: 2.0
episode_reward_mean: 1.9663299663299663
episode_reward_min: 1.0
episodes_this_iter: 297
episodes_total: 35359
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9966329966329966
  agent_1: 0.9696969696969697
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 67.31227779388428
time_total_s: 11823.050917863846
timers:
  learn_throughput: 320.847
  learn_time_ms: 51426.312
  load_throughput: 3695639.098
  load_time_ms: 4.465
  training_iteration_time_ms: 66718.759
  update_time_ms: 2.889
timesteps_total: 3184500
training_iteration: 193

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20253164556962025
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 189.35
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 19824
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.88471984863281
time_total_s: 11824.121793031693
timers:
  learn_throughput: 427.312
  learn_time_ms: 38613.478
  load_throughput: 4493780.421
  load_time_ms: 3.672
  training_iteration_time_ms: 51027.997
  update_time_ms: 2.595
timesteps_total: 3778500
training_iteration: 229

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.831081081081081
  reward for individual goal_min: 0.0
episode_len_mean: 195.83
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 21578
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.3421790599823
time_total_s: 11863.417986154556
timers:
  learn_throughput: 510.949
  learn_time_ms: 32292.873
  load_throughput: 4946289.962
  load_time_ms: 3.336
  training_iteration_time_ms: 42999.261
  update_time_ms: 2.609
timesteps_total: 4306500
training_iteration: 261

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23943661971830985
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 171.33
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 21538
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.00521469116211
time_total_s: 11836.97928071022
timers:
  learn_throughput: 440.816
  learn_time_ms: 37430.554
  load_throughput: 4680984.545
  load_time_ms: 3.525
  training_iteration_time_ms: 49120.345
  update_time_ms: 2.572
timesteps_total: 3894000
training_iteration: 236

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.972972972972973
  reward for individual goal_min: 0.0
episode_len_mean: 182.85
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 19406
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.18057632446289
time_total_s: 11857.141344308853
timers:
  learn_throughput: 439.264
  learn_time_ms: 37562.859
  load_throughput: 4709558.211
  load_time_ms: 3.504
  training_iteration_time_ms: 49129.731
  update_time_ms: 2.698
timesteps_total: 3811500
training_iteration: 231

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9943181818181818
  reward for individual goal_min: 0.5
episode_len_mean: 43.52770448548813
episode_reward_max: 2.0
episode_reward_mean: 1.9947229551451187
episode_reward_min: 1.0
episodes_this_iter: 379
episodes_total: 56260
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973614775725593
  agent_1: 0.9973614775725593
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.5264048576355
time_total_s: 11772.230910778046
timers:
  learn_throughput: 372.902
  learn_time_ms: 44247.599
  load_throughput: 3568810.483
  load_time_ms: 4.623
  training_iteration_time_ms: 57054.445
  update_time_ms: 2.865
timesteps_total: 3448500
training_iteration: 209

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9117647058823529
  reward for individual goal_min: 0.0
episode_len_mean: 199.47
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 20737
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.97234773635864
time_total_s: 11850.59340596199
timers:
  learn_throughput: 486.03
  learn_time_ms: 33948.543
  load_throughput: 4995778.213
  load_time_ms: 3.303
  training_iteration_time_ms: 44684.727
  update_time_ms: 2.544
timesteps_total: 4158000
training_iteration: 252

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1736111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.97
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 19594
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.403443813323975
time_total_s: 11855.138428926468
timers:
  learn_throughput: 408.481
  learn_time_ms: 40393.543
  load_throughput: 4277045.387
  load_time_ms: 3.858
  training_iteration_time_ms: 53008.465
  update_time_ms: 2.748
timesteps_total: 3597000
training_iteration: 218

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24675324675324675
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.13
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 24514
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.30500769615173
time_total_s: 11852.366437196732
timers:
  learn_throughput: 519.436
  learn_time_ms: 31765.203
  load_throughput: 5121553.502
  load_time_ms: 3.222
  training_iteration_time_ms: 42091.072
  update_time_ms: 2.557
timesteps_total: 4438500
training_iteration: 269

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.974619289340104
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 394
episodes_total: 74968
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.83164358139038
time_total_s: 11725.347324848175
timers:
  learn_throughput: 453.085
  learn_time_ms: 36417.013
  load_throughput: 4357622.406
  load_time_ms: 3.786
  training_iteration_time_ms: 47972.062
  update_time_ms: 2.315
timesteps_total: 4125000
training_iteration: 250

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 180.7
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 21168
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.3772246837616
time_total_s: 11871.438062906265
timers:
  learn_throughput: 461.522
  learn_time_ms: 35751.257
  load_throughput: 4825274.431
  load_time_ms: 3.419
  training_iteration_time_ms: 47000.195
  update_time_ms: 2.689
timesteps_total: 3910500
training_iteration: 237

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23026315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.62
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 19718
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.90046310424805
time_total_s: 11872.721477985382
timers:
  learn_throughput: 412.074
  learn_time_ms: 40041.392
  load_throughput: 4350527.487
  load_time_ms: 3.793
  training_iteration_time_ms: 52768.843
  update_time_ms: 3.035
timesteps_total: 3597000
training_iteration: 218

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26153846153846155
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9814814814814815
  reward for individual goal_min: 0.0
episode_len_mean: 157.43564356435644
episode_reward_max: 2.0
episode_reward_mean: 1.4158415841584158
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 20653
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7128712871287128
  agent_1: 0.7029702970297029
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.08356189727783
time_total_s: 11881.517591238022
timers:
  learn_throughput: 424.86
  learn_time_ms: 38836.337
  load_throughput: 4696263.402
  load_time_ms: 3.513
  training_iteration_time_ms: 51199.459
  update_time_ms: 2.725
timesteps_total: 3745500
training_iteration: 227

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971590909090909
  reward for individual goal_min: 0.5
episode_len_mean: 45.625
episode_reward_max: 2.0
episode_reward_mean: 1.9972222222222222
episode_reward_min: 1.0
episodes_this_iter: 360
episodes_total: 52719
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972222222222222
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 56.624144315719604
time_total_s: 11815.547119379044
timers:
  learn_throughput: 390.059
  learn_time_ms: 42301.282
  load_throughput: 4230455.162
  load_time_ms: 3.9
  training_iteration_time_ms: 54671.207
  update_time_ms: 2.711
timesteps_total: 3795000
training_iteration: 230

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18823529411764706
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 198.02
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 19910
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.43221950531006
time_total_s: 11875.554012537003
timers:
  learn_throughput: 425.735
  learn_time_ms: 38756.483
  load_throughput: 4513033.577
  load_time_ms: 3.656
  training_iteration_time_ms: 51204.004
  update_time_ms: 2.604
timesteps_total: 3795000
training_iteration: 230

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 197.49
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 21658
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.39875769615173
time_total_s: 11905.816743850708
timers:
  learn_throughput: 512.256
  learn_time_ms: 32210.436
  load_throughput: 4908924.386
  load_time_ms: 3.361
  training_iteration_time_ms: 42931.565
  update_time_ms: 2.61
timesteps_total: 4323000
training_iteration: 262

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9583333333333334
  reward for individual goal_min: 0.5
episode_len_mean: 58.430604982206404
episode_reward_max: 2.0
episode_reward_mean: 1.9572953736654803
episode_reward_min: 1.0
episodes_this_iter: 281
episodes_total: 35640
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9572953736654805
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 67.94677233695984
time_total_s: 11890.997690200806
timers:
  learn_throughput: 317.586
  learn_time_ms: 51954.482
  load_throughput: 3670744.587
  load_time_ms: 4.495
  training_iteration_time_ms: 67421.494
  update_time_ms: 2.894
timesteps_total: 3201000
training_iteration: 194

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2876712328767123
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9797297297297297
  reward for individual goal_min: 0.5
episode_len_mean: 177.94
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 21632
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.72682023048401
time_total_s: 11883.706100940704
timers:
  learn_throughput: 442.999
  learn_time_ms: 37246.161
  load_throughput: 4684121.127
  load_time_ms: 3.523
  training_iteration_time_ms: 48910.745
  update_time_ms: 2.581
timesteps_total: 3910500
training_iteration: 237

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2605633802816901
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9625
  reward for individual goal_min: 0.0
episode_len_mean: 182.2
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 19498
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.59537363052368
time_total_s: 11905.736717939377
timers:
  learn_throughput: 439.798
  learn_time_ms: 37517.186
  load_throughput: 4722541.762
  load_time_ms: 3.494
  training_iteration_time_ms: 49046.097
  update_time_ms: 2.714
timesteps_total: 3828000
training_iteration: 232

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8698630136986302
  reward for individual goal_min: 0.0
episode_len_mean: 211.89
episode_reward_max: 2.0
episode_reward_mean: 1.03
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 20818
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.48
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.643813133239746
time_total_s: 11896.23721909523
timers:
  learn_throughput: 484.945
  learn_time_ms: 34024.464
  load_throughput: 4936551.99
  load_time_ms: 3.342
  training_iteration_time_ms: 44862.196
  update_time_ms: 2.573
timesteps_total: 4174500
training_iteration: 253

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24285714285714285
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 166.05
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 24614
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.872307538986206
time_total_s: 11895.238744735718
timers:
  learn_throughput: 517.886
  learn_time_ms: 31860.315
  load_throughput: 5121629.306
  load_time_ms: 3.222
  training_iteration_time_ms: 42229.884
  update_time_ms: 2.566
timesteps_total: 4455000
training_iteration: 270

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.33759590792839
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 391
episodes_total: 56651
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.90419816970825
time_total_s: 11830.135108947754
timers:
  learn_throughput: 372.147
  learn_time_ms: 44337.313
  load_throughput: 3552123.184
  load_time_ms: 4.645
  training_iteration_time_ms: 57159.562
  update_time_ms: 2.536
timesteps_total: 3465000
training_iteration: 210

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18493150684931506
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 180.62
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 19688
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.54382872581482
time_total_s: 11906.682257652283
timers:
  learn_throughput: 408.684
  learn_time_ms: 40373.448
  load_throughput: 4357622.406
  load_time_ms: 3.786
  training_iteration_time_ms: 52882.769
  update_time_ms: 2.731
timesteps_total: 3613500
training_iteration: 219

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2152777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 174.44
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 21263
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.88221478462219
time_total_s: 11918.320277690887
timers:
  learn_throughput: 462.059
  learn_time_ms: 35709.73
  load_throughput: 4818521.567
  load_time_ms: 3.424
  training_iteration_time_ms: 46942.922
  update_time_ms: 2.679
timesteps_total: 3927000
training_iteration: 238

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974619289340102
  reward for individual goal_min: 0.5
episode_len_mean: 42.32390745501285
episode_reward_max: 2.0
episode_reward_mean: 1.9974293059125965
episode_reward_min: 1.0
episodes_this_iter: 389
episodes_total: 75357
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9974293059125964
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 48.10406446456909
time_total_s: 11773.451389312744
timers:
  learn_throughput: 453.562
  learn_time_ms: 36378.703
  load_throughput: 4338010.455
  load_time_ms: 3.804
  training_iteration_time_ms: 47917.657
  update_time_ms: 2.328
timesteps_total: 4141500
training_iteration: 251

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28289473684210525
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9726027397260274
  reward for individual goal_min: 0.0
episode_len_mean: 181.08
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 20746
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.930222511291504
time_total_s: 11932.447813749313
timers:
  learn_throughput: 423.691
  learn_time_ms: 38943.484
  load_throughput: 4694957.159
  load_time_ms: 3.514
  training_iteration_time_ms: 51171.177
  update_time_ms: 2.739
timesteps_total: 3762000
training_iteration: 228

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17532467532467533
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 183.74
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 19808
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.33676481246948
time_total_s: 11927.058242797852
timers:
  learn_throughput: 411.523
  learn_time_ms: 40094.938
  load_throughput: 4321729.541
  load_time_ms: 3.818
  training_iteration_time_ms: 52803.212
  update_time_ms: 3.053
timesteps_total: 3613500
training_iteration: 219

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20930232558139536
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 197.47
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 19993
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.663119077682495
time_total_s: 11923.217131614685
timers:
  learn_throughput: 427.674
  learn_time_ms: 38580.8
  load_throughput: 4523919.518
  load_time_ms: 3.647
  training_iteration_time_ms: 51010.446
  update_time_ms: 2.603
timesteps_total: 3811500
training_iteration: 231

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9943820224719101
  reward for individual goal_min: 0.5
episode_len_mean: 47.47550432276657
episode_reward_max: 2.0
episode_reward_mean: 1.994236311239193
episode_reward_min: 1.0
episodes_this_iter: 347
episodes_total: 53066
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9971181556195965
  agent_1: 0.9971181556195965
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.81568765640259
time_total_s: 11870.362807035446
timers:
  learn_throughput: 389.075
  learn_time_ms: 42408.322
  load_throughput: 4224902.537
  load_time_ms: 3.905
  training_iteration_time_ms: 54810.772
  update_time_ms: 2.717
timesteps_total: 3811500
training_iteration: 231

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9071428571428571
  reward for individual goal_min: 0.0
episode_len_mean: 197.92
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 21741
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.058149576187134
time_total_s: 11948.874893426895
timers:
  learn_throughput: 513.572
  learn_time_ms: 32127.907
  load_throughput: 4903290.02
  load_time_ms: 3.365
  training_iteration_time_ms: 42845.621
  update_time_ms: 2.583
timesteps_total: 4339500
training_iteration: 263

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2465753424657534
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 167.84
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 21729
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.689229011535645
time_total_s: 11929.39532995224
timers:
  learn_throughput: 444.545
  learn_time_ms: 37116.581
  load_throughput: 4719353.532
  load_time_ms: 3.496
  training_iteration_time_ms: 48762.16
  update_time_ms: 2.572
timesteps_total: 3927000
training_iteration: 238

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23076923076923078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 187.76
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 24704
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.37993121147156
time_total_s: 11936.61867594719
timers:
  learn_throughput: 518.167
  learn_time_ms: 31843.033
  load_throughput: 5111566.943
  load_time_ms: 3.228
  training_iteration_time_ms: 42184.815
  update_time_ms: 2.563
timesteps_total: 4471500
training_iteration: 271

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9285714285714286
  reward for individual goal_min: 0.0
episode_len_mean: 178.6
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 20910
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.00186514854431
time_total_s: 11941.239084243774
timers:
  learn_throughput: 484.732
  learn_time_ms: 34039.431
  load_throughput: 4934440.111
  load_time_ms: 3.344
  training_iteration_time_ms: 44890.531
  update_time_ms: 2.566
timesteps_total: 4191000
training_iteration: 254

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25757575757575757
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9333333333333333
  reward for individual goal_min: 0.0
episode_len_mean: 176.37
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 19585
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.17878699302673
time_total_s: 11953.915504932404
timers:
  learn_throughput: 441.848
  learn_time_ms: 37343.149
  load_throughput: 4703636.574
  load_time_ms: 3.508
  training_iteration_time_ms: 48943.796
  update_time_ms: 2.715
timesteps_total: 3844500
training_iteration: 233

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 176.1
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 21354
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.18912076950073
time_total_s: 11967.509398460388
timers:
  learn_throughput: 460.378
  learn_time_ms: 35840.144
  load_throughput: 4798542.257
  load_time_ms: 3.439
  training_iteration_time_ms: 47112.639
  update_time_ms: 2.61
timesteps_total: 3943500
training_iteration: 239

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.114882506527415
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 383
episodes_total: 75740
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.011998653411865
time_total_s: 11821.463387966156
timers:
  learn_throughput: 453.554
  learn_time_ms: 36379.344
  load_throughput: 4322944.344
  load_time_ms: 3.817
  training_iteration_time_ms: 47921.503
  update_time_ms: 2.324
timesteps_total: 4158000
training_iteration: 252

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9605263157894737
  reward for individual goal_min: 0.5
episode_len_mean: 56.945017182130584
episode_reward_max: 2.0
episode_reward_mean: 1.958762886597938
episode_reward_min: 1.0
episodes_this_iter: 291
episodes_total: 35931
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9587628865979382
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 67.93139863014221
time_total_s: 11958.929088830948
timers:
  learn_throughput: 314.356
  learn_time_ms: 52488.233
  load_throughput: 3585673.887
  load_time_ms: 4.602
  training_iteration_time_ms: 68236.899
  update_time_ms: 2.96
timesteps_total: 3217500
training_iteration: 195

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974358974358974
  reward for individual goal_min: 0.5
episode_len_mean: 43.50923482849604
episode_reward_max: 2.0
episode_reward_mean: 1.9973614775725594
episode_reward_min: 1.0
episodes_this_iter: 379
episodes_total: 57030
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973614775725593
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 58.59126877784729
time_total_s: 11888.726377725601
timers:
  learn_throughput: 371.948
  learn_time_ms: 44361.04
  load_throughput: 3562619.224
  load_time_ms: 4.631
  training_iteration_time_ms: 57239.65
  update_time_ms: 2.546
timesteps_total: 3481500
training_iteration: 211

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9166666666666666
  reward for individual goal_min: 0.5
episode_len_mean: 92.11666666666666
episode_reward_max: 2.0
episode_reward_mean: 1.8166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9
  agent_1: 0.9166666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3150684931506849
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9927536231884058
  reward for individual goal_min: 0.5
episode_len_mean: 165.7029702970297
episode_reward_max: 2.0
episode_reward_mean: 1.3663366336633664
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 19789
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6732673267326733
  agent_1: 0.693069306930693
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 65.18973517417908
time_total_s: 11971.871992826462
timers:
  learn_throughput: 406.981
  learn_time_ms: 40542.43
  load_throughput: 4336787.171
  load_time_ms: 3.805
  training_iteration_time_ms: 53090.451
  update_time_ms: 2.72
timesteps_total: 3630000
training_iteration: 220

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2647058823529412
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8716216216216216
  reward for individual goal_min: 0.0
episode_len_mean: 202.91
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 21822
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.25042772293091
time_total_s: 11992.125321149826
timers:
  learn_throughput: 512.202
  learn_time_ms: 32213.839
  load_throughput: 4892993.87
  load_time_ms: 3.372
  training_iteration_time_ms: 42916.147
  update_time_ms: 2.552
timesteps_total: 4356000
training_iteration: 264

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34177215189873417
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.97
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 20845
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.81529498100281
time_total_s: 11984.263108730316
timers:
  learn_throughput: 425.629
  learn_time_ms: 38766.144
  load_throughput: 4689548.772
  load_time_ms: 3.518
  training_iteration_time_ms: 51042.753
  update_time_ms: 2.736
timesteps_total: 3778500
training_iteration: 229

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17391304347826086
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.75
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 20088
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.980058670043945
time_total_s: 11972.197190284729
timers:
  learn_throughput: 429.569
  learn_time_ms: 38410.579
  load_throughput: 4540123.859
  load_time_ms: 3.634
  training_iteration_time_ms: 50751.998
  update_time_ms: 2.623
timesteps_total: 3828000
training_iteration: 232

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9930555555555556
  reward for individual goal_min: 0.5
episode_len_mean: 182.36
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 24794
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.76
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.703264236450195
time_total_s: 11978.32194018364
timers:
  learn_throughput: 520.245
  learn_time_ms: 31715.824
  load_throughput: 5021842.827
  load_time_ms: 3.286
  training_iteration_time_ms: 42043.42
  update_time_ms: 2.532
timesteps_total: 4488000
training_iteration: 272

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33766233766233766
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.975
  reward for individual goal_min: 0.0
episode_len_mean: 155.4433962264151
episode_reward_max: 2.0
episode_reward_mean: 1.4245283018867925
episode_reward_min: 0.0
episodes_this_iter: 106
episodes_total: 21835
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7735849056603774
  agent_1: 0.6509433962264151
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.004029512405396
time_total_s: 11976.399359464645
timers:
  learn_throughput: 445.829
  learn_time_ms: 37009.686
  load_throughput: 4716780.328
  load_time_ms: 3.498
  training_iteration_time_ms: 48568.612
  update_time_ms: 2.605
timesteps_total: 3943500
training_iteration: 239

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26811594202898553
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9240506329113924
  reward for individual goal_min: 0.0
episode_len_mean: 178.79
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 21003
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.84592390060425
time_total_s: 11985.085008144379
timers:
  learn_throughput: 484.397
  learn_time_ms: 34062.994
  load_throughput: 4925624.973
  load_time_ms: 3.35
  training_iteration_time_ms: 44941.971
  update_time_ms: 2.537
timesteps_total: 4207500
training_iteration: 255

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9943502824858758
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9848484848484849
  reward for individual goal_min: 0.5
episode_len_mean: 48.28947368421053
episode_reward_max: 2.0
episode_reward_mean: 1.9795321637426901
episode_reward_min: 0.0
episodes_this_iter: 342
episodes_total: 53408
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9941520467836257
  agent_1: 0.9853801169590644
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.114821434020996
time_total_s: 11926.477628469467
timers:
  learn_throughput: 387.587
  learn_time_ms: 42571.058
  load_throughput: 4195473.647
  load_time_ms: 3.933
  training_iteration_time_ms: 55050.372
  update_time_ms: 2.715
timesteps_total: 3828000
training_iteration: 232

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 81.43333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.8833333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9333333333333333
  agent_1: 0.95
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2571428571428571
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.5
episode_len_mean: 164.57
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 19906
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 64.68216562271118
time_total_s: 11991.740408420563
timers:
  learn_throughput: 409.962
  learn_time_ms: 40247.681
  load_throughput: 4312895.558
  load_time_ms: 3.826
  training_iteration_time_ms: 52976.683
  update_time_ms: 3.056
timesteps_total: 3630000
training_iteration: 220

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3493150684931507
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9358974358974359
  reward for individual goal_min: 0.0
episode_len_mean: 170.11
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 19683
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.83399844169617
time_total_s: 12002.7495033741
timers:
  learn_throughput: 443.235
  learn_time_ms: 37226.341
  load_throughput: 4703892.336
  load_time_ms: 3.508
  training_iteration_time_ms: 48784.228
  update_time_ms: 2.666
timesteps_total: 3861000
training_iteration: 234

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.75
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 396
episodes_total: 76136
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.49834585189819
time_total_s: 11869.961733818054
timers:
  learn_throughput: 452.982
  learn_time_ms: 36425.26
  load_throughput: 4309699.468
  load_time_ms: 3.829
  training_iteration_time_ms: 47966.903
  update_time_ms: 2.342
timesteps_total: 4174500
training_iteration: 253

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.90625
  reward for individual goal_min: 0.0
episode_len_mean: 193.67
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 21906
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.99514865875244
time_total_s: 12035.120469808578
timers:
  learn_throughput: 509.967
  learn_time_ms: 32355.05
  load_throughput: 4846733.012
  load_time_ms: 3.404
  training_iteration_time_ms: 43097.887
  update_time_ms: 2.527
timesteps_total: 4372500
training_iteration: 265

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 92.6
episode_reward_max: 2.0
episode_reward_mean: 1.7666666666666666
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9
  agent_1: 0.8666666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 192.43
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 21436
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.266019344329834
time_total_s: 12028.775417804718
timers:
  learn_throughput: 457.582
  learn_time_ms: 36059.147
  load_throughput: 4749376.596
  load_time_ms: 3.474
  training_iteration_time_ms: 47326.132
  update_time_ms: 2.603
timesteps_total: 3960000
training_iteration: 240

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.974025974025974
  reward for individual goal_min: 0.0
episode_len_mean: 186.35
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 19875
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.76173210144043
time_total_s: 12022.633724927902
timers:
  learn_throughput: 408.307
  learn_time_ms: 40410.755
  load_throughput: 4342202.033
  load_time_ms: 3.8
  training_iteration_time_ms: 52955.777
  update_time_ms: 2.709
timesteps_total: 3646500
training_iteration: 221

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16891891891891891
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.7
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 20932
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.596144914627075
time_total_s: 12033.859253644943
timers:
  learn_throughput: 427.453
  learn_time_ms: 38600.717
  load_throughput: 4710135.166
  load_time_ms: 3.503
  training_iteration_time_ms: 50669.569
  update_time_ms: 2.75
timesteps_total: 3795000
training_iteration: 230

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3194444444444444
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 166.41
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 24890
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.48322534561157
time_total_s: 12020.805165529251
timers:
  learn_throughput: 521.138
  learn_time_ms: 31661.466
  load_throughput: 5028337.596
  load_time_ms: 3.281
  training_iteration_time_ms: 42016.477
  update_time_ms: 2.538
timesteps_total: 4504500
training_iteration: 273

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.41688654353562
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 379
episodes_total: 57409
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.30566477775574
time_total_s: 11948.032042503357
timers:
  learn_throughput: 370.819
  learn_time_ms: 44496.046
  load_throughput: 3564527.587
  load_time_ms: 4.629
  training_iteration_time_ms: 57409.736
  update_time_ms: 2.576
timesteps_total: 3498000
training_iteration: 212

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.4
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 20180
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.411877393722534
time_total_s: 12022.609067678452
timers:
  learn_throughput: 431.141
  learn_time_ms: 38270.551
  load_throughput: 4561430.003
  load_time_ms: 3.617
  training_iteration_time_ms: 50509.41
  update_time_ms: 2.618
timesteps_total: 3844500
training_iteration: 233

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.959731543624161
  reward for individual goal_min: 0.5
episode_len_mean: 56.4808362369338
episode_reward_max: 2.0
episode_reward_mean: 1.9581881533101044
episode_reward_min: 1.0
episodes_this_iter: 287
episodes_total: 36218
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9581881533101045
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 66.6007399559021
time_total_s: 12025.52982878685
timers:
  learn_throughput: 314.63
  learn_time_ms: 52442.504
  load_throughput: 3602716.17
  load_time_ms: 4.58
  training_iteration_time_ms: 68194.736
  update_time_ms: 2.919
timesteps_total: 3234000
training_iteration: 196

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.37333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.948051948051948
  reward for individual goal_min: 0.0
episode_len_mean: 165.37864077669903
episode_reward_max: 2.0
episode_reward_mean: 1.3883495145631068
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 21106
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7087378640776699
  agent_1: 0.6796116504854369
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.80173182487488
time_total_s: 12030.886739969254
timers:
  learn_throughput: 483.652
  learn_time_ms: 34115.449
  load_throughput: 4905201.471
  load_time_ms: 3.364
  training_iteration_time_ms: 45014.31
  update_time_ms: 2.542
timesteps_total: 4224000
training_iteration: 256

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19863013698630136
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 176.78
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 20001
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.31418323516846
time_total_s: 12045.054591655731
timers:
  learn_throughput: 410.567
  learn_time_ms: 40188.371
  load_throughput: 4288654.397
  load_time_ms: 3.847
  training_iteration_time_ms: 52926.15
  update_time_ms: 3.041
timesteps_total: 3646500
training_iteration: 221

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 65.05
episode_reward_max: 2.0
episode_reward_mean: 1.9666666666666666
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9833333333333333
  agent_1: 0.9833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 160.95192307692307
episode_reward_max: 2.0
episode_reward_mean: 1.4038461538461537
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 21939
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6538461538461539
  agent_1: 0.75
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.732205867767334
time_total_s: 12032.131565332413
timers:
  learn_throughput: 449.825
  learn_time_ms: 36680.956
  load_throughput: 4729028.105
  load_time_ms: 3.489
  training_iteration_time_ms: 48163.392
  update_time_ms: 2.603
timesteps_total: 3960000
training_iteration: 240

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3219178082191781
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9324324324324325
  reward for individual goal_min: 0.0
episode_len_mean: 174.2
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 19776
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.69021201133728
time_total_s: 12052.439715385437
timers:
  learn_throughput: 441.922
  learn_time_ms: 37336.874
  load_throughput: 4704180.103
  load_time_ms: 3.508
  training_iteration_time_ms: 48940.763
  update_time_ms: 2.751
timesteps_total: 3877500
training_iteration: 235

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972222222222222
  reward for individual goal_min: 0.5
episode_len_mean: 43.835978835978835
episode_reward_max: 2.0
episode_reward_mean: 1.9973544973544974
episode_reward_min: 1.0
episodes_this_iter: 378
episodes_total: 53786
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973544973544973
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 56.6006133556366
time_total_s: 11983.078241825104
timers:
  learn_throughput: 385.57
  learn_time_ms: 42793.771
  load_throughput: 4176559.948
  load_time_ms: 3.951
  training_iteration_time_ms: 55340.801
  update_time_ms: 2.718
timesteps_total: 3844500
training_iteration: 233

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973118279569892
  reward for individual goal_min: 0.5
episode_len_mean: 42.77142857142857
episode_reward_max: 2.0
episode_reward_mean: 1.9974025974025975
episode_reward_min: 1.0
episodes_this_iter: 385
episodes_total: 76521
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9974025974025974
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 47.806286096572876
time_total_s: 11917.768019914627
timers:
  learn_throughput: 453.053
  learn_time_ms: 36419.551
  load_throughput: 4280087.326
  load_time_ms: 3.855
  training_iteration_time_ms: 47958.179
  update_time_ms: 2.344
timesteps_total: 4191000
training_iteration: 254

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2922077922077922
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8888888888888888
  reward for individual goal_min: 0.0
episode_len_mean: 201.48
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 21988
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.3378427028656
time_total_s: 12079.458312511444
timers:
  learn_throughput: 507.226
  learn_time_ms: 32529.864
  load_throughput: 4808277.3
  load_time_ms: 3.432
  training_iteration_time_ms: 43293.613
  update_time_ms: 2.532
timesteps_total: 4389000
training_iteration: 266

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29012345679012347
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.86
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 24982
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.107661724090576
time_total_s: 12062.912827253342
timers:
  learn_throughput: 520.919
  learn_time_ms: 31674.773
  load_throughput: 4993074.947
  load_time_ms: 3.305
  training_iteration_time_ms: 42053.796
  update_time_ms: 2.554
timesteps_total: 4521000
training_iteration: 274

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2323943661971831
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939024390243902
  reward for individual goal_min: 0.5
episode_len_mean: 170.39
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 21536
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.40790390968323
time_total_s: 12077.183321714401
timers:
  learn_throughput: 456.863
  learn_time_ms: 36115.859
  load_throughput: 4713760.396
  load_time_ms: 3.5
  training_iteration_time_ms: 47348.731
  update_time_ms: 2.603
timesteps_total: 3976500
training_iteration: 241

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9236111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 183.31
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 21191
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.072704553604126
time_total_s: 12073.959444522858
timers:
  learn_throughput: 489.346
  learn_time_ms: 33718.458
  load_throughput: 4914676.419
  load_time_ms: 3.357
  training_iteration_time_ms: 44516.172
  update_time_ms: 2.529
timesteps_total: 4240500
training_iteration: 257

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14583333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 183.95
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 21019
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.055691957473755
time_total_s: 12085.914945602417
timers:
  learn_throughput: 427.583
  learn_time_ms: 38588.982
  load_throughput: 4753878.745
  load_time_ms: 3.471
  training_iteration_time_ms: 50682.267
  update_time_ms: 2.775
timesteps_total: 3811500
training_iteration: 231

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.11
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 20273
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.516109466552734
time_total_s: 12071.125177145004
timers:
  learn_throughput: 434.097
  learn_time_ms: 38009.912
  load_throughput: 4589014.906
  load_time_ms: 3.596
  training_iteration_time_ms: 50113.797
  update_time_ms: 2.627
timesteps_total: 3861000
training_iteration: 234

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2357142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 169.06
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 19970
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.77930521965027
time_total_s: 12075.413030147552
timers:
  learn_throughput: 407.095
  learn_time_ms: 40531.121
  load_throughput: 4353757.054
  load_time_ms: 3.79
  training_iteration_time_ms: 53102.042
  update_time_ms: 2.697
timesteps_total: 3663000
training_iteration: 222

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.81117021276596
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 376
episodes_total: 57785
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 60.29275059700012
time_total_s: 12008.324793100357
timers:
  learn_throughput: 369.168
  learn_time_ms: 44695.071
  load_throughput: 3543320.806
  load_time_ms: 4.657
  training_iteration_time_ms: 57661.122
  update_time_ms: 2.572
timesteps_total: 3514500
training_iteration: 213

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2682926829268293
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 184.78
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 22029
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.569700479507446
time_total_s: 12078.70126581192
timers:
  learn_throughput: 451.995
  learn_time_ms: 36504.865
  load_throughput: 4764056.255
  load_time_ms: 3.463
  training_iteration_time_ms: 47889.853
  update_time_ms: 2.612
timesteps_total: 3976500
training_iteration: 241

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26282051282051283
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9577464788732394
  reward for individual goal_min: 0.0
episode_len_mean: 191.84
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 19864
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.66133904457092
time_total_s: 12101.101054430008
timers:
  learn_throughput: 444.766
  learn_time_ms: 37098.189
  load_throughput: 4708853.235
  load_time_ms: 3.504
  training_iteration_time_ms: 48695.256
  update_time_ms: 2.886
timesteps_total: 3894000
training_iteration: 236

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18493150684931506
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.57
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 20095
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.546724796295166
time_total_s: 12096.601316452026
timers:
  learn_throughput: 412.204
  learn_time_ms: 40028.709
  load_throughput: 4294136.159
  load_time_ms: 3.842
  training_iteration_time_ms: 52692.758
  update_time_ms: 2.763
timesteps_total: 3663000
training_iteration: 222

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9675324675324676
  reward for individual goal_min: 0.5
episode_len_mean: 57.66551724137931
episode_reward_max: 2.0
episode_reward_mean: 1.9655172413793103
episode_reward_min: 1.0
episodes_this_iter: 290
episodes_total: 36508
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9655172413793104
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 67.25880742073059
time_total_s: 12092.78863620758
timers:
  learn_throughput: 314.98
  learn_time_ms: 52384.318
  load_throughput: 3609499.46
  load_time_ms: 4.571
  training_iteration_time_ms: 68084.213
  update_time_ms: 2.948
timesteps_total: 3250500
training_iteration: 197

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971428571428571
  reward for individual goal_min: 0.5
episode_len_mean: 44.18983957219251
episode_reward_max: 2.0
episode_reward_mean: 1.9973262032085561
episode_reward_min: 1.0
episodes_this_iter: 374
episodes_total: 54160
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973262032085561
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 55.73603081703186
time_total_s: 12038.814272642136
timers:
  learn_throughput: 384.604
  learn_time_ms: 42901.282
  load_throughput: 4174166.807
  load_time_ms: 3.953
  training_iteration_time_ms: 55529.624
  update_time_ms: 2.696
timesteps_total: 3861000
training_iteration: 234

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26811594202898553
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9276315789473685
  reward for individual goal_min: 0.0
episode_len_mean: 183.33
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 22078
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.32681655883789
time_total_s: 12121.785129070282
timers:
  learn_throughput: 509.171
  learn_time_ms: 32405.638
  load_throughput: 4751724.479
  load_time_ms: 3.472
  training_iteration_time_ms: 43103.697
  update_time_ms: 2.514
timesteps_total: 4405500
training_iteration: 267

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9945652173913043
  reward for individual goal_min: 0.5
episode_len_mean: 42.737662337662336
episode_reward_max: 2.0
episode_reward_mean: 1.9948051948051948
episode_reward_min: 1.0
episodes_this_iter: 385
episodes_total: 76906
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9948051948051948
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.984126567840576
time_total_s: 11965.752146482468
timers:
  learn_throughput: 452.841
  learn_time_ms: 36436.628
  load_throughput: 4266840.285
  load_time_ms: 3.867
  training_iteration_time_ms: 47954.743
  update_time_ms: 2.342
timesteps_total: 4207500
training_iteration: 255

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2866666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9859154929577465
  reward for individual goal_min: 0.0
episode_len_mean: 175.91
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 25077
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.08295917510986
time_total_s: 12105.995786428452
timers:
  learn_throughput: 518.145
  learn_time_ms: 31844.389
  load_throughput: 4945618.363
  load_time_ms: 3.336
  training_iteration_time_ms: 42265.283
  update_time_ms: 2.524
timesteps_total: 4537500
training_iteration: 275

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19078947368421054
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 189.35
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 21621
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.18922400474548
time_total_s: 12126.372545719147
timers:
  learn_throughput: 454.845
  learn_time_ms: 36276.081
  load_throughput: 4718163.076
  load_time_ms: 3.497
  training_iteration_time_ms: 47551.51
  update_time_ms: 2.599
timesteps_total: 3993000
training_iteration: 242

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24242424242424243
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.94375
  reward for individual goal_min: 0.0
episode_len_mean: 162.92
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 21289
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.31346797943115
time_total_s: 12118.272912502289
timers:
  learn_throughput: 490.023
  learn_time_ms: 33671.895
  load_throughput: 4927378.464
  load_time_ms: 3.349
  training_iteration_time_ms: 44452.905
  update_time_ms: 2.529
timesteps_total: 4257000
training_iteration: 258

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 193.6
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 21104
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.42167782783508
time_total_s: 12135.336623430252
timers:
  learn_throughput: 428.623
  learn_time_ms: 38495.34
  load_throughput: 4724347.114
  load_time_ms: 3.493
  training_iteration_time_ms: 50590.017
  update_time_ms: 2.808
timesteps_total: 3828000
training_iteration: 232

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.92
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 20363
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.09032320976257
time_total_s: 12121.215500354767
timers:
  learn_throughput: 434.879
  learn_time_ms: 37941.581
  load_throughput: 4605413.952
  load_time_ms: 3.583
  training_iteration_time_ms: 50048.464
  update_time_ms: 2.641
timesteps_total: 3877500
training_iteration: 235

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17307692307692307
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9814814814814815
  reward for individual goal_min: 0.0
episode_len_mean: 189.04
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 20058
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.348413944244385
time_total_s: 12125.761444091797
timers:
  learn_throughput: 410.95
  learn_time_ms: 40150.9
  load_throughput: 4343782.78
  load_time_ms: 3.799
  training_iteration_time_ms: 52543.674
  update_time_ms: 2.689
timesteps_total: 3679500
training_iteration: 223

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9846153846153847
  reward for individual goal_min: 0.0
episode_len_mean: 169.08
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 22127
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.348626375198364
time_total_s: 12127.049892187119
timers:
  learn_throughput: 452.763
  learn_time_ms: 36442.915
  load_throughput: 4797876.916
  load_time_ms: 3.439
  training_iteration_time_ms: 47790.059
  update_time_ms: 2.598
timesteps_total: 3993000
training_iteration: 242

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2905405405405405
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9802631578947368
  reward for individual goal_min: 0.0
episode_len_mean: 176.42
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 19959
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.98149228096008
time_total_s: 12151.082546710968
timers:
  learn_throughput: 444.369
  learn_time_ms: 37131.267
  load_throughput: 4748920.332
  load_time_ms: 3.474
  training_iteration_time_ms: 48848.294
  update_time_ms: 2.865
timesteps_total: 3910500
training_iteration: 237

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15217391304347827
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.94
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 20189
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.72966694831848
time_total_s: 12150.330983400345
timers:
  learn_throughput: 412.554
  learn_time_ms: 39994.753
  load_throughput: 4306668.907
  load_time_ms: 3.831
  training_iteration_time_ms: 52680.691
  update_time_ms: 2.759
timesteps_total: 3679500
training_iteration: 223

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23076923076923078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8333333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 199.79
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 22162
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.300262451171875
time_total_s: 12166.085391521454
timers:
  learn_throughput: 507.781
  learn_time_ms: 32494.333
  load_throughput: 4768980.616
  load_time_ms: 3.46
  training_iteration_time_ms: 43230.889
  update_time_ms: 2.549
timesteps_total: 4422000
training_iteration: 268

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9948453608247423
  reward for individual goal_min: 0.5
episode_len_mean: 42.201530612244895
episode_reward_max: 2.0
episode_reward_mean: 1.9948979591836735
episode_reward_min: 1.0
episodes_this_iter: 392
episodes_total: 58177
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974489795918368
  agent_1: 0.9974489795918368
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.792285203933716
time_total_s: 12069.11707830429
timers:
  learn_throughput: 366.38
  learn_time_ms: 45035.198
  load_throughput: 3520358.109
  load_time_ms: 4.687
  training_iteration_time_ms: 58130.728
  update_time_ms: 2.585
timesteps_total: 3531000
training_iteration: 214

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26506024096385544
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 188.61
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 25162
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.495115995407104
time_total_s: 12146.490902423859
timers:
  learn_throughput: 519.931
  learn_time_ms: 31734.969
  load_throughput: 4954611.684
  load_time_ms: 3.33
  training_iteration_time_ms: 42114.253
  update_time_ms: 2.512
timesteps_total: 4554000
training_iteration: 276

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9945652173913043
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973821989528796
  reward for individual goal_min: 0.5
episode_len_mean: 44.050666666666665
episode_reward_max: 2.0
episode_reward_mean: 1.992
episode_reward_min: 0.0
episodes_this_iter: 375
episodes_total: 77281
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9946666666666667
  agent_1: 0.9973333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.246824502944946
time_total_s: 12013.998970985413
timers:
  learn_throughput: 451.735
  learn_time_ms: 36525.839
  load_throughput: 4270948.105
  load_time_ms: 3.863
  training_iteration_time_ms: 48046.983
  update_time_ms: 2.356
timesteps_total: 4224000
training_iteration: 256

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9939393939393939
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974358974358974
  reward for individual goal_min: 0.5
episode_len_mean: 45.769444444444446
episode_reward_max: 2.0
episode_reward_mean: 1.9916666666666667
episode_reward_min: 0.0
episodes_this_iter: 360
episodes_total: 54520
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9944444444444445
  agent_1: 0.9972222222222222
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.65726399421692
time_total_s: 12094.471536636353
timers:
  learn_throughput: 383.727
  learn_time_ms: 42999.297
  load_throughput: 4152253.001
  load_time_ms: 3.974
  training_iteration_time_ms: 55635.553
  update_time_ms: 2.714
timesteps_total: 3877500
training_iteration: 235

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9602649006622517
  reward for individual goal_min: 0.5
episode_len_mean: 55.88581314878893
episode_reward_max: 2.0
episode_reward_mean: 1.958477508650519
episode_reward_min: 1.0
episodes_this_iter: 289
episodes_total: 36797
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9584775086505191
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 65.49198246002197
time_total_s: 12158.280618667603
timers:
  learn_throughput: 315.739
  learn_time_ms: 52258.418
  load_throughput: 3663108.82
  load_time_ms: 4.504
  training_iteration_time_ms: 67969.516
  update_time_ms: 2.962
timesteps_total: 3267000
training_iteration: 198

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3051948051948052
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 175.76
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 21716
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.916990756988525
time_total_s: 12173.289536476135
timers:
  learn_throughput: 453.847
  learn_time_ms: 36355.872
  load_throughput: 4757506.239
  load_time_ms: 3.468
  training_iteration_time_ms: 47661.856
  update_time_ms: 2.608
timesteps_total: 4009500
training_iteration: 243

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2746478873239437
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8380281690140845
  reward for individual goal_min: 0.0
episode_len_mean: 186.04
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 21379
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.03355646133423
time_total_s: 12164.306468963623
timers:
  learn_throughput: 485.977
  learn_time_ms: 33952.251
  load_throughput: 4985952.364
  load_time_ms: 3.309
  training_iteration_time_ms: 44819.186
  update_time_ms: 2.518
timesteps_total: 4273500
training_iteration: 259

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.96
  reward for individual goal_min: 0.0
episode_len_mean: 183.17
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 21193
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.35997033119202
time_total_s: 12184.696593761444
timers:
  learn_throughput: 433.511
  learn_time_ms: 38061.284
  load_throughput: 4720222.622
  load_time_ms: 3.496
  training_iteration_time_ms: 50164.756
  update_time_ms: 2.812
timesteps_total: 3844500
training_iteration: 233

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.68
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 20458
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.373868227005005
time_total_s: 12171.589368581772
timers:
  learn_throughput: 435.932
  learn_time_ms: 37849.911
  load_throughput: 4633752.21
  load_time_ms: 3.561
  training_iteration_time_ms: 49916.193
  update_time_ms: 2.638
timesteps_total: 3894000
training_iteration: 236

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 191.47
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 20145
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.75503206253052
time_total_s: 12178.516476154327
timers:
  learn_throughput: 415.327
  learn_time_ms: 39727.712
  load_throughput: 4402306.303
  load_time_ms: 3.748
  training_iteration_time_ms: 52038.326
  update_time_ms: 2.681
timesteps_total: 3696000
training_iteration: 224

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23529411764705882
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 163.41
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 22227
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.65476846694946
time_total_s: 12174.704660654068
timers:
  learn_throughput: 456.587
  learn_time_ms: 36137.693
  load_throughput: 4816040.195
  load_time_ms: 3.426
  training_iteration_time_ms: 47470.645
  update_time_ms: 2.618
timesteps_total: 4009500
training_iteration: 243

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.358974358974359
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9430379746835443
  reward for individual goal_min: 0.0
episode_len_mean: 180.52
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 20048
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.36932945251465
time_total_s: 12197.451876163483
timers:
  learn_throughput: 445.954
  learn_time_ms: 36999.352
  load_throughput: 4711353.648
  load_time_ms: 3.502
  training_iteration_time_ms: 48669.408
  update_time_ms: 2.862
timesteps_total: 3927000
training_iteration: 238

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2635135135135135
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8881578947368421
  reward for individual goal_min: 0.0
episode_len_mean: 190.51
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 22252
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.57705760002136
time_total_s: 12209.662449121475
timers:
  learn_throughput: 506.583
  learn_time_ms: 32571.19
  load_throughput: 4721865.111
  load_time_ms: 3.494
  training_iteration_time_ms: 43325.412
  update_time_ms: 2.545
timesteps_total: 4438500
training_iteration: 269

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2152777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9753086419753086
  reward for individual goal_min: 0.0
episode_len_mean: 175.5
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 25257
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.599040269851685
time_total_s: 12189.08994269371
timers:
  learn_throughput: 520.069
  learn_time_ms: 31726.579
  load_throughput: 4986311.603
  load_time_ms: 3.309
  training_iteration_time_ms: 42084.47
  update_time_ms: 2.52
timesteps_total: 4570500
training_iteration: 277

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17567567567567569
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 179.45
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 20276
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.58193325996399
time_total_s: 12203.912916660309
timers:
  learn_throughput: 411.083
  learn_time_ms: 40137.872
  load_throughput: 4300727.456
  load_time_ms: 3.837
  training_iteration_time_ms: 52874.503
  update_time_ms: 2.739
timesteps_total: 3696000
training_iteration: 224

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.7994923857868
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 394
episodes_total: 77675
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.85634517669678
time_total_s: 12061.85531616211
timers:
  learn_throughput: 452.234
  learn_time_ms: 36485.533
  load_throughput: 4257364.601
  load_time_ms: 3.876
  training_iteration_time_ms: 48010.87
  update_time_ms: 2.36
timesteps_total: 4240500
training_iteration: 257

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.1869918699187
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 369
episodes_total: 58546
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.57392168045044
time_total_s: 12128.690999984741
timers:
  learn_throughput: 364.331
  learn_time_ms: 45288.523
  load_throughput: 3497638.087
  load_time_ms: 4.717
  training_iteration_time_ms: 58455.056
  update_time_ms: 2.591
timesteps_total: 3547500
training_iteration: 215

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2605633802816901
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 167.71
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 21814
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.3079833984375
time_total_s: 12221.597519874573
timers:
  learn_throughput: 451.442
  learn_time_ms: 36549.562
  load_throughput: 4764089.05
  load_time_ms: 3.463
  training_iteration_time_ms: 47931.865
  update_time_ms: 2.569
timesteps_total: 4026000
training_iteration: 244

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 45.966573816155986
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 359
episodes_total: 54879
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 56.33596324920654
time_total_s: 12150.807499885559
timers:
  learn_throughput: 383.306
  learn_time_ms: 43046.602
  load_throughput: 4184540.073
  load_time_ms: 3.943
  training_iteration_time_ms: 55668.402
  update_time_ms: 2.726
timesteps_total: 3894000
training_iteration: 236

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9333333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 110.26666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.8
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9
  agent_1: 0.9
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2945205479452055
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9276315789473685
  reward for individual goal_min: 0.0
episode_len_mean: 188.2
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 21468
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.292394161224365
time_total_s: 12218.598863124847
timers:
  learn_throughput: 490.15
  learn_time_ms: 33663.183
  load_throughput: 4963566.572
  load_time_ms: 3.324
  training_iteration_time_ms: 44531.646
  update_time_ms: 2.526
timesteps_total: 4290000
training_iteration: 260

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22839506172839505
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.5
episode_len_mean: 192.93
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 21284
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.213030099868774
time_total_s: 12237.909623861313
timers:
  learn_throughput: 429.435
  learn_time_ms: 38422.54
  load_throughput: 4756786.836
  load_time_ms: 3.469
  training_iteration_time_ms: 50448.666
  update_time_ms: 2.835
timesteps_total: 3861000
training_iteration: 234

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17307692307692307
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 187.58
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 20547
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.51
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.10827589035034
time_total_s: 12222.697644472122
timers:
  learn_throughput: 434.882
  learn_time_ms: 37941.316
  load_throughput: 4636049.25
  load_time_ms: 3.559
  training_iteration_time_ms: 49920.571
  update_time_ms: 2.642
timesteps_total: 3910500
training_iteration: 237

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9519230769230769
  reward for individual goal_min: 0.5
episode_len_mean: 59.58156028368794
episode_reward_max: 2.0
episode_reward_mean: 1.946808510638298
episode_reward_min: 1.0
episodes_this_iter: 282
episodes_total: 37079
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9468085106382979
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 65.75901746749878
time_total_s: 12224.039636135101
timers:
  learn_throughput: 316.61
  learn_time_ms: 52114.561
  load_throughput: 3711494.766
  load_time_ms: 4.446
  training_iteration_time_ms: 67732.012
  update_time_ms: 2.942
timesteps_total: 3283500
training_iteration: 199

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.98125
  reward for individual goal_min: 0.0
episode_len_mean: 176.72
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 20237
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.66529679298401
time_total_s: 12230.181772947311
timers:
  learn_throughput: 415.656
  learn_time_ms: 39696.309
  load_throughput: 4451463.71
  load_time_ms: 3.707
  training_iteration_time_ms: 52007.894
  update_time_ms: 2.691
timesteps_total: 3712500
training_iteration: 225

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32098765432098764
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.68
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 22325
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.038723945617676
time_total_s: 12222.743384599686
timers:
  learn_throughput: 457.325
  learn_time_ms: 36079.361
  load_throughput: 4857687.464
  load_time_ms: 3.397
  training_iteration_time_ms: 47420.543
  update_time_ms: 2.618
timesteps_total: 4026000
training_iteration: 244

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27702702702702703
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8986486486486487
  reward for individual goal_min: 0.0
episode_len_mean: 191.8
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 22338
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.22856903076172
time_total_s: 12252.891018152237
timers:
  learn_throughput: 507.052
  learn_time_ms: 32541.06
  load_throughput: 4679591.856
  load_time_ms: 3.526
  training_iteration_time_ms: 43245.072
  update_time_ms: 2.557
timesteps_total: 4455000
training_iteration: 270

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26582278481012656
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9785714285714285
  reward for individual goal_min: 0.0
episode_len_mean: 198.58
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 20129
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.30501675605774
time_total_s: 12244.75689291954
timers:
  learn_throughput: 448.668
  learn_time_ms: 36775.563
  load_throughput: 4687230.169
  load_time_ms: 3.52
  training_iteration_time_ms: 48432.44
  update_time_ms: 2.843
timesteps_total: 3943500
training_iteration: 239

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22435897435897437
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 185.42
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 25340
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.0785391330719
time_total_s: 12230.168481826782
timers:
  learn_throughput: 521.884
  learn_time_ms: 31616.209
  load_throughput: 4983403.253
  load_time_ms: 3.311
  training_iteration_time_ms: 41973.966
  update_time_ms: 2.5
timesteps_total: 4587000
training_iteration: 278

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.526315789473685
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 399
episodes_total: 78074
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.55692958831787
time_total_s: 12109.412245750427
timers:
  learn_throughput: 452.492
  learn_time_ms: 36464.697
  load_throughput: 4319652.46
  load_time_ms: 3.82
  training_iteration_time_ms: 47970.974
  update_time_ms: 2.356
timesteps_total: 4257000
training_iteration: 258

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23026315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 175.19
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 20373
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.97121715545654
time_total_s: 12257.884133815765
timers:
  learn_throughput: 410.267
  learn_time_ms: 40217.674
  load_throughput: 4290329.372
  load_time_ms: 3.846
  training_iteration_time_ms: 52962.251
  update_time_ms: 2.734
timesteps_total: 3712500
training_iteration: 225

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23972602739726026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.99375
  reward for individual goal_min: 0.5
episode_len_mean: 173.24
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 21910
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.38322353363037
time_total_s: 12269.980743408203
timers:
  learn_throughput: 449.184
  learn_time_ms: 36733.243
  load_throughput: 4719482.266
  load_time_ms: 3.496
  training_iteration_time_ms: 48162.254
  update_time_ms: 2.561
timesteps_total: 4042500
training_iteration: 245

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23648648648648649
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9539473684210527
  reward for individual goal_min: 0.0
episode_len_mean: 190.13
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 21554
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.67326831817627
time_total_s: 12262.272131443024
timers:
  learn_throughput: 491.709
  learn_time_ms: 33556.402
  load_throughput: 5030384.369
  load_time_ms: 3.28
  training_iteration_time_ms: 44426.049
  update_time_ms: 2.481
timesteps_total: 4306500
training_iteration: 261

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972972972972973
  reward for individual goal_min: 0.5
episode_len_mean: 44.924528301886795
episode_reward_max: 2.0
episode_reward_mean: 1.9973045822102427
episode_reward_min: 1.0
episodes_this_iter: 371
episodes_total: 58917
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973045822102425
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 60.274375200271606
time_total_s: 12188.965375185013
timers:
  learn_throughput: 362.091
  learn_time_ms: 45568.698
  load_throughput: 3475191.998
  load_time_ms: 4.748
  training_iteration_time_ms: 58820.229
  update_time_ms: 2.594
timesteps_total: 3564000
training_iteration: 216

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9945054945054945
  reward for individual goal_min: 0.5
episode_len_mean: 46.84943181818182
episode_reward_max: 2.0
episode_reward_mean: 1.9943181818181819
episode_reward_min: 1.0
episodes_this_iter: 352
episodes_total: 55231
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9943181818181818
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 55.32469916343689
time_total_s: 12206.132199048996
timers:
  learn_throughput: 382.826
  learn_time_ms: 43100.543
  load_throughput: 4217256.097
  load_time_ms: 3.912
  training_iteration_time_ms: 55724.06
  update_time_ms: 2.713
timesteps_total: 3910500
training_iteration: 237

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8943661971830986
  reward for individual goal_min: 0.0
episode_len_mean: 185.44
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 22426
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.611412048339844
time_total_s: 12295.502430200577
timers:
  learn_throughput: 508.553
  learn_time_ms: 32445.0
  load_throughput: 4657797.161
  load_time_ms: 3.542
  training_iteration_time_ms: 43171.7
  update_time_ms: 2.575
timesteps_total: 4471500
training_iteration: 271

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26282051282051283
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 176.95
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 21379
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.632108211517334
time_total_s: 12287.54173207283
timers:
  learn_throughput: 429.507
  learn_time_ms: 38416.102
  load_throughput: 4747552.068
  load_time_ms: 3.475
  training_iteration_time_ms: 50522.059
  update_time_ms: 2.843
timesteps_total: 3877500
training_iteration: 235

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.07
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 20642
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.73598003387451
time_total_s: 12273.433624505997
timers:
  learn_throughput: 435.765
  learn_time_ms: 37864.438
  load_throughput: 4613795.917
  load_time_ms: 3.576
  training_iteration_time_ms: 49881.797
  update_time_ms: 2.63
timesteps_total: 3927000
training_iteration: 238

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22289156626506024
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 196.16
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 25427
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.60128331184387
time_total_s: 12273.769765138626
timers:
  learn_throughput: 519.959
  learn_time_ms: 31733.249
  load_throughput: 5010208.934
  load_time_ms: 3.293
  training_iteration_time_ms: 42103.652
  update_time_ms: 2.49
timesteps_total: 4603500
training_iteration: 279

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22839506172839505
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9929577464788732
  reward for individual goal_min: 0.5
episode_len_mean: 194.71
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 22410
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.59133315086365
time_total_s: 12269.33471775055
timers:
  learn_throughput: 458.771
  learn_time_ms: 35965.668
  load_throughput: 4904262.936
  load_time_ms: 3.364
  training_iteration_time_ms: 47233.236
  update_time_ms: 2.639
timesteps_total: 4042500
training_iteration: 245

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17424242424242425
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.5
episode_len_mean: 174.47
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 20332
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.666922092437744
time_total_s: 12283.84869503975
timers:
  learn_throughput: 413.261
  learn_time_ms: 39926.317
  load_throughput: 4443460.975
  load_time_ms: 3.713
  training_iteration_time_ms: 52312.056
  update_time_ms: 2.663
timesteps_total: 3729000
training_iteration: 226

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9
  reward for individual goal_min: 0.5
episode_len_mean: 99.45
episode_reward_max: 2.0
episode_reward_mean: 1.8
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9333333333333333
  agent_1: 0.8666666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29577464788732394
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9459459459459459
  reward for individual goal_min: 0.0
episode_len_mean: 179.79
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 20221
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.217968463897705
time_total_s: 12303.974861383438
timers:
  learn_throughput: 448.419
  learn_time_ms: 36795.945
  load_throughput: 4684343.064
  load_time_ms: 3.522
  training_iteration_time_ms: 48412.083
  update_time_ms: 2.845
timesteps_total: 3960000
training_iteration: 240

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 48.86666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.9833333333333334
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9833333333333333
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.857142857142854
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 385
episodes_total: 78459
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.45607852935791
time_total_s: 12157.868324279785
timers:
  learn_throughput: 451.589
  learn_time_ms: 36537.626
  load_throughput: 4304525.952
  load_time_ms: 3.833
  training_iteration_time_ms: 48026.15
  update_time_ms: 2.333
timesteps_total: 4273500
training_iteration: 259

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9580645161290322
  reward for individual goal_min: 0.5
episode_len_mean: 56.98634812286689
episode_reward_max: 2.0
episode_reward_mean: 1.9556313993174061
episode_reward_min: 1.0
episodes_this_iter: 293
episodes_total: 37372
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9556313993174061
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 73.79089546203613
time_total_s: 12297.830531597137
timers:
  learn_throughput: 318.201
  learn_time_ms: 51854.026
  load_throughput: 3858024.551
  load_time_ms: 4.277
  training_iteration_time_ms: 67490.487
  update_time_ms: 2.983
timesteps_total: 3300000
training_iteration: 200

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-19wecnkw4d/checkpoint_000200/checkpoint-200
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 174.71
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 20469
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.798182249069214
time_total_s: 12310.682316064835
timers:
  learn_throughput: 410.259
  learn_time_ms: 40218.463
  load_throughput: 4285732.97
  load_time_ms: 3.85
  training_iteration_time_ms: 52951.417
  update_time_ms: 2.724
timesteps_total: 3729000
training_iteration: 226

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2236842105263158
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8947368421052632
  reward for individual goal_min: 0.0
episode_len_mean: 193.52
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 21641
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.03001618385315
time_total_s: 12307.302147626877
timers:
  learn_throughput: 491.841
  learn_time_ms: 33547.402
  load_throughput: 4977310.328
  load_time_ms: 3.315
  training_iteration_time_ms: 44431.66
  update_time_ms: 2.484
timesteps_total: 4323000
training_iteration: 262

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20945945945945946
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.96
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 22000
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.76187801361084
time_total_s: 12319.742621421814
timers:
  learn_throughput: 445.879
  learn_time_ms: 37005.536
  load_throughput: 4687833.42
  load_time_ms: 3.52
  training_iteration_time_ms: 48500.257
  update_time_ms: 2.565
timesteps_total: 4059000
training_iteration: 246

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8405797101449275
  reward for individual goal_min: 0.0
episode_len_mean: 207.73
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 22505
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.95958924293518
time_total_s: 12340.462019443512
timers:
  learn_throughput: 504.935
  learn_time_ms: 32677.451
  load_throughput: 4683138.513
  load_time_ms: 3.523
  training_iteration_time_ms: 43427.8
  update_time_ms: 2.586
timesteps_total: 4488000
training_iteration: 272

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23648648648648649
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.24
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 20737
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.24705219268799
time_total_s: 12320.680676698685
timers:
  learn_throughput: 438.0
  learn_time_ms: 37671.21
  load_throughput: 4595322.475
  load_time_ms: 3.591
  training_iteration_time_ms: 49618.113
  update_time_ms: 2.628
timesteps_total: 3943500
training_iteration: 239

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2077922077922078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 181.76
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 22503
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.99433898925781
time_total_s: 12316.329056739807
timers:
  learn_throughput: 459.609
  learn_time_ms: 35900.079
  load_throughput: 4929975.922
  load_time_ms: 3.347
  training_iteration_time_ms: 47131.714
  update_time_ms: 2.645
timesteps_total: 4059000
training_iteration: 246

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9944444444444445
  reward for individual goal_min: 0.5
episode_len_mean: 45.72375690607735
episode_reward_max: 2.0
episode_reward_mean: 1.9944751381215469
episode_reward_min: 1.0
episodes_this_iter: 362
episodes_total: 55593
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972375690607734
  agent_1: 0.9972375690607734
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.8795907497406
time_total_s: 12262.011789798737
timers:
  learn_throughput: 382.245
  learn_time_ms: 43165.992
  load_throughput: 4207333.986
  load_time_ms: 3.922
  training_iteration_time_ms: 55833.813
  update_time_ms: 2.651
timesteps_total: 3927000
training_iteration: 238

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 81.2
episode_reward_max: 2.0
episode_reward_mean: 1.9333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.95
  agent_1: 0.9833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9642857142857143
  reward for individual goal_min: 0.0
episode_len_mean: 181.69
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 21470
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.75140047073364
time_total_s: 12338.293132543564
timers:
  learn_throughput: 427.717
  learn_time_ms: 38576.95
  load_throughput: 4760582.502
  load_time_ms: 3.466
  training_iteration_time_ms: 50644.933
  update_time_ms: 2.832
timesteps_total: 3894000
training_iteration: 236

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9802631578947368
  reward for individual goal_min: 0.0
episode_len_mean: 179.6
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 25519
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.33776354789734
time_total_s: 12323.107528686523
timers:
  learn_throughput: 523.411
  learn_time_ms: 31524.001
  load_throughput: 5017291.913
  load_time_ms: 3.289
  training_iteration_time_ms: 41901.006
  update_time_ms: 2.468
timesteps_total: 4620000
training_iteration: 280

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9944444444444445
  reward for individual goal_min: 0.5
episode_len_mean: 44.015957446808514
episode_reward_max: 2.0
episode_reward_mean: 1.9946808510638299
episode_reward_min: 1.0
episodes_this_iter: 376
episodes_total: 59293
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973404255319149
  agent_1: 0.9973404255319149
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.748645067214966
time_total_s: 12249.714020252228
timers:
  learn_throughput: 359.561
  learn_time_ms: 45889.346
  load_throughput: 3481818.438
  load_time_ms: 4.739
  training_iteration_time_ms: 59202.117
  update_time_ms: 2.593
timesteps_total: 3580500
training_iteration: 217

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.273972602739726
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 167.7
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 20427
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.76
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.26626896858215
time_total_s: 12337.114964008331
timers:
  learn_throughput: 413.18
  learn_time_ms: 39934.173
  load_throughput: 4441065.763
  load_time_ms: 3.715
  training_iteration_time_ms: 52311.069
  update_time_ms: 2.674
timesteps_total: 3745500
training_iteration: 227

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34615384615384615
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9315068493150684
  reward for individual goal_min: 0.0
episode_len_mean: 175.14
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 20316
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.1544451713562
time_total_s: 12355.129306554794
timers:
  learn_throughput: 447.102
  learn_time_ms: 36904.315
  load_throughput: 4705011.626
  load_time_ms: 3.507
  training_iteration_time_ms: 48609.315
  update_time_ms: 2.81
timesteps_total: 3976500
training_iteration: 241

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28205128205128205
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9383561643835616
  reward for individual goal_min: 0.0
episode_len_mean: 190.19
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 21726
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.9757399559021
time_total_s: 12349.277887582779
timers:
  learn_throughput: 495.403
  learn_time_ms: 33306.192
  load_throughput: 4999531.588
  load_time_ms: 3.3
  training_iteration_time_ms: 44064.524
  update_time_ms: 2.447
timesteps_total: 4339500
training_iteration: 263

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 49.05
episode_reward_max: 2.0
episode_reward_mean: 1.9666666666666666
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9666666666666667
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.63544303797468
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 395
episodes_total: 78854
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 53.0599524974823
time_total_s: 12210.928276777267
timers:
  learn_throughput: 452.2
  learn_time_ms: 36488.268
  load_throughput: 4298029.785
  load_time_ms: 3.839
  training_iteration_time_ms: 47977.396
  update_time_ms: 2.329
timesteps_total: 4290000
training_iteration: 260

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 173.67
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 22096
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.626816511154175
time_total_s: 12368.369437932968
timers:
  learn_throughput: 446.738
  learn_time_ms: 36934.389
  load_throughput: 4721188.654
  load_time_ms: 3.495
  training_iteration_time_ms: 48425.275
  update_time_ms: 2.558
timesteps_total: 4075500
training_iteration: 247

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2361111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 163.87
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 20568
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.77239537239075
time_total_s: 12365.454711437225
timers:
  learn_throughput: 408.707
  learn_time_ms: 40371.229
  load_throughput: 4300086.119
  load_time_ms: 3.837
  training_iteration_time_ms: 53229.008
  update_time_ms: 2.69
timesteps_total: 3745500
training_iteration: 227

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2236842105263158
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9294871794871795
  reward for individual goal_min: 0.0
episode_len_mean: 192.03
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 22593
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 39.921146392822266
time_total_s: 12380.383165836334
timers:
  learn_throughput: 508.801
  learn_time_ms: 32429.151
  load_throughput: 4707443.917
  load_time_ms: 3.505
  training_iteration_time_ms: 43114.137
  update_time_ms: 2.612
timesteps_total: 4504500
training_iteration: 273

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18309859154929578
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935897435897436
  reward for individual goal_min: 0.5
episode_len_mean: 182.17
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 25612
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.35186529159546
time_total_s: 12364.459393978119
timers:
  learn_throughput: 523.412
  learn_time_ms: 31523.912
  load_throughput: 5032066.894
  load_time_ms: 3.279
  training_iteration_time_ms: 41898.193
  update_time_ms: 2.489
timesteps_total: 4636500
training_iteration: 281

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9803921568627451
  reward for individual goal_min: 0.5
episode_len_mean: 52.461290322580645
episode_reward_max: 2.0
episode_reward_mean: 1.9806451612903226
episode_reward_min: 1.0
episodes_this_iter: 310
episodes_total: 37682
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9806451612903225
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 67.6531114578247
time_total_s: 12365.483643054962
timers:
  learn_throughput: 318.406
  learn_time_ms: 51820.68
  load_throughput: 3861081.009
  load_time_ms: 4.273
  training_iteration_time_ms: 67413.646
  update_time_ms: 2.93
timesteps_total: 3316500
training_iteration: 201

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3231707317073171
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9928571428571429
  reward for individual goal_min: 0.5
episode_len_mean: 183.09
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 22593
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.50512456893921
time_total_s: 12364.834181308746
timers:
  learn_throughput: 458.39
  learn_time_ms: 35995.585
  load_throughput: 4948376.616
  load_time_ms: 3.334
  training_iteration_time_ms: 47309.421
  update_time_ms: 2.639
timesteps_total: 4075500
training_iteration: 247

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25675675675675674
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.5
episode_len_mean: 179.43
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 21559
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.88898015022278
time_total_s: 12389.182112693787
timers:
  learn_throughput: 425.833
  learn_time_ms: 38747.602
  load_throughput: 4806540.772
  load_time_ms: 3.433
  training_iteration_time_ms: 50725.529
  update_time_ms: 2.773
timesteps_total: 3910500
training_iteration: 237

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9945945945945946
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9845679012345679
  reward for individual goal_min: 0.5
episode_len_mean: 47.55619596541787
episode_reward_max: 2.0
episode_reward_mean: 1.9798270893371759
episode_reward_min: 0.0
episodes_this_iter: 347
episodes_total: 55940
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9855907780979827
  agent_1: 0.9942363112391931
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.607075929641724
time_total_s: 12317.618865728378
timers:
  learn_throughput: 383.098
  learn_time_ms: 43069.933
  load_throughput: 4215021.469
  load_time_ms: 3.915
  training_iteration_time_ms: 55763.099
  update_time_ms: 2.656
timesteps_total: 3943500
training_iteration: 239

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 97.66666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.75
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8833333333333333
  agent_1: 0.8666666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 173.09
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 20525
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.17751693725586
time_total_s: 12387.292480945587
timers:
  learn_throughput: 416.067
  learn_time_ms: 39657.031
  load_throughput: 4433895.595
  load_time_ms: 3.721
  training_iteration_time_ms: 51988.488
  update_time_ms: 2.704
timesteps_total: 3762000
training_iteration: 228

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2361111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 168.16
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 20834
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.89394450187683
time_total_s: 12383.574621200562
timers:
  learn_throughput: 436.781
  learn_time_ms: 37776.349
  load_throughput: 4595261.449
  load_time_ms: 3.591
  training_iteration_time_ms: 49625.5
  update_time_ms: 2.638
timesteps_total: 3960000
training_iteration: 240

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9947089947089947
  reward for individual goal_min: 0.5
episode_len_mean: 43.40263157894737
episode_reward_max: 2.0
episode_reward_mean: 1.9947368421052631
episode_reward_min: 1.0
episodes_this_iter: 380
episodes_total: 59673
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9947368421052631
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 59.5071976184845
time_total_s: 12309.221217870712
timers:
  learn_throughput: 358.302
  learn_time_ms: 46050.585
  load_throughput: 3446085.697
  load_time_ms: 4.788
  training_iteration_time_ms: 59415.602
  update_time_ms: 2.619
timesteps_total: 3597000
training_iteration: 218

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9788732394366197
  reward for individual goal_min: 0.0
episode_len_mean: 193.43
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 20397
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.94676327705383
time_total_s: 12403.076069831848
timers:
  learn_throughput: 447.812
  learn_time_ms: 36845.821
  load_throughput: 4491418.113
  load_time_ms: 3.674
  training_iteration_time_ms: 48544.65
  update_time_ms: 2.791
timesteps_total: 3993000
training_iteration: 242

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22794117647058823
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9647887323943662
  reward for individual goal_min: 0.5
episode_len_mean: 182.08
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 21816
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.48185849189758
time_total_s: 12392.759746074677
timers:
  learn_throughput: 497.271
  learn_time_ms: 33181.127
  load_throughput: 5013076.037
  load_time_ms: 3.291
  training_iteration_time_ms: 43912.54
  update_time_ms: 2.437
timesteps_total: 4356000
training_iteration: 264

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2777777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9671052631578947
  reward for individual goal_min: 0.5
episode_len_mean: 182.02
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 22683
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 39.672417640686035
time_total_s: 12420.05558347702
timers:
  learn_throughput: 513.463
  learn_time_ms: 32134.732
  load_throughput: 4754042.027
  load_time_ms: 3.471
  training_iteration_time_ms: 42756.492
  update_time_ms: 2.587
timesteps_total: 4521000
training_iteration: 274

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973684210526316
  reward for individual goal_min: 0.5
episode_len_mean: 42.455012853470436
episode_reward_max: 2.0
episode_reward_mean: 1.9974293059125965
episode_reward_min: 1.0
episodes_this_iter: 389
episodes_total: 79243
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974293059125964
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.053293228149414
time_total_s: 12258.981570005417
timers:
  learn_throughput: 452.459
  learn_time_ms: 36467.373
  load_throughput: 4312250.587
  load_time_ms: 3.826
  training_iteration_time_ms: 47972.281
  update_time_ms: 2.325
timesteps_total: 4306500
training_iteration: 261

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18115942028985507
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.83
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 22192
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.76
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.80988597869873
time_total_s: 12417.179323911667
timers:
  learn_throughput: 444.919
  learn_time_ms: 37085.385
  load_throughput: 4717005.371
  load_time_ms: 3.498
  training_iteration_time_ms: 48618.233
  update_time_ms: 2.563
timesteps_total: 4092000
training_iteration: 248

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24025974025974026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 178.57
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 25703
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.24873995780945
time_total_s: 12406.708133935928
timers:
  learn_throughput: 522.989
  learn_time_ms: 31549.433
  load_throughput: 5080608.445
  load_time_ms: 3.248
  training_iteration_time_ms: 41952.547
  update_time_ms: 2.499
timesteps_total: 4653000
training_iteration: 282

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.98
  reward for individual goal_min: 0.0
episode_len_mean: 172.28
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 20661
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.22338247299194
time_total_s: 12420.678093910217
timers:
  learn_throughput: 405.239
  learn_time_ms: 40716.755
  load_throughput: 4254616.411
  load_time_ms: 3.878
  training_iteration_time_ms: 53661.276
  update_time_ms: 2.687
timesteps_total: 3762000
training_iteration: 228

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24324324324324326
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 170.32
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 22691
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.503382444381714
time_total_s: 12414.337563753128
timers:
  learn_throughput: 455.016
  learn_time_ms: 36262.48
  load_throughput: 4935249.451
  load_time_ms: 3.343
  training_iteration_time_ms: 47691.11
  update_time_ms: 2.665
timesteps_total: 4092000
training_iteration: 248

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 181.92
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 21650
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.92730665206909
time_total_s: 12441.109419345856
timers:
  learn_throughput: 425.779
  learn_time_ms: 38752.493
  load_throughput: 4806540.772
  load_time_ms: 3.433
  training_iteration_time_ms: 50825.637
  update_time_ms: 2.752
timesteps_total: 3927000
training_iteration: 238

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2465753424657534
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939759036144579
  reward for individual goal_min: 0.5
episode_len_mean: 168.66
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 20933
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.01304793357849
time_total_s: 12430.58766913414
timers:
  learn_throughput: 437.4
  learn_time_ms: 37722.927
  load_throughput: 4607008.168
  load_time_ms: 3.582
  training_iteration_time_ms: 49560.469
  update_time_ms: 2.652
timesteps_total: 3976500
training_iteration: 241

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17647058823529413
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9166666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 186.31
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 21901
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.06975507736206
time_total_s: 12436.829501152039
timers:
  learn_throughput: 497.113
  learn_time_ms: 33191.634
  load_throughput: 4997906.839
  load_time_ms: 3.301
  training_iteration_time_ms: 43934.783
  update_time_ms: 2.437
timesteps_total: 4372500
training_iteration: 265

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9735099337748344
  reward for individual goal_min: 0.5
episode_len_mean: 53.12101910828026
episode_reward_max: 2.0
episode_reward_mean: 1.9745222929936306
episode_reward_min: 1.0
episodes_this_iter: 314
episodes_total: 37996
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9968152866242038
  agent_1: 0.9777070063694268
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 68.95087265968323
time_total_s: 12434.434515714645
timers:
  learn_throughput: 320.484
  learn_time_ms: 51484.565
  load_throughput: 3734768.972
  load_time_ms: 4.418
  training_iteration_time_ms: 67051.973
  update_time_ms: 2.86
timesteps_total: 3333000
training_iteration: 202

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21232876712328766
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.45
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 20618
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.59945273399353
time_total_s: 12439.89193367958
timers:
  learn_throughput: 415.611
  learn_time_ms: 39700.613
  load_throughput: 4428618.161
  load_time_ms: 3.726
  training_iteration_time_ms: 52094.152
  update_time_ms: 2.724
timesteps_total: 3778500
training_iteration: 229

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2894736842105263
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 177.82
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 20493
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.825926065444946
time_total_s: 12452.901995897293
timers:
  learn_throughput: 445.276
  learn_time_ms: 37055.709
  load_throughput: 4509093.373
  load_time_ms: 3.659
  training_iteration_time_ms: 48709.312
  update_time_ms: 2.814
timesteps_total: 4009500
training_iteration: 243

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 48.06666666666667
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971751412429378
  reward for individual goal_min: 0.5
episode_len_mean: 46.61516034985423
episode_reward_max: 2.0
episode_reward_mean: 1.9970845481049562
episode_reward_min: 1.0
episodes_this_iter: 343
episodes_total: 56283
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9970845481049563
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 59.93601989746094
time_total_s: 12377.55488562584
timers:
  learn_throughput: 384.475
  learn_time_ms: 42915.61
  load_throughput: 4178198.942
  load_time_ms: 3.949
  training_iteration_time_ms: 55575.637
  update_time_ms: 2.627
timesteps_total: 3960000
training_iteration: 240

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3402777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9013157894736842
  reward for individual goal_min: 0.0
episode_len_mean: 170.5
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 22780
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.993937492370605
time_total_s: 12463.04952096939
timers:
  learn_throughput: 513.324
  learn_time_ms: 32143.417
  load_throughput: 4811887.946
  load_time_ms: 3.429
  training_iteration_time_ms: 42756.444
  update_time_ms: 2.575
timesteps_total: 4537500
training_iteration: 275

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-18g5xov65h/checkpoint_000240/checkpoint-240
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2702702702702703
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9733333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 175.25
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 25793
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.38195562362671
time_total_s: 12447.090089559555
timers:
  learn_throughput: 525.318
  learn_time_ms: 31409.575
  load_throughput: 5130057.597
  load_time_ms: 3.216
  training_iteration_time_ms: 41742.63
  update_time_ms: 2.496
timesteps_total: 4669500
training_iteration: 283

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.97442455242967
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 391
episodes_total: 60064
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 61.26379728317261
time_total_s: 12370.485015153885
timers:
  learn_throughput: 356.538
  learn_time_ms: 46278.357
  load_throughput: 3443753.564
  load_time_ms: 4.791
  training_iteration_time_ms: 59689.188
  update_time_ms: 2.608
timesteps_total: 3613500
training_iteration: 219

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.28092783505155
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 388
episodes_total: 79631
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.32444381713867
time_total_s: 12307.306013822556
timers:
  learn_throughput: 451.847
  learn_time_ms: 36516.82
  load_throughput: 4302037.447
  load_time_ms: 3.835
  training_iteration_time_ms: 48003.021
  update_time_ms: 2.336
timesteps_total: 4323000
training_iteration: 262

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23809523809523808
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 194.61
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 22276
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.3244252204895
time_total_s: 12466.503749132156
timers:
  learn_throughput: 444.617
  learn_time_ms: 37110.597
  load_throughput: 4721446.329
  load_time_ms: 3.495
  training_iteration_time_ms: 48631.685
  update_time_ms: 2.582
timesteps_total: 4108500
training_iteration: 249

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2012987012987013
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9929577464788732
  reward for individual goal_min: 0.5
episode_len_mean: 184.79
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 20750
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.689308404922485
time_total_s: 12475.36740231514
timers:
  learn_throughput: 405.256
  learn_time_ms: 40714.991
  load_throughput: 4255819.943
  load_time_ms: 3.877
  training_iteration_time_ms: 53696.804
  update_time_ms: 3.122
timesteps_total: 3778500
training_iteration: 229

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 186.31
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 22779
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.19027042388916
time_total_s: 12462.527834177017
timers:
  learn_throughput: 454.17
  learn_time_ms: 36330.031
  load_throughput: 4915374.552
  load_time_ms: 3.357
  training_iteration_time_ms: 47809.486
  update_time_ms: 2.658
timesteps_total: 4108500
training_iteration: 249

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18243243243243243
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9473684210526315
  reward for individual goal_min: 0.0
episode_len_mean: 196.88
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 21984
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.45596194267273
time_total_s: 12479.285463094711
timers:
  learn_throughput: 500.75
  learn_time_ms: 32950.559
  load_throughput: 5017546.546
  load_time_ms: 3.288
  training_iteration_time_ms: 43600.114
  update_time_ms: 2.43
timesteps_total: 4389000
training_iteration: 266

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20945945945945946
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 181.33
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 21740
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.966253995895386
time_total_s: 12492.075673341751
timers:
  learn_throughput: 425.776
  learn_time_ms: 38752.815
  load_throughput: 4816341.847
  load_time_ms: 3.426
  training_iteration_time_ms: 50740.751
  update_time_ms: 2.78
timesteps_total: 3943500
training_iteration: 239

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1858974358974359
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9807692307692307
  reward for individual goal_min: 0.0
episode_len_mean: 188.66
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 21017
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.253310441970825
time_total_s: 12479.84097957611
timers:
  learn_throughput: 438.085
  learn_time_ms: 37663.914
  load_throughput: 4573638.833
  load_time_ms: 3.608
  training_iteration_time_ms: 49587.805
  update_time_ms: 2.632
timesteps_total: 3993000
training_iteration: 242

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2714285714285714
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9178082191780822
  reward for individual goal_min: 0.0
episode_len_mean: 181.37
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 22870
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.77049255371094
time_total_s: 12505.820013523102
timers:
  learn_throughput: 515.386
  learn_time_ms: 32014.813
  load_throughput: 4815101.964
  load_time_ms: 3.427
  training_iteration_time_ms: 42599.8
  update_time_ms: 2.563
timesteps_total: 4554000
training_iteration: 276

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25308641975308643
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9855072463768116
  reward for individual goal_min: 0.0
episode_len_mean: 192.6
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 20579
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.90454697608948
time_total_s: 12501.806542873383
timers:
  learn_throughput: 445.165
  learn_time_ms: 37064.886
  load_throughput: 4511297.863
  load_time_ms: 3.657
  training_iteration_time_ms: 48716.366
  update_time_ms: 2.83
timesteps_total: 4026000
training_iteration: 244

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22435897435897437
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 188.82
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 25883
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.314613819122314
time_total_s: 12488.404703378677
timers:
  learn_throughput: 526.098
  learn_time_ms: 31362.951
  load_throughput: 5202403.723
  load_time_ms: 3.172
  training_iteration_time_ms: 41663.36
  update_time_ms: 2.498
timesteps_total: 4686000
training_iteration: 284

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18421052631578946
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9794520547945206
  reward for individual goal_min: 0.0
episode_len_mean: 188.22
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 20703
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.35035228729248
time_total_s: 12495.242285966873
timers:
  learn_throughput: 414.544
  learn_time_ms: 39802.794
  load_throughput: 4447115.795
  load_time_ms: 3.71
  training_iteration_time_ms: 52297.114
  update_time_ms: 2.747
timesteps_total: 3795000
training_iteration: 230

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9919786096256684
  reward for individual goal_min: 0.5
episode_len_mean: 45.56989247311828
episode_reward_max: 2.0
episode_reward_mean: 1.9919354838709677
episode_reward_min: 1.0
episodes_this_iter: 372
episodes_total: 56655
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9946236559139785
  agent_1: 0.9973118279569892
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.0250198841095
time_total_s: 12433.579905509949
timers:
  learn_throughput: 383.293
  learn_time_ms: 43047.993
  load_throughput: 4162993.245
  load_time_ms: 3.963
  training_iteration_time_ms: 55695.756
  update_time_ms: 2.614
timesteps_total: 3976500
training_iteration: 241

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.343283582089555
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 402
episodes_total: 80033
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.11815619468689
time_total_s: 12355.424170017242
timers:
  learn_throughput: 452.303
  learn_time_ms: 36479.973
  load_throughput: 4310289.985
  load_time_ms: 3.828
  training_iteration_time_ms: 47965.035
  update_time_ms: 2.336
timesteps_total: 4339500
training_iteration: 263

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9779874213836478
  reward for individual goal_min: 0.5
episode_len_mean: 54.05263157894737
episode_reward_max: 2.0
episode_reward_mean: 1.9769736842105263
episode_reward_min: 1.0
episodes_this_iter: 304
episodes_total: 38300
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9769736842105263
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 67.97548961639404
time_total_s: 12502.41000533104
timers:
  learn_throughput: 319.879
  learn_time_ms: 51581.927
  load_throughput: 3771917.788
  load_time_ms: 4.374
  training_iteration_time_ms: 67118.591
  update_time_ms: 2.887
timesteps_total: 3349500
training_iteration: 203

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32098765432098764
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.04
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 22371
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.680601358413696
time_total_s: 12518.18435049057
timers:
  learn_throughput: 442.216
  learn_time_ms: 37312.101
  load_throughput: 4745110.698
  load_time_ms: 3.477
  training_iteration_time_ms: 48902.595
  update_time_ms: 2.585
timesteps_total: 4125000
training_iteration: 250

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.13333333333333
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973544973544973
  reward for individual goal_min: 0.5
episode_len_mean: 41.89340101522843
episode_reward_max: 2.0
episode_reward_mean: 1.99746192893401
episode_reward_min: 1.0
episodes_this_iter: 394
episodes_total: 60458
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974619289340102
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 64.97136545181274
time_total_s: 12435.456380605698
timers:
  learn_throughput: 355.176
  learn_time_ms: 46455.842
  load_throughput: 3430030.778
  load_time_ms: 4.81
  training_iteration_time_ms: 59921.069
  update_time_ms: 2.622
timesteps_total: 3630000
training_iteration: 220

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-184s1jdk42/checkpoint_000220/checkpoint-220
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3037974683544304
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 167.76
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 22878
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.34121751785278
time_total_s: 12511.86905169487
timers:
  learn_throughput: 452.837
  learn_time_ms: 36436.97
  load_throughput: 4924994.022
  load_time_ms: 3.35
  training_iteration_time_ms: 47935.571
  update_time_ms: 2.673
timesteps_total: 4125000
training_iteration: 250

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1597222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9578313253012049
  reward for individual goal_min: 0.0
episode_len_mean: 190.03
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 22074
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.5906343460083
time_total_s: 12520.87609744072
timers:
  learn_throughput: 502.077
  learn_time_ms: 32863.477
  load_throughput: 5051607.761
  load_time_ms: 3.266
  training_iteration_time_ms: 43451.71
  update_time_ms: 2.432
timesteps_total: 4405500
training_iteration: 267

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9857142857142858
  reward for individual goal_min: 0.0
episode_len_mean: 176.81
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 20844
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.502206325531006
time_total_s: 12530.86960864067
timers:
  learn_throughput: 404.558
  learn_time_ms: 40785.282
  load_throughput: 4275354.354
  load_time_ms: 3.859
  training_iteration_time_ms: 53874.292
  update_time_ms: 3.117
timesteps_total: 3795000
training_iteration: 230

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2792207792207792
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8591549295774648
  reward for individual goal_min: 0.0
episode_len_mean: 198.3
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 78
episodes_total: 22948
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.43684363365173
time_total_s: 12547.256857156754
timers:
  learn_throughput: 517.013
  learn_time_ms: 31914.102
  load_throughput: 4849789.837
  load_time_ms: 3.402
  training_iteration_time_ms: 42510.709
  update_time_ms: 2.555
timesteps_total: 4570500
training_iteration: 277

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22972972972972974
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.58
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 21110
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.46209692955017
time_total_s: 12529.303076505661
timers:
  learn_throughput: 438.382
  learn_time_ms: 37638.39
  load_throughput: 4570950.305
  load_time_ms: 3.61
  training_iteration_time_ms: 49492.49
  update_time_ms: 2.653
timesteps_total: 4009500
training_iteration: 243

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.273972602739726
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9859154929577465
  reward for individual goal_min: 0.0
episode_len_mean: 173.75
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 25973
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.484113931655884
time_total_s: 12529.888817310333
timers:
  learn_throughput: 528.54
  learn_time_ms: 31218.101
  load_throughput: 5235344.277
  load_time_ms: 3.152
  training_iteration_time_ms: 41503.283
  update_time_ms: 2.503
timesteps_total: 4702500
training_iteration: 285

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2948717948717949
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9305555555555556
  reward for individual goal_min: 0.0
episode_len_mean: 188.04
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 20669
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.45948839187622
time_total_s: 12550.266031265259
timers:
  learn_throughput: 446.743
  learn_time_ms: 36933.996
  load_throughput: 4521259.571
  load_time_ms: 3.649
  training_iteration_time_ms: 48593.354
  update_time_ms: 2.741
timesteps_total: 4042500
training_iteration: 245

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95
  reward for individual goal_min: 0.5
episode_len_mean: 81.38333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.8166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9
  agent_1: 0.9166666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2564102564102564
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 184.18
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 21829
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.402719497680664
time_total_s: 12553.478392839432
timers:
  learn_throughput: 423.76
  learn_time_ms: 38937.141
  load_throughput: 4814097.122
  load_time_ms: 3.427
  training_iteration_time_ms: 50931.128
  update_time_ms: 2.761
timesteps_total: 3960000
training_iteration: 240

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973544973544973
  reward for individual goal_min: 0.5
episode_len_mean: 42.8984375
episode_reward_max: 2.0
episode_reward_mean: 1.9973958333333333
episode_reward_min: 1.0
episodes_this_iter: 384
episodes_total: 80417
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973958333333334
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 48.09354376792908
time_total_s: 12403.517713785172
timers:
  learn_throughput: 452.009
  learn_time_ms: 36503.698
  load_throughput: 4324159.83
  load_time_ms: 3.816
  training_iteration_time_ms: 47993.476
  update_time_ms: 2.341
timesteps_total: 4356000
training_iteration: 264

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2733333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.37
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 20799
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.796067237854004
time_total_s: 12552.038353204727
timers:
  learn_throughput: 409.575
  learn_time_ms: 40285.684
  load_throughput: 4430120.665
  load_time_ms: 3.725
  training_iteration_time_ms: 52900.709
  update_time_ms: 2.761
timesteps_total: 3811500
training_iteration: 231

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.994475138121547
  reward for individual goal_min: 0.5
episode_len_mean: 44.83967391304348
episode_reward_max: 2.0
episode_reward_mean: 1.9945652173913044
episode_reward_min: 1.0
episodes_this_iter: 368
episodes_total: 57023
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972826086956522
  agent_1: 0.9972826086956522
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.0263557434082
time_total_s: 12489.606261253357
timers:
  learn_throughput: 383.189
  learn_time_ms: 43059.743
  load_throughput: 4178577.354
  load_time_ms: 3.949
  training_iteration_time_ms: 55686.152
  update_time_ms: 2.609
timesteps_total: 3993000
training_iteration: 242

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9850746268656716
  reward for individual goal_min: 0.5
episode_len_mean: 181.45
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 22459
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.00609755516052
time_total_s: 12567.19044804573
timers:
  learn_throughput: 442.905
  learn_time_ms: 37254.059
  load_throughput: 4748333.836
  load_time_ms: 3.475
  training_iteration_time_ms: 48962.328
  update_time_ms: 2.627
timesteps_total: 4141500
training_iteration: 251

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16176470588235295
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9523809523809523
  reward for individual goal_min: 0.0
episode_len_mean: 187.38
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 22163
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.00685691833496
time_total_s: 12561.882954359055
timers:
  learn_throughput: 506.035
  learn_time_ms: 32606.426
  load_throughput: 5087144.023
  load_time_ms: 3.243
  training_iteration_time_ms: 43121.037
  update_time_ms: 2.414
timesteps_total: 4422000
training_iteration: 268

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3291139240506329
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.92
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 22977
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.68583059310913
time_total_s: 12560.55488228798
timers:
  learn_throughput: 450.587
  learn_time_ms: 36618.912
  load_throughput: 4868794.304
  load_time_ms: 3.389
  training_iteration_time_ms: 48147.024
  update_time_ms: 2.668
timesteps_total: 4141500
training_iteration: 251

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3860759493670886
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8671875
  reward for individual goal_min: 0.0
episode_len_mean: 190.02
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 23037
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.622785568237305
time_total_s: 12589.87964272499
timers:
  learn_throughput: 518.742
  learn_time_ms: 31807.742
  load_throughput: 4836199.581
  load_time_ms: 3.412
  training_iteration_time_ms: 42342.772
  update_time_ms: 2.521
timesteps_total: 4587000
training_iteration: 278

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21830985915492956
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 175.57
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 26069
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.36894750595093
time_total_s: 12570.257764816284
timers:
  learn_throughput: 528.682
  learn_time_ms: 31209.661
  load_throughput: 5256459.186
  load_time_ms: 3.139
  training_iteration_time_ms: 41490.747
  update_time_ms: 2.505
timesteps_total: 4719000
training_iteration: 286

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9746376811594203
  reward for individual goal_min: 0.5
episode_len_mean: 54.83112582781457
episode_reward_max: 2.0
episode_reward_mean: 1.9768211920529801
episode_reward_min: 1.0
episodes_this_iter: 302
episodes_total: 38602
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9933774834437086
  agent_1: 0.9834437086092715
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 69.23992037773132
time_total_s: 12571.64992570877
timers:
  learn_throughput: 318.329
  learn_time_ms: 51833.179
  load_throughput: 3746637.577
  load_time_ms: 4.404
  training_iteration_time_ms: 67246.892
  update_time_ms: 2.951
timesteps_total: 3366000
training_iteration: 204

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.6
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 20940
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.15027904510498
time_total_s: 12585.019887685776
timers:
  learn_throughput: 403.794
  learn_time_ms: 40862.432
  load_throughput: 4321027.966
  load_time_ms: 3.819
  training_iteration_time_ms: 53957.669
  update_time_ms: 3.111
timesteps_total: 3811500
training_iteration: 231

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.03045685279188
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 394
episodes_total: 60852
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 60.85374903678894
time_total_s: 12496.310129642487
timers:
  learn_throughput: 353.681
  learn_time_ms: 46652.155
  load_throughput: 3392701.228
  load_time_ms: 4.863
  training_iteration_time_ms: 60159.592
  update_time_ms: 2.595
timesteps_total: 3646500
training_iteration: 221

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14935064935064934
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 191.79
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 21196
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.95367074012756
time_total_s: 12578.256747245789
timers:
  learn_throughput: 437.801
  learn_time_ms: 37688.368
  load_throughput: 4526730.637
  load_time_ms: 3.645
  training_iteration_time_ms: 49536.158
  update_time_ms: 2.645
timesteps_total: 4026000
training_iteration: 244

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2702702702702703
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9733333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 178.84
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 20758
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.425015926361084
time_total_s: 12599.69104719162
timers:
  learn_throughput: 445.708
  learn_time_ms: 37019.753
  load_throughput: 4466489.141
  load_time_ms: 3.694
  training_iteration_time_ms: 48669.426
  update_time_ms: 2.609
timesteps_total: 4059000
training_iteration: 246

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939024390243902
  reward for individual goal_min: 0.5
episode_len_mean: 175.36
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 1.0
episodes_this_iter: 94
episodes_total: 21923
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.809576749801636
time_total_s: 12600.287969589233
timers:
  learn_throughput: 427.386
  learn_time_ms: 38606.742
  load_throughput: 4857926.155
  load_time_ms: 3.397
  training_iteration_time_ms: 50406.285
  update_time_ms: 2.763
timesteps_total: 3976500
training_iteration: 241

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.84935064935065
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 385
episodes_total: 80802
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.966965436935425
time_total_s: 12452.484679222107
timers:
  learn_throughput: 450.981
  learn_time_ms: 36586.927
  load_throughput: 4354167.935
  load_time_ms: 3.789
  training_iteration_time_ms: 48091.28
  update_time_ms: 2.359
timesteps_total: 4372500
training_iteration: 265

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.48
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 20893
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.93600296974182
time_total_s: 12603.974356174469
timers:
  learn_throughput: 410.001
  learn_time_ms: 40243.794
  load_throughput: 4368266.921
  load_time_ms: 3.777
  training_iteration_time_ms: 52816.194
  update_time_ms: 2.749
timesteps_total: 3828000
training_iteration: 232

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2468354430379747
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9459459459459459
  reward for individual goal_min: 0.0
episode_len_mean: 199.39
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 22245
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.7849497795105
time_total_s: 12604.667904138565
timers:
  learn_throughput: 509.839
  learn_time_ms: 32363.189
  load_throughput: 5128650.956
  load_time_ms: 3.217
  training_iteration_time_ms: 42807.762
  update_time_ms: 2.407
timesteps_total: 4438500
training_iteration: 269

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2638888888888889
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.5
episode_len_mean: 177.14
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 22555
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.46218276023865
time_total_s: 12618.65263080597
timers:
  learn_throughput: 440.065
  learn_time_ms: 37494.481
  load_throughput: 4725540.693
  load_time_ms: 3.492
  training_iteration_time_ms: 49177.595
  update_time_ms: 2.633
timesteps_total: 4158000
training_iteration: 252

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2468354430379747
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9054054054054054
  reward for individual goal_min: 0.0
episode_len_mean: 205.42
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 23118
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.45
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.51763200759888
time_total_s: 12630.39727473259
timers:
  learn_throughput: 523.922
  learn_time_ms: 31493.226
  load_throughput: 4853020.673
  load_time_ms: 3.4
  training_iteration_time_ms: 42036.931
  update_time_ms: 2.509
timesteps_total: 4603500
training_iteration: 279

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9809782608695652
  reward for individual goal_min: 0.5
episode_len_mean: 47.63976945244957
episode_reward_max: 2.0
episode_reward_mean: 1.9798270893371759
episode_reward_min: 1.0
episodes_this_iter: 347
episodes_total: 57370
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9884726224783862
  agent_1: 0.9913544668587896
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.207932472229004
time_total_s: 12543.814193725586
timers:
  learn_throughput: 384.818
  learn_time_ms: 42877.424
  load_throughput: 4175073.359
  load_time_ms: 3.952
  training_iteration_time_ms: 55447.692
  update_time_ms: 2.628
timesteps_total: 4009500
training_iteration: 243

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30128205128205127
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 166.14
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 23076
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.76
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.12971067428589
time_total_s: 12607.684592962265
timers:
  learn_throughput: 451.859
  learn_time_ms: 36515.806
  load_throughput: 4840258.498
  load_time_ms: 3.409
  training_iteration_time_ms: 48025.428
  update_time_ms: 2.657
timesteps_total: 4158000
training_iteration: 252

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2714285714285714
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.0
episode_len_mean: 166.26
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 26169
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.76
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.46414566040039
time_total_s: 12613.721910476685
timers:
  learn_throughput: 527.868
  learn_time_ms: 31257.803
  load_throughput: 5265858.291
  load_time_ms: 3.133
  training_iteration_time_ms: 41577.306
  update_time_ms: 2.484
timesteps_total: 4735500
training_iteration: 287

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28289473684210525
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9929577464788732
  reward for individual goal_min: 0.5
episode_len_mean: 177.5
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 21290
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.413285970687866
time_total_s: 12628.670033216476
timers:
  learn_throughput: 438.033
  learn_time_ms: 37668.416
  load_throughput: 4529189.529
  load_time_ms: 3.643
  training_iteration_time_ms: 49568.312
  update_time_ms: 2.62
timesteps_total: 4042500
training_iteration: 245

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3142857142857143
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.935064935064935
  reward for individual goal_min: 0.0
episode_len_mean: 169.86
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 20851
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.39659309387207
time_total_s: 12647.087640285492
timers:
  learn_throughput: 448.155
  learn_time_ms: 36817.633
  load_throughput: 4442234.532
  load_time_ms: 3.714
  training_iteration_time_ms: 48410.99
  update_time_ms: 2.621
timesteps_total: 4075500
training_iteration: 247

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18571428571428572
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9927536231884058
  reward for individual goal_min: 0.5
episode_len_mean: 172.31
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 21037
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.396503925323486
time_total_s: 12641.4163916111
timers:
  learn_throughput: 400.382
  learn_time_ms: 41210.68
  load_throughput: 4286476.228
  load_time_ms: 3.849
  training_iteration_time_ms: 54442.882
  update_time_ms: 3.112
timesteps_total: 3828000
training_iteration: 232

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3472222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9733333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 160.41584158415841
episode_reward_max: 2.0
episode_reward_mean: 1.4158415841584158
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 22024
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6633663366336634
  agent_1: 0.7524752475247525
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.44985842704773
time_total_s: 12648.737828016281
timers:
  learn_throughput: 428.53
  learn_time_ms: 38503.698
  load_throughput: 4867972.37
  load_time_ms: 3.39
  training_iteration_time_ms: 50308.948
  update_time_ms: 2.752
timesteps_total: 3993000
training_iteration: 242

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.10687022900763
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 393
episodes_total: 61245
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.65314173698425
time_total_s: 12555.96327137947
timers:
  learn_throughput: 353.522
  learn_time_ms: 46673.147
  load_throughput: 3404467.532
  load_time_ms: 4.847
  training_iteration_time_ms: 60194.143
  update_time_ms: 2.577
timesteps_total: 3663000
training_iteration: 222

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9709677419354839
  reward for individual goal_min: 0.5
episode_len_mean: 53.963815789473685
episode_reward_max: 2.0
episode_reward_mean: 1.9703947368421053
episode_reward_min: 1.0
episodes_this_iter: 304
episodes_total: 38906
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9703947368421053
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 66.17687368392944
time_total_s: 12637.8267993927
timers:
  learn_throughput: 319.427
  learn_time_ms: 51655.071
  load_throughput: 3612136.978
  load_time_ms: 4.568
  training_iteration_time_ms: 67070.432
  update_time_ms: 2.98
timesteps_total: 3382500
training_iteration: 205

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19135802469135801
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9210526315789473
  reward for individual goal_min: 0.0
episode_len_mean: 204.78
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 22326
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.110889196395874
time_total_s: 12646.778793334961
timers:
  learn_throughput: 510.574
  learn_time_ms: 32316.577
  load_throughput: 5195568.835
  load_time_ms: 3.176
  training_iteration_time_ms: 42781.412
  update_time_ms: 2.418
timesteps_total: 4455000
training_iteration: 270

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9921465968586387
  reward for individual goal_min: 0.5
episode_len_mean: 42.65721649484536
episode_reward_max: 2.0
episode_reward_mean: 1.9922680412371134
episode_reward_min: 1.0
episodes_this_iter: 388
episodes_total: 81190
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9948453608247423
  agent_1: 0.9974226804123711
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.98136329650879
time_total_s: 12500.466042518616
timers:
  learn_throughput: 451.563
  learn_time_ms: 36539.78
  load_throughput: 4366007.154
  load_time_ms: 3.779
  training_iteration_time_ms: 48064.522
  update_time_ms: 2.339
timesteps_total: 4389000
training_iteration: 266

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3108108108108108
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.974025974025974
  reward for individual goal_min: 0.0
episode_len_mean: 158.25
episode_reward_max: 2.0
episode_reward_mean: 1.3846153846153846
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 20997
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.625
  agent_1: 0.7596153846153846
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.51588034629822
time_total_s: 12657.490236520767
timers:
  learn_throughput: 407.228
  learn_time_ms: 40517.825
  load_throughput: 4366365.254
  load_time_ms: 3.779
  training_iteration_time_ms: 53132.776
  update_time_ms: 2.751
timesteps_total: 3844500
training_iteration: 233

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1794871794871795
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939024390243902
  reward for individual goal_min: 0.5
episode_len_mean: 189.29
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 26253
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 39.401525020599365
time_total_s: 12653.123435497284
timers:
  learn_throughput: 529.655
  learn_time_ms: 31152.355
  load_throughput: 5276175.867
  load_time_ms: 3.127
  training_iteration_time_ms: 41409.549
  update_time_ms: 2.514
timesteps_total: 4752000
training_iteration: 288

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30405405405405406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 163.87
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 22655
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.634846210479736
time_total_s: 12668.287477016449
timers:
  learn_throughput: 437.976
  learn_time_ms: 37673.298
  load_throughput: 4710744.328
  load_time_ms: 3.503
  training_iteration_time_ms: 49449.481
  update_time_ms: 2.661
timesteps_total: 4174500
training_iteration: 253

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9
  reward for individual goal_min: 0.5
episode_len_mean: 96.11666666666666
episode_reward_max: 2.0
episode_reward_mean: 1.8666666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.95
  agent_1: 0.9166666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23780487804878048
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9025974025974026
  reward for individual goal_min: 0.0
episode_len_mean: 204.13
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 23197
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.92975974082947
time_total_s: 12680.32703447342
timers:
  learn_throughput: 527.669
  learn_time_ms: 31269.599
  load_throughput: 4896455.755
  load_time_ms: 3.37
  training_iteration_time_ms: 41765.397
  update_time_ms: 2.515
timesteps_total: 4620000
training_iteration: 280

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3466666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 159.28155339805826
episode_reward_max: 2.0
episode_reward_mean: 1.4368932038834952
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 23179
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7475728155339806
  agent_1: 0.6893203883495146
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.427242279052734
time_total_s: 12654.111835241318
timers:
  learn_throughput: 452.735
  learn_time_ms: 36445.161
  load_throughput: 4844968.601
  load_time_ms: 3.406
  training_iteration_time_ms: 47902.373
  update_time_ms: 2.656
timesteps_total: 4174500
training_iteration: 253

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.984375
  reward for individual goal_min: 0.5
episode_len_mean: 48.02332361516035
episode_reward_max: 2.0
episode_reward_mean: 1.9854227405247813
episode_reward_min: 1.0
episodes_this_iter: 343
episodes_total: 57713
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9883381924198251
  agent_1: 0.9970845481049563
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.248414516448975
time_total_s: 12598.062608242035
timers:
  learn_throughput: 385.494
  learn_time_ms: 42802.204
  load_throughput: 4212532.778
  load_time_ms: 3.917
  training_iteration_time_ms: 55299.646
  update_time_ms: 2.627
timesteps_total: 4026000
training_iteration: 244

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17857142857142858
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9802631578947368
  reward for individual goal_min: 0.0
episode_len_mean: 174.61
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 21384
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.620012283325195
time_total_s: 12679.290045499802
timers:
  learn_throughput: 437.925
  learn_time_ms: 37677.693
  load_throughput: 4533105.562
  load_time_ms: 3.64
  training_iteration_time_ms: 49593.034
  update_time_ms: 2.623
timesteps_total: 4059000
training_iteration: 246

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2152777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.974025974025974
  reward for individual goal_min: 0.0
episode_len_mean: 179.96
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 20945
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.77511739730835
time_total_s: 12694.8627576828
timers:
  learn_throughput: 447.087
  learn_time_ms: 36905.592
  load_throughput: 4478455.197
  load_time_ms: 3.684
  training_iteration_time_ms: 48551.649
  update_time_ms: 2.597
timesteps_total: 4092000
training_iteration: 248

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2708333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 167.2233009708738
episode_reward_max: 2.0
episode_reward_mean: 1.3689320388349515
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 22127
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5922330097087378
  agent_1: 0.7766990291262136
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.233556270599365
time_total_s: 12698.97138428688
timers:
  learn_throughput: 427.397
  learn_time_ms: 38605.776
  load_throughput: 4853531.198
  load_time_ms: 3.4
  training_iteration_time_ms: 50395.885
  update_time_ms: 2.767
timesteps_total: 4009500
training_iteration: 243

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2638888888888889
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 161.62745098039215
episode_reward_max: 2.0
episode_reward_mean: 1.3137254901960784
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 21139
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6176470588235294
  agent_1: 0.696078431372549
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.728442430496216
time_total_s: 12696.144834041595
timers:
  learn_throughput: 399.609
  learn_time_ms: 41290.334
  load_throughput: 4267471.743
  load_time_ms: 3.866
  training_iteration_time_ms: 54542.495
  update_time_ms: 3.114
timesteps_total: 3844500
training_iteration: 233

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22941176470588234
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9621212121212122
  reward for individual goal_min: 0.0
episode_len_mean: 199.49
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 22409
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.42256951332092
time_total_s: 12691.201362848282
timers:
  learn_throughput: 509.838
  learn_time_ms: 32363.213
  load_throughput: 5200449.063
  load_time_ms: 3.173
  training_iteration_time_ms: 42856.38
  update_time_ms: 2.426
timesteps_total: 4471500
training_iteration: 271

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.310126582278481
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.38
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 26345
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.812206745147705
time_total_s: 12693.935642242432
timers:
  learn_throughput: 533.572
  learn_time_ms: 30923.675
  load_throughput: 5256339.415
  load_time_ms: 3.139
  training_iteration_time_ms: 41130.75
  update_time_ms: 2.499
timesteps_total: 4768500
training_iteration: 289

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9953051643192489
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971428571428571
  reward for individual goal_min: 0.5
episode_len_mean: 42.44072164948454
episode_reward_max: 2.0
episode_reward_mean: 1.9922680412371134
episode_reward_min: 0.0
episodes_this_iter: 388
episodes_total: 81578
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974226804123711
  agent_1: 0.9948453608247423
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.84417796134949
time_total_s: 12549.310220479965
timers:
  learn_throughput: 450.61
  learn_time_ms: 36617.031
  load_throughput: 4340214.107
  load_time_ms: 3.802
  training_iteration_time_ms: 48163.147
  update_time_ms: 2.338
timesteps_total: 4405500
training_iteration: 267

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973821989528796
  reward for individual goal_min: 0.5
episode_len_mean: 42.00769230769231
episode_reward_max: 2.0
episode_reward_mean: 1.9974358974358974
episode_reward_min: 1.0
episodes_this_iter: 390
episodes_total: 61635
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9974358974358974
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 60.31926107406616
time_total_s: 12616.282532453537
timers:
  learn_throughput: 353.511
  learn_time_ms: 46674.648
  load_throughput: 3371265.674
  load_time_ms: 4.894
  training_iteration_time_ms: 60196.063
  update_time_ms: 2.574
timesteps_total: 3679500
training_iteration: 223

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27702702702702703
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8461538461538461
  reward for individual goal_min: 0.0
episode_len_mean: 195.56
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 23285
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.11761736869812
time_total_s: 12722.444651842117
timers:
  learn_throughput: 528.441
  learn_time_ms: 31223.895
  load_throughput: 4924328.194
  load_time_ms: 3.351
  training_iteration_time_ms: 41716.003
  update_time_ms: 2.503
timesteps_total: 4636500
training_iteration: 281

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20481927710843373
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 192.2
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 22738
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.411728858947754
time_total_s: 12714.699205875397
timers:
  learn_throughput: 439.796
  learn_time_ms: 37517.409
  load_throughput: 4665616.05
  load_time_ms: 3.537
  training_iteration_time_ms: 49259.774
  update_time_ms: 2.65
timesteps_total: 4191000
training_iteration: 254

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9727891156462585
  reward for individual goal_min: 0.5
episode_len_mean: 57.15625
episode_reward_max: 2.0
episode_reward_mean: 1.9722222222222223
episode_reward_min: 1.0
episodes_this_iter: 288
episodes_total: 39194
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9722222222222222
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 66.70806884765625
time_total_s: 12704.534868240356
timers:
  learn_throughput: 319.276
  learn_time_ms: 51679.412
  load_throughput: 3550610.58
  load_time_ms: 4.647
  training_iteration_time_ms: 67081.158
  update_time_ms: 2.987
timesteps_total: 3399000
training_iteration: 206

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9859154929577465
  reward for individual goal_min: 0.0
episode_len_mean: 159.38834951456312
episode_reward_max: 2.0
episode_reward_mean: 1.3592233009708738
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 23282
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6407766990291263
  agent_1: 0.7184466019417476
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.368265867233276
time_total_s: 12705.480101108551
timers:
  learn_throughput: 449.707
  learn_time_ms: 36690.551
  load_throughput: 4869034.087
  load_time_ms: 3.389
  training_iteration_time_ms: 48235.116
  update_time_ms: 2.626
timesteps_total: 4191000
training_iteration: 254

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9855072463768116
  reward for individual goal_min: 0.0
episode_len_mean: 177.09
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 21091
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.14731550216675
time_total_s: 12714.637552022934
timers:
  learn_throughput: 404.169
  learn_time_ms: 40824.472
  load_throughput: 4336569.771
  load_time_ms: 3.805
  training_iteration_time_ms: 53572.25
  update_time_ms: 2.802
timesteps_total: 3861000
training_iteration: 234

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971264367816092
  reward for individual goal_min: 0.5
episode_len_mean: 45.98882681564246
episode_reward_max: 2.0
episode_reward_mean: 1.9972067039106145
episode_reward_min: 1.0
episodes_this_iter: 358
episodes_total: 58071
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972067039106145
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 54.838889360427856
time_total_s: 12652.901497602463
timers:
  learn_throughput: 386.204
  learn_time_ms: 42723.564
  load_throughput: 4258386.261
  load_time_ms: 3.875
  training_iteration_time_ms: 55218.251
  update_time_ms: 2.622
timesteps_total: 4042500
training_iteration: 245

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9375
  reward for individual goal_min: 0.0
episode_len_mean: 194.15
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 22491
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.81931757926941
time_total_s: 12732.020680427551
timers:
  learn_throughput: 514.755
  learn_time_ms: 32054.06
  load_throughput: 5238474.919
  load_time_ms: 3.15
  training_iteration_time_ms: 42435.1
  update_time_ms: 2.415
timesteps_total: 4488000
training_iteration: 272

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 174.99
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 21479
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.594970703125
time_total_s: 12728.885016202927
timers:
  learn_throughput: 440.039
  learn_time_ms: 37496.685
  load_throughput: 4544357.213
  load_time_ms: 3.631
  training_iteration_time_ms: 49441.402
  update_time_ms: 2.614
timesteps_total: 4075500
training_iteration: 247

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32098765432098764
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9552238805970149
  reward for individual goal_min: 0.0
episode_len_mean: 183.0
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 21033
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.588592529296875
time_total_s: 12745.451350212097
timers:
  learn_throughput: 444.051
  learn_time_ms: 37157.888
  load_throughput: 4514152.202
  load_time_ms: 3.655
  training_iteration_time_ms: 48880.172
  update_time_ms: 2.592
timesteps_total: 4108500
training_iteration: 249

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20833333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9539473684210527
  reward for individual goal_min: 0.0
episode_len_mean: 181.69
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 22215
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.908589124679565
time_total_s: 12748.87997341156
timers:
  learn_throughput: 431.699
  learn_time_ms: 38221.08
  load_throughput: 4846563.301
  load_time_ms: 3.404
  training_iteration_time_ms: 50065.632
  update_time_ms: 2.75
timesteps_total: 4026000
training_iteration: 244

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23648648648648649
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.51
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 26439
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.85715651512146
time_total_s: 12734.792798757553
timers:
  learn_throughput: 533.35
  learn_time_ms: 30936.529
  load_throughput: 5275049.811
  load_time_ms: 3.128
  training_iteration_time_ms: 41131.515
  update_time_ms: 2.53
timesteps_total: 4785000
training_iteration: 290

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2898550724637681
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9875
  reward for individual goal_min: 0.5
episode_len_mean: 153.23584905660377
episode_reward_max: 2.0
episode_reward_mean: 1.4056603773584906
episode_reward_min: 0.0
episodes_this_iter: 106
episodes_total: 21245
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.660377358490566
  agent_1: 0.7452830188679245
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.15616512298584
time_total_s: 12750.300999164581
timers:
  learn_throughput: 399.183
  learn_time_ms: 41334.399
  load_throughput: 4254982.631
  load_time_ms: 3.878
  training_iteration_time_ms: 54599.547
  update_time_ms: 3.133
timesteps_total: 3861000
training_iteration: 234

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2987012987012987
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8910256410256411
  reward for individual goal_min: 0.0
episode_len_mean: 193.32
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 23370
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.89765691757202
time_total_s: 12764.34230875969
timers:
  learn_throughput: 533.132
  learn_time_ms: 30949.186
  load_throughput: 4923907.763
  load_time_ms: 3.351
  training_iteration_time_ms: 41409.668
  update_time_ms: 2.487
timesteps_total: 4653000
training_iteration: 282

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.82077922077922
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 385
episodes_total: 81963
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.943954944610596
time_total_s: 12597.254175424576
timers:
  learn_throughput: 450.288
  learn_time_ms: 36643.218
  load_throughput: 4309565.282
  load_time_ms: 3.829
  training_iteration_time_ms: 48202.171
  update_time_ms: 2.35
timesteps_total: 4422000
training_iteration: 268

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22077922077922077
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.7
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 22831
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.7855167388916
time_total_s: 12763.484722614288
timers:
  learn_throughput: 439.875
  learn_time_ms: 37510.684
  load_throughput: 4714049.371
  load_time_ms: 3.5
  training_iteration_time_ms: 49300.281
  update_time_ms: 2.666
timesteps_total: 4207500
training_iteration: 255

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2647058823529412
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 161.01980198019803
episode_reward_max: 2.0
episode_reward_mean: 1.4158415841584158
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 23383
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7029702970297029
  agent_1: 0.7128712871287128
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.952980756759644
time_total_s: 12753.43308186531
timers:
  learn_throughput: 448.527
  learn_time_ms: 36787.109
  load_throughput: 4819293.325
  load_time_ms: 3.424
  training_iteration_time_ms: 48371.015
  update_time_ms: 2.635
timesteps_total: 4207500
training_iteration: 255

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971590909090909
  reward for individual goal_min: 0.5
episode_len_mean: 44.71739130434783
episode_reward_max: 2.0
episode_reward_mean: 1.997282608695652
episode_reward_min: 1.0
episodes_this_iter: 368
episodes_total: 62003
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972826086956522
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 60.14697599411011
time_total_s: 12676.429508447647
timers:
  learn_throughput: 353.899
  learn_time_ms: 46623.418
  load_throughput: 3353183.358
  load_time_ms: 4.921
  training_iteration_time_ms: 60132.177
  update_time_ms: 2.594
timesteps_total: 3696000
training_iteration: 224

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2916666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9852941176470589
  reward for individual goal_min: 0.0
episode_len_mean: 183.39
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 21180
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.78234052658081
time_total_s: 12769.419892549515
timers:
  learn_throughput: 401.954
  learn_time_ms: 41049.522
  load_throughput: 4321513.647
  load_time_ms: 3.818
  training_iteration_time_ms: 53883.672
  update_time_ms: 2.814
timesteps_total: 3877500
training_iteration: 235

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2236842105263158
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9214285714285714
  reward for individual goal_min: 0.0
episode_len_mean: 185.3
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 22581
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.04456615447998
time_total_s: 12775.065246582031
timers:
  learn_throughput: 513.937
  learn_time_ms: 32105.082
  load_throughput: 5267702.051
  load_time_ms: 3.132
  training_iteration_time_ms: 42542.127
  update_time_ms: 2.41
timesteps_total: 4504500
training_iteration: 273

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3181818181818182
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9539473684210527
  reward for individual goal_min: 0.0
episode_len_mean: 177.36
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 21126
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.424426794052124
time_total_s: 12789.87577700615
timers:
  learn_throughput: 447.783
  learn_time_ms: 36848.203
  load_throughput: 4512915.86
  load_time_ms: 3.656
  training_iteration_time_ms: 48552.191
  update_time_ms: 2.624
timesteps_total: 4125000
training_iteration: 250

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3092105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 168.04
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 26539
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.78
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 39.737144470214844
time_total_s: 12774.529943227768
timers:
  learn_throughput: 535.746
  learn_time_ms: 30798.189
  load_throughput: 5293084.101
  load_time_ms: 3.117
  training_iteration_time_ms: 40969.95
  update_time_ms: 2.528
timesteps_total: 4801500
training_iteration: 291

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971590909090909
  reward for individual goal_min: 0.5
episode_len_mean: 46.49008498583569
episode_reward_max: 2.0
episode_reward_mean: 1.9971671388101984
episode_reward_min: 1.0
episodes_this_iter: 353
episodes_total: 58424
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9971671388101983
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 56.44736957550049
time_total_s: 12709.348867177963
timers:
  learn_throughput: 386.311
  learn_time_ms: 42711.756
  load_throughput: 4207589.783
  load_time_ms: 3.921
  training_iteration_time_ms: 55229.961
  update_time_ms: 2.596
timesteps_total: 4059000
training_iteration: 246

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9872611464968153
  reward for individual goal_min: 0.5
episode_len_mean: 50.948012232415905
episode_reward_max: 2.0
episode_reward_mean: 1.9877675840978593
episode_reward_min: 1.0
episodes_this_iter: 327
episodes_total: 39521
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9877675840978594
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 69.0842924118042
time_total_s: 12773.61916065216
timers:
  learn_throughput: 318.419
  learn_time_ms: 51818.504
  load_throughput: 3552487.86
  load_time_ms: 4.645
  training_iteration_time_ms: 67262.902
  update_time_ms: 2.986
timesteps_total: 3415500
training_iteration: 207

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21830985915492956
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 173.81
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 21576
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.43144249916077
time_total_s: 12778.316458702087
timers:
  learn_throughput: 441.334
  learn_time_ms: 37386.651
  load_throughput: 4549046.952
  load_time_ms: 3.627
  training_iteration_time_ms: 49310.915
  update_time_ms: 2.624
timesteps_total: 4092000
training_iteration: 248

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24390243902439024
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9041095890410958
  reward for individual goal_min: 0.0
episode_len_mean: 212.1
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 79
episodes_total: 23449
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 39.569077253341675
time_total_s: 12803.911386013031
timers:
  learn_throughput: 533.673
  learn_time_ms: 30917.803
  load_throughput: 4889640.515
  load_time_ms: 3.374
  training_iteration_time_ms: 41374.389
  update_time_ms: 2.47
timesteps_total: 4669500
training_iteration: 283

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.12
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 22308
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.01447796821594
time_total_s: 12797.894451379776
timers:
  learn_throughput: 431.932
  learn_time_ms: 38200.478
  load_throughput: 4880227.348
  load_time_ms: 3.381
  training_iteration_time_ms: 50003.853
  update_time_ms: 2.749
timesteps_total: 4042500
training_iteration: 245

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.66
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 21339
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.235597372055054
time_total_s: 12802.536596536636
timers:
  learn_throughput: 400.109
  learn_time_ms: 41238.761
  load_throughput: 4259539.493
  load_time_ms: 3.874
  training_iteration_time_ms: 54425.708
  update_time_ms: 3.143
timesteps_total: 3877500
training_iteration: 235

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.566750629722925
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 397
episodes_total: 82360
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.97136831283569
time_total_s: 12645.225543737411
timers:
  learn_throughput: 450.867
  learn_time_ms: 36596.156
  load_throughput: 4309082.283
  load_time_ms: 3.829
  training_iteration_time_ms: 48153.176
  update_time_ms: 2.384
timesteps_total: 4438500
training_iteration: 269

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.32
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 22926
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.513463497161865
time_total_s: 12813.99818611145
timers:
  learn_throughput: 439.405
  learn_time_ms: 37550.796
  load_throughput: 4585244.746
  load_time_ms: 3.598
  training_iteration_time_ms: 49375.283
  update_time_ms: 2.688
timesteps_total: 4224000
training_iteration: 256

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3092105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.49
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 23477
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.489506244659424
time_total_s: 12801.92258810997
timers:
  learn_throughput: 447.509
  learn_time_ms: 36870.781
  load_throughput: 4771479.513
  load_time_ms: 3.458
  training_iteration_time_ms: 48520.813
  update_time_ms: 2.637
timesteps_total: 4224000
training_iteration: 256

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16447368421052633
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9333333333333333
  reward for individual goal_min: 0.0
episode_len_mean: 199.76
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 22661
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.020299196243286
time_total_s: 12817.085545778275
timers:
  learn_throughput: 516.116
  learn_time_ms: 31969.563
  load_throughput: 5259255.409
  load_time_ms: 3.137
  training_iteration_time_ms: 42396.303
  update_time_ms: 2.425
timesteps_total: 4521000
training_iteration: 274

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2785714285714286
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.62
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 26633
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.27845215797424
time_total_s: 12814.808395385742
timers:
  learn_throughput: 538.364
  learn_time_ms: 30648.39
  load_throughput: 5314872.362
  load_time_ms: 3.104
  training_iteration_time_ms: 40773.126
  update_time_ms: 2.543
timesteps_total: 4818000
training_iteration: 292

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26704545454545453
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 191.89
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 21268
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.134321451187134
time_total_s: 12821.554214000702
timers:
  learn_throughput: 402.279
  learn_time_ms: 41016.287
  load_throughput: 4324321.947
  load_time_ms: 3.816
  training_iteration_time_ms: 53730.362
  update_time_ms: 2.824
timesteps_total: 3894000
training_iteration: 236

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9923469387755102
  reward for individual goal_min: 0.5
episode_len_mean: 43.347258485639685
episode_reward_max: 2.0
episode_reward_mean: 1.9921671018276763
episode_reward_min: 1.0
episodes_this_iter: 383
episodes_total: 62386
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9947780678851175
  agent_1: 0.9973890339425587
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.95884609222412
time_total_s: 12735.388354539871
timers:
  learn_throughput: 354.078
  learn_time_ms: 46599.947
  load_throughput: 3351624.379
  load_time_ms: 4.923
  training_iteration_time_ms: 60070.795
  update_time_ms: 2.609
timesteps_total: 3712500
training_iteration: 225

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1987179487179487
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8958333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 198.2
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 23532
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.90174603462219
time_total_s: 12847.813132047653
timers:
  learn_throughput: 528.969
  learn_time_ms: 31192.737
  load_throughput: 4905514.396
  load_time_ms: 3.364
  training_iteration_time_ms: 41797.159
  update_time_ms: 2.471
timesteps_total: 4686000
training_iteration: 284

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2028985507246377
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9166666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 176.95
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 21221
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.11261820793152
time_total_s: 12840.98839521408
timers:
  learn_throughput: 447.477
  learn_time_ms: 36873.383
  load_throughput: 4527263.664
  load_time_ms: 3.645
  training_iteration_time_ms: 48547.884
  update_time_ms: 2.635
timesteps_total: 4141500
training_iteration: 251

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2391304347826087
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.5
episode_len_mean: 171.71
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 21670
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.88774633407593
time_total_s: 12828.204205036163
timers:
  learn_throughput: 438.916
  learn_time_ms: 37592.607
  load_throughput: 4630837.627
  load_time_ms: 3.563
  training_iteration_time_ms: 49574.851
  update_time_ms: 2.621
timesteps_total: 4108500
training_iteration: 249

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2905405405405405
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 168.05
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 22406
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.53557109832764
time_total_s: 12848.430022478104
timers:
  learn_throughput: 432.074
  learn_time_ms: 38187.906
  load_throughput: 4857994.356
  load_time_ms: 3.396
  training_iteration_time_ms: 49982.482
  update_time_ms: 2.739
timesteps_total: 4059000
training_iteration: 246

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9823529411764705
  reward for individual goal_min: 0.5
episode_len_mean: 47.87031700288185
episode_reward_max: 2.0
episode_reward_mean: 1.9827089337175792
episode_reward_min: 1.0
episodes_this_iter: 347
episodes_total: 58771
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9827089337175793
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 55.87009263038635
time_total_s: 12765.21895980835
timers:
  learn_throughput: 386.201
  learn_time_ms: 42723.849
  load_throughput: 4208536.505
  load_time_ms: 3.921
  training_iteration_time_ms: 55283.734
  update_time_ms: 2.583
timesteps_total: 4075500
training_iteration: 247

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9953051643192489
  reward for individual goal_min: 0.5
episode_len_mean: 42.148337595907925
episode_reward_max: 2.0
episode_reward_mean: 1.9948849104859334
episode_reward_min: 1.0
episodes_this_iter: 391
episodes_total: 82751
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974424552429667
  agent_1: 0.9974424552429667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.80673885345459
time_total_s: 12693.032282590866
timers:
  learn_throughput: 451.351
  learn_time_ms: 36556.92
  load_throughput: 4264290.045
  load_time_ms: 3.869
  training_iteration_time_ms: 48099.292
  update_time_ms: 2.41
timesteps_total: 4455000
training_iteration: 270

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9818181818181818
  reward for individual goal_min: 0.5
episode_len_mean: 50.80555555555556
episode_reward_max: 2.0
episode_reward_mean: 1.9814814814814814
episode_reward_min: 1.0
episodes_this_iter: 324
episodes_total: 39845
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9814814814814815
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 69.61347699165344
time_total_s: 12843.232637643814
timers:
  learn_throughput: 316.114
  learn_time_ms: 52196.307
  load_throughput: 3430540.858
  load_time_ms: 4.81
  training_iteration_time_ms: 67673.122
  update_time_ms: 3.076
timesteps_total: 3432000
training_iteration: 208

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15492957746478872
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9802631578947368
  reward for individual goal_min: 0.5
episode_len_mean: 187.29
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 23014
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.1485641002655
time_total_s: 12862.146750211716
timers:
  learn_throughput: 439.876
  learn_time_ms: 37510.607
  load_throughput: 4520462.197
  load_time_ms: 3.65
  training_iteration_time_ms: 49327.346
  update_time_ms: 2.687
timesteps_total: 4240500
training_iteration: 257

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21518987341772153
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.06
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 21430
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.08843374252319
time_total_s: 12858.62503027916
timers:
  learn_throughput: 398.462
  learn_time_ms: 41409.225
  load_throughput: 4252420.412
  load_time_ms: 3.88
  training_iteration_time_ms: 54741.102
  update_time_ms: 3.16
timesteps_total: 3894000
training_iteration: 236

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95
  reward for individual goal_min: 0.5
episode_len_mean: 174.3
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 22755
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.46711850166321
time_total_s: 12858.552664279938
timers:
  learn_throughput: 518.872
  learn_time_ms: 31799.756
  load_throughput: 5297054.42
  load_time_ms: 3.115
  training_iteration_time_ms: 42135.74
  update_time_ms: 2.439
timesteps_total: 4537500
training_iteration: 275

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.65
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 23574
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.81509065628052
time_total_s: 12850.73767876625
timers:
  learn_throughput: 446.739
  learn_time_ms: 36934.293
  load_throughput: 4706355.476
  load_time_ms: 3.506
  training_iteration_time_ms: 48551.808
  update_time_ms: 2.628
timesteps_total: 4240500
training_iteration: 257

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9928571428571429
  reward for individual goal_min: 0.5
episode_len_mean: 180.22
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 26727
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.822452545166016
time_total_s: 12857.630847930908
timers:
  learn_throughput: 535.848
  learn_time_ms: 30792.327
  load_throughput: 5328540.322
  load_time_ms: 3.097
  training_iteration_time_ms: 41017.03
  update_time_ms: 2.535
timesteps_total: 4834500
training_iteration: 293

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18674698795180722
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8618421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 216.98
episode_reward_max: 2.0
episode_reward_mean: 1.04
episode_reward_min: 0.0
episodes_this_iter: 74
episodes_total: 23606
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 39.97805976867676
time_total_s: 12887.79119181633
timers:
  learn_throughput: 532.895
  learn_time_ms: 30962.97
  load_throughput: 4879607.973
  load_time_ms: 3.381
  training_iteration_time_ms: 41495.566
  update_time_ms: 2.495
timesteps_total: 4702500
training_iteration: 285

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2361111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9938271604938271
  reward for individual goal_min: 0.5
episode_len_mean: 172.84
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 21361
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.12680792808533
time_total_s: 12873.681021928787
timers:
  learn_throughput: 403.399
  learn_time_ms: 40902.468
  load_throughput: 4333528.45
  load_time_ms: 3.808
  training_iteration_time_ms: 53616.137
  update_time_ms: 2.822
timesteps_total: 3910500
training_iteration: 237

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3051948051948052
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9726027397260274
  reward for individual goal_min: 0.0
episode_len_mean: 178.27
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 21309
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.89739489555359
time_total_s: 12888.885790109634
timers:
  learn_throughput: 447.318
  learn_time_ms: 36886.526
  load_throughput: 4662347.14
  load_time_ms: 3.539
  training_iteration_time_ms: 48542.979
  update_time_ms: 2.608
timesteps_total: 4158000
training_iteration: 252

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1987179487179487
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9873417721518988
  reward for individual goal_min: 0.0
episode_len_mean: 184.11
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 21759
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.19049835205078
time_total_s: 12877.394703388214
timers:
  learn_throughput: 441.769
  learn_time_ms: 37349.796
  load_throughput: 4648442.773
  load_time_ms: 3.55
  training_iteration_time_ms: 49343.436
  update_time_ms: 2.637
timesteps_total: 4125000
training_iteration: 250

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.024324324324326
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 370
episodes_total: 62756
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.30098605155945
time_total_s: 12793.68934059143
timers:
  learn_throughput: 355.516
  learn_time_ms: 46411.428
  load_throughput: 3331376.528
  load_time_ms: 4.953
  training_iteration_time_ms: 59874.182
  update_time_ms: 2.635
timesteps_total: 3729000
training_iteration: 226

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.176056338028169
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.975609756097561
  reward for individual goal_min: 0.0
episode_len_mean: 180.51
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 22496
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.88097667694092
time_total_s: 12898.310999155045
timers:
  learn_throughput: 434.073
  learn_time_ms: 38012.013
  load_throughput: 4848872.385
  load_time_ms: 3.403
  training_iteration_time_ms: 49881.659
  update_time_ms: 2.733
timesteps_total: 4075500
training_iteration: 247

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9975
  reward for individual goal_min: 0.5
episode_len_mean: 42.815104166666664
episode_reward_max: 2.0
episode_reward_mean: 1.9973958333333333
episode_reward_min: 1.0
episodes_this_iter: 384
episodes_total: 83135
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973958333333334
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 47.34092926979065
time_total_s: 12740.373211860657
timers:
  learn_throughput: 451.926
  learn_time_ms: 36510.371
  load_throughput: 4252472.672
  load_time_ms: 3.88
  training_iteration_time_ms: 48028.301
  update_time_ms: 2.408
timesteps_total: 4471500
training_iteration: 271

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.985207100591716
  reward for individual goal_min: 0.5
episode_len_mean: 45.206043956043956
episode_reward_max: 2.0
episode_reward_mean: 1.9862637362637363
episode_reward_min: 1.0
episodes_this_iter: 364
episodes_total: 59135
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9862637362637363
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 57.113879442214966
time_total_s: 12822.332839250565
timers:
  learn_throughput: 385.305
  learn_time_ms: 42823.236
  load_throughput: 4169238.036
  load_time_ms: 3.958
  training_iteration_time_ms: 55406.499
  update_time_ms: 2.586
timesteps_total: 4092000
training_iteration: 248

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21794871794871795
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9615384615384616
  reward for individual goal_min: 0.0
episode_len_mean: 196.08
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 22841
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.9923415184021
time_total_s: 12899.54500579834
timers:
  learn_throughput: 521.093
  learn_time_ms: 31664.208
  load_throughput: 5276175.867
  load_time_ms: 3.127
  training_iteration_time_ms: 41989.413
  update_time_ms: 2.419
timesteps_total: 4554000
training_iteration: 276

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3150684931506849
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9929577464788732
  reward for individual goal_min: 0.5
episode_len_mean: 169.86
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 23109
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.34862494468689
time_total_s: 12911.495375156403
timers:
  learn_throughput: 439.402
  learn_time_ms: 37551.008
  load_throughput: 4527619.085
  load_time_ms: 3.644
  training_iteration_time_ms: 49381.153
  update_time_ms: 2.684
timesteps_total: 4257000
training_iteration: 258

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30666666666666664
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9925373134328358
  reward for individual goal_min: 0.5
episode_len_mean: 171.12
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 26822
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.290311336517334
time_total_s: 12899.921159267426
timers:
  learn_throughput: 535.448
  learn_time_ms: 30815.309
  load_throughput: 5312954.652
  load_time_ms: 3.106
  training_iteration_time_ms: 41114.715
  update_time_ms: 2.531
timesteps_total: 4851000
training_iteration: 294

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.13194444444444445
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.99375
  reward for individual goal_min: 0.5
episode_len_mean: 180.69
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 21522
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.67471170425415
time_total_s: 12912.299741983414
timers:
  learn_throughput: 398.95
  learn_time_ms: 41358.52
  load_throughput: 4226115.121
  load_time_ms: 3.904
  training_iteration_time_ms: 54631.25
  update_time_ms: 3.212
timesteps_total: 3910500
training_iteration: 237

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22077922077922077
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.6
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 23664
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.85584616661072
time_total_s: 12899.593524932861
timers:
  learn_throughput: 447.863
  learn_time_ms: 36841.627
  load_throughput: 4706675.553
  load_time_ms: 3.506
  training_iteration_time_ms: 48486.568
  update_time_ms: 2.612
timesteps_total: 4257000
training_iteration: 258

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20945945945945946
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8670886075949367
  reward for individual goal_min: 0.0
episode_len_mean: 199.49
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 23693
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.711955308914185
time_total_s: 12930.503147125244
timers:
  learn_throughput: 533.271
  learn_time_ms: 30941.096
  load_throughput: 4918064.214
  load_time_ms: 3.355
  training_iteration_time_ms: 41489.536
  update_time_ms: 2.49
timesteps_total: 4719000
training_iteration: 286

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9906832298136646
  reward for individual goal_min: 0.5
episode_len_mean: 50.42638036809816
episode_reward_max: 2.0
episode_reward_mean: 1.99079754601227
episode_reward_min: 1.0
episodes_this_iter: 326
episodes_total: 40171
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9969325153374233
  agent_1: 0.9938650306748467
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 69.85829281806946
time_total_s: 12913.090930461884
timers:
  learn_throughput: 314.593
  learn_time_ms: 52448.68
  load_throughput: 3382189.142
  load_time_ms: 4.878
  training_iteration_time_ms: 68082.173
  update_time_ms: 3.139
timesteps_total: 3448500
training_iteration: 209

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2987012987012987
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9657534246575342
  reward for individual goal_min: 0.0
episode_len_mean: 183.75
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 21398
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.40327262878418
time_total_s: 12937.289062738419
timers:
  learn_throughput: 448.596
  learn_time_ms: 36781.412
  load_throughput: 4668102.231
  load_time_ms: 3.535
  training_iteration_time_ms: 48400.834
  update_time_ms: 2.583
timesteps_total: 4174500
training_iteration: 253

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935897435897436
  reward for individual goal_min: 0.5
episode_len_mean: 180.24
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 21452
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.86522626876831
time_total_s: 12928.546248197556
timers:
  learn_throughput: 399.506
  learn_time_ms: 41300.992
  load_throughput: 4329976.6
  load_time_ms: 3.811
  training_iteration_time_ms: 54084.844
  update_time_ms: 2.832
timesteps_total: 3927000
training_iteration: 238

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.98125
  reward for individual goal_min: 0.0
episode_len_mean: 180.92
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 21844
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.26015090942383
time_total_s: 12926.654854297638
timers:
  learn_throughput: 439.745
  learn_time_ms: 37521.786
  load_throughput: 4640899.13
  load_time_ms: 3.555
  training_iteration_time_ms: 49568.014
  update_time_ms: 2.62
timesteps_total: 4141500
training_iteration: 251

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26973684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9714285714285714
  reward for individual goal_min: 0.0
episode_len_mean: 184.48
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 22588
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.395010471343994
time_total_s: 12952.706009626389
timers:
  learn_throughput: 430.647
  learn_time_ms: 38314.41
  load_throughput: 4807475.669
  load_time_ms: 3.432
  training_iteration_time_ms: 50128.24
  update_time_ms: 2.779
timesteps_total: 4092000
training_iteration: 248

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974619289340102
  reward for individual goal_min: 0.5
episode_len_mean: 44.00262467191601
episode_reward_max: 2.0
episode_reward_mean: 1.9973753280839894
episode_reward_min: 1.0
episodes_this_iter: 381
episodes_total: 63137
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973753280839895
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 58.95889163017273
time_total_s: 12852.648232221603
timers:
  learn_throughput: 356.708
  learn_time_ms: 46256.324
  load_throughput: 3331472.749
  load_time_ms: 4.953
  training_iteration_time_ms: 59707.136
  update_time_ms: 2.64
timesteps_total: 3745500
training_iteration: 227

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.372093023255815
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 387
episodes_total: 83522
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.120197772979736
time_total_s: 12788.493409633636
timers:
  learn_throughput: 452.353
  learn_time_ms: 36475.953
  load_throughput: 4245637.618
  load_time_ms: 3.886
  training_iteration_time_ms: 48007.627
  update_time_ms: 2.418
timesteps_total: 4488000
training_iteration: 272

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.225
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9492753623188406
  reward for individual goal_min: 0.0
episode_len_mean: 190.28
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 22928
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.723872900009155
time_total_s: 12944.268878698349
timers:
  learn_throughput: 519.144
  learn_time_ms: 31783.095
  load_throughput: 5283506.966
  load_time_ms: 3.123
  training_iteration_time_ms: 42302.575
  update_time_ms: 2.439
timesteps_total: 4570500
training_iteration: 277

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 182.76
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 26915
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.986013650894165
time_total_s: 12940.90717291832
timers:
  learn_throughput: 535.889
  learn_time_ms: 30789.941
  load_throughput: 5309041.924
  load_time_ms: 3.108
  training_iteration_time_ms: 41064.876
  update_time_ms: 2.525
timesteps_total: 4867500
training_iteration: 295

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9859154929577465
  reward for individual goal_min: 0.0
episode_len_mean: 183.84
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 23197
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.876389026641846
time_total_s: 12961.371764183044
timers:
  learn_throughput: 438.507
  learn_time_ms: 37627.636
  load_throughput: 4541911.637
  load_time_ms: 3.633
  training_iteration_time_ms: 49436.522
  update_time_ms: 2.657
timesteps_total: 4273500
training_iteration: 259

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9909638554216867
  reward for individual goal_min: 0.5
episode_len_mean: 46.069637883008355
episode_reward_max: 2.0
episode_reward_mean: 1.9916434540389971
episode_reward_min: 1.0
episodes_this_iter: 359
episodes_total: 59494
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9916434540389972
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 56.13101243972778
time_total_s: 12878.463851690292
timers:
  learn_throughput: 384.702
  learn_time_ms: 42890.334
  load_throughput: 4136048.385
  load_time_ms: 3.989
  training_iteration_time_ms: 55458.262
  update_time_ms: 2.588
timesteps_total: 4108500
training_iteration: 249

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2662337662337662
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8642857142857143
  reward for individual goal_min: 0.0
episode_len_mean: 201.87
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 23777
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.46
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.395793437957764
time_total_s: 12972.898940563202
timers:
  learn_throughput: 531.798
  learn_time_ms: 31026.822
  load_throughput: 4880124.108
  load_time_ms: 3.381
  training_iteration_time_ms: 41585.478
  update_time_ms: 2.482
timesteps_total: 4735500
training_iteration: 287

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2605633802816901
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 160.76237623762376
episode_reward_max: 2.0
episode_reward_mean: 1.3564356435643565
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 23765
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7227722772277227
  agent_1: 0.6336633663366337
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.43443274497986
time_total_s: 12949.027957677841
timers:
  learn_throughput: 446.84
  learn_time_ms: 36925.961
  load_throughput: 4712316.052
  load_time_ms: 3.501
  training_iteration_time_ms: 48611.167
  update_time_ms: 2.608
timesteps_total: 4273500
training_iteration: 259

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16216216216216217
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 179.66
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 21613
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.571744441986084
time_total_s: 12967.8714864254
timers:
  learn_throughput: 398.548
  learn_time_ms: 41400.29
  load_throughput: 4253858.012
  load_time_ms: 3.879
  training_iteration_time_ms: 54665.972
  update_time_ms: 3.217
timesteps_total: 3927000
training_iteration: 238

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24342105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9333333333333333
  reward for individual goal_min: 0.0
episode_len_mean: 190.82
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 21488
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.85234832763672
time_total_s: 12985.141411066055
timers:
  learn_throughput: 449.56
  learn_time_ms: 36702.518
  load_throughput: 4678389.746
  load_time_ms: 3.527
  training_iteration_time_ms: 48295.468
  update_time_ms: 2.592
timesteps_total: 4191000
training_iteration: 254

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1728395061728395
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 198.87
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 21931
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.23010468482971
time_total_s: 12977.884958982468
timers:
  learn_throughput: 437.369
  learn_time_ms: 37725.625
  load_throughput: 4648161.785
  load_time_ms: 3.55
  training_iteration_time_ms: 49765.975
  update_time_ms: 2.621
timesteps_total: 4158000
training_iteration: 252

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9772727272727273
  reward for individual goal_min: 0.5
episode_len_mean: 182.74
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 21547
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.25076508522034
time_total_s: 12985.797013282776
timers:
  learn_throughput: 396.205
  learn_time_ms: 41645.154
  load_throughput: 4304820.482
  load_time_ms: 3.833
  training_iteration_time_ms: 54549.77
  update_time_ms: 2.82
timesteps_total: 3943500
training_iteration: 239

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 190.97
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 26999
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.63683032989502
time_total_s: 12982.544003248215
timers:
  learn_throughput: 534.541
  learn_time_ms: 30867.627
  load_throughput: 5276900.014
  load_time_ms: 3.127
  training_iteration_time_ms: 41191.827
  update_time_ms: 2.524
timesteps_total: 4884000
training_iteration: 296

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.948051948051948
  reward for individual goal_min: 0.0
episode_len_mean: 178.31
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 23018
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.5262131690979
time_total_s: 12986.795091867447
timers:
  learn_throughput: 517.861
  learn_time_ms: 31861.817
  load_throughput: 5242601.983
  load_time_ms: 3.147
  training_iteration_time_ms: 42454.248
  update_time_ms: 2.449
timesteps_total: 4587000
training_iteration: 278

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 157.05882352941177
episode_reward_max: 2.0
episode_reward_mean: 1.4313725490196079
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 22690
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6862745098039216
  agent_1: 0.7450980392156863
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.63044738769531
time_total_s: 13001.336457014084
timers:
  learn_throughput: 433.193
  learn_time_ms: 38089.22
  load_throughput: 4845647.069
  load_time_ms: 3.405
  training_iteration_time_ms: 49894.441
  update_time_ms: 2.75
timesteps_total: 4108500
training_iteration: 249

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9772727272727273
  reward for individual goal_min: 0.5
episode_len_mean: 50.423312883435585
episode_reward_max: 2.0
episode_reward_mean: 1.978527607361963
episode_reward_min: 1.0
episodes_this_iter: 326
episodes_total: 40497
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9785276073619632
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 69.22073292732239
time_total_s: 12982.311663389206
timers:
  learn_throughput: 313.125
  learn_time_ms: 52694.528
  load_throughput: 3355313.055
  load_time_ms: 4.918
  training_iteration_time_ms: 68349.48
  update_time_ms: 3.155
timesteps_total: 3465000
training_iteration: 210

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972826086956522
  reward for individual goal_min: 0.5
episode_len_mean: 41.92947103274559
episode_reward_max: 2.0
episode_reward_mean: 1.9974811083123425
episode_reward_min: 1.0
episodes_this_iter: 397
episodes_total: 83919
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974811083123426
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.65581679344177
time_total_s: 12837.149226427078
timers:
  learn_throughput: 451.965
  learn_time_ms: 36507.289
  load_throughput: 4215098.486
  load_time_ms: 3.914
  training_iteration_time_ms: 48061.292
  update_time_ms: 2.423
timesteps_total: 4504500
training_iteration: 273

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24324324324324326
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8846153846153846
  reward for individual goal_min: 0.0
episode_len_mean: 193.57
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 23863
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.79763054847717
time_total_s: 13013.696571111679
timers:
  learn_throughput: 534.514
  learn_time_ms: 30869.148
  load_throughput: 4826452.238
  load_time_ms: 3.419
  training_iteration_time_ms: 41403.151
  update_time_ms: 2.466
timesteps_total: 4752000
training_iteration: 288

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9951690821256038
  reward for individual goal_min: 0.5
episode_len_mean: 44.0857908847185
episode_reward_max: 2.0
episode_reward_mean: 1.9946380697050938
episode_reward_min: 1.0
episodes_this_iter: 373
episodes_total: 63510
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973190348525469
  agent_1: 0.9973190348525469
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.255184173583984
time_total_s: 12910.903416395187
timers:
  learn_throughput: 357.686
  learn_time_ms: 46129.797
  load_throughput: 3335840.588
  load_time_ms: 4.946
  training_iteration_time_ms: 59582.479
  update_time_ms: 2.631
timesteps_total: 3762000
training_iteration: 228

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9946808510638298
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9911242603550295
  reward for individual goal_min: 0.5
episode_len_mean: 46.32773109243698
episode_reward_max: 2.0
episode_reward_mean: 1.9859943977591037
episode_reward_min: 0.0
episodes_this_iter: 357
episodes_total: 59851
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9943977591036415
  agent_1: 0.9915966386554622
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.82168245315552
time_total_s: 12934.285534143448
timers:
  learn_throughput: 383.647
  learn_time_ms: 43008.271
  load_throughput: 4131209.169
  load_time_ms: 3.994
  training_iteration_time_ms: 55565.215
  update_time_ms: 2.637
timesteps_total: 4125000
training_iteration: 250

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 63.96666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.9333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9666666666666667
  agent_1: 0.9666666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2077922077922078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 190.38
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 23851
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.56141757965088
time_total_s: 13004.589375257492
timers:
  learn_throughput: 448.199
  learn_time_ms: 36814.021
  load_throughput: 4692569.569
  load_time_ms: 3.516
  training_iteration_time_ms: 48435.758
  update_time_ms: 2.593
timesteps_total: 4290000
training_iteration: 260

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 92.5
episode_reward_max: 2.0
episode_reward_mean: 1.8166666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9
  agent_1: 0.9166666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29605263157894735
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 167.17
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 23295
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.27068519592285
time_total_s: 13023.642449378967
timers:
  learn_throughput: 440.812
  learn_time_ms: 37430.917
  load_throughput: 4517688.346
  load_time_ms: 3.652
  training_iteration_time_ms: 49193.152
  update_time_ms: 2.647
timesteps_total: 4290000
training_iteration: 260

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.9
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 21711
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.77
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.551865100860596
time_total_s: 13020.42335152626
timers:
  learn_throughput: 400.08
  learn_time_ms: 41241.701
  load_throughput: 4241552.322
  load_time_ms: 3.89
  training_iteration_time_ms: 54451.615
  update_time_ms: 2.773
timesteps_total: 3943500
training_iteration: 239

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20945945945945946
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9459459459459459
  reward for individual goal_min: 0.0
episode_len_mean: 194.96
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 21571
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.065568685531616
time_total_s: 13034.206979751587
timers:
  learn_throughput: 448.416
  learn_time_ms: 36796.222
  load_throughput: 4659710.207
  load_time_ms: 3.541
  training_iteration_time_ms: 48356.319
  update_time_ms: 2.573
timesteps_total: 4207500
training_iteration: 255

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2328767123287671
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.07
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 22027
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.005388498306274
time_total_s: 13027.890347480774
timers:
  learn_throughput: 437.442
  learn_time_ms: 37719.303
  load_throughput: 4621097.348
  load_time_ms: 3.571
  training_iteration_time_ms: 49820.441
  update_time_ms: 2.598
timesteps_total: 4174500
training_iteration: 253

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2894736842105263
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.0
episode_len_mean: 175.15
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 27095
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.083839893341064
time_total_s: 13026.627843141556
timers:
  learn_throughput: 532.975
  learn_time_ms: 30958.308
  load_throughput: 5255181.903
  load_time_ms: 3.14
  training_iteration_time_ms: 41253.53
  update_time_ms: 2.542
timesteps_total: 4900500
training_iteration: 297

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24050632911392406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9324324324324325
  reward for individual goal_min: 0.0
episode_len_mean: 188.27
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 23106
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.035794258117676
time_total_s: 13032.830886125565
timers:
  learn_throughput: 514.163
  learn_time_ms: 32090.974
  load_throughput: 5153091.288
  load_time_ms: 3.202
  training_iteration_time_ms: 42779.591
  update_time_ms: 2.453
timesteps_total: 4603500
training_iteration: 279

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2714285714285714
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9358974358974359
  reward for individual goal_min: 0.0
episode_len_mean: 180.98
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 23953
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.16934108734131
time_total_s: 13056.86591219902
timers:
  learn_throughput: 529.357
  learn_time_ms: 31169.919
  load_throughput: 4851251.682
  load_time_ms: 3.401
  training_iteration_time_ms: 41668.314
  update_time_ms: 2.479
timesteps_total: 4768500
training_iteration: 289

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22077922077922077
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9807692307692307
  reward for individual goal_min: 0.0
episode_len_mean: 182.68
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 22782
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.3115611076355
time_total_s: 13050.64801812172
timers:
  learn_throughput: 435.412
  learn_time_ms: 37895.179
  load_throughput: 4852850.521
  load_time_ms: 3.4
  training_iteration_time_ms: 49675.735
  update_time_ms: 2.737
timesteps_total: 4125000
training_iteration: 250

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974747474747475
  reward for individual goal_min: 0.5
episode_len_mean: 42.045801526717554
episode_reward_max: 2.0
episode_reward_mean: 1.9974554707379135
episode_reward_min: 1.0
episodes_this_iter: 393
episodes_total: 84312
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974554707379135
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 49.160271406173706
time_total_s: 12886.309497833252
timers:
  learn_throughput: 450.816
  learn_time_ms: 36600.289
  load_throughput: 4206720.198
  load_time_ms: 3.922
  training_iteration_time_ms: 48167.785
  update_time_ms: 2.41
timesteps_total: 4521000
training_iteration: 274

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 78.91666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.8666666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.95
  agent_1: 0.9166666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20422535211267606
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9805194805194806
  reward for individual goal_min: 0.5
episode_len_mean: 180.99
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 21639
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 64.23658418655396
time_total_s: 13050.03359746933
timers:
  learn_throughput: 398.131
  learn_time_ms: 41443.649
  load_throughput: 4287724.42
  load_time_ms: 3.848
  training_iteration_time_ms: 54290.926
  update_time_ms: 2.803
timesteps_total: 3960000
training_iteration: 240

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9975369458128078
  reward for individual goal_min: 0.5
episode_len_mean: 44.47043010752688
episode_reward_max: 2.0
episode_reward_mean: 1.9973118279569892
episode_reward_min: 1.0
episodes_this_iter: 372
episodes_total: 63882
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973118279569892
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 59.057363748550415
time_total_s: 12969.960780143738
timers:
  learn_throughput: 359.019
  learn_time_ms: 45958.563
  load_throughput: 3338028.805
  load_time_ms: 4.943
  training_iteration_time_ms: 59362.448
  update_time_ms: 2.631
timesteps_total: 3778500
training_iteration: 229

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9930555555555556
  reward for individual goal_min: 0.5
episode_len_mean: 165.13
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 23949
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.42210817337036
time_total_s: 13053.011483430862
timers:
  learn_throughput: 448.451
  learn_time_ms: 36793.304
  load_throughput: 4746477.556
  load_time_ms: 3.476
  training_iteration_time_ms: 48409.338
  update_time_ms: 2.607
timesteps_total: 4306500
training_iteration: 261

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9875
  reward for individual goal_min: 0.5
episode_len_mean: 52.37341772151899
episode_reward_max: 2.0
episode_reward_mean: 1.9873417721518987
episode_reward_min: 1.0
episodes_this_iter: 316
episodes_total: 40813
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9968354430379747
  agent_1: 0.990506329113924
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 69.39959406852722
time_total_s: 13051.711257457733
timers:
  learn_throughput: 312.367
  learn_time_ms: 52822.488
  load_throughput: 3427737.296
  load_time_ms: 4.814
  training_iteration_time_ms: 68523.486
  update_time_ms: 3.253
timesteps_total: 3481500
training_iteration: 211

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.175
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 193.96
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 23382
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.38260746002197
time_total_s: 13074.02505683899
timers:
  learn_throughput: 439.149
  learn_time_ms: 37572.662
  load_throughput: 4533283.725
  load_time_ms: 3.64
  training_iteration_time_ms: 49330.835
  update_time_ms: 2.615
timesteps_total: 4306500
training_iteration: 261

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 44.244623655913976
episode_reward_max: 2.0
episode_reward_mean: 1.9946236559139785
episode_reward_min: 1.0
episodes_this_iter: 372
episodes_total: 60223
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973118279569892
  agent_1: 0.9973118279569892
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.42751860618591
time_total_s: 12991.713052749634
timers:
  learn_throughput: 383.395
  learn_time_ms: 43036.528
  load_throughput: 4128966.237
  load_time_ms: 3.996
  training_iteration_time_ms: 55705.907
  update_time_ms: 2.634
timesteps_total: 4141500
training_iteration: 251

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9436619718309859
  reward for individual goal_min: 0.0
episode_len_mean: 184.78
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 21663
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.417964220047
time_total_s: 13083.624943971634
timers:
  learn_throughput: 448.061
  learn_time_ms: 36825.345
  load_throughput: 4708180.501
  load_time_ms: 3.505
  training_iteration_time_ms: 48355.859
  update_time_ms: 2.568
timesteps_total: 4224000
training_iteration: 256

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23780487804878048
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 194.78
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 27177
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.762084007263184
time_total_s: 13068.389927148819
timers:
  learn_throughput: 530.01
  learn_time_ms: 31131.515
  load_throughput: 5205416.773
  load_time_ms: 3.17
  training_iteration_time_ms: 41489.512
  update_time_ms: 2.564
timesteps_total: 4917000
training_iteration: 298

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 70.16666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.9333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9666666666666667
  agent_1: 0.9666666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21428571428571427
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 180.75
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 21801
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.5810546875
time_total_s: 13084.00440621376
timers:
  learn_throughput: 401.545
  learn_time_ms: 41091.25
  load_throughput: 4255244.256
  load_time_ms: 3.878
  training_iteration_time_ms: 54206.922
  update_time_ms: 2.805
timesteps_total: 3960000
training_iteration: 240

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-29ws591pkw/checkpoint_000240/checkpoint-240
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21794871794871795
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9423076923076923
  reward for individual goal_min: 0.0
episode_len_mean: 192.78
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 24037
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.62403106689453
time_total_s: 13098.489943265915
timers:
  learn_throughput: 527.886
  learn_time_ms: 31256.773
  load_throughput: 4803771.605
  load_time_ms: 3.435
  training_iteration_time_ms: 41779.35
  update_time_ms: 2.466
timesteps_total: 4785000
training_iteration: 290

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17307692307692307
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 185.81
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 22114
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.532843828201294
time_total_s: 13078.423191308975
timers:
  learn_throughput: 436.321
  learn_time_ms: 37816.186
  load_throughput: 4663132.517
  load_time_ms: 3.538
  training_iteration_time_ms: 49978.379
  update_time_ms: 2.585
timesteps_total: 4191000
training_iteration: 254

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1962025316455696
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9605263157894737
  reward for individual goal_min: 0.0
episode_len_mean: 190.73
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 22870
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.34272575378418
time_total_s: 13098.990743875504
timers:
  learn_throughput: 435.222
  learn_time_ms: 37911.659
  load_throughput: 4790006.645
  load_time_ms: 3.445
  training_iteration_time_ms: 49829.196
  update_time_ms: 2.7
timesteps_total: 4141500
training_iteration: 251

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95
  reward for individual goal_min: 0.5
episode_len_mean: 98.95
episode_reward_max: 2.0
episode_reward_mean: 1.8833333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9666666666666667
  agent_1: 0.9166666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9577464788732394
  reward for individual goal_min: 0.0
episode_len_mean: 179.29
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 23197
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.70222234725952
time_total_s: 13088.533108472824
timers:
  learn_throughput: 509.359
  learn_time_ms: 32393.628
  load_throughput: 5076657.913
  load_time_ms: 3.25
  training_iteration_time_ms: 43149.743
  update_time_ms: 2.452
timesteps_total: 4620000
training_iteration: 280

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19istq_qwi/checkpoint_000280/checkpoint-280
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9975609756097561
  reward for individual goal_min: 0.5
episode_len_mean: 40.66093366093366
episode_reward_max: 2.0
episode_reward_mean: 1.9975429975429975
episode_reward_min: 1.0
episodes_this_iter: 407
episodes_total: 84719
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9975429975429976
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 48.08685612678528
time_total_s: 12934.396353960037
timers:
  learn_throughput: 451.906
  learn_time_ms: 36512.047
  load_throughput: 4202479.733
  load_time_ms: 3.926
  training_iteration_time_ms: 48079.449
  update_time_ms: 2.4
timesteps_total: 4537500
training_iteration: 275

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21232876712328766
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 174.44
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 21731
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.61293840408325
time_total_s: 13104.646535873413
timers:
  learn_throughput: 400.026
  learn_time_ms: 41247.331
  load_throughput: 4319814.239
  load_time_ms: 3.82
  training_iteration_time_ms: 54072.214
  update_time_ms: 2.817
timesteps_total: 3976500
training_iteration: 241

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2974683544303797
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.99
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 24045
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.88142418861389
time_total_s: 13101.892907619476
timers:
  learn_throughput: 446.456
  learn_time_ms: 36957.709
  load_throughput: 4755119.967
  load_time_ms: 3.47
  training_iteration_time_ms: 48584.472
  update_time_ms: 2.619
timesteps_total: 4323000
training_iteration: 262

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21794871794871795
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 189.0
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 23470
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.96143984794617
time_total_s: 13122.986496686935
timers:
  learn_throughput: 441.793
  learn_time_ms: 37347.789
  load_throughput: 4516125.866
  load_time_ms: 3.654
  training_iteration_time_ms: 49092.901
  update_time_ms: 2.61
timesteps_total: 4323000
training_iteration: 262

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17567567567567569
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.99375
  reward for individual goal_min: 0.5
episode_len_mean: 185.07
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 27269
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 40.575209856033325
time_total_s: 13108.965137004852
timers:
  learn_throughput: 530.451
  learn_time_ms: 31105.634
  load_throughput: 5185330.686
  load_time_ms: 3.182
  training_iteration_time_ms: 41465.504
  update_time_ms: 2.57
timesteps_total: 4933500
training_iteration: 299

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22058823529411764
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.98125
  reward for individual goal_min: 0.0
episode_len_mean: 172.83
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 21756
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.571415424346924
time_total_s: 13131.19635939598
timers:
  learn_throughput: 447.562
  learn_time_ms: 36866.43
  load_throughput: 4699867.301
  load_time_ms: 3.511
  training_iteration_time_ms: 48373.361
  update_time_ms: 2.56
timesteps_total: 4240500
training_iteration: 257

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.542635658914726
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 387
episodes_total: 64269
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 60.597054958343506
time_total_s: 13030.557835102081
timers:
  learn_throughput: 358.999
  learn_time_ms: 45961.103
  load_throughput: 3342091.04
  load_time_ms: 4.937
  training_iteration_time_ms: 59400.09
  update_time_ms: 2.614
timesteps_total: 3795000
training_iteration: 230

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9917582417582418
  reward for individual goal_min: 0.5
episode_len_mean: 46.0
episode_reward_max: 2.0
episode_reward_mean: 1.991549295774648
episode_reward_min: 1.0
episodes_this_iter: 355
episodes_total: 60578
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9915492957746479
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 55.06294131278992
time_total_s: 13046.775994062424
timers:
  learn_throughput: 384.172
  learn_time_ms: 42949.519
  load_throughput: 4112502.585
  load_time_ms: 4.012
  training_iteration_time_ms: 55609.489
  update_time_ms: 2.656
timesteps_total: 4158000
training_iteration: 252

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20422535211267606
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9090909090909091
  reward for individual goal_min: 0.0
episode_len_mean: 196.07
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 24120
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.087149143218994
time_total_s: 13141.577092409134
timers:
  learn_throughput: 525.778
  learn_time_ms: 31382.085
  load_throughput: 4778859.94
  load_time_ms: 3.453
  training_iteration_time_ms: 41876.238
  update_time_ms: 2.491
timesteps_total: 4801500
training_iteration: 291

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9638157894736842
  reward for individual goal_min: 0.5
episode_len_mean: 56.54325259515571
episode_reward_max: 2.0
episode_reward_mean: 1.9619377162629759
episode_reward_min: 1.0
episodes_this_iter: 289
episodes_total: 41102
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9965397923875432
  agent_1: 0.9653979238754326
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 67.44485425949097
time_total_s: 13119.156111717224
timers:
  learn_throughput: 312.879
  learn_time_ms: 52735.978
  load_throughput: 3466557.937
  load_time_ms: 4.76
  training_iteration_time_ms: 68372.764
  update_time_ms: 3.252
timesteps_total: 3498000
training_iteration: 212

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18674698795180722
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 192.56
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 22204
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.71038603782654
time_total_s: 13127.133577346802
timers:
  learn_throughput: 437.453
  learn_time_ms: 37718.337
  load_throughput: 4670811.719
  load_time_ms: 3.533
  training_iteration_time_ms: 49808.605
  update_time_ms: 2.592
timesteps_total: 4207500
training_iteration: 255

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.208955223880597
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9470588235294117
  reward for individual goal_min: 0.0
episode_len_mean: 178.88
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 23291
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.81083941459656
time_total_s: 13130.34394788742
timers:
  learn_throughput: 512.231
  learn_time_ms: 32212.037
  load_throughput: 5027862.69
  load_time_ms: 3.282
  training_iteration_time_ms: 42888.159
  update_time_ms: 2.468
timesteps_total: 4636500
training_iteration: 281

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1858974358974359
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 180.23
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 21894
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.20856857299805
time_total_s: 13138.212974786758
timers:
  learn_throughput: 401.194
  learn_time_ms: 41127.195
  load_throughput: 4219107.236
  load_time_ms: 3.911
  training_iteration_time_ms: 54212.784
  update_time_ms: 2.826
timesteps_total: 3976500
training_iteration: 241

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2602739726027397
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9855072463768116
  reward for individual goal_min: 0.0
episode_len_mean: 173.5
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 22963
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.754960775375366
time_total_s: 13150.745704650879
timers:
  learn_throughput: 431.743
  learn_time_ms: 38217.166
  load_throughput: 4791598.538
  load_time_ms: 3.444
  training_iteration_time_ms: 50159.759
  update_time_ms: 2.671
timesteps_total: 4158000
training_iteration: 252

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.4256926952141
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 397
episodes_total: 85116
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.01173210144043
time_total_s: 12982.408086061478
timers:
  learn_throughput: 451.486
  learn_time_ms: 36545.955
  load_throughput: 4218181.464
  load_time_ms: 3.912
  training_iteration_time_ms: 48081.984
  update_time_ms: 2.403
timesteps_total: 4554000
training_iteration: 276

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24324324324324326
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9726027397260274
  reward for individual goal_min: 0.0
episode_len_mean: 185.15
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 24134
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.0421998500824
time_total_s: 13147.935107469559
timers:
  learn_throughput: 447.079
  learn_time_ms: 36906.228
  load_throughput: 4738385.529
  load_time_ms: 3.482
  training_iteration_time_ms: 48546.264
  update_time_ms: 2.601
timesteps_total: 4339500
training_iteration: 263

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2642857142857143
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9539473684210527
  reward for individual goal_min: 0.0
episode_len_mean: 170.66
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 21828
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.36286449432373
time_total_s: 13159.009400367737
timers:
  learn_throughput: 397.999
  learn_time_ms: 41457.355
  load_throughput: 4365841.897
  load_time_ms: 3.779
  training_iteration_time_ms: 54315.29
  update_time_ms: 2.854
timesteps_total: 3993000
training_iteration: 242

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22297297297297297
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9814814814814815
  reward for individual goal_min: 0.0
episode_len_mean: 174.85
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 23564
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.192405223846436
time_total_s: 13171.178901910782
timers:
  learn_throughput: 442.856
  learn_time_ms: 37258.172
  load_throughput: 4512062.59
  load_time_ms: 3.657
  training_iteration_time_ms: 48949.141
  update_time_ms: 2.84
timesteps_total: 4339500
training_iteration: 263

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26119402985074625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.879746835443038
  reward for individual goal_min: 0.0
episode_len_mean: 186.12
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 24205
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.77
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.74663043022156
time_total_s: 13184.323722839355
timers:
  learn_throughput: 524.646
  learn_time_ms: 31449.779
  load_throughput: 4783583.505
  load_time_ms: 3.449
  training_iteration_time_ms: 41961.224
  update_time_ms: 2.489
timesteps_total: 4818000
training_iteration: 292

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95
  reward for individual goal_min: 0.5
episode_len_mean: 78.35
episode_reward_max: 2.0
episode_reward_mean: 1.95
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.95
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2361111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.29
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 27361
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.06145906448364
time_total_s: 13161.026596069336
timers:
  learn_throughput: 526.905
  learn_time_ms: 31314.958
  load_throughput: 5168176.36
  load_time_ms: 3.193
  training_iteration_time_ms: 41712.64
  update_time_ms: 2.555
timesteps_total: 4950000
training_iteration: 300

Starting final evaluation!
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9594594594594594
  reward for individual goal_min: 0.0
episode_len_mean: 198.56
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 21841
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.791226625442505
time_total_s: 13179.987586021423
timers:
  learn_throughput: 447.272
  learn_time_ms: 36890.302
  load_throughput: 4695020.861
  load_time_ms: 3.514
  training_iteration_time_ms: 48474.935
  update_time_ms: 2.56
timesteps_total: 4257000
training_iteration: 258

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22784810126582278
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9266666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 201.29
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 23373
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.77687692642212
time_total_s: 13174.120824813843
timers:
  learn_throughput: 508.965
  learn_time_ms: 32418.743
  load_throughput: 5052160.925
  load_time_ms: 3.266
  training_iteration_time_ms: 43184.403
  update_time_ms: 2.498
timesteps_total: 4653000
training_iteration: 282

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.98
  reward for individual goal_min: 0.5
episode_len_mean: 47.288951841359776
episode_reward_max: 2.0
episode_reward_mean: 1.980169971671388
episode_reward_min: 1.0
episodes_this_iter: 353
episodes_total: 60931
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9858356940509915
  agent_1: 0.9943342776203966
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.180684328079224
time_total_s: 13101.956678390503
timers:
  learn_throughput: 383.567
  learn_time_ms: 43017.285
  load_throughput: 4120607.558
  load_time_ms: 4.004
  training_iteration_time_ms: 55706.264
  update_time_ms: 2.657
timesteps_total: 4174500
training_iteration: 253

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2054794520547945
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.88
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 22297
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.94577670097351
time_total_s: 13175.079354047775
timers:
  learn_throughput: 439.862
  learn_time_ms: 37511.8
  load_throughput: 4522914.281
  load_time_ms: 3.648
  training_iteration_time_ms: 49541.233
  update_time_ms: 2.582
timesteps_total: 4224000
training_iteration: 256

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.85194805194805
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 385
episodes_total: 64654
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.81349563598633
time_total_s: 13089.371330738068
timers:
  learn_throughput: 360.214
  learn_time_ms: 45806.093
  load_throughput: 3289822.213
  load_time_ms: 5.015
  training_iteration_time_ms: 59195.717
  update_time_ms: 2.634
timesteps_total: 3811500
training_iteration: 231

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23684210526315788
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9779411764705882
  reward for individual goal_min: 0.0
episode_len_mean: 184.22
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 23053
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.08107328414917
time_total_s: 13197.826777935028
timers:
  learn_throughput: 434.666
  learn_time_ms: 37960.192
  load_throughput: 4763203.733
  load_time_ms: 3.464
  training_iteration_time_ms: 49845.023
  update_time_ms: 2.639
timesteps_total: 4174500
training_iteration: 253

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17567567567567569
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 180.46
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 21987
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.891077280044556
time_total_s: 13195.104052066803
timers:
  learn_throughput: 400.47
  learn_time_ms: 41201.599
  load_throughput: 4215868.808
  load_time_ms: 3.914
  training_iteration_time_ms: 54262.134
  update_time_ms: 2.836
timesteps_total: 3993000
training_iteration: 242

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.75757575757576
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 396
episodes_total: 85512
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.631123065948486
time_total_s: 13031.039209127426
timers:
  learn_throughput: 451.681
  learn_time_ms: 36530.187
  load_throughput: 4222221.843
  load_time_ms: 3.908
  training_iteration_time_ms: 48060.587
  update_time_ms: 2.403
timesteps_total: 4570500
training_iteration: 277

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9761904761904762
  reward for individual goal_min: 0.5
episode_len_mean: 53.363344051446944
episode_reward_max: 2.0
episode_reward_mean: 1.9742765273311897
episode_reward_min: 1.0
episodes_this_iter: 311
episodes_total: 41413
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9967845659163987
  agent_1: 0.977491961414791
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 68.65118741989136
time_total_s: 13187.807299137115
timers:
  learn_throughput: 312.721
  learn_time_ms: 52762.719
  load_throughput: 3413131.325
  load_time_ms: 4.834
  training_iteration_time_ms: 68439.618
  update_time_ms: 3.257
timesteps_total: 3514500
training_iteration: 213

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31756756756756754
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 155.52884615384616
episode_reward_max: 2.0
episode_reward_mean: 1.3942307692307692
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 24238
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6538461538461539
  agent_1: 0.7403846153846154
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.03964924812317
time_total_s: 13196.974756717682
timers:
  learn_throughput: 449.342
  learn_time_ms: 36720.387
  load_throughput: 4759862.169
  load_time_ms: 3.466
  training_iteration_time_ms: 48313.45
  update_time_ms: 2.596
timesteps_total: 4356000
training_iteration: 264

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2647058823529412
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.918918918918919
  reward for individual goal_min: 0.0
episode_len_mean: 185.25
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 24296
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.21861743927002
time_total_s: 13225.542340278625
timers:
  learn_throughput: 522.287
  learn_time_ms: 31591.828
  load_throughput: 4821106.25
  load_time_ms: 3.422
  training_iteration_time_ms: 42125.916
  update_time_ms: 2.492
timesteps_total: 4834500
training_iteration: 293

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.05
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 23652
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.63931179046631
time_total_s: 13219.818213701248
timers:
  learn_throughput: 440.962
  learn_time_ms: 37418.231
  load_throughput: 4520757.488
  load_time_ms: 3.65
  training_iteration_time_ms: 49171.879
  update_time_ms: 2.855
timesteps_total: 4356000
training_iteration: 264

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2987012987012987
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.62
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 21922
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.13064885139465
time_total_s: 13213.140049219131
timers:
  learn_throughput: 398.099
  learn_time_ms: 41446.942
  load_throughput: 4357924.247
  load_time_ms: 3.786
  training_iteration_time_ms: 54376.682
  update_time_ms: 2.869
timesteps_total: 4009500
training_iteration: 243

Evaluation results over 10 episodes for each goal
{'[0, 0, 1]': 1.8, '[0, 1, 0]': 1.9, '[1, 0, 0]': 2.0, '[0, 1, 1]': 2.0, '[1, 0, 1]': 2.0, '[1, 1, 0]': 1.8}
Moviepy - Building video /gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19ecg2ejpn/trained_agent.mp4.
Moviepy - Writing video /gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19ecg2ejpn/trained_agent.mp4

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19863013698630136
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9583333333333334
  reward for individual goal_min: 0.0
episode_len_mean: 188.91
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 21925
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.31655263900757
time_total_s: 13228.30413866043
timers:
  learn_throughput: 448.488
  learn_time_ms: 36790.261
  load_throughput: 4642082.048
  load_time_ms: 3.554
  training_iteration_time_ms: 48247.763
  update_time_ms: 2.562
timesteps_total: 4273500
training_iteration: 259

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24647887323943662
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 174.97
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 23468
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.46328282356262
time_total_s: 13218.584107637405
timers:
  learn_throughput: 506.495
  learn_time_ms: 32576.821
  load_throughput: 5071152.341
  load_time_ms: 3.254
  training_iteration_time_ms: 43326.268
  update_time_ms: 2.519
timesteps_total: 4669500
training_iteration: 283

Moviepy - Done !
Moviepy - video ready /gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19ecg2ejpn/trained_agent.mp4
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1643835616438356
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.94
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 22389
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.59576106071472
time_total_s: 13223.67511510849
timers:
  learn_throughput: 440.424
  learn_time_ms: 37463.94
  load_throughput: 4506744.291
  load_time_ms: 3.661
  training_iteration_time_ms: 49441.446
  update_time_ms: 2.583
timesteps_total: 4240500
training_iteration: 257

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22857142857142856
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9675324675324676
  reward for individual goal_min: 0.0
episode_len_mean: 177.82
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 23143
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.915712118148804
time_total_s: 13244.742490053177
timers:
  learn_throughput: 436.788
  learn_time_ms: 37775.763
  load_throughput: 4757506.239
  load_time_ms: 3.468
  training_iteration_time_ms: 49546.051
  update_time_ms: 2.626
timesteps_total: 4191000
training_iteration: 254

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9888888888888889
  reward for individual goal_min: 0.5
episode_len_mean: 47.65706051873199
episode_reward_max: 2.0
episode_reward_mean: 1.9884726224783862
episode_reward_min: 1.0
episodes_this_iter: 347
episodes_total: 61278
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9884726224783862
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 54.643152475357056
time_total_s: 13156.59983086586
timers:
  learn_throughput: 383.323
  learn_time_ms: 43044.695
  load_throughput: 4053654.474
  load_time_ms: 4.07
  training_iteration_time_ms: 55745.324
  update_time_ms: 2.675
timesteps_total: 4191000
training_iteration: 254

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.84771573604061
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 394
episodes_total: 85906
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.21074032783508
time_total_s: 13079.249949455261
timers:
  learn_throughput: 451.285
  learn_time_ms: 36562.27
  load_throughput: 4241942.297
  load_time_ms: 3.89
  training_iteration_time_ms: 48086.987
  update_time_ms: 2.384
timesteps_total: 4587000
training_iteration: 278

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.675257731958766
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 388
episodes_total: 65042
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.47199320793152
time_total_s: 13148.843323946
timers:
  learn_throughput: 360.444
  learn_time_ms: 45776.89
  load_throughput: 3278585.214
  load_time_ms: 5.033
  training_iteration_time_ms: 59177.892
  update_time_ms: 2.614
timesteps_total: 3828000
training_iteration: 232

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17307692307692307
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 185.18
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 22074
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.554511070251465
time_total_s: 13248.658563137054
timers:
  learn_throughput: 400.885
  learn_time_ms: 41158.971
  load_throughput: 4201357.188
  load_time_ms: 3.927
  training_iteration_time_ms: 54144.853
  update_time_ms: 2.812
timesteps_total: 4009500
training_iteration: 243

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9113924050632911
  reward for individual goal_min: 0.0
episode_len_mean: 183.13
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 24388
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.07871770858765
time_total_s: 13268.621057987213
timers:
  learn_throughput: 523.199
  learn_time_ms: 31536.78
  load_throughput: 4794486.543
  load_time_ms: 3.441
  training_iteration_time_ms: 42043.369
  update_time_ms: 2.516
timesteps_total: 4851000
training_iteration: 294

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2635135135135135
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 171.16
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 24335
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.43285250663757
time_total_s: 13245.40760922432
timers:
  learn_throughput: 449.367
  learn_time_ms: 36718.28
  load_throughput: 4784079.525
  load_time_ms: 3.449
  training_iteration_time_ms: 48361.505
  update_time_ms: 2.565
timesteps_total: 4372500
training_iteration: 265

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.28
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 23745
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.171000719070435
time_total_s: 13268.989214420319
timers:
  learn_throughput: 439.995
  learn_time_ms: 37500.422
  load_throughput: 4472030.655
  load_time_ms: 3.69
  training_iteration_time_ms: 49210.545
  update_time_ms: 2.836
timesteps_total: 4372500
training_iteration: 265

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9177215189873418
  reward for individual goal_min: 0.0
episode_len_mean: 198.24
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 23549
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.32411074638367
time_total_s: 13261.908218383789
timers:
  learn_throughput: 504.892
  learn_time_ms: 32680.241
  load_throughput: 5090923.643
  load_time_ms: 3.241
  training_iteration_time_ms: 43456.37
  update_time_ms: 2.521
timesteps_total: 4686000
training_iteration: 284

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2865853658536585
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9772727272727273
  reward for individual goal_min: 0.0
episode_len_mean: 184.18
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 22012
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.120054721832275
time_total_s: 13267.260103940964
timers:
  learn_throughput: 400.025
  learn_time_ms: 41247.436
  load_throughput: 4356196.087
  load_time_ms: 3.788
  training_iteration_time_ms: 54073.751
  update_time_ms: 2.835
timesteps_total: 4026000
training_iteration: 244

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.974025974025974
  reward for individual goal_min: 0.5
episode_len_mean: 55.12
episode_reward_max: 2.0
episode_reward_mean: 1.9733333333333334
episode_reward_min: 1.0
episodes_this_iter: 300
episodes_total: 41713
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9733333333333334
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 68.05902624130249
time_total_s: 13255.866325378418
timers:
  learn_throughput: 313.541
  learn_time_ms: 52624.674
  load_throughput: 3431136.143
  load_time_ms: 4.809
  training_iteration_time_ms: 68321.965
  update_time_ms: 3.201
timesteps_total: 3531000
training_iteration: 214

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28378378378378377
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 167.14
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 22485
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.616329193115234
time_total_s: 13272.291444301605
timers:
  learn_throughput: 441.228
  learn_time_ms: 37395.632
  load_throughput: 4500501.775
  load_time_ms: 3.666
  training_iteration_time_ms: 49360.223
  update_time_ms: 2.592
timesteps_total: 4257000
training_iteration: 258

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9
  reward for individual goal_min: 0.5
episode_len_mean: 118.9
episode_reward_max: 2.0
episode_reward_mean: 1.6666666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8833333333333333
  agent_1: 0.7833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27564102564102566
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9692307692307692
  reward for individual goal_min: 0.0
episode_len_mean: 183.94
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 22017
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.63860034942627
time_total_s: 13289.942739009857
timers:
  learn_throughput: 444.157
  learn_time_ms: 37149.052
  load_throughput: 4671631.486
  load_time_ms: 3.532
  training_iteration_time_ms: 48611.526
  update_time_ms: 2.533
timesteps_total: 4290000
training_iteration: 260

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18243243243243243
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 179.33
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 23236
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.624688386917114
time_total_s: 13295.367178440094
timers:
  learn_throughput: 434.616
  learn_time_ms: 37964.551
  load_throughput: 4793623.096
  load_time_ms: 3.442
  training_iteration_time_ms: 49707.127
  update_time_ms: 2.634
timesteps_total: 4207500
training_iteration: 255

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974093264248705
  reward for individual goal_min: 0.5
episode_len_mean: 43.335958005249346
episode_reward_max: 2.0
episode_reward_mean: 1.9973753280839894
episode_reward_min: 1.0
episodes_this_iter: 381
episodes_total: 86287
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973753280839895
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 48.363972187042236
time_total_s: 13127.613921642303
timers:
  learn_throughput: 450.822
  learn_time_ms: 36599.786
  load_throughput: 4208587.692
  load_time_ms: 3.921
  training_iteration_time_ms: 48127.216
  update_time_ms: 2.366
timesteps_total: 4603500
training_iteration: 279

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9884393063583815
  reward for individual goal_min: 0.5
episode_len_mean: 46.15297450424929
episode_reward_max: 2.0
episode_reward_mean: 1.9886685552407932
episode_reward_min: 1.0
episodes_this_iter: 353
episodes_total: 61631
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9915014164305949
  agent_1: 0.9971671388101983
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.75198197364807
time_total_s: 13211.351812839508
timers:
  learn_throughput: 383.122
  learn_time_ms: 43067.177
  load_throughput: 4013315.627
  load_time_ms: 4.111
  training_iteration_time_ms: 55736.857
  update_time_ms: 2.657
timesteps_total: 4207500
training_iteration: 255

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2462686567164179
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9423076923076923
  reward for individual goal_min: 0.0
episode_len_mean: 179.22
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 24482
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.17128896713257
time_total_s: 13311.792346954346
timers:
  learn_throughput: 518.953
  learn_time_ms: 31794.77
  load_throughput: 4814699.977
  load_time_ms: 3.427
  training_iteration_time_ms: 42362.566
  update_time_ms: 2.518
timesteps_total: 4867500
training_iteration: 295

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2721518987341772
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9921875
  reward for individual goal_min: 0.5
episode_len_mean: 177.25
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 22167
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.33174681663513
time_total_s: 13306.99030995369
timers:
  learn_throughput: 397.807
  learn_time_ms: 41477.421
  load_throughput: 4224257.828
  load_time_ms: 3.906
  training_iteration_time_ms: 54562.624
  update_time_ms: 2.822
timesteps_total: 4026000
training_iteration: 244

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.5374677002584
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 387
episodes_total: 65429
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.53429460525513
time_total_s: 13207.377618551254
timers:
  learn_throughput: 361.299
  learn_time_ms: 45668.561
  load_throughput: 3309756.524
  load_time_ms: 4.985
  training_iteration_time_ms: 58999.69
  update_time_ms: 2.612
timesteps_total: 3844500
training_iteration: 233

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28846153846153844
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 157.90825688073394
episode_reward_max: 2.0
episode_reward_mean: 1.4036697247706422
episode_reward_min: 0.0
episodes_this_iter: 109
episodes_total: 24444
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6880733944954128
  agent_1: 0.7155963302752294
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.37895584106445
time_total_s: 13292.786565065384
timers:
  learn_throughput: 450.316
  learn_time_ms: 36640.919
  load_throughput: 4802404.88
  load_time_ms: 3.436
  training_iteration_time_ms: 48249.862
  update_time_ms: 2.565
timesteps_total: 4389000
training_iteration: 266

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22093023255813954
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9044117647058824
  reward for individual goal_min: 0.0
episode_len_mean: 206.53
episode_reward_max: 2.0
episode_reward_mean: 1.09
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 23629
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.65589427947998
time_total_s: 13304.564112663269
timers:
  learn_throughput: 504.309
  learn_time_ms: 32718.024
  load_throughput: 5096247.073
  load_time_ms: 3.238
  training_iteration_time_ms: 43575.592
  update_time_ms: 2.505
timesteps_total: 4702500
training_iteration: 285

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.93
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 23834
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.50603365898132
time_total_s: 13317.4952480793
timers:
  learn_throughput: 441.358
  learn_time_ms: 37384.654
  load_throughput: 4587129.052
  load_time_ms: 3.597
  training_iteration_time_ms: 49009.923
  update_time_ms: 2.808
timesteps_total: 4389000
training_iteration: 266

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22077922077922077
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9927536231884058
  reward for individual goal_min: 0.5
episode_len_mean: 186.82
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 22100
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.30871319770813
time_total_s: 13320.568817138672
timers:
  learn_throughput: 401.035
  learn_time_ms: 41143.543
  load_throughput: 4367605.284
  load_time_ms: 3.778
  training_iteration_time_ms: 53926.716
  update_time_ms: 2.833
timesteps_total: 4042500
training_iteration: 245

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17721518987341772
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 193.24
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 22570
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.53343057632446
time_total_s: 13318.82487487793
timers:
  learn_throughput: 443.911
  learn_time_ms: 37169.623
  load_throughput: 4474806.572
  load_time_ms: 3.687
  training_iteration_time_ms: 49024.893
  update_time_ms: 2.592
timesteps_total: 4273500
training_iteration: 259

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2597402597402597
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 179.82
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 22106
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.47720909118652
time_total_s: 13337.419948101044
timers:
  learn_throughput: 448.039
  learn_time_ms: 36827.135
  load_throughput: 4683550.527
  load_time_ms: 3.523
  training_iteration_time_ms: 48248.153
  update_time_ms: 2.507
timesteps_total: 4306500
training_iteration: 261

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.96
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 23331
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.82626557350159
time_total_s: 13342.193444013596
timers:
  learn_throughput: 438.081
  learn_time_ms: 37664.304
  load_throughput: 4804471.936
  load_time_ms: 3.434
  training_iteration_time_ms: 49335.759
  update_time_ms: 2.633
timesteps_total: 4224000
training_iteration: 256

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9305555555555556
  reward for individual goal_min: 0.0
episode_len_mean: 194.3
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 24563
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.579294204711914
time_total_s: 13354.371641159058
timers:
  learn_throughput: 519.708
  learn_time_ms: 31748.576
  load_throughput: 4786064.634
  load_time_ms: 3.448
  training_iteration_time_ms: 42349.366
  update_time_ms: 2.521
timesteps_total: 4884000
training_iteration: 296

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9907407407407407
  reward for individual goal_min: 0.5
episode_len_mean: 52.48571428571429
episode_reward_max: 2.0
episode_reward_mean: 1.9904761904761905
episode_reward_min: 1.0
episodes_this_iter: 315
episodes_total: 42028
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9904761904761905
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 68.3920407295227
time_total_s: 13324.25836610794
timers:
  learn_throughput: 312.041
  learn_time_ms: 52877.589
  load_throughput: 3588704.652
  load_time_ms: 4.598
  training_iteration_time_ms: 68543.617
  update_time_ms: 3.159
timesteps_total: 3547500
training_iteration: 215

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.233333333333334
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974747474747475
  reward for individual goal_min: 0.5
episode_len_mean: 42.58354755784062
episode_reward_max: 2.0
episode_reward_mean: 1.9974293059125965
episode_reward_min: 1.0
episodes_this_iter: 389
episodes_total: 86676
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9974293059125964
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 52.13030982017517
time_total_s: 13179.744231462479
timers:
  learn_throughput: 451.061
  learn_time_ms: 36580.425
  load_throughput: 4244752.237
  load_time_ms: 3.887
  training_iteration_time_ms: 48109.245
  update_time_ms: 2.353
timesteps_total: 4620000
training_iteration: 280

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-172kuwc_58/checkpoint_000280/checkpoint-280
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974358974358974
  reward for individual goal_min: 0.5
episode_len_mean: 45.31165311653117
episode_reward_max: 2.0
episode_reward_mean: 1.997289972899729
episode_reward_min: 1.0
episodes_this_iter: 369
episodes_total: 62000
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.997289972899729
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 55.66251254081726
time_total_s: 13267.014325380325
timers:
  learn_throughput: 383.792
  learn_time_ms: 42991.99
  load_throughput: 4026930.14
  load_time_ms: 4.097
  training_iteration_time_ms: 55657.331
  update_time_ms: 2.664
timesteps_total: 4224000
training_iteration: 256

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 163.19
episode_reward_max: 2.0
episode_reward_mean: 1.41
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 24539
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.68578767776489
time_total_s: 13338.472352743149
timers:
  learn_throughput: 453.673
  learn_time_ms: 36369.794
  load_throughput: 4851659.796
  load_time_ms: 3.401
  training_iteration_time_ms: 47936.79
  update_time_ms: 2.564
timesteps_total: 4405500
training_iteration: 267

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15584415584415584
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9358974358974359
  reward for individual goal_min: 0.0
episode_len_mean: 199.4
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 23715
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.55577278137207
time_total_s: 13348.119885444641
timers:
  learn_throughput: 500.588
  learn_time_ms: 32961.258
  load_throughput: 5111378.18
  load_time_ms: 3.228
  training_iteration_time_ms: 43832.075
  update_time_ms: 2.499
timesteps_total: 4719000
training_iteration: 286

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 202.7
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 23914
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.828405141830444
time_total_s: 13366.32365322113
timers:
  learn_throughput: 440.771
  learn_time_ms: 37434.396
  load_throughput: 4629567.521
  load_time_ms: 3.564
  training_iteration_time_ms: 49077.878
  update_time_ms: 2.821
timesteps_total: 4405500
training_iteration: 267

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9927536231884058
  reward for individual goal_min: 0.5
episode_len_mean: 184.6
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 22256
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.20600247383118
time_total_s: 13363.19631242752
timers:
  learn_throughput: 395.373
  learn_time_ms: 41732.777
  load_throughput: 4220548.136
  load_time_ms: 3.909
  training_iteration_time_ms: 54959.578
  update_time_ms: 2.851
timesteps_total: 4042500
training_iteration: 245

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.946666666666665
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 375
episodes_total: 65804
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.390055894851685
time_total_s: 13265.767674446106
timers:
  learn_throughput: 362.287
  learn_time_ms: 45543.998
  load_throughput: 3332306.892
  load_time_ms: 4.952
  training_iteration_time_ms: 58823.425
  update_time_ms: 2.587
timesteps_total: 3861000
training_iteration: 234

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3148148148148148
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9393939393939394
  reward for individual goal_min: 0.0
episode_len_mean: 183.51
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 22198
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.350953817367554
time_total_s: 13383.770901918411
timers:
  learn_throughput: 449.659
  learn_time_ms: 36694.467
  load_throughput: 4760877.246
  load_time_ms: 3.466
  training_iteration_time_ms: 48093.416
  update_time_ms: 2.513
timesteps_total: 4323000
training_iteration: 262

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21686746987951808
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.0
episode_len_mean: 190.86
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 22188
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.305039167404175
time_total_s: 13375.873856306076
timers:
  learn_throughput: 399.233
  learn_time_ms: 41329.244
  load_throughput: 4401186.436
  load_time_ms: 3.749
  training_iteration_time_ms: 54243.447
  update_time_ms: 2.849
timesteps_total: 4059000
training_iteration: 246

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2088607594936709
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 186.09
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 23420
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.34157943725586
time_total_s: 13387.535023450851
timers:
  learn_throughput: 442.431
  learn_time_ms: 37293.987
  load_throughput: 4817079.377
  load_time_ms: 3.425
  training_iteration_time_ms: 48881.906
  update_time_ms: 2.634
timesteps_total: 4240500
training_iteration: 257

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9166666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 189.69
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 24649
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.42462730407715
time_total_s: 13396.796268463135
timers:
  learn_throughput: 520.108
  learn_time_ms: 31724.162
  load_throughput: 4837720.877
  load_time_ms: 3.411
  training_iteration_time_ms: 42352.152
  update_time_ms: 2.539
timesteps_total: 4900500
training_iteration: 297

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 94.75
episode_reward_max: 2.0
episode_reward_mean: 1.7666666666666666
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8833333333333333
  agent_1: 0.8833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24050632911392406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 184.46
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 22660
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.21917247772217
time_total_s: 13381.044047355652
timers:
  learn_throughput: 441.541
  learn_time_ms: 37369.127
  load_throughput: 4478455.197
  load_time_ms: 3.684
  training_iteration_time_ms: 49276.432
  update_time_ms: 2.565
timesteps_total: 4290000
training_iteration: 260

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.734177215189874
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 395
episodes_total: 87071
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.00274348258972
time_total_s: 13227.746974945068
timers:
  learn_throughput: 450.338
  learn_time_ms: 36639.137
  load_throughput: 4239993.138
  load_time_ms: 3.892
  training_iteration_time_ms: 48174.768
  update_time_ms: 2.353
timesteps_total: 4636500
training_iteration: 281

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1780821917808219
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9246575342465754
  reward for individual goal_min: 0.0
episode_len_mean: 198.57
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 23797
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.48
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.69901919364929
time_total_s: 13391.81890463829
timers:
  learn_throughput: 500.823
  learn_time_ms: 32945.775
  load_throughput: 5107945.117
  load_time_ms: 3.23
  training_iteration_time_ms: 43729.812
  update_time_ms: 2.481
timesteps_total: 4735500
training_iteration: 287

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24324324324324326
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.54
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 24633
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.76
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.87726640701294
time_total_s: 13386.349619150162
timers:
  learn_throughput: 454.147
  learn_time_ms: 36331.864
  load_throughput: 4862465.73
  load_time_ms: 3.393
  training_iteration_time_ms: 47839.682
  update_time_ms: 2.89
timesteps_total: 4422000
training_iteration: 268

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972527472527473
  reward for individual goal_min: 0.5
episode_len_mean: 44.62330623306233
episode_reward_max: 2.0
episode_reward_mean: 1.997289972899729
episode_reward_min: 1.0
episodes_this_iter: 369
episodes_total: 62369
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.997289972899729
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 54.370131969451904
time_total_s: 13321.384457349777
timers:
  learn_throughput: 384.705
  learn_time_ms: 42890.006
  load_throughput: 4016087.093
  load_time_ms: 4.108
  training_iteration_time_ms: 55507.508
  update_time_ms: 2.659
timesteps_total: 4240500
training_iteration: 257

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9940476190476191
  reward for individual goal_min: 0.5
episode_len_mean: 53.41479099678457
episode_reward_max: 2.0
episode_reward_mean: 1.9935691318327975
episode_reward_min: 1.0
episodes_this_iter: 311
episodes_total: 42339
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9935691318327974
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 68.72525930404663
time_total_s: 13392.983625411987
timers:
  learn_throughput: 311.223
  learn_time_ms: 53016.731
  load_throughput: 3651338.849
  load_time_ms: 4.519
  training_iteration_time_ms: 68744.592
  update_time_ms: 3.266
timesteps_total: 3564000
training_iteration: 216

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3026315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.99375
  reward for individual goal_min: 0.5
episode_len_mean: 163.6320754716981
episode_reward_max: 2.0
episode_reward_mean: 1.3679245283018868
episode_reward_min: 0.0
episodes_this_iter: 106
episodes_total: 24020
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6886792452830188
  agent_1: 0.6792452830188679
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.94490885734558
time_total_s: 13416.268562078476
timers:
  learn_throughput: 441.229
  learn_time_ms: 37395.581
  load_throughput: 4611029.263
  load_time_ms: 3.578
  training_iteration_time_ms: 49137.268
  update_time_ms: 2.81
timesteps_total: 4422000
training_iteration: 268

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9947368421052631
  reward for individual goal_min: 0.5
episode_len_mean: 149.51818181818183
episode_reward_max: 2.0
episode_reward_mean: 1.4545454545454546
episode_reward_min: 0.0
episodes_this_iter: 110
episodes_total: 22366
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7363636363636363
  agent_1: 0.7181818181818181
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.14668560028076
time_total_s: 13418.342998027802
timers:
  learn_throughput: 394.721
  learn_time_ms: 41801.684
  load_throughput: 4189860.209
  load_time_ms: 3.938
  training_iteration_time_ms: 54878.521
  update_time_ms: 2.866
timesteps_total: 4059000
training_iteration: 246

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974619289340102
  reward for individual goal_min: 0.5
episode_len_mean: 42.960835509138384
episode_reward_max: 2.0
episode_reward_mean: 1.9973890339425588
episode_reward_min: 1.0
episodes_this_iter: 383
episodes_total: 66187
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973890339425587
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 58.72654151916504
time_total_s: 13324.494215965271
timers:
  learn_throughput: 362.578
  learn_time_ms: 45507.499
  load_throughput: 3326684.516
  load_time_ms: 4.96
  training_iteration_time_ms: 58800.004
  update_time_ms: 2.579
timesteps_total: 3877500
training_iteration: 235

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2792207792207792
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.972972972972973
  reward for individual goal_min: 0.0
episode_len_mean: 179.5
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 22288
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.80569362640381
time_total_s: 13431.576595544815
timers:
  learn_throughput: 450.601
  learn_time_ms: 36617.741
  load_throughput: 4771644.006
  load_time_ms: 3.458
  training_iteration_time_ms: 48033.837
  update_time_ms: 2.507
timesteps_total: 4339500
training_iteration: 263

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2602739726027397
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9383561643835616
  reward for individual goal_min: 0.0
episode_len_mean: 186.58
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 24740
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.93749713897705
time_total_s: 13439.733765602112
timers:
  learn_throughput: 516.932
  learn_time_ms: 31919.071
  load_throughput: 4898916.669
  load_time_ms: 3.368
  training_iteration_time_ms: 42566.229
  update_time_ms: 2.576
timesteps_total: 4917000
training_iteration: 298

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19736842105263158
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 188.97
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 23509
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.24256348609924
time_total_s: 13434.77758693695
timers:
  learn_throughput: 450.657
  learn_time_ms: 36613.25
  load_throughput: 4881638.734
  load_time_ms: 3.38
  training_iteration_time_ms: 48166.845
  update_time_ms: 2.585
timesteps_total: 4257000
training_iteration: 258

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.6
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 22283
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.7974750995636
time_total_s: 13429.67133140564
timers:
  learn_throughput: 397.485
  learn_time_ms: 41510.985
  load_throughput: 4367302.1
  load_time_ms: 3.778
  training_iteration_time_ms: 54410.764
  update_time_ms: 2.843
timesteps_total: 4075500
training_iteration: 247

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.13970588235294118
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.44
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 22758
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.67277002334595
time_total_s: 13430.716817378998
timers:
  learn_throughput: 441.623
  learn_time_ms: 37362.17
  load_throughput: 4518632.253
  load_time_ms: 3.652
  training_iteration_time_ms: 49317.12
  update_time_ms: 2.557
timesteps_total: 4306500
training_iteration: 261

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9444444444444444
  reward for individual goal_min: 0.0
episode_len_mean: 188.72
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 23887
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.28651428222656
time_total_s: 13434.105418920517
timers:
  learn_throughput: 500.738
  learn_time_ms: 32951.383
  load_throughput: 5123714.815
  load_time_ms: 3.22
  training_iteration_time_ms: 43706.07
  update_time_ms: 2.485
timesteps_total: 4752000
training_iteration: 288

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9920634920634921
  reward for individual goal_min: 0.5
episode_len_mean: 42.27577319587629
episode_reward_max: 2.0
episode_reward_mean: 1.9922680412371134
episode_reward_min: 1.0
episodes_this_iter: 388
episodes_total: 87459
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974226804123711
  agent_1: 0.9948453608247423
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.66510057449341
time_total_s: 13275.412075519562
timers:
  learn_throughput: 451.004
  learn_time_ms: 36585.023
  load_throughput: 4278499.688
  load_time_ms: 3.856
  training_iteration_time_ms: 48119.753
  update_time_ms: 2.346
timesteps_total: 4653000
training_iteration: 282

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3157894736842105
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9852941176470589
  reward for individual goal_min: 0.0
episode_len_mean: 166.61764705882354
episode_reward_max: 2.0
episode_reward_mean: 1.3431372549019607
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 24735
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6470588235294118
  agent_1: 0.696078431372549
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.15333032608032
time_total_s: 13434.502949476242
timers:
  learn_throughput: 455.035
  learn_time_ms: 36260.973
  load_throughput: 4855812.857
  load_time_ms: 3.398
  training_iteration_time_ms: 47711.655
  update_time_ms: 2.887
timesteps_total: 4438500
training_iteration: 269

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21951219512195122
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 189.16
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 24106
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.37736463546753
time_total_s: 13461.645926713943
timers:
  learn_throughput: 446.271
  learn_time_ms: 36973.074
  load_throughput: 4613457.593
  load_time_ms: 3.576
  training_iteration_time_ms: 48687.385
  update_time_ms: 2.827
timesteps_total: 4438500
training_iteration: 269

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.777188328912466
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 377
episodes_total: 62746
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 56.051554441452026
time_total_s: 13377.43601179123
timers:
  learn_throughput: 385.459
  learn_time_ms: 42806.069
  load_throughput: 4076313.26
  load_time_ms: 4.048
  training_iteration_time_ms: 55401.219
  update_time_ms: 2.66
timesteps_total: 4257000
training_iteration: 258

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19285714285714287
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9873417721518988
  reward for individual goal_min: 0.0
episode_len_mean: 173.78
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 24833
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.56222486495972
time_total_s: 13482.295990467072
timers:
  learn_throughput: 519.228
  learn_time_ms: 31777.976
  load_throughput: 4888120.921
  load_time_ms: 3.376
  training_iteration_time_ms: 42505.486
  update_time_ms: 2.583
timesteps_total: 4933500
training_iteration: 299

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2702702702702703
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.99375
  reward for individual goal_min: 0.5
episode_len_mean: 166.05
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 22464
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.87056827545166
time_total_s: 13473.213566303253
timers:
  learn_throughput: 393.77
  learn_time_ms: 41902.678
  load_throughput: 4198095.007
  load_time_ms: 3.93
  training_iteration_time_ms: 54998.041
  update_time_ms: 2.83
timesteps_total: 4075500
training_iteration: 247

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24342105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9571428571428572
  reward for individual goal_min: 0.0
episode_len_mean: 177.63
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 22378
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.345492124557495
time_total_s: 13479.922087669373
timers:
  learn_throughput: 450.756
  learn_time_ms: 36605.14
  load_throughput: 4726799.443
  load_time_ms: 3.491
  training_iteration_time_ms: 48083.411
  update_time_ms: 2.49
timesteps_total: 4356000
training_iteration: 264

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27631578947368424
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9779411764705882
  reward for individual goal_min: 0.0
episode_len_mean: 175.44
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 23602
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.73897171020508
time_total_s: 13482.516558647156
timers:
  learn_throughput: 452.257
  learn_time_ms: 36483.642
  load_throughput: 4883050.936
  load_time_ms: 3.379
  training_iteration_time_ms: 48077.861
  update_time_ms: 2.593
timesteps_total: 4273500
training_iteration: 259

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9910714285714286
  reward for individual goal_min: 0.5
episode_len_mean: 48.55917159763314
episode_reward_max: 2.0
episode_reward_mean: 1.9911242603550297
episode_reward_min: 1.0
episodes_this_iter: 338
episodes_total: 42677
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9970414201183432
  agent_1: 0.9940828402366864
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 70.28452658653259
time_total_s: 13463.26815199852
timers:
  learn_throughput: 310.543
  learn_time_ms: 53132.66
  load_throughput: 3667476.55
  load_time_ms: 4.499
  training_iteration_time_ms: 68863.95
  update_time_ms: 3.238
timesteps_total: 3580500
training_iteration: 217

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.00797872340426
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 376
episodes_total: 66563
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 56.994425535202026
time_total_s: 13381.488641500473
timers:
  learn_throughput: 363.241
  learn_time_ms: 45424.351
  load_throughput: 3363254.103
  load_time_ms: 4.906
  training_iteration_time_ms: 58669.536
  update_time_ms: 2.553
timesteps_total: 3894000
training_iteration: 236

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.363013698630137
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.94
  reward for individual goal_min: 0.0
episode_len_mean: 160.72277227722773
episode_reward_max: 2.0
episode_reward_mean: 1.386138613861386
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 23988
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6633663366336634
  agent_1: 0.7227722772277227
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.78169322013855
time_total_s: 13477.887112140656
timers:
  learn_throughput: 502.907
  learn_time_ms: 32809.248
  load_throughput: 5208002.167
  load_time_ms: 3.168
  training_iteration_time_ms: 43480.644
  update_time_ms: 2.507
timesteps_total: 4768500
training_iteration: 289

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2076923076923077
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939759036144579
  reward for individual goal_min: 0.5
episode_len_mean: 158.873786407767
episode_reward_max: 2.0
episode_reward_mean: 1.3786407766990292
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 22861
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6893203883495146
  agent_1: 0.6893203883495146
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.12178921699524
time_total_s: 13479.838606595993
timers:
  learn_throughput: 442.955
  learn_time_ms: 37249.828
  load_throughput: 4514682.271
  load_time_ms: 3.655
  training_iteration_time_ms: 49105.799
  update_time_ms: 2.565
timesteps_total: 4323000
training_iteration: 262

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24305555555555555
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 171.53
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 22377
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.57785725593567
time_total_s: 13485.249188661575
timers:
  learn_throughput: 397.604
  learn_time_ms: 41498.568
  load_throughput: 4375668.844
  load_time_ms: 3.771
  training_iteration_time_ms: 54468.835
  update_time_ms: 2.819
timesteps_total: 4092000
training_iteration: 248

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.52061855670103
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 388
episodes_total: 87847
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.41617512702942
time_total_s: 13322.828250646591
timers:
  learn_throughput: 452.487
  learn_time_ms: 36465.11
  load_throughput: 4275856.24
  load_time_ms: 3.859
  training_iteration_time_ms: 47996.037
  update_time_ms: 2.336
timesteps_total: 4669500
training_iteration: 283

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2894736842105263
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 168.7
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 24832
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.03381586074829
time_total_s: 13482.53676533699
timers:
  learn_throughput: 454.85
  learn_time_ms: 36275.69
  load_throughput: 4890504.342
  load_time_ms: 3.374
  training_iteration_time_ms: 47756.275
  update_time_ms: 2.883
timesteps_total: 4455000
training_iteration: 270

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20833333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935897435897436
  reward for individual goal_min: 0.5
episode_len_mean: 178.63
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 24198
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.018470287323
time_total_s: 13506.664397001266
timers:
  learn_throughput: 449.368
  learn_time_ms: 36718.213
  load_throughput: 4645634.423
  load_time_ms: 3.552
  training_iteration_time_ms: 48264.533
  update_time_ms: 2.843
timesteps_total: 4455000
training_iteration: 270

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9946236559139785
  reward for individual goal_min: 0.5
episode_len_mean: 45.78212290502793
episode_reward_max: 2.0
episode_reward_mean: 1.994413407821229
episode_reward_min: 1.0
episodes_this_iter: 358
episodes_total: 63104
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972067039106145
  agent_1: 0.9972067039106145
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.379141330718994
time_total_s: 13429.815153121948
timers:
  learn_throughput: 388.721
  learn_time_ms: 42446.894
  load_throughput: 4062482.595
  load_time_ms: 4.062
  training_iteration_time_ms: 55026.138
  update_time_ms: 2.685
timesteps_total: 4273500
training_iteration: 259

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17682926829268292
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.96
  reward for individual goal_min: 0.0
episode_len_mean: 200.92
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 22459
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.169976472854614
time_total_s: 13526.092064142227
timers:
  learn_throughput: 454.045
  learn_time_ms: 36340.031
  load_throughput: 4750256.78
  load_time_ms: 3.473
  training_iteration_time_ms: 47793.502
  update_time_ms: 2.479
timesteps_total: 4372500
training_iteration: 265

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 93.98333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.8333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9166666666666666
  agent_1: 0.9166666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20422535211267606
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9295774647887324
  reward for individual goal_min: 0.0
episode_len_mean: 182.55
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 24922
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.612428188323975
time_total_s: 13537.908418655396
timers:
  learn_throughput: 514.454
  learn_time_ms: 32072.847
  load_throughput: 4932787.068
  load_time_ms: 3.345
  training_iteration_time_ms: 42874.709
  update_time_ms: 2.588
timesteps_total: 4950000
training_iteration: 300

Starting final evaluation!
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23376623376623376
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9662162162162162
  reward for individual goal_min: 0.0
episode_len_mean: 193.69
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 24073
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.99292302131653
time_total_s: 13519.880035161972
timers:
  learn_throughput: 507.91
  learn_time_ms: 32486.047
  load_throughput: 5259934.941
  load_time_ms: 3.137
  training_iteration_time_ms: 43098.687
  update_time_ms: 2.49
timesteps_total: 4785000
training_iteration: 290

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30128205128205127
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 162.11428571428573
episode_reward_max: 2.0
episode_reward_mean: 1.3714285714285714
episode_reward_min: 0.0
episodes_this_iter: 105
episodes_total: 22569
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6857142857142857
  agent_1: 0.6857142857142857
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.45697903633118
time_total_s: 13529.670545339584
timers:
  learn_throughput: 393.254
  learn_time_ms: 41957.634
  load_throughput: 4147674.088
  load_time_ms: 3.978
  training_iteration_time_ms: 55086.351
  update_time_ms: 2.834
timesteps_total: 4092000
training_iteration: 248

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23026315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9859154929577465
  reward for individual goal_min: 0.0
episode_len_mean: 178.07
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 22952
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.59628343582153
time_total_s: 13529.434890031815
timers:
  learn_throughput: 443.483
  learn_time_ms: 37205.452
  load_throughput: 4510533.396
  load_time_ms: 3.658
  training_iteration_time_ms: 49065.052
  update_time_ms: 2.578
timesteps_total: 4339500
training_iteration: 263

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95
  reward for individual goal_min: 0.5
episode_len_mean: 98.9
episode_reward_max: 2.0
episode_reward_mean: 1.65
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8333333333333334
  agent_1: 0.8166666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20270270270270271
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.71
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 23692
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 64.11530780792236
time_total_s: 13546.631866455078
timers:
  learn_throughput: 450.046
  learn_time_ms: 36662.923
  load_throughput: 4894343.423
  load_time_ms: 3.371
  training_iteration_time_ms: 48297.905
  update_time_ms: 2.612
timesteps_total: 4290000
training_iteration: 260

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.784415584415584
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 385
episodes_total: 66948
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.19054293632507
time_total_s: 13439.679184436798
timers:
  learn_throughput: 363.858
  learn_time_ms: 45347.416
  load_throughput: 3366886.533
  load_time_ms: 4.901
  training_iteration_time_ms: 58592.62
  update_time_ms: 2.552
timesteps_total: 3910500
training_iteration: 237

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9977064220183486
  reward for individual goal_min: 0.5
episode_len_mean: 42.28644501278772
episode_reward_max: 2.0
episode_reward_mean: 1.9974424552429668
episode_reward_min: 1.0
episodes_this_iter: 391
episodes_total: 88238
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974424552429667
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 47.51323986053467
time_total_s: 13370.341490507126
timers:
  learn_throughput: 454.137
  learn_time_ms: 36332.634
  load_throughput: 4266182.715
  load_time_ms: 3.868
  training_iteration_time_ms: 47831.312
  update_time_ms: 2.342
timesteps_total: 4686000
training_iteration: 284

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22142857142857142
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 172.27
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 22474
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.405070543289185
time_total_s: 13538.654259204865
timers:
  learn_throughput: 400.803
  learn_time_ms: 41167.346
  load_throughput: 4359708.706
  load_time_ms: 3.785
  training_iteration_time_ms: 54084.736
  update_time_ms: 2.821
timesteps_total: 4108500
training_iteration: 249

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3355263157894737
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9938271604938271
  reward for individual goal_min: 0.5
episode_len_mean: 154.55555555555554
episode_reward_max: 2.0
episode_reward_mean: 1.4166666666666667
episode_reward_min: 0.0
episodes_this_iter: 108
episodes_total: 24940
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7222222222222222
  agent_1: 0.6944444444444444
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.79415559768677
time_total_s: 13530.330920934677
timers:
  learn_throughput: 456.009
  learn_time_ms: 36183.523
  load_throughput: 4884877.677
  load_time_ms: 3.378
  training_iteration_time_ms: 47693.419
  update_time_ms: 2.87
timesteps_total: 4471500
training_iteration: 271

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24305555555555555
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9772727272727273
  reward for individual goal_min: 0.5
episode_len_mean: 182.95
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 24290
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.142284631729126
time_total_s: 13556.806681632996
timers:
  learn_throughput: 450.373
  learn_time_ms: 36636.28
  load_throughput: 4643670.596
  load_time_ms: 3.553
  training_iteration_time_ms: 48240.476
  update_time_ms: 2.837
timesteps_total: 4471500
training_iteration: 271

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9970760233918129
  reward for individual goal_min: 0.5
episode_len_mean: 48.489614243323444
episode_reward_max: 2.0
episode_reward_mean: 1.997032640949555
episode_reward_min: 1.0
episodes_this_iter: 337
episodes_total: 43014
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9970326409495549
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 70.92950105667114
time_total_s: 13534.197653055191
timers:
  learn_throughput: 309.959
  learn_time_ms: 53232.77
  load_throughput: 3745826.419
  load_time_ms: 4.405
  training_iteration_time_ms: 68995.618
  update_time_ms: 3.241
timesteps_total: 3597000
training_iteration: 218

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20481927710843373
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9657534246575342
  reward for individual goal_min: 0.0
episode_len_mean: 199.09
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 22542
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.54
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.35879302024841
time_total_s: 13571.450857162476
timers:
  learn_throughput: 458.661
  learn_time_ms: 35974.288
  load_throughput: 4785965.339
  load_time_ms: 3.448
  training_iteration_time_ms: 47387.364
  update_time_ms: 2.493
timesteps_total: 4389000
training_iteration: 266

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27205882352941174
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9797297297297297
  reward for individual goal_min: 0.0
episode_len_mean: 164.95
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 24173
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.693145513534546
time_total_s: 13563.573180675507
timers:
  learn_throughput: 505.974
  learn_time_ms: 32610.364
  load_throughput: 5337252.325
  load_time_ms: 3.091
  training_iteration_time_ms: 43287.266
  update_time_ms: 2.503
timesteps_total: 4801500
training_iteration: 291

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 44.88333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.9833333333333334
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9833333333333333
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.989247311827957
  reward for individual goal_min: 0.5
episode_len_mean: 45.32786885245902
episode_reward_max: 2.0
episode_reward_mean: 1.989071038251366
episode_reward_min: 1.0
episodes_this_iter: 366
episodes_total: 63470
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.994535519125683
  agent_1: 0.994535519125683
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.86086583137512
time_total_s: 13487.676018953323
timers:
  learn_throughput: 391.084
  learn_time_ms: 42190.413
  load_throughput: 4045195.637
  load_time_ms: 4.079
  training_iteration_time_ms: 54771.409
  update_time_ms: 2.627
timesteps_total: 4290000
training_iteration: 260

Evaluation results over 10 episodes for each goal
{'[0, 0, 1]': 2.0, '[0, 1, 0]': 2.0, '[1, 0, 0]': 1.9, '[0, 1, 1]': 2.0, '[1, 0, 1]': 2.0, '[1, 1, 0]': 1.6}
Moviepy - Building video /gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-091_srff7m/trained_agent.mp4.
Moviepy - Writing video /gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-091_srff7m/trained_agent.mp4

Moviepy - Done !
Moviepy - video ready /gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-091_srff7m/trained_agent.mp4
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1506849315068493
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.43
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 22656
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.408029317855835
time_total_s: 13587.07857465744
timers:
  learn_throughput: 390.13
  learn_time_ms: 42293.645
  load_throughput: 4131110.527
  load_time_ms: 3.994
  training_iteration_time_ms: 55572.408
  update_time_ms: 2.865
timesteps_total: 4108500
training_iteration: 249

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2012987012987013
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 186.48
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 23042
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.53
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.8778772354126
time_total_s: 13577.312767267227
timers:
  learn_throughput: 445.705
  learn_time_ms: 37020.006
  load_throughput: 4520757.488
  load_time_ms: 3.65
  training_iteration_time_ms: 48799.477
  update_time_ms: 2.585
timesteps_total: 4356000
training_iteration: 264

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19186046511627908
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 203.07
episode_reward_max: 2.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 23773
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.9365336894989
time_total_s: 13594.568400144577
timers:
  learn_throughput: 449.604
  learn_time_ms: 36698.961
  load_throughput: 4914048.27
  load_time_ms: 3.358
  training_iteration_time_ms: 48257.669
  update_time_ms: 2.627
timesteps_total: 4306500
training_iteration: 261

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974619289340102
  reward for individual goal_min: 0.5
episode_len_mean: 42.18622448979592
episode_reward_max: 2.0
episode_reward_mean: 1.9974489795918366
episode_reward_min: 1.0
episodes_this_iter: 392
episodes_total: 88630
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9974489795918368
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 47.87230324745178
time_total_s: 13418.213793754578
timers:
  learn_throughput: 454.516
  learn_time_ms: 36302.36
  load_throughput: 4272292.762
  load_time_ms: 3.862
  training_iteration_time_ms: 47799.963
  update_time_ms: 2.353
timesteps_total: 4702500
training_iteration: 285

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30405405405405406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.98
  reward for individual goal_min: 0.5
episode_len_mean: 170.75
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 25036
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.91686773300171
time_total_s: 13577.247788667679
timers:
  learn_throughput: 458.147
  learn_time_ms: 36014.606
  load_throughput: 4907462.382
  load_time_ms: 3.362
  training_iteration_time_ms: 47497.088
  update_time_ms: 2.879
timesteps_total: 4488000
training_iteration: 272

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23943661971830985
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.98
  reward for individual goal_min: 0.5
episode_len_mean: 179.85
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 24382
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.06662178039551
time_total_s: 13601.873303413391
timers:
  learn_throughput: 454.208
  learn_time_ms: 36326.975
  load_throughput: 4647849.616
  load_time_ms: 3.55
  training_iteration_time_ms: 47851.056
  update_time_ms: 2.865
timesteps_total: 4488000
training_iteration: 272

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9859154929577465
  reward for individual goal_min: 0.0
episode_len_mean: 179.1
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 22563
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.50416445732117
time_total_s: 13594.158423662186
timers:
  learn_throughput: 398.389
  learn_time_ms: 41416.786
  load_throughput: 4384262.121
  load_time_ms: 3.763
  training_iteration_time_ms: 54359.027
  update_time_ms: 2.834
timesteps_total: 4125000
training_iteration: 250

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974093264248705
  reward for individual goal_min: 0.5
episode_len_mean: 43.356955380577425
episode_reward_max: 2.0
episode_reward_mean: 1.9973753280839894
episode_reward_min: 1.0
episodes_this_iter: 381
episodes_total: 67329
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973753280839895
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 57.54080009460449
time_total_s: 13497.219984531403
timers:
  learn_throughput: 364.326
  learn_time_ms: 45289.081
  load_throughput: 3365609.38
  load_time_ms: 4.903
  training_iteration_time_ms: 58520.109
  update_time_ms: 2.552
timesteps_total: 3927000
training_iteration: 238

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9605263157894737
  reward for individual goal_min: 0.0
episode_len_mean: 193.42
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 22627
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.68308758735657
time_total_s: 13616.133944749832
timers:
  learn_throughput: 462.475
  learn_time_ms: 35677.614
  load_throughput: 4786130.833
  load_time_ms: 3.447
  training_iteration_time_ms: 47098.398
  update_time_ms: 2.509
timesteps_total: 4405500
training_iteration: 267

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9444444444444444
  reward for individual goal_min: 0.0
episode_len_mean: 178.02
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 24265
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.298054695129395
time_total_s: 13608.871235370636
timers:
  learn_throughput: 504.325
  learn_time_ms: 32716.991
  load_throughput: 5357249.152
  load_time_ms: 3.08
  training_iteration_time_ms: 43439.189
  update_time_ms: 2.475
timesteps_total: 4818000
training_iteration: 292

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939759036144579
  reward for individual goal_min: 0.5
episode_len_mean: 49.445427728613566
episode_reward_max: 2.0
episode_reward_mean: 1.9941002949852507
episode_reward_min: 1.0
episodes_this_iter: 339
episodes_total: 43353
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9941002949852508
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 71.11535167694092
time_total_s: 13605.313004732132
timers:
  learn_throughput: 309.135
  learn_time_ms: 53374.709
  load_throughput: 3784706.957
  load_time_ms: 4.36
  training_iteration_time_ms: 69121.003
  update_time_ms: 3.146
timesteps_total: 3613500
training_iteration: 219

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9890710382513661
  reward for individual goal_min: 0.5
episode_len_mean: 46.374647887323945
episode_reward_max: 2.0
episode_reward_mean: 1.9887323943661972
episode_reward_min: 1.0
episodes_this_iter: 355
episodes_total: 63825
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9915492957746479
  agent_1: 0.9971830985915493
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.65859937667847
time_total_s: 13539.334618330002
timers:
  learn_throughput: 394.979
  learn_time_ms: 41774.362
  load_throughput: 4041014.837
  load_time_ms: 4.083
  training_iteration_time_ms: 54194.843
  update_time_ms: 2.624
timesteps_total: 4306500
training_iteration: 261

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21794871794871795
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.5
episode_len_mean: 188.2
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 23131
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.8150360584259
time_total_s: 13627.127803325653
timers:
  learn_throughput: 444.472
  learn_time_ms: 37122.724
  load_throughput: 4512945.289
  load_time_ms: 3.656
  training_iteration_time_ms: 48909.691
  update_time_ms: 2.597
timesteps_total: 4372500
training_iteration: 265

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 185.07
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 23864
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.51869559288025
time_total_s: 13644.087095737457
timers:
  learn_throughput: 451.508
  learn_time_ms: 36544.243
  load_throughput: 4922962.057
  load_time_ms: 3.352
  training_iteration_time_ms: 48033.853
  update_time_ms: 2.617
timesteps_total: 4323000
training_iteration: 262

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.5
episode_len_mean: 170.86
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 25130
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.41064190864563
time_total_s: 13624.658430576324
timers:
  learn_throughput: 456.795
  learn_time_ms: 36121.233
  load_throughput: 4926782.137
  load_time_ms: 3.349
  training_iteration_time_ms: 47633.986
  update_time_ms: 2.88
timesteps_total: 4504500
training_iteration: 273

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.66062176165803
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 386
episodes_total: 89016
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.43163323402405
time_total_s: 13466.645426988602
timers:
  learn_throughput: 454.293
  learn_time_ms: 36320.179
  load_throughput: 4284035.062
  load_time_ms: 3.852
  training_iteration_time_ms: 47842.291
  update_time_ms: 2.372
timesteps_total: 4719000
training_iteration: 286

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1891891891891892
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.52
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 22748
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.75825619697571
time_total_s: 13641.836830854416
timers:
  learn_throughput: 389.187
  learn_time_ms: 42396.049
  load_throughput: 4088281.24
  load_time_ms: 4.036
  training_iteration_time_ms: 55743.366
  update_time_ms: 3.197
timesteps_total: 4125000
training_iteration: 250

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22602739726027396
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9662162162162162
  reward for individual goal_min: 0.0
episode_len_mean: 180.33
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 24470
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.231698989868164
time_total_s: 13650.10500240326
timers:
  learn_throughput: 454.826
  learn_time_ms: 36277.585
  load_throughput: 4661091.085
  load_time_ms: 3.54
  training_iteration_time_ms: 47854.725
  update_time_ms: 2.601
timesteps_total: 4504500
training_iteration: 273

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21830985915492956
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 168.3
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 22663
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.099488258361816
time_total_s: 13650.257911920547
timers:
  learn_throughput: 397.708
  learn_time_ms: 41487.722
  load_throughput: 4340377.429
  load_time_ms: 3.802
  training_iteration_time_ms: 54507.866
  update_time_ms: 2.822
timesteps_total: 4141500
training_iteration: 251

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8888888888888888
  reward for individual goal_min: 0.0
episode_len_mean: 195.78
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 22711
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.49
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.86876368522644
time_total_s: 13664.002708435059
timers:
  learn_throughput: 463.601
  learn_time_ms: 35590.926
  load_throughput: 4849042.257
  load_time_ms: 3.403
  training_iteration_time_ms: 47006.172
  update_time_ms: 2.519
timesteps_total: 4422000
training_iteration: 268

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.5617128463476
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 397
episodes_total: 67726
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.10512018203735
time_total_s: 13555.32510471344
timers:
  learn_throughput: 364.774
  learn_time_ms: 45233.49
  load_throughput: 3375952.624
  load_time_ms: 4.888
  training_iteration_time_ms: 58424.114
  update_time_ms: 2.595
timesteps_total: 3943500
training_iteration: 239

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28289473684210525
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.8819444444444444
  reward for individual goal_min: 0.0
episode_len_mean: 186.19
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 24350
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.262786626815796
time_total_s: 13654.134021997452
timers:
  learn_throughput: 504.169
  learn_time_ms: 32727.092
  load_throughput: 5290737.122
  load_time_ms: 3.119
  training_iteration_time_ms: 43519.31
  update_time_ms: 2.479
timesteps_total: 4834500
training_iteration: 293

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9918918918918919
  reward for individual goal_min: 0.5
episode_len_mean: 46.813559322033896
episode_reward_max: 2.0
episode_reward_mean: 1.9915254237288136
episode_reward_min: 1.0
episodes_this_iter: 354
episodes_total: 64179
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9943502824858758
  agent_1: 0.9971751412429378
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.72351288795471
time_total_s: 13591.058131217957
timers:
  learn_throughput: 397.382
  learn_time_ms: 41521.773
  load_throughput: 4035453.862
  load_time_ms: 4.089
  training_iteration_time_ms: 53861.536
  update_time_ms: 2.621
timesteps_total: 4323000
training_iteration: 262

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 168.51
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 25228
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.30088210105896
time_total_s: 13670.959312677383
timers:
  learn_throughput: 459.21
  learn_time_ms: 35931.297
  load_throughput: 4904193.43
  load_time_ms: 3.364
  training_iteration_time_ms: 47360.036
  update_time_ms: 2.902
timesteps_total: 4521000
training_iteration: 274

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.13513513513513514
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9691358024691358
  reward for individual goal_min: 0.0
episode_len_mean: 185.54
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 23219
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.56910228729248
time_total_s: 13676.696905612946
timers:
  learn_throughput: 442.873
  learn_time_ms: 37256.713
  load_throughput: 4664735.508
  load_time_ms: 3.537
  training_iteration_time_ms: 49071.962
  update_time_ms: 2.626
timesteps_total: 4389000
training_iteration: 266

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20987654320987653
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 189.3
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 23951
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.95154333114624
time_total_s: 13694.038639068604
timers:
  learn_throughput: 448.357
  learn_time_ms: 36801.023
  load_throughput: 4992174.509
  load_time_ms: 3.305
  training_iteration_time_ms: 48320.966
  update_time_ms: 2.635
timesteps_total: 4339500
training_iteration: 263

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.58701298701299
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 385
episodes_total: 89401
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.75266432762146
time_total_s: 13515.398091316223
timers:
  learn_throughput: 453.957
  learn_time_ms: 36347.026
  load_throughput: 4290542.161
  load_time_ms: 3.846
  training_iteration_time_ms: 47854.868
  update_time_ms: 2.378
timesteps_total: 4735500
training_iteration: 287

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21621621621621623
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9805194805194806
  reward for individual goal_min: 0.0
episode_len_mean: 177.78
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 24563
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.59379458427429
time_total_s: 13698.698796987534
timers:
  learn_throughput: 455.133
  learn_time_ms: 36253.146
  load_throughput: 4701783.125
  load_time_ms: 3.509
  training_iteration_time_ms: 47850.263
  update_time_ms: 2.589
timesteps_total: 4521000
training_iteration: 274

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2733333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 166.73
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 22848
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.02644467353821
time_total_s: 13696.863275527954
timers:
  learn_throughput: 388.018
  learn_time_ms: 42523.758
  load_throughput: 4086181.17
  load_time_ms: 4.038
  training_iteration_time_ms: 55825.181
  update_time_ms: 3.19
timesteps_total: 4141500
training_iteration: 251

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 48.61666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.9833333333333334
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9833333333333333
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2191780821917808
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.974025974025974
  reward for individual goal_min: 0.0
episode_len_mean: 187.11
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 24438
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.50340557098389
time_total_s: 13697.637427568436
timers:
  learn_throughput: 503.368
  learn_time_ms: 32779.209
  load_throughput: 5263535.389
  load_time_ms: 3.135
  training_iteration_time_ms: 43537.325
  update_time_ms: 2.482
timesteps_total: 4851000
training_iteration: 294

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 47.84795321637427
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 342
episodes_total: 43695
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 78.93600463867188
time_total_s: 13684.249009370804
timers:
  learn_throughput: 308.025
  learn_time_ms: 53567.051
  load_throughput: 3766723.779
  load_time_ms: 4.38
  training_iteration_time_ms: 69358.563
  update_time_ms: 3.078
timesteps_total: 3630000
training_iteration: 220

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-19wecnkw4d/checkpoint_000220/checkpoint-220
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2638888888888889
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9594594594594594
  reward for individual goal_min: 0.0
episode_len_mean: 173.74
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 22808
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.1869854927063
time_total_s: 13712.189693927765
timers:
  learn_throughput: 463.976
  learn_time_ms: 35562.206
  load_throughput: 4869376.675
  load_time_ms: 3.389
  training_iteration_time_ms: 46993.15
  update_time_ms: 2.512
timesteps_total: 4438500
training_iteration: 269

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2463768115942029
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 155.58252427184465
episode_reward_max: 2.0
episode_reward_mean: 1.3786407766990292
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 22766
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7184466019417476
  agent_1: 0.6601941747572816
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.919153451919556
time_total_s: 13705.177065372467
timers:
  learn_throughput: 397.404
  learn_time_ms: 41519.418
  load_throughput: 4317281.098
  load_time_ms: 3.822
  training_iteration_time_ms: 54563.537
  update_time_ms: 2.788
timesteps_total: 4158000
training_iteration: 252

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.81666666666667
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.202099737532805
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 381
episodes_total: 68107
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 62.89519023895264
time_total_s: 13618.220294952393
timers:
  learn_throughput: 366.368
  learn_time_ms: 45036.707
  load_throughput: 3410154.478
  load_time_ms: 4.838
  training_iteration_time_ms: 58138.822
  update_time_ms: 2.619
timesteps_total: 3960000
training_iteration: 240

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-184s1jdk42/checkpoint_000240/checkpoint-240
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34810126582278483
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9848484848484849
  reward for individual goal_min: 0.5
episode_len_mean: 178.27
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 25321
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.06993007659912
time_total_s: 13718.029242753983
timers:
  learn_throughput: 460.478
  learn_time_ms: 35832.364
  load_throughput: 4884257.121
  load_time_ms: 3.378
  training_iteration_time_ms: 47223.972
  update_time_ms: 2.904
timesteps_total: 4537500
training_iteration: 275

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972972972972973
  reward for individual goal_min: 0.5
episode_len_mean: 44.86684782608695
episode_reward_max: 2.0
episode_reward_mean: 1.997282608695652
episode_reward_min: 1.0
episodes_this_iter: 368
episodes_total: 64547
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972826086956522
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 52.16219663619995
time_total_s: 13643.220327854156
timers:
  learn_throughput: 399.673
  learn_time_ms: 41283.801
  load_throughput: 4040849.672
  load_time_ms: 4.083
  training_iteration_time_ms: 53559.325
  update_time_ms: 2.611
timesteps_total: 4339500
training_iteration: 263

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.13855421686746988
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 197.8
episode_reward_max: 2.0
episode_reward_mean: 1.11
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 23302
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.48
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.86304187774658
time_total_s: 13727.559947490692
timers:
  learn_throughput: 441.346
  learn_time_ms: 37385.672
  load_throughput: 4668480.11
  load_time_ms: 3.534
  training_iteration_time_ms: 49298.738
  update_time_ms: 2.615
timesteps_total: 4405500
training_iteration: 267

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2905405405405405
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 164.0
episode_reward_max: 2.0
episode_reward_mean: 1.3823529411764706
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 24053
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6862745098039216
  agent_1: 0.696078431372549
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.51878213882446
time_total_s: 13744.557421207428
timers:
  learn_throughput: 444.916
  learn_time_ms: 37085.637
  load_throughput: 4831405.314
  load_time_ms: 3.415
  training_iteration_time_ms: 48681.005
  update_time_ms: 2.646
timesteps_total: 4356000
training_iteration: 264

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9946808510638298
  reward for individual goal_min: 0.5
episode_len_mean: 42.997416020671835
episode_reward_max: 2.0
episode_reward_mean: 1.9948320413436693
episode_reward_min: 1.0
episodes_this_iter: 387
episodes_total: 89788
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974160206718347
  agent_1: 0.9974160206718347
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.95532846450806
time_total_s: 13563.353419780731
timers:
  learn_throughput: 454.282
  learn_time_ms: 36321.084
  load_throughput: 4282762.513
  load_time_ms: 3.853
  training_iteration_time_ms: 47829.296
  update_time_ms: 2.375
timesteps_total: 4752000
training_iteration: 288

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28289473684210525
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9807692307692307
  reward for individual goal_min: 0.0
episode_len_mean: 174.0
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 24657
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.52120661735535
time_total_s: 13747.220003604889
timers:
  learn_throughput: 455.805
  learn_time_ms: 36199.66
  load_throughput: 4712508.58
  load_time_ms: 3.501
  training_iteration_time_ms: 47785.316
  update_time_ms: 2.604
timesteps_total: 4537500
training_iteration: 275

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22142857142857142
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9121621621621622
  reward for individual goal_min: 0.0
episode_len_mean: 183.38
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 24527
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.87188768386841
time_total_s: 13742.509315252304
timers:
  learn_throughput: 500.245
  learn_time_ms: 32983.861
  load_throughput: 5272317.104
  load_time_ms: 3.13
  training_iteration_time_ms: 43758.853
  update_time_ms: 2.484
timesteps_total: 4867500
training_iteration: 295

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24074074074074073
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.07
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 22938
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.39537000656128
time_total_s: 13752.258645534515
timers:
  learn_throughput: 389.266
  learn_time_ms: 42387.433
  load_throughput: 4110060.22
  load_time_ms: 4.015
  training_iteration_time_ms: 55675.453
  update_time_ms: 3.194
timesteps_total: 4158000
training_iteration: 252

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3287671232876712
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9383561643835616
  reward for individual goal_min: 0.0
episode_len_mean: 177.85
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 22900
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.92972803115845
time_total_s: 13761.119421958923
timers:
  learn_throughput: 463.151
  learn_time_ms: 35625.555
  load_throughput: 4826856.19
  load_time_ms: 3.418
  training_iteration_time_ms: 47079.964
  update_time_ms: 2.519
timesteps_total: 4455000
training_iteration: 270

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2345679012345679
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.88
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 22855
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.198811292648315
time_total_s: 13758.375876665115
timers:
  learn_throughput: 397.716
  learn_time_ms: 41486.893
  load_throughput: 4290515.561
  load_time_ms: 3.846
  training_iteration_time_ms: 54470.613
  update_time_ms: 2.771
timesteps_total: 4174500
training_iteration: 253

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9970059880239521
  reward for individual goal_min: 0.5
episode_len_mean: 48.79585798816568
episode_reward_max: 2.0
episode_reward_mean: 1.9970414201183433
episode_reward_min: 1.0
episodes_this_iter: 338
episodes_total: 44033
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9970414201183432
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 69.0181519985199
time_total_s: 13753.267161369324
timers:
  learn_throughput: 308.014
  learn_time_ms: 53568.949
  load_throughput: 3593008.572
  load_time_ms: 4.592
  training_iteration_time_ms: 69319.881
  update_time_ms: 3.026
timesteps_total: 3646500
training_iteration: 221

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.37037037037037035
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 162.2135922330097
episode_reward_max: 2.0
episode_reward_mean: 1.3786407766990292
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 25424
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6990291262135923
  agent_1: 0.6796116504854369
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.65927863121033
time_total_s: 13764.688521385193
timers:
  learn_throughput: 461.485
  learn_time_ms: 35754.173
  load_throughput: 4932294.886
  load_time_ms: 3.345
  training_iteration_time_ms: 47152.47
  update_time_ms: 2.907
timesteps_total: 4554000
training_iteration: 276

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1527777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9938271604938271
  reward for individual goal_min: 0.5
episode_len_mean: 183.3
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 23393
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.49087309837341
time_total_s: 13773.050820589066
timers:
  learn_throughput: 443.3
  learn_time_ms: 37220.864
  load_throughput: 4727832.764
  load_time_ms: 3.49
  training_iteration_time_ms: 48986.097
  update_time_ms: 2.593
timesteps_total: 4422000
training_iteration: 268

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.037333333333336
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 375
episodes_total: 68482
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.3429000377655
time_total_s: 13676.563194990158
timers:
  learn_throughput: 366.716
  learn_time_ms: 44993.978
  load_throughput: 3500645.742
  load_time_ms: 4.713
  training_iteration_time_ms: 58092.535
  update_time_ms: 2.608
timesteps_total: 3976500
training_iteration: 241

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.60054347826087
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 368
episodes_total: 64915
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 52.900208473205566
time_total_s: 13696.120536327362
timers:
  learn_throughput: 401.329
  learn_time_ms: 41113.416
  load_throughput: 4083673.571
  load_time_ms: 4.04
  training_iteration_time_ms: 53384.866
  update_time_ms: 2.581
timesteps_total: 4356000
training_iteration: 264

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.092269326683294
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 401
episodes_total: 90189
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.365331411361694
time_total_s: 13611.718751192093
timers:
  learn_throughput: 454.453
  learn_time_ms: 36307.409
  load_throughput: 4280378.521
  load_time_ms: 3.855
  training_iteration_time_ms: 47828.355
  update_time_ms: 2.371
timesteps_total: 4768500
training_iteration: 289

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25882352941176473
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 188.53
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 24139
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.18902277946472
time_total_s: 13794.746443986893
timers:
  learn_throughput: 445.598
  learn_time_ms: 37028.878
  load_throughput: 4787952.014
  load_time_ms: 3.446
  training_iteration_time_ms: 48637.472
  update_time_ms: 2.636
timesteps_total: 4372500
training_iteration: 265

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.273972602739726
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9929577464788732
  reward for individual goal_min: 0.5
episode_len_mean: 176.37
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 24750
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.327767848968506
time_total_s: 13795.547771453857
timers:
  learn_throughput: 456.465
  learn_time_ms: 36147.375
  load_throughput: 4714916.508
  load_time_ms: 3.5
  training_iteration_time_ms: 47767.606
  update_time_ms: 2.613
timesteps_total: 4554000
training_iteration: 276

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9493670886075949
  reward for individual goal_min: 0.0
episode_len_mean: 190.76
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 24615
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.70328736305237
time_total_s: 13785.212602615356
timers:
  learn_throughput: 501.775
  learn_time_ms: 32883.249
  load_throughput: 5286533.955
  load_time_ms: 3.121
  training_iteration_time_ms: 43673.387
  update_time_ms: 2.495
timesteps_total: 4884000
training_iteration: 296

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2077922077922078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9506172839506173
  reward for individual goal_min: 0.0
episode_len_mean: 191.84
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 22985
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.56795310974121
time_total_s: 13809.687375068665
timers:
  learn_throughput: 461.183
  learn_time_ms: 35777.584
  load_throughput: 4798874.997
  load_time_ms: 3.438
  training_iteration_time_ms: 47189.135
  update_time_ms: 2.525
timesteps_total: 4471500
training_iteration: 271

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.09
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 23034
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.987870931625366
time_total_s: 13806.24651646614
timers:
  learn_throughput: 389.353
  learn_time_ms: 42377.992
  load_throughput: 4128547.498
  load_time_ms: 3.997
  training_iteration_time_ms: 55718.82
  update_time_ms: 3.192
timesteps_total: 4174500
training_iteration: 253

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2972972972972973
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9859154929577465
  reward for individual goal_min: 0.0
episode_len_mean: 170.25
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 22950
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.61904764175415
time_total_s: 13810.99492430687
timers:
  learn_throughput: 399.18
  learn_time_ms: 41334.715
  load_throughput: 4300192.995
  load_time_ms: 3.837
  training_iteration_time_ms: 54320.561
  update_time_ms: 2.774
timesteps_total: 4191000
training_iteration: 254

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2987012987012987
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 168.89
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 25517
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.83048915863037
time_total_s: 13810.519010543823
timers:
  learn_throughput: 461.229
  learn_time_ms: 35774.006
  load_throughput: 4878094.607
  load_time_ms: 3.382
  training_iteration_time_ms: 47167.338
  update_time_ms: 2.894
timesteps_total: 4570500
training_iteration: 277

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23417721518987342
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 181.18
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 23488
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.731592893600464
time_total_s: 13820.782413482666
timers:
  learn_throughput: 442.781
  learn_time_ms: 37264.451
  load_throughput: 4743029.381
  load_time_ms: 3.479
  training_iteration_time_ms: 49105.831
  update_time_ms: 2.606
timesteps_total: 4438500
training_iteration: 269

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25308641975308643
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.972972972972973
  reward for individual goal_min: 0.0
episode_len_mean: 187.75
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 24703
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.07602095603943
time_total_s: 13828.288623571396
timers:
  learn_throughput: 501.886
  learn_time_ms: 32875.995
  load_throughput: 5282176.19
  load_time_ms: 3.124
  training_iteration_time_ms: 43611.034
  update_time_ms: 2.51
timesteps_total: 4900500
training_iteration: 297

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.378865979381445
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 388
episodes_total: 90577
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.06895565986633
time_total_s: 13659.78770685196
timers:
  learn_throughput: 453.792
  learn_time_ms: 36360.298
  load_throughput: 4239032.213
  load_time_ms: 3.892
  training_iteration_time_ms: 47872.685
  update_time_ms: 2.358
timesteps_total: 4785000
training_iteration: 290

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1794871794871795
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 192.28
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 24227
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.79324173927307
time_total_s: 13844.539685726166
timers:
  learn_throughput: 443.117
  learn_time_ms: 37236.251
  load_throughput: 4776814.859
  load_time_ms: 3.454
  training_iteration_time_ms: 48934.669
  update_time_ms: 2.994
timesteps_total: 4389000
training_iteration: 266

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23170731707317074
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9927536231884058
  reward for individual goal_min: 0.5
episode_len_mean: 197.54
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 24834
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.31939077377319
time_total_s: 13843.86716222763
timers:
  learn_throughput: 456.903
  learn_time_ms: 36112.725
  load_throughput: 4718935.195
  load_time_ms: 3.497
  training_iteration_time_ms: 47716.902
  update_time_ms: 2.595
timesteps_total: 4570500
training_iteration: 277

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 45.043596730245234
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 367
episodes_total: 65282
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 52.878570795059204
time_total_s: 13748.999107122421
timers:
  learn_throughput: 403.096
  learn_time_ms: 40933.165
  load_throughput: 4090697.782
  load_time_ms: 4.034
  training_iteration_time_ms: 53197.124
  update_time_ms: 2.581
timesteps_total: 4372500
training_iteration: 265

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.05714285714286
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 385
episodes_total: 68867
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.91905927658081
time_total_s: 13735.482254266739
timers:
  learn_throughput: 366.873
  learn_time_ms: 44974.709
  load_throughput: 3538592.151
  load_time_ms: 4.663
  training_iteration_time_ms: 58037.055
  update_time_ms: 2.639
timesteps_total: 3993000
training_iteration: 242

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9914772727272727
  reward for individual goal_min: 0.5
episode_len_mean: 49.14454277286136
episode_reward_max: 2.0
episode_reward_mean: 1.991150442477876
episode_reward_min: 1.0
episodes_this_iter: 339
episodes_total: 44372
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9970501474926253
  agent_1: 0.9941002949852508
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 70.95649790763855
time_total_s: 13824.223659276962
timers:
  learn_throughput: 306.283
  learn_time_ms: 53871.762
  load_throughput: 3629470.416
  load_time_ms: 4.546
  training_iteration_time_ms: 69669.646
  update_time_ms: 3.058
timesteps_total: 3663000
training_iteration: 222

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2534246575342466
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9493670886075949
  reward for individual goal_min: 0.0
episode_len_mean: 187.48
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 23075
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.76
  agent_1: 0.53
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.21257948875427
time_total_s: 13856.899954557419
timers:
  learn_throughput: 460.753
  learn_time_ms: 35810.924
  load_throughput: 4809045.779
  load_time_ms: 3.431
  training_iteration_time_ms: 47275.234
  update_time_ms: 2.543
timesteps_total: 4488000
training_iteration: 272

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9797297297297297
  reward for individual goal_min: 0.0
episode_len_mean: 171.42
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 23131
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.21786570549011
time_total_s: 13861.46438217163
timers:
  learn_throughput: 391.383
  learn_time_ms: 42158.206
  load_throughput: 4093988.867
  load_time_ms: 4.03
  training_iteration_time_ms: 55407.57
  update_time_ms: 3.181
timesteps_total: 4191000
training_iteration: 254

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2222222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9794520547945206
  reward for individual goal_min: 0.0
episode_len_mean: 190.45
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 23037
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.50359082221985
time_total_s: 13865.49851512909
timers:
  learn_throughput: 398.552
  learn_time_ms: 41399.878
  load_throughput: 4308948.135
  load_time_ms: 3.829
  training_iteration_time_ms: 54439.961
  update_time_ms: 2.767
timesteps_total: 4207500
training_iteration: 255

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24675324675324675
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.83
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 25613
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.91326427459717
time_total_s: 13856.43227481842
timers:
  learn_throughput: 463.027
  learn_time_ms: 35635.091
  load_throughput: 4882155.298
  load_time_ms: 3.38
  training_iteration_time_ms: 46970.44
  update_time_ms: 2.566
timesteps_total: 4587000
training_iteration: 278

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14189189189189189
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 184.27
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 23575
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.509602785110474
time_total_s: 13866.292016267776
timers:
  learn_throughput: 448.707
  learn_time_ms: 36772.351
  load_throughput: 4763039.822
  load_time_ms: 3.464
  training_iteration_time_ms: 48486.228
  update_time_ms: 2.606
timesteps_total: 4455000
training_iteration: 270

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2571428571428571
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9390243902439024
  reward for individual goal_min: 0.0
episode_len_mean: 175.58
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 24799
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.69787073135376
time_total_s: 13870.98649430275
timers:
  learn_throughput: 500.73
  learn_time_ms: 32951.883
  load_throughput: 5273281.265
  load_time_ms: 3.129
  training_iteration_time_ms: 43652.39
  update_time_ms: 2.505
timesteps_total: 4917000
training_iteration: 298

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2866666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 177.67
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 24927
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.5587739944458
time_total_s: 13889.425936222076
timers:
  learn_throughput: 460.743
  learn_time_ms: 35811.696
  load_throughput: 4704563.846
  load_time_ms: 3.507
  training_iteration_time_ms: 47278.353
  update_time_ms: 2.602
timesteps_total: 4587000
training_iteration: 278

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.82687338501292
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 387
episodes_total: 90964
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.53305625915527
time_total_s: 13707.320763111115
timers:
  learn_throughput: 454.399
  learn_time_ms: 36311.737
  load_throughput: 4240149.005
  load_time_ms: 3.891
  training_iteration_time_ms: 47826.408
  update_time_ms: 2.356
timesteps_total: 4801500
training_iteration: 291

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9929577464788732
  reward for individual goal_min: 0.5
episode_len_mean: 178.66
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 24318
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.08385443687439
time_total_s: 13896.62354016304
timers:
  learn_throughput: 435.905
  learn_time_ms: 37852.3
  load_throughput: 4760811.744
  load_time_ms: 3.466
  training_iteration_time_ms: 49608.701
  update_time_ms: 3.007
timesteps_total: 4405500
training_iteration: 267

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9970588235294118
  reward for individual goal_min: 0.5
episode_len_mean: 46.30252100840336
episode_reward_max: 2.0
episode_reward_mean: 1.9971988795518207
episode_reward_min: 1.0
episodes_this_iter: 357
episodes_total: 65639
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9971988795518207
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 52.28247928619385
time_total_s: 13801.281586408615
timers:
  learn_throughput: 405.479
  learn_time_ms: 40692.58
  load_throughput: 4102288.427
  load_time_ms: 4.022
  training_iteration_time_ms: 52859.861
  update_time_ms: 2.571
timesteps_total: 4389000
training_iteration: 266

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2152777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9512195121951219
  reward for individual goal_min: 0.0
episode_len_mean: 187.07
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 23160
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.40043258666992
time_total_s: 13904.300387144089
timers:
  learn_throughput: 461.202
  learn_time_ms: 35776.089
  load_throughput: 4781336.171
  load_time_ms: 3.451
  training_iteration_time_ms: 47234.998
  update_time_ms: 2.863
timesteps_total: 4504500
training_iteration: 273

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.63492063492063
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 378
episodes_total: 69245
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.4775755405426
time_total_s: 13793.959829807281
timers:
  learn_throughput: 367.264
  learn_time_ms: 44926.75
  load_throughput: 3536350.007
  load_time_ms: 4.666
  training_iteration_time_ms: 58031.154
  update_time_ms: 2.645
timesteps_total: 4009500
training_iteration: 243

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20394736842105263
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 176.75
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 23222
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.3724889755249
time_total_s: 13916.836871147156
timers:
  learn_throughput: 391.906
  learn_time_ms: 42101.89
  load_throughput: 4078234.961
  load_time_ms: 4.046
  training_iteration_time_ms: 55324.269
  update_time_ms: 3.175
timesteps_total: 4207500
training_iteration: 255

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25903614457831325
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 186.43
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 25700
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.370535135269165
time_total_s: 13902.80280995369
timers:
  learn_throughput: 465.599
  learn_time_ms: 35438.213
  load_throughput: 4891403.046
  load_time_ms: 3.373
  training_iteration_time_ms: 46791.918
  update_time_ms: 2.586
timesteps_total: 4603500
training_iteration: 279

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9937888198757764
  reward for individual goal_min: 0.5
episode_len_mean: 48.77744807121662
episode_reward_max: 2.0
episode_reward_mean: 1.9940652818991098
episode_reward_min: 1.0
episodes_this_iter: 337
episodes_total: 44709
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9940652818991098
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 70.29027819633484
time_total_s: 13894.513937473297
timers:
  learn_throughput: 305.617
  learn_time_ms: 53989.197
  load_throughput: 3690849.728
  load_time_ms: 4.471
  training_iteration_time_ms: 69833.993
  update_time_ms: 3.094
timesteps_total: 3679500
training_iteration: 223

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2839506172839506
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9485294117647058
  reward for individual goal_min: 0.0
episode_len_mean: 192.45
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 24884
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.64466214179993
time_total_s: 13914.63115644455
timers:
  learn_throughput: 500.494
  learn_time_ms: 32967.447
  load_throughput: 5278952.844
  load_time_ms: 3.126
  training_iteration_time_ms: 43638.419
  update_time_ms: 2.491
timesteps_total: 4933500
training_iteration: 299

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25949367088607594
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.98
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 23666
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.934922218322754
time_total_s: 13913.2269384861
timers:
  learn_throughput: 450.801
  learn_time_ms: 36601.533
  load_throughput: 4749995.951
  load_time_ms: 3.474
  training_iteration_time_ms: 48213.03
  update_time_ms: 2.612
timesteps_total: 4471500
training_iteration: 271

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2028985507246377
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.0
episode_len_mean: 164.79611650485438
episode_reward_max: 2.0
episode_reward_mean: 1.3203883495145632
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 23140
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6310679611650486
  agent_1: 0.6893203883495146
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.75097060203552
time_total_s: 13919.249485731125
timers:
  learn_throughput: 399.819
  learn_time_ms: 41268.656
  load_throughput: 4259749.238
  load_time_ms: 3.873
  training_iteration_time_ms: 54284.644
  update_time_ms: 2.757
timesteps_total: 4224000
training_iteration: 256

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.30434782608695654
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 163.73267326732673
episode_reward_max: 2.0
episode_reward_mean: 1.3663366336633664
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 25028
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6732673267326733
  agent_1: 0.693069306930693
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.58350706100464
time_total_s: 13939.009443283081
timers:
  learn_throughput: 456.84
  learn_time_ms: 36117.665
  load_throughput: 4714498.958
  load_time_ms: 3.5
  training_iteration_time_ms: 47698.8
  update_time_ms: 2.578
timesteps_total: 4603500
training_iteration: 279

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.38693467336683
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 398
episodes_total: 91362
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.47908592224121
time_total_s: 13755.799849033356
timers:
  learn_throughput: 453.445
  learn_time_ms: 36388.084
  load_throughput: 4230119.008
  load_time_ms: 3.901
  training_iteration_time_ms: 47917.659
  update_time_ms: 2.354
timesteps_total: 4818000
training_iteration: 292

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24050632911392406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 185.11
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 24406
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.73742318153381
time_total_s: 13945.360963344574
timers:
  learn_throughput: 434.696
  learn_time_ms: 37957.583
  load_throughput: 4706003.441
  load_time_ms: 3.506
  training_iteration_time_ms: 49758.16
  update_time_ms: 3.017
timesteps_total: 4422000
training_iteration: 268

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24324324324324326
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 184.12
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 23253
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.20572113990784
time_total_s: 13951.506108283997
timers:
  learn_throughput: 461.814
  learn_time_ms: 35728.638
  load_throughput: 4834105.14
  load_time_ms: 3.413
  training_iteration_time_ms: 47120.773
  update_time_ms: 2.839
timesteps_total: 4521000
training_iteration: 274

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9945054945054945
  reward for individual goal_min: 0.5
episode_len_mean: 45.03551912568306
episode_reward_max: 2.0
episode_reward_mean: 1.994535519125683
episode_reward_min: 1.0
episodes_this_iter: 366
episodes_total: 66005
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.994535519125683
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 51.94189119338989
time_total_s: 13853.223477602005
timers:
  learn_throughput: 407.573
  learn_time_ms: 40483.578
  load_throughput: 4113113.63
  load_time_ms: 4.012
  training_iteration_time_ms: 52616.773
  update_time_ms: 2.563
timesteps_total: 4405500
training_iteration: 267

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9946808510638298
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973821989528796
  reward for individual goal_min: 0.5
episode_len_mean: 43.437994722955146
episode_reward_max: 2.0
episode_reward_mean: 1.992084432717678
episode_reward_min: 0.0
episodes_this_iter: 379
episodes_total: 69624
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973614775725593
  agent_1: 0.9947229551451188
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.28995728492737
time_total_s: 13852.249787092209
timers:
  learn_throughput: 367.448
  learn_time_ms: 44904.286
  load_throughput: 3521397.039
  load_time_ms: 4.686
  training_iteration_time_ms: 58021.279
  update_time_ms: 2.637
timesteps_total: 4026000
training_iteration: 244

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20422535211267606
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 166.32
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 23320
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.92896866798401
time_total_s: 13970.76583981514
timers:
  learn_throughput: 393.597
  learn_time_ms: 41921.057
  load_throughput: 4115143.601
  load_time_ms: 4.01
  training_iteration_time_ms: 55202.978
  update_time_ms: 3.136
timesteps_total: 4224000
training_iteration: 256

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 65.66666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.9333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.95
  agent_1: 0.9833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2808219178082192
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 155.66037735849056
episode_reward_max: 2.0
episode_reward_mean: 1.4339622641509433
episode_reward_min: 0.0
episodes_this_iter: 106
episodes_total: 25806
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6981132075471698
  agent_1: 0.7358490566037735
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.76169776916504
time_total_s: 13956.564507722855
timers:
  learn_throughput: 467.589
  learn_time_ms: 35287.381
  load_throughput: 4895105.038
  load_time_ms: 3.371
  training_iteration_time_ms: 46616.432
  update_time_ms: 2.569
timesteps_total: 4620000
training_iteration: 280

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24647887323943662
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9743589743589743
  reward for individual goal_min: 0.0
episode_len_mean: 161.58252427184465
episode_reward_max: 2.0
episode_reward_mean: 1.3398058252427185
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 23769
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6407766990291263
  agent_1: 0.6990291262135923
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.52757906913757
time_total_s: 13962.754517555237
timers:
  learn_throughput: 451.026
  learn_time_ms: 36583.221
  load_throughput: 4761172.027
  load_time_ms: 3.466
  training_iteration_time_ms: 48253.525
  update_time_ms: 2.62
timesteps_total: 4488000
training_iteration: 272

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2191780821917808
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 181.23
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 23224
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.652753829956055
time_total_s: 13970.902239561081
timers:
  learn_throughput: 401.766
  learn_time_ms: 41068.672
  load_throughput: 4260142.567
  load_time_ms: 3.873
  training_iteration_time_ms: 54070.192
  update_time_ms: 2.755
timesteps_total: 4240500
training_iteration: 257

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95
  reward for individual goal_min: 0.5
episode_len_mean: 113.63333333333334
episode_reward_max: 2.0
episode_reward_mean: 1.6833333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.85
  agent_1: 0.8333333333333334
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9652777777777778
  reward for individual goal_min: 0.0
episode_len_mean: 198.41
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 24967
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.97180223464966
time_total_s: 13970.6029586792
timers:
  learn_throughput: 498.49
  learn_time_ms: 33099.957
  load_throughput: 5289726.135
  load_time_ms: 3.119
  training_iteration_time_ms: 43782.099
  update_time_ms: 2.498
timesteps_total: 4950000
training_iteration: 300

Starting final evaluation!
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.9236641221374
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 393
episodes_total: 91755
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.003063440322876
time_total_s: 13803.802912473679
timers:
  learn_throughput: 452.512
  learn_time_ms: 36463.089
  load_throughput: 4264815.618
  load_time_ms: 3.869
  training_iteration_time_ms: 47976.559
  update_time_ms: 2.351
timesteps_total: 4834500
training_iteration: 293

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 47.66959064327485
episode_reward_max: 2.0
episode_reward_mean: 1.9941520467836258
episode_reward_min: 1.0
episodes_this_iter: 342
episodes_total: 45051
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9970760233918129
  agent_1: 0.9970760233918129
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 71.67276906967163
time_total_s: 13966.186706542969
timers:
  learn_throughput: 303.845
  learn_time_ms: 54303.955
  load_throughput: 3642038.522
  load_time_ms: 4.53
  training_iteration_time_ms: 70194.442
  update_time_ms: 3.15
timesteps_total: 3696000
training_iteration: 224

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3194444444444444
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 162.5142857142857
episode_reward_max: 2.0
episode_reward_mean: 1.3904761904761904
episode_reward_min: 0.0
episodes_this_iter: 105
episodes_total: 24511
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6571428571428571
  agent_1: 0.7333333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.31705188751221
time_total_s: 13998.678015232086
timers:
  learn_throughput: 428.406
  learn_time_ms: 38514.843
  load_throughput: 4708500.827
  load_time_ms: 3.504
  training_iteration_time_ms: 50315.971
  update_time_ms: 3.031
timesteps_total: 4438500
training_iteration: 269

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9166666666666666
  reward for individual goal_min: 0.5
episode_len_mean: 104.58333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.7166666666666666
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8166666666666667
  agent_1: 0.9
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2569444444444444
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.92
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 25118
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.872140884399414
time_total_s: 13997.88158416748
timers:
  learn_throughput: 455.884
  learn_time_ms: 36193.427
  load_throughput: 4722799.585
  load_time_ms: 3.494
  training_iteration_time_ms: 47894.376
  update_time_ms: 2.577
timesteps_total: 4620000
training_iteration: 280

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21739130434782608
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95
  reward for individual goal_min: 0.0
episode_len_mean: 176.33
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 23348
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.68214440345764
time_total_s: 14000.188252687454
timers:
  learn_throughput: 458.866
  learn_time_ms: 35958.243
  load_throughput: 4820166.044
  load_time_ms: 3.423
  training_iteration_time_ms: 47371.896
  update_time_ms: 2.873
timesteps_total: 4537500
training_iteration: 275

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9917582417582418
  reward for individual goal_min: 0.5
episode_len_mean: 46.45042492917847
episode_reward_max: 2.0
episode_reward_mean: 1.9915014164305949
episode_reward_min: 1.0
episodes_this_iter: 353
episodes_total: 66358
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9915014164305949
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 51.10495615005493
time_total_s: 13904.32843375206
timers:
  learn_throughput: 411.824
  learn_time_ms: 40065.66
  load_throughput: 4090818.684
  load_time_ms: 4.033
  training_iteration_time_ms: 52122.828
  update_time_ms: 2.559
timesteps_total: 4422000
training_iteration: 268

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22972972972972974
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.75
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 25903
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.84954500198364
time_total_s: 14001.414052724838
timers:
  learn_throughput: 470.163
  learn_time_ms: 35094.222
  load_throughput: 4913001.711
  load_time_ms: 3.358
  training_iteration_time_ms: 46322.172
  update_time_ms: 2.59
timesteps_total: 4636500
training_iteration: 281

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.171355498721226
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 391
episodes_total: 70015
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 56.88343048095703
time_total_s: 13909.133217573166
timers:
  learn_throughput: 368.625
  learn_time_ms: 44760.92
  load_throughput: 3552469.624
  load_time_ms: 4.645
  training_iteration_time_ms: 57837.091
  update_time_ms: 2.645
timesteps_total: 4042500
training_iteration: 245

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23170731707317074
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 185.66
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 23860
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.050421476364136
time_total_s: 14010.804939031601
timers:
  learn_throughput: 452.213
  learn_time_ms: 36487.199
  load_throughput: 4782096.062
  load_time_ms: 3.45
  training_iteration_time_ms: 48098.841
  update_time_ms: 2.64
timesteps_total: 4504500
training_iteration: 273

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19863013698630136
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.6
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 23415
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.937954902648926
time_total_s: 14026.703794717789
timers:
  learn_throughput: 392.581
  learn_time_ms: 42029.498
  load_throughput: 4133405.164
  load_time_ms: 3.992
  training_iteration_time_ms: 55309.689
  update_time_ms: 3.141
timesteps_total: 4240500
training_iteration: 257

Evaluation results over 10 episodes for each goal
{'[0, 0, 1]': 1.8, '[0, 1, 0]': 2.0, '[1, 0, 0]': 1.9, '[0, 1, 1]': 2.0, '[1, 0, 1]': 2.0, '[1, 1, 0]': 1.6}
Moviepy - Building video /gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19istq_qwi/trained_agent.mp4.
Moviepy - Writing video /gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19istq_qwi/trained_agent.mp4

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18421052631578946
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.5
episode_len_mean: 187.49
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 23315
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.96722412109375
time_total_s: 14024.869463682175
timers:
  learn_throughput: 402.486
  learn_time_ms: 40995.258
  load_throughput: 4279372.743
  load_time_ms: 3.856
  training_iteration_time_ms: 53922.139
  update_time_ms: 2.748
timesteps_total: 4257000
training_iteration: 258

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.53400503778337
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 397
episodes_total: 92152
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.918198585510254
time_total_s: 13851.721111059189
timers:
  learn_throughput: 452.036
  learn_time_ms: 36501.515
  load_throughput: 4288973.339
  load_time_ms: 3.847
  training_iteration_time_ms: 48016.541
  update_time_ms: 2.353
timesteps_total: 4851000
training_iteration: 294

Moviepy - Done !
Moviepy - video ready /gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19istq_qwi/trained_agent.mp4
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2564102564102564
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.99
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 25207
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.22794270515442
time_total_s: 14042.109526872635
timers:
  learn_throughput: 460.954
  learn_time_ms: 35795.364
  load_throughput: 4761761.699
  load_time_ms: 3.465
  training_iteration_time_ms: 47302.997
  update_time_ms: 2.565
timesteps_total: 4636500
training_iteration: 281

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23076923076923078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 185.26
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 24600
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.03260779380798
time_total_s: 14047.710623025894
timers:
  learn_throughput: 431.1
  learn_time_ms: 38274.221
  load_throughput: 4706739.574
  load_time_ms: 3.506
  training_iteration_time_ms: 50067.609
  update_time_ms: 3.035
timesteps_total: 4455000
training_iteration: 270

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.32666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.92
  reward for individual goal_min: 0.0
episode_len_mean: 175.78
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 23440
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.76
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.97716546058655
time_total_s: 14048.16541814804
timers:
  learn_throughput: 455.649
  learn_time_ms: 36212.047
  load_throughput: 4813695.303
  load_time_ms: 3.428
  training_iteration_time_ms: 47633.521
  update_time_ms: 2.897
timesteps_total: 4554000
training_iteration: 276

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.78100263852243
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 379
episodes_total: 66737
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 52.65965938568115
time_total_s: 13956.988093137741
timers:
  learn_throughput: 411.022
  learn_time_ms: 40143.872
  load_throughput: 4125520.325
  load_time_ms: 3.999
  training_iteration_time_ms: 52150.352
  update_time_ms: 2.513
timesteps_total: 4438500
training_iteration: 269

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9967948717948718
  reward for individual goal_min: 0.5
episode_len_mean: 51.20061728395062
episode_reward_max: 2.0
episode_reward_mean: 1.9969135802469136
episode_reward_min: 1.0
episodes_this_iter: 324
episodes_total: 45375
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9969135802469136
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 69.87053847312927
time_total_s: 14036.057245016098
timers:
  learn_throughput: 303.431
  learn_time_ms: 54378.121
  load_throughput: 3512443.017
  load_time_ms: 4.698
  training_iteration_time_ms: 70342.144
  update_time_ms: 3.124
timesteps_total: 3712500
training_iteration: 225

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.275
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.63
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 25995
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.71881341934204
time_total_s: 14046.13286614418
timers:
  learn_throughput: 472.495
  learn_time_ms: 34921.01
  load_throughput: 4851829.864
  load_time_ms: 3.401
  training_iteration_time_ms: 46102.298
  update_time_ms: 2.632
timesteps_total: 4653000
training_iteration: 282

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9652777777777778
  reward for individual goal_min: 0.0
episode_len_mean: 177.89
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 23952
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.022064447402954
time_total_s: 14060.827003479004
timers:
  learn_throughput: 450.606
  learn_time_ms: 36617.352
  load_throughput: 4779817.112
  load_time_ms: 3.452
  training_iteration_time_ms: 48313.211
  update_time_ms: 2.632
timesteps_total: 4521000
training_iteration: 274

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9975247524752475
  reward for individual goal_min: 0.5
episode_len_mean: 42.90414507772021
episode_reward_max: 2.0
episode_reward_mean: 1.9974093264248705
episode_reward_min: 1.0
episodes_this_iter: 386
episodes_total: 70401
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974093264248705
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 56.69779586791992
time_total_s: 13965.831013441086
timers:
  learn_throughput: 368.997
  learn_time_ms: 44715.84
  load_throughput: 3546698.372
  load_time_ms: 4.652
  training_iteration_time_ms: 57807.072
  update_time_ms: 2.643
timesteps_total: 4059000
training_iteration: 246

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2328767123287671
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9927536231884058
  reward for individual goal_min: 0.5
episode_len_mean: 173.89
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 23509
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.09337830543518
time_total_s: 14081.797173023224
timers:
  learn_throughput: 393.633
  learn_time_ms: 41917.17
  load_throughput: 4172430.35
  load_time_ms: 3.955
  training_iteration_time_ms: 55173.624
  update_time_ms: 3.133
timesteps_total: 4257000
training_iteration: 258

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.526992287917736
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 389
episodes_total: 92541
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.171191453933716
time_total_s: 13899.892302513123
timers:
  learn_throughput: 451.531
  learn_time_ms: 36542.328
  load_throughput: 4251218.802
  load_time_ms: 3.881
  training_iteration_time_ms: 48056.69
  update_time_ms: 2.347
timesteps_total: 4867500
training_iteration: 295

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28846153846153844
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 172.65
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 23410
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.586994886398315
time_total_s: 14078.456458568573
timers:
  learn_throughput: 401.685
  learn_time_ms: 41076.983
  load_throughput: 4295362.157
  load_time_ms: 3.841
  training_iteration_time_ms: 53940.003
  update_time_ms: 2.742
timesteps_total: 4273500
training_iteration: 259

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22916666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9743589743589743
  reward for individual goal_min: 0.0
episode_len_mean: 182.14
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 25299
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.80918097496033
time_total_s: 14089.918707847595
timers:
  learn_throughput: 458.327
  learn_time_ms: 36000.472
  load_throughput: 4825106.22
  load_time_ms: 3.42
  training_iteration_time_ms: 47577.094
  update_time_ms: 2.535
timesteps_total: 4653000
training_iteration: 282

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9155844155844156
  reward for individual goal_min: 0.0
episode_len_mean: 195.73
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 23521
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.52
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.545734167099
time_total_s: 14096.71115231514
timers:
  learn_throughput: 451.34
  learn_time_ms: 36557.801
  load_throughput: 4827091.86
  load_time_ms: 3.418
  training_iteration_time_ms: 48019.989
  update_time_ms: 2.885
timesteps_total: 4570500
training_iteration: 277

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16216216216216217
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 185.04
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 24691
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.95652794837952
time_total_s: 14099.667150974274
timers:
  learn_throughput: 427.65
  learn_time_ms: 38582.966
  load_throughput: 4590323.749
  load_time_ms: 3.595
  training_iteration_time_ms: 50468.897
  update_time_ms: 3.029
timesteps_total: 4471500
training_iteration: 271

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.41
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 26088
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.717262744903564
time_total_s: 14090.850128889084
timers:
  learn_throughput: 475.478
  learn_time_ms: 34701.94
  load_throughput: 4862499.895
  load_time_ms: 3.393
  training_iteration_time_ms: 45832.944
  update_time_ms: 2.624
timesteps_total: 4669500
training_iteration: 283

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.96875
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 384
episodes_total: 67121
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 55.3800847530365
time_total_s: 14012.368177890778
timers:
  learn_throughput: 409.009
  learn_time_ms: 40341.371
  load_throughput: 4129212.594
  load_time_ms: 3.996
  training_iteration_time_ms: 52360.689
  update_time_ms: 2.515
timesteps_total: 4455000
training_iteration: 270

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15753424657534246
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.01
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 24041
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.707061767578125
time_total_s: 14109.534065246582
timers:
  learn_throughput: 451.992
  learn_time_ms: 36505.039
  load_throughput: 4785005.704
  load_time_ms: 3.448
  training_iteration_time_ms: 48202.445
  update_time_ms: 2.627
timesteps_total: 4537500
training_iteration: 275

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.996875
  reward for individual goal_min: 0.5
episode_len_mean: 49.13095238095238
episode_reward_max: 2.0
episode_reward_mean: 1.9970238095238095
episode_reward_min: 1.0
episodes_this_iter: 336
episodes_total: 45711
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9970238095238095
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 68.99314737319946
time_total_s: 14105.050392389297
timers:
  learn_throughput: 303.344
  learn_time_ms: 54393.638
  load_throughput: 3501123.905
  load_time_ms: 4.713
  training_iteration_time_ms: 70368.585
  update_time_ms: 3.018
timesteps_total: 3729000
training_iteration: 226

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9925742574257426
  reward for individual goal_min: 0.5
episode_len_mean: 42.52307692307692
episode_reward_max: 2.0
episode_reward_mean: 1.9923076923076923
episode_reward_min: 1.0
episodes_this_iter: 390
episodes_total: 92931
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974358974358974
  agent_1: 0.9948717948717949
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.95068621635437
time_total_s: 13947.842988729477
timers:
  learn_throughput: 452.064
  learn_time_ms: 36499.246
  load_throughput: 4247409.49
  load_time_ms: 3.885
  training_iteration_time_ms: 48008.367
  update_time_ms: 2.323
timesteps_total: 4884000
training_iteration: 296

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24025974025974026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 189.08
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 25386
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.36702346801758
time_total_s: 14135.285731315613
timers:
  learn_throughput: 461.115
  learn_time_ms: 35782.846
  load_throughput: 4876891.463
  load_time_ms: 3.383
  training_iteration_time_ms: 47290.409
  update_time_ms: 2.541
timesteps_total: 4669500
training_iteration: 283

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.774535809018566
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 377
episodes_total: 70778
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 55.32598829269409
time_total_s: 14021.15700173378
timers:
  learn_throughput: 371.111
  learn_time_ms: 44461.143
  load_throughput: 3533443.072
  load_time_ms: 4.67
  training_iteration_time_ms: 57520.762
  update_time_ms: 2.65
timesteps_total: 4075500
training_iteration: 247

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.84
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 23599
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.94700002670288
time_total_s: 14134.744173049927
timers:
  learn_throughput: 396.609
  learn_time_ms: 41602.646
  load_throughput: 4200490.176
  load_time_ms: 3.928
  training_iteration_time_ms: 54727.234
  update_time_ms: 3.106
timesteps_total: 4273500
training_iteration: 259

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34459459459459457
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.925
  reward for individual goal_min: 0.0
episode_len_mean: 170.78
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 23618
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.53517150878906
time_total_s: 14146.246323823929
timers:
  learn_throughput: 449.006
  learn_time_ms: 36747.85
  load_throughput: 4780543.498
  load_time_ms: 3.451
  training_iteration_time_ms: 48186.714
  update_time_ms: 2.896
timesteps_total: 4587000
training_iteration: 278

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29012345679012347
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.74
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 24780
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.558340549468994
time_total_s: 14148.225491523743
timers:
  learn_throughput: 430.003
  learn_time_ms: 38371.864
  load_throughput: 4602198.223
  load_time_ms: 3.585
  training_iteration_time_ms: 50373.159
  update_time_ms: 3.033
timesteps_total: 4488000
training_iteration: 272

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 80.23333333333333
episode_reward_max: 2.0
episode_reward_mean: 1.7333333333333334
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8666666666666667
  agent_1: 0.8666666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2328767123287671
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9930555555555556
  reward for individual goal_min: 0.5
episode_len_mean: 179.41
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 23504
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.807127475738525
time_total_s: 14142.263586044312
timers:
  learn_throughput: 403.534
  learn_time_ms: 40888.761
  load_throughput: 4279690.306
  load_time_ms: 3.855
  training_iteration_time_ms: 53678.708
  update_time_ms: 2.716
timesteps_total: 4290000
training_iteration: 260

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3231707317073171
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9930555555555556
  reward for individual goal_min: 0.5
episode_len_mean: 179.12
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 26177
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.78117108345032
time_total_s: 14136.631299972534
timers:
  learn_throughput: 475.955
  learn_time_ms: 34667.119
  load_throughput: 4864482.245
  load_time_ms: 3.392
  training_iteration_time_ms: 45780.905
  update_time_ms: 2.599
timesteps_total: 4686000
training_iteration: 284

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.04043126684636
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 371
episodes_total: 67492
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 53.7406952381134
time_total_s: 14066.108873128891
timers:
  learn_throughput: 407.376
  learn_time_ms: 40503.151
  load_throughput: 4119185.044
  load_time_ms: 4.006
  training_iteration_time_ms: 52567.923
  update_time_ms: 2.519
timesteps_total: 4471500
training_iteration: 271

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21794871794871795
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 180.98
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 24134
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.21357560157776
time_total_s: 14157.74764084816
timers:
  learn_throughput: 453.308
  learn_time_ms: 36399.112
  load_throughput: 4816408.886
  load_time_ms: 3.426
  training_iteration_time_ms: 48066.576
  update_time_ms: 2.612
timesteps_total: 4554000
training_iteration: 276

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.77
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 25477
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.675570011138916
time_total_s: 14180.961301326752
timers:
  learn_throughput: 463.665
  learn_time_ms: 35586.022
  load_throughput: 4842426.04
  load_time_ms: 3.407
  training_iteration_time_ms: 46998.672
  update_time_ms: 2.555
timesteps_total: 4686000
training_iteration: 284

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974489795918368
  reward for individual goal_min: 0.5
episode_len_mean: 41.70558375634518
episode_reward_max: 2.0
episode_reward_mean: 1.99746192893401
episode_reward_min: 1.0
episodes_this_iter: 394
episodes_total: 93325
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9974619289340102
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 47.86172342300415
time_total_s: 13995.704712152481
timers:
  learn_throughput: 453.06
  learn_time_ms: 36418.979
  load_throughput: 4264999.599
  load_time_ms: 3.869
  training_iteration_time_ms: 47918.516
  update_time_ms: 2.332
timesteps_total: 4900500
training_iteration: 297

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.387909319899244
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 397
episodes_total: 71175
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.14968180656433
time_total_s: 14078.306683540344
timers:
  learn_throughput: 371.563
  learn_time_ms: 44406.951
  load_throughput: 3561152.643
  load_time_ms: 4.633
  training_iteration_time_ms: 57481.79
  update_time_ms: 2.651
timesteps_total: 4092000
training_iteration: 248

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3961038961038961
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9202898550724637
  reward for individual goal_min: 0.0
episode_len_mean: 172.04
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 23712
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.75
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.55851864814758
time_total_s: 14197.804842472076
timers:
  learn_throughput: 447.036
  learn_time_ms: 36909.784
  load_throughput: 4749344.002
  load_time_ms: 3.474
  training_iteration_time_ms: 48523.717
  update_time_ms: 2.949
timesteps_total: 4603500
training_iteration: 279

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2222222222222222
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 185.35
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 26267
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.77301335334778
time_total_s: 14180.404313325882
timers:
  learn_throughput: 479.291
  learn_time_ms: 34425.837
  load_throughput: 4861987.481
  load_time_ms: 3.394
  training_iteration_time_ms: 45451.303
  update_time_ms: 2.593
timesteps_total: 4702500
training_iteration: 285

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 60.15
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19444444444444445
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9764705882352941
  reward for individual goal_min: 0.0
episode_len_mean: 176.86
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 24872
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.41343331336975
time_total_s: 14201.638924837112
timers:
  learn_throughput: 426.244
  learn_time_ms: 38710.212
  load_throughput: 4579661.717
  load_time_ms: 3.603
  training_iteration_time_ms: 50719.284
  update_time_ms: 3.023
timesteps_total: 4504500
training_iteration: 273

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21323529411764705
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 161.43564356435644
episode_reward_max: 2.0
episode_reward_mean: 1.3366336633663367
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 23700
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.594059405940594
  agent_1: 0.7425742574257426
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.56962585449219
time_total_s: 14197.313798904419
timers:
  learn_throughput: 396.534
  learn_time_ms: 41610.545
  load_throughput: 4243971.325
  load_time_ms: 3.888
  training_iteration_time_ms: 54705.712
  update_time_ms: 2.743
timesteps_total: 4290000
training_iteration: 260

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-29ws591pkw/checkpoint_000260/checkpoint-260
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21830985915492956
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.01
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 23600
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.83464479446411
time_total_s: 14194.098230838776
timers:
  learn_throughput: 406.044
  learn_time_ms: 40635.961
  load_throughput: 4291074.225
  load_time_ms: 3.845
  training_iteration_time_ms: 53252.067
  update_time_ms: 2.711
timesteps_total: 4306500
training_iteration: 261

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9815950920245399
  reward for individual goal_min: 0.5
episode_len_mean: 50.03323262839879
episode_reward_max: 2.0
episode_reward_mean: 1.9818731117824773
episode_reward_min: 1.0
episodes_this_iter: 331
episodes_total: 46042
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9969788519637462
  agent_1: 0.9848942598187311
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 69.90451407432556
time_total_s: 14174.954906463623
timers:
  learn_throughput: 303.719
  learn_time_ms: 54326.457
  load_throughput: 3400051.881
  load_time_ms: 4.853
  training_iteration_time_ms: 70331.278
  update_time_ms: 3.028
timesteps_total: 3745500
training_iteration: 227

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 184.44
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 24219
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.38529634475708
time_total_s: 14204.132937192917
timers:
  learn_throughput: 456.92
  learn_time_ms: 36111.353
  load_throughput: 4814599.491
  load_time_ms: 3.427
  training_iteration_time_ms: 47618.979
  update_time_ms: 2.6
timesteps_total: 4570500
training_iteration: 277

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973821989528796
  reward for individual goal_min: 0.5
episode_len_mean: 45.34877384196185
episode_reward_max: 2.0
episode_reward_mean: 1.997275204359673
episode_reward_min: 1.0
episodes_this_iter: 367
episodes_total: 67859
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.997275204359673
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 56.584831953048706
time_total_s: 14122.69370508194
timers:
  learn_throughput: 402.762
  learn_time_ms: 40967.096
  load_throughput: 4133380.477
  load_time_ms: 3.992
  training_iteration_time_ms: 53053.243
  update_time_ms: 2.508
timesteps_total: 4488000
training_iteration: 272

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29285714285714287
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9802631578947368
  reward for individual goal_min: 0.0
episode_len_mean: 166.72
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 25573
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.37198042869568
time_total_s: 14226.333281755447
timers:
  learn_throughput: 466.815
  learn_time_ms: 35345.882
  load_throughput: 4832214.945
  load_time_ms: 3.415
  training_iteration_time_ms: 46683.778
  update_time_ms: 2.554
timesteps_total: 4702500
training_iteration: 285

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9975124378109452
  reward for individual goal_min: 0.5
episode_len_mean: 41.437185929648244
episode_reward_max: 2.0
episode_reward_mean: 1.9974874371859297
episode_reward_min: 1.0
episodes_this_iter: 398
episodes_total: 93723
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9974874371859297
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 48.521997690200806
time_total_s: 14044.226709842682
timers:
  learn_throughput: 452.498
  learn_time_ms: 36464.23
  load_throughput: 4253439.701
  load_time_ms: 3.879
  training_iteration_time_ms: 47975.275
  update_time_ms: 2.352
timesteps_total: 4917000
training_iteration: 298

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2986111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939024390243902
  reward for individual goal_min: 0.5
episode_len_mean: 157.3846153846154
episode_reward_max: 2.0
episode_reward_mean: 1.4038461538461537
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 26371
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6634615384615384
  agent_1: 0.7403846153846154
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.02709221839905
time_total_s: 14227.431405544281
timers:
  learn_throughput: 478.729
  learn_time_ms: 34466.232
  load_throughput: 4855642.51
  load_time_ms: 3.398
  training_iteration_time_ms: 45487.85
  update_time_ms: 2.595
timesteps_total: 4719000
training_iteration: 286

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16049382716049382
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939759036144579
  reward for individual goal_min: 0.5
episode_len_mean: 197.41
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 24956
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.96679353713989
time_total_s: 14249.605718374252
timers:
  learn_throughput: 429.449
  learn_time_ms: 38421.293
  load_throughput: 4747942.92
  load_time_ms: 3.475
  training_iteration_time_ms: 50464.024
  update_time_ms: 3.034
timesteps_total: 4521000
training_iteration: 274

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.455012853470436
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 389
episodes_total: 71564
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.17088031768799
time_total_s: 14135.477563858032
timers:
  learn_throughput: 372.422
  learn_time_ms: 44304.604
  load_throughput: 3557509.754
  load_time_ms: 4.638
  training_iteration_time_ms: 57388.599
  update_time_ms: 2.612
timesteps_total: 4108500
training_iteration: 249

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 184.03
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 23690
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.12035822868347
time_total_s: 14245.21858906746
timers:
  learn_throughput: 409.054
  learn_time_ms: 40336.926
  load_throughput: 4307258.594
  load_time_ms: 3.831
  training_iteration_time_ms: 52872.038
  update_time_ms: 2.697
timesteps_total: 4323000
training_iteration: 262

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2246376811594203
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 164.88
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 23798
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.56158900260925
time_total_s: 14251.875387907028
timers:
  learn_throughput: 397.844
  learn_time_ms: 41473.493
  load_throughput: 4284406.364
  load_time_ms: 3.851
  training_iteration_time_ms: 54659.075
  update_time_ms: 2.74
timesteps_total: 4306500
training_iteration: 261

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9
  reward for individual goal_min: 0.5
episode_len_mean: 103.81666666666666
episode_reward_max: 2.0
episode_reward_mean: 1.8666666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9666666666666667
  agent_1: 0.9
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9315068493150684
  reward for individual goal_min: 0.0
episode_len_mean: 188.22
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 23800
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.09610939025879
time_total_s: 14258.900951862335
timers:
  learn_throughput: 446.766
  learn_time_ms: 36932.089
  load_throughput: 4742411.841
  load_time_ms: 3.479
  training_iteration_time_ms: 48583.816
  update_time_ms: 2.958
timesteps_total: 4620000
training_iteration: 280

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16891891891891891
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9928571428571429
  reward for individual goal_min: 0.5
episode_len_mean: 185.1
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 24308
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.82311701774597
time_total_s: 14251.956054210663
timers:
  learn_throughput: 456.217
  learn_time_ms: 36166.969
  load_throughput: 4784410.262
  load_time_ms: 3.449
  training_iteration_time_ms: 47852.011
  update_time_ms: 2.6
timesteps_total: 4587000
training_iteration: 278

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31645569620253167
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 176.88
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 25667
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.678566694259644
time_total_s: 14272.011848449707
timers:
  learn_throughput: 469.834
  learn_time_ms: 35118.82
  load_throughput: 4870816.072
  load_time_ms: 3.388
  training_iteration_time_ms: 46418.759
  update_time_ms: 2.559
timesteps_total: 4719000
training_iteration: 286

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971264367816092
  reward for individual goal_min: 0.5
episode_len_mean: 49.75903614457831
episode_reward_max: 2.0
episode_reward_mean: 1.9969879518072289
episode_reward_min: 1.0
episodes_this_iter: 332
episodes_total: 46374
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9969879518072289
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 70.35603666305542
time_total_s: 14245.310943126678
timers:
  learn_throughput: 304.38
  learn_time_ms: 54208.58
  load_throughput: 3247934.596
  load_time_ms: 5.08
  training_iteration_time_ms: 70274.074
  update_time_ms: 2.933
timesteps_total: 3762000
training_iteration: 228

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9921465968586387
  reward for individual goal_min: 0.5
episode_len_mean: 46.12011173184357
episode_reward_max: 2.0
episode_reward_mean: 1.9916201117318435
episode_reward_min: 1.0
episodes_this_iter: 358
episodes_total: 68217
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9916201117318436
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 56.11305046081543
time_total_s: 14178.806755542755
timers:
  learn_throughput: 399.422
  learn_time_ms: 41309.735
  load_throughput: 4097358.026
  load_time_ms: 4.027
  training_iteration_time_ms: 53448.358
  update_time_ms: 2.51
timesteps_total: 4504500
training_iteration: 273

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9975609756097561
  reward for individual goal_min: 0.5
episode_len_mean: 42.01526717557252
episode_reward_max: 2.0
episode_reward_mean: 1.9974554707379135
episode_reward_min: 1.0
episodes_this_iter: 393
episodes_total: 94116
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974554707379135
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 48.10568308830261
time_total_s: 14092.332392930984
timers:
  learn_throughput: 452.676
  learn_time_ms: 36449.937
  load_throughput: 4292245.232
  load_time_ms: 3.844
  training_iteration_time_ms: 47949.678
  update_time_ms: 2.359
timesteps_total: 4933500
training_iteration: 299

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2733333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.02
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 26465
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.548930644989014
time_total_s: 14271.98033618927
timers:
  learn_throughput: 479.435
  learn_time_ms: 34415.539
  load_throughput: 4922716.933
  load_time_ms: 3.352
  training_iteration_time_ms: 45359.49
  update_time_ms: 2.602
timesteps_total: 4735500
training_iteration: 287

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18181818181818182
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 164.06
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 25053
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.192129135131836
time_total_s: 14300.797847509384
timers:
  learn_throughput: 428.826
  learn_time_ms: 38477.142
  load_throughput: 4740430.3
  load_time_ms: 3.481
  training_iteration_time_ms: 50564.278
  update_time_ms: 3.023
timesteps_total: 4537500
training_iteration: 275

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23972602739726026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9857142857142858
  reward for individual goal_min: 0.0
episode_len_mean: 174.45
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 23785
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.47476530075073
time_total_s: 14298.69335436821
timers:
  learn_throughput: 409.381
  learn_time_ms: 40304.799
  load_throughput: 4366447.901
  load_time_ms: 3.779
  training_iteration_time_ms: 52899.694
  update_time_ms: 2.693
timesteps_total: 4339500
training_iteration: 263

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3194444444444444
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9671052631578947
  reward for individual goal_min: 0.0
episode_len_mean: 167.47
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 23899
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.73
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.57184195518494
time_total_s: 14309.47279381752
timers:
  learn_throughput: 445.443
  learn_time_ms: 37041.76
  load_throughput: 4741664.508
  load_time_ms: 3.48
  training_iteration_time_ms: 48783.998
  update_time_ms: 2.987
timesteps_total: 4636500
training_iteration: 281

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18666666666666668
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.25
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 23886
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.12622928619385
time_total_s: 14306.001617193222
timers:
  learn_throughput: 398.897
  learn_time_ms: 41364.045
  load_throughput: 4296535.505
  load_time_ms: 3.84
  training_iteration_time_ms: 54532.089
  update_time_ms: 2.743
timesteps_total: 4323000
training_iteration: 262

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9970414201183432
  reward for individual goal_min: 0.5
episode_len_mean: 42.7279792746114
episode_reward_max: 2.0
episode_reward_mean: 1.9974093264248705
episode_reward_min: 1.0
episodes_this_iter: 386
episodes_total: 71950
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974093264248705
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 57.3547306060791
time_total_s: 14192.832294464111
timers:
  learn_throughput: 372.935
  learn_time_ms: 44243.615
  load_throughput: 3531297.537
  load_time_ms: 4.673
  training_iteration_time_ms: 57349.889
  update_time_ms: 2.593
timesteps_total: 4125000
training_iteration: 250

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14084507042253522
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.74
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 24398
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.67809081077576
time_total_s: 14297.634145021439
timers:
  learn_throughput: 458.222
  learn_time_ms: 36008.768
  load_throughput: 4757866.021
  load_time_ms: 3.468
  training_iteration_time_ms: 47646.855
  update_time_ms: 2.579
timesteps_total: 4603500
training_iteration: 279

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28205128205128205
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.51
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 25755
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.00922989845276
time_total_s: 14318.02107834816
timers:
  learn_throughput: 472.011
  learn_time_ms: 34956.785
  load_throughput: 4887188.91
  load_time_ms: 3.376
  training_iteration_time_ms: 46187.604
  update_time_ms: 2.568
timesteps_total: 4735500
training_iteration: 287

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26973684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 178.31
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 26557
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 43.63775634765625
time_total_s: 14315.618092536926
timers:
  learn_throughput: 481.315
  learn_time_ms: 34281.071
  load_throughput: 4902143.864
  load_time_ms: 3.366
  training_iteration_time_ms: 45132.1
  update_time_ms: 2.587
timesteps_total: 4752000
training_iteration: 288

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.0
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973118279569892
  reward for individual goal_min: 0.5
episode_len_mean: 42.33678756476684
episode_reward_max: 2.0
episode_reward_mean: 1.9974093264248705
episode_reward_min: 1.0
episodes_this_iter: 386
episodes_total: 94502
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9974093264248705
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 53.12498092651367
time_total_s: 14145.457373857498
timers:
  learn_throughput: 451.898
  learn_time_ms: 36512.668
  load_throughput: 4331765.354
  load_time_ms: 3.809
  training_iteration_time_ms: 48020.68
  update_time_ms: 2.359
timesteps_total: 4950000
training_iteration: 300

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-172kuwc_58/checkpoint_000300/checkpoint-300
Starting final evaluation!
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.995
  reward for individual goal_min: 0.5
episode_len_mean: 44.83967391304348
episode_reward_max: 2.0
episode_reward_mean: 1.9945652173913044
episode_reward_min: 1.0
episodes_this_iter: 368
episodes_total: 68585
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9945652173913043
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 56.39560008049011
time_total_s: 14235.202355623245
timers:
  learn_throughput: 396.476
  learn_time_ms: 41616.606
  load_throughput: 4051921.919
  load_time_ms: 4.072
  training_iteration_time_ms: 53797.932
  update_time_ms: 2.521
timesteps_total: 4521000
training_iteration: 274

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 48.40469208211144
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 341
episodes_total: 46715
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 70.79824137687683
time_total_s: 14316.109184503555
timers:
  learn_throughput: 304.512
  learn_time_ms: 54185.005
  load_throughput: 3157842.633
  load_time_ms: 5.225
  training_iteration_time_ms: 70241.938
  update_time_ms: 2.925
timesteps_total: 3778500
training_iteration: 229

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20253164556962025
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.98
  reward for individual goal_min: 0.0
episode_len_mean: 190.16
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 25141
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.45174217224121
time_total_s: 14351.249589681625
timers:
  learn_throughput: 427.948
  learn_time_ms: 38556.046
  load_throughput: 4718999.55
  load_time_ms: 3.497
  training_iteration_time_ms: 50630.014
  update_time_ms: 2.642
timesteps_total: 4554000
training_iteration: 276

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9675324675324676
  reward for individual goal_min: 0.0
episode_len_mean: 190.87
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 81
episodes_total: 23980
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.756420373916626
time_total_s: 14358.229214191437
timers:
  learn_throughput: 443.426
  learn_time_ms: 37210.242
  load_throughput: 4714081.481
  load_time_ms: 3.5
  training_iteration_time_ms: 48938.353
  update_time_ms: 2.994
timesteps_total: 4653000
training_iteration: 282

Evaluation results over 10 episodes for each goal
{'[0, 0, 1]': 2.0, '[0, 1, 0]': 2.0, '[1, 0, 0]': 2.0, '[0, 1, 1]': 2.0, '[1, 0, 1]': 2.0, '[1, 1, 0]': 2.0}
Moviepy - Building video /gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-172kuwc_58/trained_agent.mp4.
Moviepy - Writing video /gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-172kuwc_58/trained_agent.mp4

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28846153846153844
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.5
episode_len_mean: 178.92
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 23878
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.7681610584259
time_total_s: 14350.461515426636
timers:
  learn_throughput: 409.614
  learn_time_ms: 40281.837
  load_throughput: 4352990.282
  load_time_ms: 3.79
  training_iteration_time_ms: 52814.613
  update_time_ms: 2.699
timesteps_total: 4356000
training_iteration: 264

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.11842105263157894
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 190.26
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 23975
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.34997224807739
time_total_s: 14357.3515894413
timers:
  learn_throughput: 400.769
  learn_time_ms: 41170.84
  load_throughput: 4273321.601
  load_time_ms: 3.861
  training_iteration_time_ms: 54268.187
  update_time_ms: 2.748
timesteps_total: 4339500
training_iteration: 263

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Moviepy - Done !
Moviepy - video ready /gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-172kuwc_58/trained_agent.mp4
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22784810126582278
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.0
episode_len_mean: 188.31
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 25845
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.814422607421875
time_total_s: 14364.835500955582
timers:
  learn_throughput: 470.536
  learn_time_ms: 35066.416
  load_throughput: 4942651.373
  load_time_ms: 3.338
  training_iteration_time_ms: 46313.362
  update_time_ms: 2.555
timesteps_total: 4752000
training_iteration: 288

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 77.51666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.9
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.95
  agent_1: 0.95
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.58291457286432
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 398
episodes_total: 72348
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 56.42940163612366
time_total_s: 14249.261696100235
timers:
  learn_throughput: 374.317
  learn_time_ms: 44080.285
  load_throughput: 3525810.356
  load_time_ms: 4.68
  training_iteration_time_ms: 57157.168
  update_time_ms: 2.592
timesteps_total: 4141500
training_iteration: 251

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23376623376623376
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.3
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 24494
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.8339478969574
time_total_s: 14354.468092918396
timers:
  learn_throughput: 455.452
  learn_time_ms: 36227.758
  load_throughput: 4757571.65
  load_time_ms: 3.468
  training_iteration_time_ms: 47888.747
  update_time_ms: 2.574
timesteps_total: 4620000
training_iteration: 280

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-28ftt3iad9/checkpoint_000280/checkpoint-280
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2534246575342466
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9880952380952381
  reward for individual goal_min: 0.0
episode_len_mean: 165.24
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 26654
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 44.982325077056885
time_total_s: 14360.600417613983
timers:
  learn_throughput: 482.553
  learn_time_ms: 34193.11
  load_throughput: 4914432.12
  load_time_ms: 3.357
  training_iteration_time_ms: 44993.398
  update_time_ms: 2.55
timesteps_total: 4768500
training_iteration: 289

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973821989528796
  reward for individual goal_min: 0.5
episode_len_mean: 43.37894736842105
episode_reward_max: 2.0
episode_reward_mean: 1.9973684210526317
episode_reward_min: 1.0
episodes_this_iter: 380
episodes_total: 68965
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973684210526316
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 56.79846143722534
time_total_s: 14292.00081706047
timers:
  learn_throughput: 393.284
  learn_time_ms: 41954.392
  load_throughput: 4034512.84
  load_time_ms: 4.09
  training_iteration_time_ms: 54189.79
  update_time_ms: 2.512
timesteps_total: 4537500
training_iteration: 275

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23026315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9924242424242424
  reward for individual goal_min: 0.5
episode_len_mean: 181.34
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 25232
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.75728678703308
time_total_s: 14403.006876468658
timers:
  learn_throughput: 429.286
  learn_time_ms: 38435.897
  load_throughput: 4732617.758
  load_time_ms: 3.486
  training_iteration_time_ms: 50597.626
  update_time_ms: 2.636
timesteps_total: 4570500
training_iteration: 277

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9551282051282052
  reward for individual goal_min: 0.0
episode_len_mean: 160.52884615384616
episode_reward_max: 2.0
episode_reward_mean: 1.4038461538461537
episode_reward_min: 0.0
episodes_this_iter: 104
episodes_total: 24084
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7019230769230769
  agent_1: 0.7019230769230769
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.49646067619324
time_total_s: 14408.72567486763
timers:
  learn_throughput: 440.636
  learn_time_ms: 37445.843
  load_throughput: 4703380.839
  load_time_ms: 3.508
  training_iteration_time_ms: 49247.241
  update_time_ms: 2.702
timesteps_total: 4669500
training_iteration: 283

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2077922077922078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.46
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 23964
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.77162981033325
time_total_s: 14402.233145236969
timers:
  learn_throughput: 411.864
  learn_time_ms: 40061.803
  load_throughput: 4360752.606
  load_time_ms: 3.784
  training_iteration_time_ms: 52541.429
  update_time_ms: 2.675
timesteps_total: 4372500
training_iteration: 265

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2605633802816901
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.98125
  reward for individual goal_min: 0.0
episode_len_mean: 170.96
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 25942
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.28771948814392
time_total_s: 14410.123220443726
timers:
  learn_throughput: 474.966
  learn_time_ms: 34739.316
  load_throughput: 4931416.234
  load_time_ms: 3.346
  training_iteration_time_ms: 45883.724
  update_time_ms: 2.582
timesteps_total: 4768500
training_iteration: 289

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2054794520547945
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.03
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 24069
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.03464961051941
time_total_s: 14408.386239051819
timers:
  learn_throughput: 404.219
  learn_time_ms: 40819.501
  load_throughput: 4292697.838
  load_time_ms: 3.844
  training_iteration_time_ms: 53849.753
  update_time_ms: 2.763
timesteps_total: 4356000
training_iteration: 264

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25675675675675674
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 171.3
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 24589
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.775025844573975
time_total_s: 14401.24311876297
timers:
  learn_throughput: 455.596
  learn_time_ms: 36216.282
  load_throughput: 4787322.722
  load_time_ms: 3.447
  training_iteration_time_ms: 47873.035
  update_time_ms: 2.605
timesteps_total: 4636500
training_iteration: 281

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.08683473389356
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 357
episodes_total: 47072
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 70.49409055709839
time_total_s: 14386.603275060654
timers:
  learn_throughput: 305.229
  learn_time_ms: 54057.795
  load_throughput: 3143641.765
  load_time_ms: 5.249
  training_iteration_time_ms: 70130.812
  update_time_ms: 2.942
timesteps_total: 3795000
training_iteration: 230

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.255639097744364
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 399
episodes_total: 72747
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 56.68021273612976
time_total_s: 14305.941908836365
timers:
  learn_throughput: 375.96
  learn_time_ms: 43887.677
  load_throughput: 3498734.397
  load_time_ms: 4.716
  training_iteration_time_ms: 56933.203
  update_time_ms: 2.56
timesteps_total: 4158000
training_iteration: 252

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2894736842105263
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9797297297297297
  reward for individual goal_min: 0.0
episode_len_mean: 173.01
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 26753
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.66635704040527
time_total_s: 14406.266774654388
timers:
  learn_throughput: 482.398
  learn_time_ms: 34204.117
  load_throughput: 4928887.464
  load_time_ms: 3.348
  training_iteration_time_ms: 44931.859
  update_time_ms: 2.582
timesteps_total: 4785000
training_iteration: 290

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23026315789473684
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.5
episode_len_mean: 179.85
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 25326
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.86425852775574
time_total_s: 14452.871134996414
timers:
  learn_throughput: 426.891
  learn_time_ms: 38651.516
  load_throughput: 4754695.266
  load_time_ms: 3.47
  training_iteration_time_ms: 50710.273
  update_time_ms: 2.643
timesteps_total: 4587000
training_iteration: 278

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9551282051282052
  reward for individual goal_min: 0.0
episode_len_mean: 188.2
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 24167
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.90435314178467
time_total_s: 14455.630028009415
timers:
  learn_throughput: 441.854
  learn_time_ms: 37342.64
  load_throughput: 4682473.105
  load_time_ms: 3.524
  training_iteration_time_ms: 49217.141
  update_time_ms: 2.712
timesteps_total: 4686000
training_iteration: 284

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9887640449438202
  reward for individual goal_min: 0.5
episode_len_mean: 45.86426592797784
episode_reward_max: 2.0
episode_reward_mean: 1.9889196675900278
episode_reward_min: 1.0
episodes_this_iter: 361
episodes_total: 69326
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9944598337950139
  agent_1: 0.9944598337950139
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 56.668073654174805
time_total_s: 14348.668890714645
timers:
  learn_throughput: 390.296
  learn_time_ms: 42275.557
  load_throughput: 4014875.561
  load_time_ms: 4.11
  training_iteration_time_ms: 54628.317
  update_time_ms: 2.542
timesteps_total: 4554000
training_iteration: 276

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22297297297297297
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.43
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 26035
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.570390939712524
time_total_s: 14457.693611383438
timers:
  learn_throughput: 474.325
  learn_time_ms: 34786.305
  load_throughput: 4956634.365
  load_time_ms: 3.329
  training_iteration_time_ms: 45943.382
  update_time_ms: 2.571
timesteps_total: 4785000
training_iteration: 290

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20512820512820512
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 188.01
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 24676
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.95556092262268
time_total_s: 14448.198679685593
timers:
  learn_throughput: 458.223
  learn_time_ms: 36008.694
  load_throughput: 4809279.713
  load_time_ms: 3.431
  training_iteration_time_ms: 47615.929
  update_time_ms: 2.615
timesteps_total: 4653000
training_iteration: 282

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3181818181818182
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 168.09
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 24063
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.88362407684326
time_total_s: 14456.116769313812
timers:
  learn_throughput: 411.94
  learn_time_ms: 40054.413
  load_throughput: 4384762.122
  load_time_ms: 3.763
  training_iteration_time_ms: 52554.617
  update_time_ms: 2.668
timesteps_total: 4389000
training_iteration: 266

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22077922077922077
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9873417721518988
  reward for individual goal_min: 0.0
episode_len_mean: 172.53
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 24167
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.35497760772705
time_total_s: 14461.741216659546
timers:
  learn_throughput: 405.795
  learn_time_ms: 40660.968
  load_throughput: 4305570.376
  load_time_ms: 3.832
  training_iteration_time_ms: 53648.053
  update_time_ms: 2.733
timesteps_total: 4372500
training_iteration: 265

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3375
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9852941176470589
  reward for individual goal_min: 0.0
episode_len_mean: 172.6
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 26845
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 42.86672639846802
time_total_s: 14449.133501052856
timers:
  learn_throughput: 484.604
  learn_time_ms: 34048.421
  load_throughput: 4875379.782
  load_time_ms: 3.384
  training_iteration_time_ms: 44733.648
  update_time_ms: 2.551
timesteps_total: 4801500
training_iteration: 291

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.986979166666664
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 384
episodes_total: 73131
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.18884587287903
time_total_s: 14363.130754709244
timers:
  learn_throughput: 376.627
  learn_time_ms: 43809.965
  load_throughput: 3524212.392
  load_time_ms: 4.682
  training_iteration_time_ms: 56804.687
  update_time_ms: 2.547
timesteps_total: 4174500
training_iteration: 253

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 48.98224852071006
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 338
episodes_total: 47410
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 70.29582285881042
time_total_s: 14456.899097919464
timers:
  learn_throughput: 304.711
  learn_time_ms: 54149.635
  load_throughput: 3258133.61
  load_time_ms: 5.064
  training_iteration_time_ms: 70258.358
  update_time_ms: 2.89
timesteps_total: 3811500
training_iteration: 231

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1987179487179487
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9567901234567902
  reward for individual goal_min: 0.0
episode_len_mean: 198.01
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 24251
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.122740030288696
time_total_s: 14502.752768039703
timers:
  learn_throughput: 443.274
  learn_time_ms: 37223.023
  load_throughput: 4706195.453
  load_time_ms: 3.506
  training_iteration_time_ms: 49061.378
  update_time_ms: 2.712
timesteps_total: 4702500
training_iteration: 285

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2535211267605634
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.38
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 25421
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.451884031295776
time_total_s: 14503.32301902771
timers:
  learn_throughput: 430.225
  learn_time_ms: 38352.018
  load_throughput: 4713439.354
  load_time_ms: 3.501
  training_iteration_time_ms: 50423.688
  update_time_ms: 2.614
timesteps_total: 4603500
training_iteration: 279

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.31547619047619047
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.35
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 26127
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.94822549819946
time_total_s: 14503.641836881638
timers:
  learn_throughput: 472.46
  learn_time_ms: 34923.614
  load_throughput: 4996607.8
  load_time_ms: 3.302
  training_iteration_time_ms: 46115.331
  update_time_ms: 2.582
timesteps_total: 4801500
training_iteration: 291

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19444444444444445
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.64
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 24770
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.536680698394775
time_total_s: 14493.735360383987
timers:
  learn_throughput: 460.766
  learn_time_ms: 35809.971
  load_throughput: 4799806.915
  load_time_ms: 3.438
  training_iteration_time_ms: 47364.176
  update_time_ms: 2.576
timesteps_total: 4669500
training_iteration: 283

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972826086956522
  reward for individual goal_min: 0.5
episode_len_mean: 42.62041884816754
episode_reward_max: 2.0
episode_reward_mean: 1.9973821989528795
episode_reward_min: 1.0
episodes_this_iter: 382
episodes_total: 69708
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973821989528796
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 57.79234337806702
time_total_s: 14406.461234092712
timers:
  learn_throughput: 385.529
  learn_time_ms: 42798.294
  load_throughput: 4015551.133
  load_time_ms: 4.109
  training_iteration_time_ms: 55212.828
  update_time_ms: 2.558
timesteps_total: 4570500
training_iteration: 277

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2911392405063291
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.22
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 26939
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.10568642616272
time_total_s: 14495.23918747902
timers:
  learn_throughput: 483.525
  learn_time_ms: 34124.396
  load_throughput: 4955959.955
  load_time_ms: 3.329
  training_iteration_time_ms: 44872.241
  update_time_ms: 2.486
timesteps_total: 4818000
training_iteration: 292

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21153846153846154
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.56
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 24154
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.04537630081177
time_total_s: 14507.162145614624
timers:
  learn_throughput: 411.95
  learn_time_ms: 40053.382
  load_throughput: 4406482.824
  load_time_ms: 3.744
  training_iteration_time_ms: 52493.49
  update_time_ms: 2.688
timesteps_total: 4405500
training_iteration: 267

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 165.76
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 24266
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.33965086936951
time_total_s: 14514.080867528915
timers:
  learn_throughput: 406.905
  learn_time_ms: 40550.048
  load_throughput: 4294136.159
  load_time_ms: 3.842
  training_iteration_time_ms: 53489.011
  update_time_ms: 2.75
timesteps_total: 4389000
training_iteration: 266

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973821989528796
  reward for individual goal_min: 0.5
episode_len_mean: 42.375321336760926
episode_reward_max: 2.0
episode_reward_mean: 1.9974293059125965
episode_reward_min: 1.0
episodes_this_iter: 389
episodes_total: 73520
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9974293059125964
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 58.623069524765015
time_total_s: 14421.753824234009
timers:
  learn_throughput: 376.112
  learn_time_ms: 43869.899
  load_throughput: 3525361.344
  load_time_ms: 4.68
  training_iteration_time_ms: 56838.043
  update_time_ms: 2.548
timesteps_total: 4191000
training_iteration: 254

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24342105263157895
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9485294117647058
  reward for individual goal_min: 0.0
episode_len_mean: 190.15
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 24341
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.75477981567383
time_total_s: 14553.507547855377
timers:
  learn_throughput: 440.7
  learn_time_ms: 37440.417
  load_throughput: 4691456.191
  load_time_ms: 3.517
  training_iteration_time_ms: 49339.14
  update_time_ms: 2.81
timesteps_total: 4719000
training_iteration: 286

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18243243243243243
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 183.94
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 26221
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.95869183540344
time_total_s: 14551.600528717041
timers:
  learn_throughput: 472.154
  learn_time_ms: 34946.256
  load_throughput: 4964741.633
  load_time_ms: 3.323
  training_iteration_time_ms: 46129.907
  update_time_ms: 2.602
timesteps_total: 4818000
training_iteration: 292

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2702702702702703
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.08
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 24866
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.02882647514343
time_total_s: 14541.76418685913
timers:
  learn_throughput: 462.905
  learn_time_ms: 35644.497
  load_throughput: 4810015.082
  load_time_ms: 3.43
  training_iteration_time_ms: 47164.934
  update_time_ms: 2.577
timesteps_total: 4686000
training_iteration: 284

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 47.64553314121037
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 347
episodes_total: 47757
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 70.8106336593628
time_total_s: 14527.709731578827
timers:
  learn_throughput: 304.775
  learn_time_ms: 54138.322
  load_throughput: 3296246.606
  load_time_ms: 5.006
  training_iteration_time_ms: 70243.339
  update_time_ms: 2.868
timesteps_total: 3828000
training_iteration: 232

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26973684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 172.83
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 27034
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.48768615722656
time_total_s: 14541.726873636246
timers:
  learn_throughput: 482.264
  learn_time_ms: 34213.608
  load_throughput: 4930221.769
  load_time_ms: 3.347
  training_iteration_time_ms: 45049.249
  update_time_ms: 2.488
timesteps_total: 4834500
training_iteration: 293

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 87.71666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.85
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9333333333333333
  agent_1: 0.9166666666666666
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1891891891891892
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9797297297297297
  reward for individual goal_min: 0.0
episode_len_mean: 184.23
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 25505
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.6369252204895
time_total_s: 14565.9599442482
timers:
  learn_throughput: 427.418
  learn_time_ms: 38603.877
  load_throughput: 4685104.153
  load_time_ms: 3.522
  training_iteration_time_ms: 50669.794
  update_time_ms: 2.624
timesteps_total: 4620000
training_iteration: 280

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18181818181818182
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 186.39
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 24242
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.12554454803467
time_total_s: 14561.287690162659
timers:
  learn_throughput: 412.036
  learn_time_ms: 40045.078
  load_throughput: 4413789.725
  load_time_ms: 3.738
  training_iteration_time_ms: 52509.768
  update_time_ms: 2.702
timesteps_total: 4422000
training_iteration: 268

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1917808219178082
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.11
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 24359
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.86743092536926
time_total_s: 14567.948298454285
timers:
  learn_throughput: 409.168
  learn_time_ms: 40325.774
  load_throughput: 4282789.017
  load_time_ms: 3.853
  training_iteration_time_ms: 53282.029
  update_time_ms: 2.726
timesteps_total: 4405500
training_iteration: 267

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.997093023255814
  reward for individual goal_min: 0.5
episode_len_mean: 45.33604336043361
episode_reward_max: 2.0
episode_reward_mean: 1.997289972899729
episode_reward_min: 1.0
episodes_this_iter: 369
episodes_total: 70077
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.997289972899729
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 56.885759353637695
time_total_s: 14463.34699344635
timers:
  learn_throughput: 381.312
  learn_time_ms: 43271.619
  load_throughput: 4031152.273
  load_time_ms: 4.093
  training_iteration_time_ms: 55790.819
  update_time_ms: 2.56
timesteps_total: 4587000
training_iteration: 278

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.35333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 154.31428571428572
episode_reward_max: 2.0
episode_reward_mean: 1.4285714285714286
episode_reward_min: 0.0
episodes_this_iter: 105
episodes_total: 26326
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7142857142857143
  agent_1: 0.7142857142857143
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.89585828781128
time_total_s: 14599.496387004852
timers:
  learn_throughput: 468.786
  learn_time_ms: 35197.329
  load_throughput: 4876204.219
  load_time_ms: 3.384
  training_iteration_time_ms: 46382.753
  update_time_ms: 2.607
timesteps_total: 4834500
training_iteration: 293

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.005102040816325
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 392
episodes_total: 73912
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.42546033859253
time_total_s: 14480.179284572601
timers:
  learn_throughput: 374.907
  learn_time_ms: 44010.973
  load_throughput: 3501673.067
  load_time_ms: 4.712
  training_iteration_time_ms: 56991.323
  update_time_ms: 2.552
timesteps_total: 4207500
training_iteration: 255

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2702702702702703
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.55
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 24963
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.10810208320618
time_total_s: 14588.872288942337
timers:
  learn_throughput: 464.047
  learn_time_ms: 35556.756
  load_throughput: 4789012.248
  load_time_ms: 3.445
  training_iteration_time_ms: 47005.313
  update_time_ms: 2.593
timesteps_total: 4702500
training_iteration: 285

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2112676056338028
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9605263157894737
  reward for individual goal_min: 0.0
episode_len_mean: 181.81
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 24430
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.711820125579834
time_total_s: 14605.219367980957
timers:
  learn_throughput: 438.131
  learn_time_ms: 37659.988
  load_throughput: 4645322.594
  load_time_ms: 3.552
  training_iteration_time_ms: 49655.394
  update_time_ms: 2.819
timesteps_total: 4735500
training_iteration: 287

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3780487804878049
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 155.45794392523365
episode_reward_max: 2.0
episode_reward_mean: 1.4299065420560748
episode_reward_min: 0.0
episodes_this_iter: 107
episodes_total: 27141
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6728971962616822
  agent_1: 0.7570093457943925
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.57257127761841
time_total_s: 14588.299444913864
timers:
  learn_throughput: 481.281
  learn_time_ms: 34283.499
  load_throughput: 4957202.433
  load_time_ms: 3.328
  training_iteration_time_ms: 45128.177
  update_time_ms: 2.494
timesteps_total: 4851000
training_iteration: 294

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19736842105263158
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 184.12
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 25595
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.724398612976074
time_total_s: 14620.684342861176
timers:
  learn_throughput: 424.474
  learn_time_ms: 38871.669
  load_throughput: 4808978.945
  load_time_ms: 3.431
  training_iteration_time_ms: 50946.831
  update_time_ms: 2.632
timesteps_total: 4636500
training_iteration: 281

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2716049382716049
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 179.07
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 24335
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.01370620727539
time_total_s: 14612.301396369934
timers:
  learn_throughput: 414.16
  learn_time_ms: 39839.662
  load_throughput: 4387736.708
  load_time_ms: 3.76
  training_iteration_time_ms: 52252.867
  update_time_ms: 2.696
timesteps_total: 4438500
training_iteration: 269

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3076923076923077
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9791666666666666
  reward for individual goal_min: 0.5
episode_len_mean: 172.27
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 24457
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.35531711578369
time_total_s: 14623.303615570068
timers:
  learn_throughput: 408.717
  learn_time_ms: 40370.274
  load_throughput: 4261322.989
  load_time_ms: 3.872
  training_iteration_time_ms: 53308.244
  update_time_ms: 2.712
timesteps_total: 4422000
training_iteration: 268

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.813648293963254
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 381
episodes_total: 70458
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 54.95911455154419
time_total_s: 14518.306107997894
timers:
  learn_throughput: 379.757
  learn_time_ms: 43448.845
  load_throughput: 4024260.694
  load_time_ms: 4.1
  training_iteration_time_ms: 56020.414
  update_time_ms: 2.593
timesteps_total: 4603500
training_iteration: 279

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9939759036144579
  reward for individual goal_min: 0.5
episode_len_mean: 48.5727002967359
episode_reward_max: 2.0
episode_reward_mean: 1.9940652818991098
episode_reward_min: 1.0
episodes_this_iter: 337
episodes_total: 48094
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9940652818991098
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 70.07635021209717
time_total_s: 14597.786081790924
timers:
  learn_throughput: 304.974
  learn_time_ms: 54103.046
  load_throughput: 3258440.416
  load_time_ms: 5.064
  training_iteration_time_ms: 70220.758
  update_time_ms: 2.821
timesteps_total: 3844500
training_iteration: 233

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9594594594594594
  reward for individual goal_min: 0.0
episode_len_mean: 185.31
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 26411
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.85105514526367
time_total_s: 14647.347442150116
timers:
  learn_throughput: 466.794
  learn_time_ms: 35347.46
  load_throughput: 4889260.528
  load_time_ms: 3.375
  training_iteration_time_ms: 46600.203
  update_time_ms: 2.586
timesteps_total: 4851000
training_iteration: 294

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19594594594594594
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 179.43
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 25055
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.71842956542969
time_total_s: 14636.590718507767
timers:
  learn_throughput: 465.027
  learn_time_ms: 35481.798
  load_throughput: 4756198.396
  load_time_ms: 3.469
  training_iteration_time_ms: 46956.101
  update_time_ms: 2.579
timesteps_total: 4719000
training_iteration: 286

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24025974025974026
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.43
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 27237
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.12639784812927
time_total_s: 14633.425842761993
timers:
  learn_throughput: 479.807
  learn_time_ms: 34388.798
  load_throughput: 4977059.763
  load_time_ms: 3.315
  training_iteration_time_ms: 45263.305
  update_time_ms: 2.505
timesteps_total: 4867500
training_iteration: 295

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21621621621621623
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9492753623188406
  reward for individual goal_min: 0.0
episode_len_mean: 192.58
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 82
episodes_total: 24512
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.241586685180664
time_total_s: 14656.460954666138
timers:
  learn_throughput: 436.756
  learn_time_ms: 37778.542
  load_throughput: 4607652.299
  load_time_ms: 3.581
  training_iteration_time_ms: 49825.863
  update_time_ms: 2.809
timesteps_total: 4752000
training_iteration: 288

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.31713554987212
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 391
episodes_total: 74303
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.56000852584839
time_total_s: 14538.73929309845
timers:
  learn_throughput: 373.784
  learn_time_ms: 44143.167
  load_throughput: 3493453.675
  load_time_ms: 4.723
  training_iteration_time_ms: 57176.863
  update_time_ms: 2.547
timesteps_total: 4224000
training_iteration: 256

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2289156626506024
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 192.32
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 25678
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.51356053352356
time_total_s: 14667.1979033947
timers:
  learn_throughput: 425.286
  learn_time_ms: 38797.457
  load_throughput: 4778694.95
  load_time_ms: 3.453
  training_iteration_time_ms: 50742.282
  update_time_ms: 2.647
timesteps_total: 4653000
training_iteration: 282

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17532467532467533
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9882352941176471
  reward for individual goal_min: 0.0
episode_len_mean: 182.53
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 24422
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.7708306312561
time_total_s: 14663.07222700119
timers:
  learn_throughput: 416.4
  learn_time_ms: 39625.383
  load_throughput: 4401466.35
  load_time_ms: 3.749
  training_iteration_time_ms: 52040.792
  update_time_ms: 2.726
timesteps_total: 4455000
training_iteration: 270

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20238095238095238
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 190.68
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 24543
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.64883470535278
time_total_s: 14678.952450275421
timers:
  learn_throughput: 406.682
  learn_time_ms: 40572.227
  load_throughput: 4232214.381
  load_time_ms: 3.899
  training_iteration_time_ms: 53578.395
  update_time_ms: 2.703
timesteps_total: 4438500
training_iteration: 269

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.31666666666667
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.994475138121547
  reward for individual goal_min: 0.5
episode_len_mean: 43.341145833333336
episode_reward_max: 2.0
episode_reward_mean: 1.9947916666666667
episode_reward_min: 1.0
episodes_this_iter: 384
episodes_total: 70842
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973958333333334
  agent_1: 0.9973958333333334
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.3767614364624
time_total_s: 14579.682869434357
timers:
  learn_throughput: 378.592
  learn_time_ms: 43582.566
  load_throughput: 4021548.045
  load_time_ms: 4.103
  training_iteration_time_ms: 56151.699
  update_time_ms: 2.611
timesteps_total: 4620000
training_iteration: 280

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-18g5xov65h/checkpoint_000280/checkpoint-280
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29285714285714287
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 164.74
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 26510
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.48730158805847
time_total_s: 14694.834743738174
timers:
  learn_throughput: 465.024
  learn_time_ms: 35482.057
  load_throughput: 4907079.62
  load_time_ms: 3.362
  training_iteration_time_ms: 46811.568
  update_time_ms: 2.595
timesteps_total: 4867500
training_iteration: 295

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 184.34
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 25145
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.41700839996338
time_total_s: 14683.00772690773
timers:
  learn_throughput: 465.127
  learn_time_ms: 35474.155
  load_throughput: 4784542.57
  load_time_ms: 3.449
  training_iteration_time_ms: 46959.039
  update_time_ms: 2.586
timesteps_total: 4735500
training_iteration: 287

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2792207792207792
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 173.01
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 27331
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.06792736053467
time_total_s: 14680.493770122528
timers:
  learn_throughput: 480.752
  learn_time_ms: 34321.227
  load_throughput: 4909377.088
  load_time_ms: 3.361
  training_iteration_time_ms: 45267.842
  update_time_ms: 2.513
timesteps_total: 4884000
training_iteration: 296

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 47.95072463768116
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 345
episodes_total: 48439
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 71.38714861869812
time_total_s: 14669.173230409622
timers:
  learn_throughput: 305.341
  learn_time_ms: 54037.915
  load_throughput: 3210537.069
  load_time_ms: 5.139
  training_iteration_time_ms: 70191.324
  update_time_ms: 2.793
timesteps_total: 3861000
training_iteration: 234

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.27692307692307694
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9577464788732394
  reward for individual goal_min: 0.0
episode_len_mean: 175.51
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 24604
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.74
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.786479234695435
time_total_s: 14706.247433900833
timers:
  learn_throughput: 437.498
  learn_time_ms: 37714.484
  load_throughput: 4613703.642
  load_time_ms: 3.576
  training_iteration_time_ms: 49648.848
  update_time_ms: 2.783
timesteps_total: 4768500
training_iteration: 289

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9928571428571429
  reward for individual goal_min: 0.5
episode_len_mean: 172.02
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 25774
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.20477914810181
time_total_s: 14722.4026825428
timers:
  learn_throughput: 424.88
  learn_time_ms: 38834.479
  load_throughput: 4809847.933
  load_time_ms: 3.43
  training_iteration_time_ms: 50921.308
  update_time_ms: 2.657
timesteps_total: 4669500
training_iteration: 283

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.25575447570333
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 391
episodes_total: 74694
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.24716591835022
time_total_s: 14597.9864590168
timers:
  learn_throughput: 371.008
  learn_time_ms: 44473.377
  load_throughput: 3483167.797
  load_time_ms: 4.737
  training_iteration_time_ms: 57568.338
  update_time_ms: 2.661
timesteps_total: 4240500
training_iteration: 257

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22289156626506024
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9868421052631579
  reward for individual goal_min: 0.0
episode_len_mean: 187.14
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 24514
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.355804204940796
time_total_s: 14715.428031206131
timers:
  learn_throughput: 416.051
  learn_time_ms: 39658.585
  load_throughput: 4419172.946
  load_time_ms: 3.734
  training_iteration_time_ms: 52093.082
  update_time_ms: 2.72
timesteps_total: 4471500
training_iteration: 271

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1987179487179487
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 184.22
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 24629
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.16108441352844
time_total_s: 14733.11353468895
timers:
  learn_throughput: 407.032
  learn_time_ms: 40537.356
  load_throughput: 4215303.877
  load_time_ms: 3.914
  training_iteration_time_ms: 53539.804
  update_time_ms: 2.706
timesteps_total: 4455000
training_iteration: 270

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 178.38
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 26600
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.8751323223114
time_total_s: 14741.709876060486
timers:
  learn_throughput: 463.603
  learn_time_ms: 35590.814
  load_throughput: 4920162.095
  load_time_ms: 3.354
  training_iteration_time_ms: 46931.419
  update_time_ms: 2.581
timesteps_total: 4884000
training_iteration: 296

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.12318840579710146
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.72
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 25236
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.63409161567688
time_total_s: 14730.641818523407
timers:
  learn_throughput: 464.221
  learn_time_ms: 35543.445
  load_throughput: 4802504.858
  load_time_ms: 3.436
  training_iteration_time_ms: 46940.263
  update_time_ms: 2.573
timesteps_total: 4752000
training_iteration: 288

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3108108108108108
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9940476190476191
  reward for individual goal_min: 0.5
episode_len_mean: 153.62616822429908
episode_reward_max: 2.0
episode_reward_mean: 1.439252336448598
episode_reward_min: 0.0
episodes_this_iter: 107
episodes_total: 27438
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6915887850467289
  agent_1: 0.7476635514018691
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.508039712905884
time_total_s: 14727.001809835434
timers:
  learn_throughput: 479.148
  learn_time_ms: 34436.089
  load_throughput: 4888189.973
  load_time_ms: 3.375
  training_iteration_time_ms: 45463.288
  update_time_ms: 2.518
timesteps_total: 4900500
training_iteration: 297

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.58760107816712
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 371
episodes_total: 71213
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.12648320198059
time_total_s: 14636.809352636337
timers:
  learn_throughput: 376.172
  learn_time_ms: 43862.897
  load_throughput: 4030213.257
  load_time_ms: 4.094
  training_iteration_time_ms: 56490.454
  update_time_ms: 2.609
timesteps_total: 4636500
training_iteration: 281

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19863013698630136
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.961038961038961
  reward for individual goal_min: 0.0
episode_len_mean: 184.45
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 24694
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.968191623687744
time_total_s: 14756.21562552452
timers:
  learn_throughput: 436.878
  learn_time_ms: 37768.001
  load_throughput: 4581025.875
  load_time_ms: 3.602
  training_iteration_time_ms: 49692.616
  update_time_ms: 2.79
timesteps_total: 4785000
training_iteration: 290

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29878048780487804
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 187.42
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 25861
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.4013934135437
time_total_s: 14770.804075956345
timers:
  learn_throughput: 424.0
  learn_time_ms: 38915.132
  load_throughput: 4763203.733
  load_time_ms: 3.464
  training_iteration_time_ms: 50964.809
  update_time_ms: 2.642
timesteps_total: 4686000
training_iteration: 284

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973118279569892
  reward for individual goal_min: 0.5
episode_len_mean: 49.17910447761194
episode_reward_max: 2.0
episode_reward_mean: 1.9970149253731344
episode_reward_min: 1.0
episodes_this_iter: 335
episodes_total: 48774
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9970149253731343
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 70.28867721557617
time_total_s: 14739.461907625198
timers:
  learn_throughput: 304.929
  learn_time_ms: 54110.99
  load_throughput: 3339720.202
  load_time_ms: 4.941
  training_iteration_time_ms: 70232.517
  update_time_ms: 2.847
timesteps_total: 3877500
training_iteration: 235

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935897435897436
  reward for individual goal_min: 0.5
episode_len_mean: 170.8
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 24609
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.20793080329895
time_total_s: 14767.63596200943
timers:
  learn_throughput: 415.598
  learn_time_ms: 39701.846
  load_throughput: 4452351.499
  load_time_ms: 3.706
  training_iteration_time_ms: 52201.992
  update_time_ms: 2.75
timesteps_total: 4488000
training_iteration: 272

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.69620253164557
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 395
episodes_total: 75089
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 60.014981269836426
time_total_s: 14658.001440286636
timers:
  learn_throughput: 369.079
  learn_time_ms: 44705.852
  load_throughput: 3467582.724
  load_time_ms: 4.758
  training_iteration_time_ms: 57854.995
  update_time_ms: 2.646
timesteps_total: 4257000
training_iteration: 258

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22435897435897437
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 181.46
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 24719
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.60520815849304
time_total_s: 14785.718742847443
timers:
  learn_throughput: 408.299
  learn_time_ms: 40411.607
  load_throughput: 4185653.649
  load_time_ms: 3.942
  training_iteration_time_ms: 53344.296
  update_time_ms: 2.696
timesteps_total: 4471500
training_iteration: 271

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1506849315068493
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.59
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 25322
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.19524359703064
time_total_s: 14776.837062120438
timers:
  learn_throughput: 464.143
  learn_time_ms: 35549.36
  load_throughput: 4807976.657
  load_time_ms: 3.432
  training_iteration_time_ms: 46992.05
  update_time_ms: 2.59
timesteps_total: 4768500
training_iteration: 289

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2625
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.98
  reward for individual goal_min: 0.0
episode_len_mean: 184.38
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 26691
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.07339596748352
time_total_s: 14790.78327202797
timers:
  learn_throughput: 460.576
  learn_time_ms: 35824.691
  load_throughput: 4901241.209
  load_time_ms: 3.366
  training_iteration_time_ms: 47237.845
  update_time_ms: 2.581
timesteps_total: 4900500
training_iteration: 297

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2535211267605634
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 167.1
episode_reward_max: 2.0
episode_reward_mean: 1.39
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 27533
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.75
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.349950551986694
time_total_s: 14772.35176038742
timers:
  learn_throughput: 478.166
  learn_time_ms: 34506.836
  load_throughput: 4921841.69
  load_time_ms: 3.352
  training_iteration_time_ms: 45634.469
  update_time_ms: 2.529
timesteps_total: 4917000
training_iteration: 298

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16417910447761194
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9715909090909091
  reward for individual goal_min: 0.0
episode_len_mean: 176.05
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 24788
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.44843411445618
time_total_s: 14802.664059638977
timers:
  learn_throughput: 439.952
  learn_time_ms: 37504.06
  load_throughput: 4575906.903
  load_time_ms: 3.606
  training_iteration_time_ms: 49280.307
  update_time_ms: 2.774
timesteps_total: 4801500
training_iteration: 291

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.44473007712082
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 389
episodes_total: 71602
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.2711124420166
time_total_s: 14694.080465078354
timers:
  learn_throughput: 376.045
  learn_time_ms: 43877.709
  load_throughput: 4055174.644
  load_time_ms: 4.069
  training_iteration_time_ms: 56558.753
  update_time_ms: 2.63
timesteps_total: 4653000
training_iteration: 282

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18421052631578946
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 190.2
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 25949
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.05850911140442
time_total_s: 14821.862585067749
timers:
  learn_throughput: 423.599
  learn_time_ms: 38951.934
  load_throughput: 4791764.421
  load_time_ms: 3.443
  training_iteration_time_ms: 50951.252
  update_time_ms: 2.658
timesteps_total: 4702500
training_iteration: 285

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22727272727272727
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9814814814814815
  reward for individual goal_min: 0.0
episode_len_mean: 177.82
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 24698
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.0971622467041
time_total_s: 14820.733124256134
timers:
  learn_throughput: 415.517
  learn_time_ms: 39709.524
  load_throughput: 4466863.914
  load_time_ms: 3.694
  training_iteration_time_ms: 52164.176
  update_time_ms: 2.752
timesteps_total: 4504500
training_iteration: 273

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2733333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9940476190476191
  reward for individual goal_min: 0.5
episode_len_mean: 158.85849056603774
episode_reward_max: 2.0
episode_reward_mean: 1.3962264150943395
episode_reward_min: 0.0
episodes_this_iter: 106
episodes_total: 27639
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6981132075471698
  agent_1: 0.6981132075471698
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.20073676109314
time_total_s: 14818.552497148514
timers:
  learn_throughput: 476.096
  learn_time_ms: 34656.891
  load_throughput: 4900130.706
  load_time_ms: 3.367
  training_iteration_time_ms: 45756.024
  update_time_ms: 2.525
timesteps_total: 4933500
training_iteration: 299

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 188.16
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 25412
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.55
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.37990760803223
time_total_s: 14826.21696972847
timers:
  learn_throughput: 462.333
  learn_time_ms: 35688.591
  load_throughput: 4802005.01
  load_time_ms: 3.436
  training_iteration_time_ms: 47136.905
  update_time_ms: 2.607
timesteps_total: 4785000
training_iteration: 290

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29577464788732394
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9802631578947368
  reward for individual goal_min: 0.0
episode_len_mean: 169.27
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 26788
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.89256262779236
time_total_s: 14841.675834655762
timers:
  learn_throughput: 456.67
  learn_time_ms: 36131.087
  load_throughput: 4829584.636
  load_time_ms: 3.416
  training_iteration_time_ms: 47645.572
  update_time_ms: 2.587
timesteps_total: 4917000
training_iteration: 298

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.974554707379134
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 393
episodes_total: 75482
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 60.53347063064575
time_total_s: 14718.534910917282
timers:
  learn_throughput: 367.03
  learn_time_ms: 44955.469
  load_throughput: 3421027.405
  load_time_ms: 4.823
  training_iteration_time_ms: 58191.068
  update_time_ms: 2.627
timesteps_total: 4273500
training_iteration: 259

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17763157894736842
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9929577464788732
  reward for individual goal_min: 0.5
episode_len_mean: 184.95
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 24809
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.5
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.62681841850281
time_total_s: 14841.345561265945
timers:
  learn_throughput: 406.811
  learn_time_ms: 40559.386
  load_throughput: 4176786.809
  load_time_ms: 3.95
  training_iteration_time_ms: 53494.662
  update_time_ms: 2.699
timesteps_total: 4488000
training_iteration: 272

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 48.505847953216374
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 342
episodes_total: 49116
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 71.9981324672699
time_total_s: 14811.460040092468
timers:
  learn_throughput: 303.415
  learn_time_ms: 54380.957
  load_throughput: 3326220.838
  load_time_ms: 4.961
  training_iteration_time_ms: 70532.539
  update_time_ms: 2.93
timesteps_total: 3894000
training_iteration: 236

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2088607594936709
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9675324675324676
  reward for individual goal_min: 0.0
episode_len_mean: 191.35
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 24873
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.51
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.738550662994385
time_total_s: 14853.402610301971
timers:
  learn_throughput: 438.806
  learn_time_ms: 37602.024
  load_throughput: 4592303.65
  load_time_ms: 3.593
  training_iteration_time_ms: 49478.564
  update_time_ms: 2.778
timesteps_total: 4818000
training_iteration: 292

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9916666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 44.72826086956522
episode_reward_max: 2.0
episode_reward_mean: 1.9918478260869565
episode_reward_min: 1.0
episodes_this_iter: 368
episodes_total: 71970
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9918478260869565
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 58.59880065917969
time_total_s: 14752.679265737534
timers:
  learn_throughput: 374.643
  learn_time_ms: 44041.9
  load_throughput: 4033384.193
  load_time_ms: 4.091
  training_iteration_time_ms: 56807.268
  update_time_ms: 2.629
timesteps_total: 4669500
training_iteration: 283

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9928571428571429
  reward for individual goal_min: 0.5
episode_len_mean: 181.25
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 26042
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.34914302825928
time_total_s: 14873.211728096008
timers:
  learn_throughput: 422.493
  learn_time_ms: 39053.938
  load_throughput: 4799074.664
  load_time_ms: 3.438
  training_iteration_time_ms: 51040.495
  update_time_ms: 2.683
timesteps_total: 4719000
training_iteration: 286

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25949367088607594
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 177.48
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 24795
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.75790238380432
time_total_s: 14872.491026639938
timers:
  learn_throughput: 415.54
  learn_time_ms: 39707.329
  load_throughput: 4478339.276
  load_time_ms: 3.684
  training_iteration_time_ms: 52163.195
  update_time_ms: 2.741
timesteps_total: 4521000
training_iteration: 274

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 171.12
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 25508
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.08680295944214
time_total_s: 14874.303772687912
timers:
  learn_throughput: 461.229
  learn_time_ms: 35773.959
  load_throughput: 4722992.971
  load_time_ms: 3.494
  training_iteration_time_ms: 47267.913
  update_time_ms: 2.568
timesteps_total: 4801500
training_iteration: 291

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22297297297297297
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.53
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 26876
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.54392862319946
time_total_s: 14889.219763278961
timers:
  learn_throughput: 453.552
  learn_time_ms: 36379.481
  load_throughput: 4835827.854
  load_time_ms: 3.412
  training_iteration_time_ms: 47871.441
  update_time_ms: 2.566
timesteps_total: 4933500
training_iteration: 299

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95
  reward for individual goal_min: 0.5
episode_len_mean: 78.25
episode_reward_max: 2.0
episode_reward_mean: 1.8833333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9333333333333333
  agent_1: 0.95
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2357142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 165.39
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 27737
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.52281975746155
time_total_s: 14872.075316905975
timers:
  learn_throughput: 476.995
  learn_time_ms: 34591.593
  load_throughput: 4872805.21
  load_time_ms: 3.386
  training_iteration_time_ms: 45756.531
  update_time_ms: 2.517
timesteps_total: 4950000
training_iteration: 300

Starting final evaluation!
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25675675675675674
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935897435897436
  reward for individual goal_min: 0.5
episode_len_mean: 169.49
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 24905
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.29861259460449
time_total_s: 14891.64417386055
timers:
  learn_throughput: 407.588
  learn_time_ms: 40482.088
  load_throughput: 4202377.659
  load_time_ms: 3.926
  training_iteration_time_ms: 53389.385
  update_time_ms: 2.699
timesteps_total: 4504500
training_iteration: 273

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21084337349397592
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.967948717948718
  reward for individual goal_min: 0.0
episode_len_mean: 196.73
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 24958
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.87079930305481
time_total_s: 14901.273409605026
timers:
  learn_throughput: 441.506
  learn_time_ms: 37372.047
  load_throughput: 4586429.854
  load_time_ms: 3.598
  training_iteration_time_ms: 49216.094
  update_time_ms: 2.749
timesteps_total: 4834500
training_iteration: 293

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.983333333333334
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974226804123711
  reward for individual goal_min: 0.5
episode_len_mean: 42.11704834605598
episode_reward_max: 2.0
episode_reward_mean: 1.9974554707379135
episode_reward_min: 1.0
episodes_this_iter: 393
episodes_total: 75875
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974554707379135
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 63.1547417640686
time_total_s: 14781.68965268135
timers:
  learn_throughput: 365.983
  learn_time_ms: 45084.121
  load_throughput: 3421145.786
  load_time_ms: 4.823
  training_iteration_time_ms: 58332.224
  update_time_ms: 2.632
timesteps_total: 4290000
training_iteration: 260

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-184s1jdk42/checkpoint_000260/checkpoint-260
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9942196531791907
  reward for individual goal_min: 0.5
episode_len_mean: 47.182857142857145
episode_reward_max: 2.0
episode_reward_mean: 1.9942857142857142
episode_reward_min: 1.0
episodes_this_iter: 350
episodes_total: 49466
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9942857142857143
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 71.35294890403748
time_total_s: 14882.812988996506
timers:
  learn_throughput: 302.672
  learn_time_ms: 54514.485
  load_throughput: 3251169.333
  load_time_ms: 5.075
  training_iteration_time_ms: 70675.841
  update_time_ms: 2.921
timesteps_total: 3910500
training_iteration: 237

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23809523809523808
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 197.34
episode_reward_max: 2.0
episode_reward_mean: 1.15
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 26125
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.41140866279602
time_total_s: 14922.623136758804
timers:
  learn_throughput: 424.591
  learn_time_ms: 38860.96
  load_throughput: 4780378.391
  load_time_ms: 3.452
  training_iteration_time_ms: 50805.808
  update_time_ms: 2.675
timesteps_total: 4735500
training_iteration: 287

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9941520467836257
  reward for individual goal_min: 0.5
episode_len_mean: 44.44235924932976
episode_reward_max: 2.0
episode_reward_mean: 1.9946380697050938
episode_reward_min: 1.0
episodes_this_iter: 373
episodes_total: 72343
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9946380697050938
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 57.485296964645386
time_total_s: 14810.164562702179
timers:
  learn_throughput: 373.914
  learn_time_ms: 44127.837
  load_throughput: 4027750.418
  load_time_ms: 4.097
  training_iteration_time_ms: 56915.908
  update_time_ms: 2.662
timesteps_total: 4686000
training_iteration: 284

Evaluation results over 10 episodes for each goal
{'[0, 0, 1]': 1.9, '[0, 1, 0]': 2.0, '[1, 0, 0]': 1.8, '[0, 1, 1]': 2.0, '[1, 0, 1]': 2.0, '[1, 1, 0]': 2.0}
Moviepy - Building video /gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-28met_xp4k/trained_agent.mp4.
Moviepy - Writing video /gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-28met_xp4k/trained_agent.mp4

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 182.74
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 24885
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.86170721054077
time_total_s: 14924.35273385048
timers:
  learn_throughput: 415.021
  learn_time_ms: 39757.06
  load_throughput: 4472695.405
  load_time_ms: 3.689
  training_iteration_time_ms: 52172.206
  update_time_ms: 2.748
timesteps_total: 4537500
training_iteration: 275

Moviepy - Done !
Moviepy - video ready /gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-28met_xp4k/trained_agent.mp4
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19230769230769232
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9863013698630136
  reward for individual goal_min: 0.0
episode_len_mean: 187.64
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 25600
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.69021511077881
time_total_s: 14921.99398779869
timers:
  learn_throughput: 459.53
  learn_time_ms: 35906.248
  load_throughput: 4698016.822
  load_time_ms: 3.512
  training_iteration_time_ms: 47341.558
  update_time_ms: 2.54
timesteps_total: 4818000
training_iteration: 292

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26973684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.5
episode_len_mean: 176.31
episode_reward_max: 2.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 25000
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.60907506942749
time_total_s: 14944.253248929977
timers:
  learn_throughput: 405.947
  learn_time_ms: 40645.65
  load_throughput: 4231800.316
  load_time_ms: 3.899
  training_iteration_time_ms: 53546.437
  update_time_ms: 2.681
timesteps_total: 4521000
training_iteration: 274

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23648648648648649
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 179.82
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 25049
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.95469951629639
time_total_s: 14951.228109121323
timers:
  learn_throughput: 437.664
  learn_time_ms: 37700.16
  load_throughput: 4580449.798
  load_time_ms: 3.602
  training_iteration_time_ms: 49521.327
  update_time_ms: 2.731
timesteps_total: 4851000
training_iteration: 294

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.8333333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9166666666666666
  reward for individual goal_min: 0.5
episode_len_mean: 100.76666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.75
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.8833333333333333
  agent_1: 0.8666666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2857142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 173.29
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 26969
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.983739137649536
time_total_s: 14950.20350241661
timers:
  learn_throughput: 453.422
  learn_time_ms: 36389.941
  load_throughput: 4821946.03
  load_time_ms: 3.422
  training_iteration_time_ms: 47876.448
  update_time_ms: 2.59
timesteps_total: 4950000
training_iteration: 300

Starting final evaluation!
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.14358974358974
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 390
episodes_total: 76265
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.99176216125488
time_total_s: 14840.681414842606
timers:
  learn_throughput: 364.266
  learn_time_ms: 45296.57
  load_throughput: 3420756.851
  load_time_ms: 4.823
  training_iteration_time_ms: 58588.943
  update_time_ms: 2.63
timesteps_total: 4306500
training_iteration: 261

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20253164556962025
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 191.34
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 26211
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.560182094573975
time_total_s: 14973.183318853378
timers:
  learn_throughput: 424.892
  learn_time_ms: 38833.434
  load_throughput: 4753944.057
  load_time_ms: 3.471
  training_iteration_time_ms: 50875.53
  update_time_ms: 2.686
timesteps_total: 4752000
training_iteration: 288

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9939393939393939
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9975
  reward for individual goal_min: 0.5
episode_len_mean: 44.917808219178085
episode_reward_max: 2.0
episode_reward_mean: 1.9917808219178081
episode_reward_min: 0.0
episodes_this_iter: 365
episodes_total: 72708
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972602739726028
  agent_1: 0.9945205479452055
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.38368105888367
time_total_s: 14868.548243761063
timers:
  learn_throughput: 372.818
  learn_time_ms: 44257.558
  load_throughput: 4034512.84
  load_time_ms: 4.09
  training_iteration_time_ms: 57074.537
  update_time_ms: 2.679
timesteps_total: 4702500
training_iteration: 285

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2037037037037037
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 194.03
episode_reward_max: 2.0
episode_reward_mean: 1.17
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 25685
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.627302169799805
time_total_s: 14970.62128996849
timers:
  learn_throughput: 457.287
  learn_time_ms: 36082.389
  load_throughput: 4699324.768
  load_time_ms: 3.511
  training_iteration_time_ms: 47651.046
  update_time_ms: 2.535
timesteps_total: 4834500
training_iteration: 293

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2638888888888889
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 163.36274509803923
episode_reward_max: 2.0
episode_reward_mean: 1.3627450980392157
episode_reward_min: 0.0
episodes_this_iter: 102
episodes_total: 24987
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6274509803921569
  agent_1: 0.7352941176470589
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.07474732398987
time_total_s: 14977.427481174469
timers:
  learn_throughput: 415.565
  learn_time_ms: 39704.993
  load_throughput: 4500326.18
  load_time_ms: 3.666
  training_iteration_time_ms: 52091.118
  update_time_ms: 2.748
timesteps_total: 4554000
training_iteration: 276

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.0819209039548
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 354
episodes_total: 49820
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 70.64501881599426
time_total_s: 14953.4580078125
timers:
  learn_throughput: 302.256
  learn_time_ms: 54589.47
  load_throughput: 3354272.254
  load_time_ms: 4.919
  training_iteration_time_ms: 70704.521
  update_time_ms: 2.972
timesteps_total: 3927000
training_iteration: 238

Evaluation results over 10 episodes for each goal
{'[0, 0, 1]': 2.0, '[0, 1, 0]': 2.0, '[1, 0, 0]': 1.8, '[0, 1, 1]': 1.8, '[1, 0, 1]': 2.0, '[1, 1, 0]': 2.0}
Moviepy - Building video /gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-18pddilqzj/trained_agent.mp4.
Moviepy - Writing video /gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-18pddilqzj/trained_agent.mp4

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17721518987341772
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9565217391304348
  reward for individual goal_min: 0.0
episode_len_mean: 204.12
episode_reward_max: 2.0
episode_reward_mean: 1.14
episode_reward_min: 0.0
episodes_this_iter: 80
episodes_total: 25129
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.5
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.885950803756714
time_total_s: 15001.11405992508
timers:
  learn_throughput: 435.328
  learn_time_ms: 37902.423
  load_throughput: 4536403.836
  load_time_ms: 3.637
  training_iteration_time_ms: 49797.532
  update_time_ms: 2.722
timesteps_total: 4867500
training_iteration: 295

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.275
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.987012987012987
  reward for individual goal_min: 0.0
episode_len_mean: 174.82
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 25093
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.043381452560425
time_total_s: 14997.296630382538
timers:
  learn_throughput: 406.037
  learn_time_ms: 40636.67
  load_throughput: 4218310.019
  load_time_ms: 3.912
  training_iteration_time_ms: 53515.275
  update_time_ms: 2.676
timesteps_total: 4537500
training_iteration: 275

Moviepy - Done !
Moviepy - video ready /gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-18pddilqzj/trained_agent.mp4
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2357142857142857
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.13
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 26306
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.904980421066284
time_total_s: 15025.088299274445
timers:
  learn_throughput: 423.171
  learn_time_ms: 38991.364
  load_throughput: 4741014.845
  load_time_ms: 3.48
  training_iteration_time_ms: 51021.028
  update_time_ms: 2.707
timesteps_total: 4768500
training_iteration: 289

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.87309644670051
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 394
episodes_total: 76659
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.30923819541931
time_total_s: 14899.990653038025
timers:
  learn_throughput: 362.44
  learn_time_ms: 45524.752
  load_throughput: 3409885.641
  load_time_ms: 4.839
  training_iteration_time_ms: 58851.831
  update_time_ms: 2.646
timesteps_total: 4323000
training_iteration: 262

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18354430379746836
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 188.74
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 25772
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.58
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.31511950492859
time_total_s: 15015.93640947342
timers:
  learn_throughput: 459.793
  learn_time_ms: 35885.706
  load_throughput: 4714980.753
  load_time_ms: 3.499
  training_iteration_time_ms: 47379.635
  update_time_ms: 2.529
timesteps_total: 4851000
training_iteration: 294

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24305555555555555
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 170.0
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 99
episodes_total: 25086
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.2813880443573
time_total_s: 15030.708869218826
timers:
  learn_throughput: 413.9
  learn_time_ms: 39864.721
  load_throughput: 4479469.76
  load_time_ms: 3.683
  training_iteration_time_ms: 52315.079
  update_time_ms: 2.735
timesteps_total: 4570500
training_iteration: 277

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9916201117318436
  reward for individual goal_min: 0.5
episode_len_mean: 44.849726775956285
episode_reward_max: 2.0
episode_reward_mean: 1.9918032786885247
episode_reward_min: 1.0
episodes_this_iter: 366
episodes_total: 73074
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9918032786885246
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 58.403706312179565
time_total_s: 14926.951950073242
timers:
  learn_throughput: 371.581
  learn_time_ms: 44404.87
  load_throughput: 4018512.351
  load_time_ms: 4.106
  training_iteration_time_ms: 57247.491
  update_time_ms: 2.649
timesteps_total: 4719000
training_iteration: 286

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2077922077922078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9594594594594594
  reward for individual goal_min: 0.0
episode_len_mean: 198.03
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 25215
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.0608925819397
time_total_s: 15049.174952507019
timers:
  learn_throughput: 438.079
  learn_time_ms: 37664.427
  load_throughput: 4516538.492
  load_time_ms: 3.653
  training_iteration_time_ms: 49528.359
  update_time_ms: 2.633
timesteps_total: 4884000
training_iteration: 296

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2905405405405405
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 161.60194174757282
episode_reward_max: 2.0
episode_reward_mean: 1.3398058252427185
episode_reward_min: 0.0
episodes_this_iter: 103
episodes_total: 25196
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6601941747572816
  agent_1: 0.6796116504854369
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.817282915115356
time_total_s: 15050.113913297653
timers:
  learn_throughput: 405.361
  learn_time_ms: 40704.428
  load_throughput: 4218130.044
  load_time_ms: 3.912
  training_iteration_time_ms: 53562.975
  update_time_ms: 2.683
timesteps_total: 4554000
training_iteration: 276

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9942528735632183
  reward for individual goal_min: 0.5
episode_len_mean: 48.58600583090379
episode_reward_max: 2.0
episode_reward_mean: 1.9941690962099126
episode_reward_min: 1.0
episodes_this_iter: 343
episodes_total: 50163
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9941690962099126
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 70.86731243133545
time_total_s: 15024.325320243835
timers:
  learn_throughput: 302.003
  learn_time_ms: 54635.174
  load_throughput: 3263664.985
  load_time_ms: 5.056
  training_iteration_time_ms: 70710.864
  update_time_ms: 2.979
timesteps_total: 3943500
training_iteration: 239

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.9
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 26396
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.66462802886963
time_total_s: 15075.752927303314
timers:
  learn_throughput: 424.316
  learn_time_ms: 38886.068
  load_throughput: 4733750.761
  load_time_ms: 3.486
  training_iteration_time_ms: 50938.267
  update_time_ms: 2.701
timesteps_total: 4785000
training_iteration: 290

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14473684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.48
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 25865
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.81228709220886
time_total_s: 15061.748696565628
timers:
  learn_throughput: 460.909
  learn_time_ms: 35798.841
  load_throughput: 4759076.599
  load_time_ms: 3.467
  training_iteration_time_ms: 47249.772
  update_time_ms: 2.524
timesteps_total: 4867500
training_iteration: 295

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9941860465116279
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9976303317535545
  reward for individual goal_min: 0.5
episode_len_mean: 43.174934725848566
episode_reward_max: 2.0
episode_reward_mean: 1.9921671018276763
episode_reward_min: 0.0
episodes_this_iter: 383
episodes_total: 77042
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9947780678851175
  agent_1: 0.9973890339425587
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 59.058409690856934
time_total_s: 14959.049062728882
timers:
  learn_throughput: 361.318
  learn_time_ms: 45666.193
  load_throughput: 3399500.729
  load_time_ms: 4.854
  training_iteration_time_ms: 59038.126
  update_time_ms: 2.684
timesteps_total: 4339500
training_iteration: 263

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2676056338028169
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.0
episode_len_mean: 160.74257425742573
episode_reward_max: 2.0
episode_reward_mean: 1.3663366336633664
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 25187
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6435643564356436
  agent_1: 0.7227722772277227
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.429795265197754
time_total_s: 15084.138664484024
timers:
  learn_throughput: 414.423
  learn_time_ms: 39814.374
  load_throughput: 4460242.585
  load_time_ms: 3.699
  training_iteration_time_ms: 52245.512
  update_time_ms: 2.722
timesteps_total: 4587000
training_iteration: 278

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18461538461538463
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9487179487179487
  reward for individual goal_min: 0.0
episode_len_mean: 184.36
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 25304
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.582622051239014
time_total_s: 15097.757574558258
timers:
  learn_throughput: 441.034
  learn_time_ms: 37412.108
  load_throughput: 4534828.386
  load_time_ms: 3.639
  training_iteration_time_ms: 49215.852
  update_time_ms: 2.618
timesteps_total: 4900500
training_iteration: 297

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.145833333333336
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 384
episodes_total: 73458
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.99334931373596
time_total_s: 14984.945299386978
timers:
  learn_throughput: 371.556
  learn_time_ms: 44407.851
  load_throughput: 3875502.792
  load_time_ms: 4.258
  training_iteration_time_ms: 57267.821
  update_time_ms: 2.68
timesteps_total: 4735500
training_iteration: 287

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21232876712328766
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 173.91
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 25292
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.03638529777527
time_total_s: 15102.150298595428
timers:
  learn_throughput: 406.473
  learn_time_ms: 40593.063
  load_throughput: 4219132.958
  load_time_ms: 3.911
  training_iteration_time_ms: 53380.009
  update_time_ms: 2.687
timesteps_total: 4570500
training_iteration: 277

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 173.45
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 26493
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.74
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.04365515708923
time_total_s: 15125.796582460403
timers:
  learn_throughput: 429.288
  learn_time_ms: 38435.702
  load_throughput: 4718323.913
  load_time_ms: 3.497
  training_iteration_time_ms: 50470.136
  update_time_ms: 2.725
timesteps_total: 4801500
training_iteration: 291

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.15753424657534246
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.47
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 25955
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.34966850280762
time_total_s: 15111.098365068436
timers:
  learn_throughput: 459.469
  learn_time_ms: 35911.0
  load_throughput: 4819897.482
  load_time_ms: 3.423
  training_iteration_time_ms: 47412.95
  update_time_ms: 2.538
timesteps_total: 4884000
training_iteration: 296

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 52.416666666666664
episode_reward_max: 2.0
episode_reward_mean: 1.9833333333333334
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9833333333333333
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9882352941176471
  reward for individual goal_min: 0.5
episode_len_mean: 49.55855855855856
episode_reward_max: 2.0
episode_reward_mean: 1.987987987987988
episode_reward_min: 1.0
episodes_this_iter: 333
episodes_total: 50496
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.993993993993994
  agent_1: 0.993993993993994
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 77.26604843139648
time_total_s: 15101.591368675232
timers:
  learn_throughput: 302.171
  learn_time_ms: 54604.86
  load_throughput: 3333318.049
  load_time_ms: 4.95
  training_iteration_time_ms: 70623.88
  update_time_ms: 2.962
timesteps_total: 3960000
training_iteration: 240

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-19wecnkw4d/checkpoint_000240/checkpoint-240
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.34
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9941860465116279
  reward for individual goal_min: 0.5
episode_len_mean: 151.55555555555554
episode_reward_max: 2.0
episode_reward_mean: 1.4814814814814814
episode_reward_min: 0.0
episodes_this_iter: 108
episodes_total: 25295
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7037037037037037
  agent_1: 0.7777777777777778
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.046494245529175
time_total_s: 15134.185158729553
timers:
  learn_throughput: 415.234
  learn_time_ms: 39736.628
  load_throughput: 4483445.475
  load_time_ms: 3.68
  training_iteration_time_ms: 52148.213
  update_time_ms: 2.757
timesteps_total: 4603500
training_iteration: 279

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9407894736842105
  reward for individual goal_min: 0.0
episode_len_mean: 183.34
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 25393
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.67047047615051
time_total_s: 15144.428045034409
timers:
  learn_throughput: 444.836
  learn_time_ms: 37092.363
  load_throughput: 4544983.943
  load_time_ms: 3.63
  training_iteration_time_ms: 48758.879
  update_time_ms: 2.642
timesteps_total: 4917000
training_iteration: 298

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.931472081218274
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 394
episodes_total: 77436
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.226704597473145
time_total_s: 15018.275767326355
timers:
  learn_throughput: 360.833
  learn_time_ms: 45727.556
  load_throughput: 3412104.819
  load_time_ms: 4.836
  training_iteration_time_ms: 59098.38
  update_time_ms: 2.686
timesteps_total: 4356000
training_iteration: 264

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25333333333333335
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9810126582278481
  reward for individual goal_min: 0.0
episode_len_mean: 166.77
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 25389
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.04737734794617
time_total_s: 15156.197675943375
timers:
  learn_throughput: 407.412
  learn_time_ms: 40499.583
  load_throughput: 4245325.089
  load_time_ms: 3.887
  training_iteration_time_ms: 53249.379
  update_time_ms: 2.692
timesteps_total: 4587000
training_iteration: 278

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9948186528497409
  reward for individual goal_min: 0.5
episode_len_mean: 42.53213367609254
episode_reward_max: 2.0
episode_reward_mean: 1.9948586118251928
episode_reward_min: 1.0
episodes_this_iter: 389
episodes_total: 73847
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974293059125964
  agent_1: 0.9974293059125964
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.02558517456055
time_total_s: 15042.970884561539
timers:
  learn_throughput: 370.789
  learn_time_ms: 44499.676
  load_throughput: 3814075.36
  load_time_ms: 4.326
  training_iteration_time_ms: 57381.015
  update_time_ms: 2.686
timesteps_total: 4752000
training_iteration: 288

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 172.05
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 26585
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.1488573551178
time_total_s: 15173.945439815521
timers:
  learn_throughput: 427.882
  learn_time_ms: 38562.059
  load_throughput: 4688913.31
  load_time_ms: 3.519
  training_iteration_time_ms: 50633.841
  update_time_ms: 2.72
timesteps_total: 4818000
training_iteration: 292

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19078947368421054
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 185.23
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 26044
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.54
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.568047761917114
time_total_s: 15158.666412830353
timers:
  learn_throughput: 458.931
  learn_time_ms: 35953.087
  load_throughput: 4806307.105
  load_time_ms: 3.433
  training_iteration_time_ms: 47528.088
  update_time_ms: 2.537
timesteps_total: 4900500
training_iteration: 297

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2631578947368421
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9861111111111112
  reward for individual goal_min: 0.0
episode_len_mean: 179.77
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 25486
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.95196723937988
time_total_s: 15192.380012273788
timers:
  learn_throughput: 445.962
  learn_time_ms: 36998.65
  load_throughput: 4583240.573
  load_time_ms: 3.6
  training_iteration_time_ms: 48575.336
  update_time_ms: 2.645
timesteps_total: 4933500
training_iteration: 299

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95
  reward for individual goal_min: 0.5
episode_len_mean: 75.7
episode_reward_max: 2.0
episode_reward_mean: 1.8833333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.95
  agent_1: 0.9333333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 176.4
episode_reward_max: 2.0
episode_reward_mean: 1.3
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 25384
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.67
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.97314095497131
time_total_s: 15197.158299684525
timers:
  learn_throughput: 413.419
  learn_time_ms: 39911.074
  load_throughput: 4462313.237
  load_time_ms: 3.698
  training_iteration_time_ms: 52374.099
  update_time_ms: 2.732
timesteps_total: 4620000
training_iteration: 280

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.59644670050761
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 394
episodes_total: 77830
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.57807970046997
time_total_s: 15076.853847026825
timers:
  learn_throughput: 360.69
  learn_time_ms: 45745.669
  load_throughput: 3418999.289
  load_time_ms: 4.826
  training_iteration_time_ms: 59114.14
  update_time_ms: 2.661
timesteps_total: 4372500
training_iteration: 265

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971098265895953
  reward for individual goal_min: 0.5
episode_len_mean: 47.73546511627907
episode_reward_max: 2.0
episode_reward_mean: 1.997093023255814
episode_reward_min: 1.0
episodes_this_iter: 344
episodes_total: 50840
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.997093023255814
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 71.50421333312988
time_total_s: 15173.095582008362
timers:
  learn_throughput: 301.633
  learn_time_ms: 54702.241
  load_throughput: 3209078.119
  load_time_ms: 5.142
  training_iteration_time_ms: 70744.822
  update_time_ms: 3.003
timesteps_total: 3976500
training_iteration: 241

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2848101265822785
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.11
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 25484
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.95365262031555
time_total_s: 15210.15132856369
timers:
  learn_throughput: 408.476
  learn_time_ms: 40394.046
  load_throughput: 4249965.672
  load_time_ms: 3.882
  training_iteration_time_ms: 53080.014
  update_time_ms: 2.697
timesteps_total: 4603500
training_iteration: 279

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9923076923076923
  reward for individual goal_min: 0.5
episode_len_mean: 45.530386740331494
episode_reward_max: 2.0
episode_reward_mean: 1.9917127071823204
episode_reward_min: 1.0
episodes_this_iter: 362
episodes_total: 74209
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.994475138121547
  agent_1: 0.9972375690607734
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.806151151657104
time_total_s: 15098.777035713196
timers:
  learn_throughput: 370.169
  learn_time_ms: 44574.284
  load_throughput: 3755339.009
  load_time_ms: 4.394
  training_iteration_time_ms: 57466.611
  update_time_ms: 2.665
timesteps_total: 4768500
training_iteration: 289

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.14492753623188406
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.57
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 26140
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 46.44045543670654
time_total_s: 15205.10686826706
timers:
  learn_throughput: 460.108
  learn_time_ms: 35861.18
  load_throughput: 4772400.819
  load_time_ms: 3.457
  training_iteration_time_ms: 47408.728
  update_time_ms: 2.532
timesteps_total: 4917000
training_iteration: 298

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25949367088607594
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9652777777777778
  reward for individual goal_min: 0.0
episode_len_mean: 184.64
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 26676
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.81096911430359
time_total_s: 15224.756408929825
timers:
  learn_throughput: 430.922
  learn_time_ms: 38290.006
  load_throughput: 4609309.463
  load_time_ms: 3.58
  training_iteration_time_ms: 50194.479
  update_time_ms: 2.704
timesteps_total: 4834500
training_iteration: 293

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9
  reward for individual goal_min: 0.0
episode_len_mean: 122.56666666666666
episode_reward_max: 2.0
episode_reward_mean: 1.6333333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.85
  agent_1: 0.7833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20253164556962025
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9683544303797469
  reward for individual goal_min: 0.0
episode_len_mean: 195.57
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 84
episodes_total: 25570
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.57
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 60.66213536262512
time_total_s: 15253.042147636414
timers:
  learn_throughput: 449.938
  learn_time_ms: 36671.754
  load_throughput: 4635304.015
  load_time_ms: 3.56
  training_iteration_time_ms: 48226.221
  update_time_ms: 2.637
timesteps_total: 4950000
training_iteration: 300

Starting final evaluation!
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2710843373493976
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.19
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 25476
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.614234924316406
time_total_s: 15248.772534608841
timers:
  learn_throughput: 413.929
  learn_time_ms: 39861.868
  load_throughput: 4453526.217
  load_time_ms: 3.705
  training_iteration_time_ms: 52299.97
  update_time_ms: 2.728
timesteps_total: 4636500
training_iteration: 281

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21052631578947367
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9931506849315068
  reward for individual goal_min: 0.5
episode_len_mean: 182.9
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 26226
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 41.63086152076721
time_total_s: 15246.737729787827
timers:
  learn_throughput: 464.82
  learn_time_ms: 35497.589
  load_throughput: 4804872.217
  load_time_ms: 3.434
  training_iteration_time_ms: 46952.359
  update_time_ms: 2.543
timesteps_total: 4933500
training_iteration: 299

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.994475138121547
  reward for individual goal_min: 0.5
episode_len_mean: 44.218666666666664
episode_reward_max: 2.0
episode_reward_mean: 1.9946666666666666
episode_reward_min: 1.0
episodes_this_iter: 375
episodes_total: 78205
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9946666666666667
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 56.32350397109985
time_total_s: 15133.177350997925
timers:
  learn_throughput: 361.841
  learn_time_ms: 45600.186
  load_throughput: 3420029.947
  load_time_ms: 4.825
  training_iteration_time_ms: 58890.97
  update_time_ms: 2.661
timesteps_total: 4389000
training_iteration: 266

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21739130434782608
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 164.21782178217822
episode_reward_max: 2.0
episode_reward_mean: 1.4158415841584158
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 26777
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6831683168316832
  agent_1: 0.7326732673267327
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.165531635284424
time_total_s: 15272.92194056511
timers:
  learn_throughput: 429.932
  learn_time_ms: 38378.118
  load_throughput: 4591877.065
  load_time_ms: 3.593
  training_iteration_time_ms: 50170.644
  update_time_ms: 2.742
timesteps_total: 4851000
training_iteration: 294

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.27365728900256
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 391
episodes_total: 74600
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 55.65751504898071
time_total_s: 15154.434550762177
timers:
  learn_throughput: 371.181
  learn_time_ms: 44452.758
  load_throughput: 3752000.043
  load_time_ms: 4.398
  training_iteration_time_ms: 57362.664
  update_time_ms: 2.639
timesteps_total: 4785000
training_iteration: 290

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9166666666666666
  reward for individual goal_min: 0.5
episode_len_mean: 79.76666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.9166666666666667
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9333333333333333
  agent_1: 0.9833333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.25
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.63
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 25581
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.80351161956787
time_total_s: 15271.954840183258
timers:
  learn_throughput: 410.653
  learn_time_ms: 40179.876
  load_throughput: 4256212.546
  load_time_ms: 3.877
  training_iteration_time_ms: 52860.169
  update_time_ms: 2.733
timesteps_total: 4620000
training_iteration: 280

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 45.155313351498634
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 367
episodes_total: 51207
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 72.67823648452759
time_total_s: 15245.77381849289
timers:
  learn_throughput: 300.758
  learn_time_ms: 54861.428
  load_throughput: 3122268.411
  load_time_ms: 5.285
  training_iteration_time_ms: 70931.441
  update_time_ms: 3.023
timesteps_total: 3993000
training_iteration: 242

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2468354430379747
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 184.59
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 25566
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.05391025543213
time_total_s: 15299.826444864273
timers:
  learn_throughput: 414.523
  learn_time_ms: 39804.815
  load_throughput: 4441464.786
  load_time_ms: 3.715
  training_iteration_time_ms: 52184.453
  update_time_ms: 2.7
timesteps_total: 4653000
training_iteration: 282

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Evaluation results over 10 episodes for each goal
{'[0, 0, 1]': 1.7, '[0, 1, 0]': 1.8, '[1, 0, 0]': 1.8, '[0, 1, 1]': 2.0, '[1, 0, 1]': 2.0, '[1, 1, 0]': 1.8}
Moviepy - Building video /gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19mcvnks1j/trained_agent.mp4.
Moviepy - Writing video /gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19mcvnks1j/trained_agent.mp4

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.7
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 99.21666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.6666666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.85
  agent_1: 0.8166666666666667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.12162162162162163
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9932432432432432
  reward for individual goal_min: 0.5
episode_len_mean: 190.78
episode_reward_max: 2.0
episode_reward_mean: 1.16
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 26311
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.56
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.47097206115723
time_total_s: 15302.208701848984
timers:
  learn_throughput: 469.558
  learn_time_ms: 35139.434
  load_throughput: 4793523.488
  load_time_ms: 3.442
  training_iteration_time_ms: 46529.344
  update_time_ms: 2.594
timesteps_total: 4950000
training_iteration: 300

Starting final evaluation!
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2318840579710145
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9871794871794872
  reward for individual goal_min: 0.0
episode_len_mean: 167.42
episode_reward_max: 2.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 26871
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.72
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.55888223648071
time_total_s: 15321.48082280159
timers:
  learn_throughput: 432.302
  learn_time_ms: 38167.791
  load_throughput: 4580419.482
  load_time_ms: 3.602
  training_iteration_time_ms: 49920.959
  update_time_ms: 2.739
timesteps_total: 4867500
training_iteration: 295

Moviepy - Done !
Moviepy - video ready /gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19mcvnks1j/trained_agent.mp4
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.95685279187817
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 394
episodes_total: 78599
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.12004256248474
time_total_s: 15190.29739356041
timers:
  learn_throughput: 363.162
  learn_time_ms: 45434.24
  load_throughput: 3434490.603
  load_time_ms: 4.804
  training_iteration_time_ms: 58678.213
  update_time_ms: 2.558
timesteps_total: 4405500
training_iteration: 267

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19078947368421054
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 180.64
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 25670
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.17699337005615
time_total_s: 15324.131833553314
timers:
  learn_throughput: 411.047
  learn_time_ms: 40141.366
  load_throughput: 4218721.448
  load_time_ms: 3.911
  training_iteration_time_ms: 52817.422
  update_time_ms: 2.718
timesteps_total: 4636500
training_iteration: 281

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.63756613756614
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 378
episodes_total: 74978
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.102500677108765
time_total_s: 15211.537051439285
timers:
  learn_throughput: 370.659
  learn_time_ms: 44515.365
  load_throughput: 3750170.206
  load_time_ms: 4.4
  training_iteration_time_ms: 57359.98
  update_time_ms: 2.672
timesteps_total: 4801500
training_iteration: 291

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.39142091152815
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 373
episodes_total: 51580
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 72.93814277648926
time_total_s: 15318.711961269379
timers:
  learn_throughput: 299.382
  learn_time_ms: 55113.459
  load_throughput: 3122268.411
  load_time_ms: 5.285
  training_iteration_time_ms: 71216.666
  update_time_ms: 3.057
timesteps_total: 4009500
training_iteration: 243

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2534246575342466
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9933333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 173.81
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 25662
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.102012395858765
time_total_s: 15351.928457260132
timers:
  learn_throughput: 415.489
  learn_time_ms: 39712.225
  load_throughput: 4406090.062
  load_time_ms: 3.745
  training_iteration_time_ms: 52085.02
  update_time_ms: 2.705
timesteps_total: 4669500
training_iteration: 283

Evaluation results over 10 episodes for each goal
{'[0, 0, 1]': 2.0, '[0, 1, 0]': 2.0, '[1, 0, 0]': 2.0, '[0, 1, 1]': 1.6, '[1, 0, 1]': 1.6, '[1, 1, 0]': 2.0}
Moviepy - Building video /gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-28ftt3iad9/trained_agent.mp4.
Moviepy - Writing video /gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-28ftt3iad9/trained_agent.mp4

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.16
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 188.96
episode_reward_max: 2.0
episode_reward_mean: 1.24
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 26959
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 45.15071511268616
time_total_s: 15366.631537914276
timers:
  learn_throughput: 439.423
  learn_time_ms: 37549.212
  load_throughput: 4648224.224
  load_time_ms: 3.55
  training_iteration_time_ms: 49301.713
  update_time_ms: 2.726
timesteps_total: 4884000
training_iteration: 296

Moviepy - Done !
Moviepy - video ready /gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-28ftt3iad9/trained_agent.mp4
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23076923076923078
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 179.61
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 25760
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.62
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.90406513214111
time_total_s: 15377.035898685455
timers:
  learn_throughput: 413.393
  learn_time_ms: 39913.548
  load_throughput: 4235840.913
  load_time_ms: 3.895
  training_iteration_time_ms: 52545.311
  update_time_ms: 2.699
timesteps_total: 4653000
training_iteration: 282

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.54040404040404
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 396
episodes_total: 78995
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 60.512187480926514
time_total_s: 15250.809581041336
timers:
  learn_throughput: 362.538
  learn_time_ms: 45512.454
  load_throughput: 3436776.068
  load_time_ms: 4.801
  training_iteration_time_ms: 58714.811
  update_time_ms: 2.578
timesteps_total: 4422000
training_iteration: 268

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9947643979057592
  reward for individual goal_min: 0.5
episode_len_mean: 45.52486187845304
episode_reward_max: 2.0
episode_reward_mean: 1.9944751381215469
episode_reward_min: 1.0
episodes_this_iter: 362
episodes_total: 75340
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.994475138121547
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 57.97608160972595
time_total_s: 15269.513133049011
timers:
  learn_throughput: 370.362
  learn_time_ms: 44550.968
  load_throughput: 3704977.515
  load_time_ms: 4.453
  training_iteration_time_ms: 57431.373
  update_time_ms: 2.65
timesteps_total: 4818000
training_iteration: 292

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28378378378378377
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9855072463768116
  reward for individual goal_min: 0.0
episode_len_mean: 171.98
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 25759
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.68
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.562994718551636
time_total_s: 15403.491451978683
timers:
  learn_throughput: 416.057
  learn_time_ms: 39658.066
  load_throughput: 4423692.567
  load_time_ms: 3.73
  training_iteration_time_ms: 52065.189
  update_time_ms: 2.724
timesteps_total: 4686000
training_iteration: 284

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20987654320987653
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 192.31
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 27048
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.1873037815094
time_total_s: 15416.818841695786
timers:
  learn_throughput: 437.839
  learn_time_ms: 37685.065
  load_throughput: 4679243.813
  load_time_ms: 3.526
  training_iteration_time_ms: 49379.124
  update_time_ms: 2.73
timesteps_total: 4900500
training_iteration: 297

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19594594594594594
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 181.3
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 25853
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.090364933013916
time_total_s: 15429.12626361847
timers:
  learn_throughput: 412.043
  learn_time_ms: 40044.35
  load_throughput: 4215945.855
  load_time_ms: 3.914
  training_iteration_time_ms: 52724.761
  update_time_ms: 2.722
timesteps_total: 4669500
training_iteration: 283

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.344632768361585
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 354
episodes_total: 51934
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 73.30301260948181
time_total_s: 15392.01497387886
timers:
  learn_throughput: 298.178
  learn_time_ms: 55336.029
  load_throughput: 3165917.922
  load_time_ms: 5.212
  training_iteration_time_ms: 71408.515
  update_time_ms: 3.042
timesteps_total: 4026000
training_iteration: 244

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.148337595907925
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 391
episodes_total: 79386
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 62.02266478538513
time_total_s: 15312.832245826721
timers:
  learn_throughput: 361.462
  learn_time_ms: 45647.917
  load_throughput: 3406344.306
  load_time_ms: 4.844
  training_iteration_time_ms: 58863.162
  update_time_ms: 2.679
timesteps_total: 4438500
training_iteration: 269

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.992167101827675
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 383
episodes_total: 75723
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.97436046600342
time_total_s: 15329.487493515015
timers:
  learn_throughput: 369.286
  learn_time_ms: 44680.865
  load_throughput: 3688882.398
  load_time_ms: 4.473
  training_iteration_time_ms: 57567.811
  update_time_ms: 2.663
timesteps_total: 4834500
training_iteration: 293

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2361111111111111
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.993421052631579
  reward for individual goal_min: 0.5
episode_len_mean: 179.4
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 25851
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.75
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.198951959609985
time_total_s: 15452.690403938293
timers:
  learn_throughput: 418.878
  learn_time_ms: 39390.919
  load_throughput: 4436482.15
  load_time_ms: 3.719
  training_iteration_time_ms: 51799.003
  update_time_ms: 2.713
timesteps_total: 4702500
training_iteration: 285

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1927710843373494
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 198.53
episode_reward_max: 2.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 83
episodes_total: 27131
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.40089797973633
time_total_s: 15467.219739675522
timers:
  learn_throughput: 437.454
  learn_time_ms: 37718.227
  load_throughput: 4781699.567
  load_time_ms: 3.451
  training_iteration_time_ms: 49362.868
  update_time_ms: 2.724
timesteps_total: 4917000
training_iteration: 298

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19078947368421054
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935064935064936
  reward for individual goal_min: 0.5
episode_len_mean: 181.05
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 25946
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.41432595252991
time_total_s: 15479.540589571
timers:
  learn_throughput: 414.015
  learn_time_ms: 39853.588
  load_throughput: 4199266.77
  load_time_ms: 3.929
  training_iteration_time_ms: 52505.571
  update_time_ms: 2.734
timesteps_total: 4686000
training_iteration: 284

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973821989528796
  reward for individual goal_min: 0.5
episode_len_mean: 48.71091445427729
episode_reward_max: 2.0
episode_reward_mean: 1.9970501474926254
episode_reward_min: 1.0
episodes_this_iter: 339
episodes_total: 52273
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9970501474926253
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 69.97220611572266
time_total_s: 15461.987179994583
timers:
  learn_throughput: 298.338
  learn_time_ms: 55306.324
  load_throughput: 3150209.89
  load_time_ms: 5.238
  training_iteration_time_ms: 71376.781
  update_time_ms: 3.018
timesteps_total: 4042500
training_iteration: 245

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9950980392156863
  reward for individual goal_min: 0.5
episode_len_mean: 42.31713554987212
episode_reward_max: 2.0
episode_reward_mean: 1.9948849104859334
episode_reward_min: 1.0
episodes_this_iter: 391
episodes_total: 79777
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9948849104859335
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 61.725905656814575
time_total_s: 15374.558151483536
timers:
  learn_throughput: 359.342
  learn_time_ms: 45917.205
  load_throughput: 3404953.284
  load_time_ms: 4.846
  training_iteration_time_ms: 59158.988
  update_time_ms: 2.673
timesteps_total: 4455000
training_iteration: 270

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9926470588235294
  reward for individual goal_min: 0.5
episode_len_mean: 44.65405405405406
episode_reward_max: 2.0
episode_reward_mean: 1.991891891891892
episode_reward_min: 1.0
episodes_this_iter: 370
episodes_total: 76093
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9945945945945946
  agent_1: 0.9972972972972973
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 57.71802258491516
time_total_s: 15387.20551609993
timers:
  learn_throughput: 369.082
  learn_time_ms: 44705.483
  load_throughput: 3680720.763
  load_time_ms: 4.483
  training_iteration_time_ms: 57591.084
  update_time_ms: 2.684
timesteps_total: 4851000
training_iteration: 294

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20588235294117646
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9876543209876543
  reward for individual goal_min: 0.5
episode_len_mean: 172.26
episode_reward_max: 2.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 96
episodes_total: 25947
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.92121458053589
time_total_s: 15502.61161851883
timers:
  learn_throughput: 421.048
  learn_time_ms: 39187.944
  load_throughput: 4441008.766
  load_time_ms: 3.715
  training_iteration_time_ms: 51484.076
  update_time_ms: 2.693
timesteps_total: 4719000
training_iteration: 286

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.22
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9794520547945206
  reward for individual goal_min: 0.0
episode_len_mean: 184.48
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 27221
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.5883629322052
time_total_s: 15516.808102607727
timers:
  learn_throughput: 439.31
  learn_time_ms: 37558.935
  load_throughput: 4789012.248
  load_time_ms: 3.445
  training_iteration_time_ms: 49131.301
  update_time_ms: 2.705
timesteps_total: 4933500
training_iteration: 299

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.17123287671232876
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 183.35
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 26037
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.9194860458374
time_total_s: 15531.460075616837
timers:
  learn_throughput: 414.756
  learn_time_ms: 39782.421
  load_throughput: 4215663.361
  load_time_ms: 3.914
  training_iteration_time_ms: 52393.307
  update_time_ms: 2.742
timesteps_total: 4702500
training_iteration: 285

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1956521739130435
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9873417721518988
  reward for individual goal_min: 0.0
episode_len_mean: 174.34
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 26042
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.19702076911926
time_total_s: 15554.808639287949
timers:
  learn_throughput: 421.906
  learn_time_ms: 39108.208
  load_throughput: 4461162.638
  load_time_ms: 3.699
  training_iteration_time_ms: 51375.717
  update_time_ms: 2.676
timesteps_total: 4735500
training_iteration: 287

Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9333333333333333
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.95
  reward for individual goal_min: 0.5
episode_len_mean: 87.66666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.8833333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.95
  agent_1: 0.9333333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2602739726027397
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 166.59
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 100
episodes_total: 27321
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.495854139328
time_total_s: 15568.303956747055
timers:
  learn_throughput: 447.695
  learn_time_ms: 36855.469
  load_throughput: 4898049.868
  load_time_ms: 3.369
  training_iteration_time_ms: 48299.412
  update_time_ms: 2.695
timesteps_total: 4950000
training_iteration: 300

Starting final evaluation!
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.994535519125683
  reward for individual goal_min: 0.5
episode_len_mean: 45.01639344262295
episode_reward_max: 2.0
episode_reward_mean: 1.994535519125683
episode_reward_min: 1.0
episodes_this_iter: 366
episodes_total: 76459
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972677595628415
  agent_1: 0.9972677595628415
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 58.17248010635376
time_total_s: 15445.377996206284
timers:
  learn_throughput: 369.365
  learn_time_ms: 44671.277
  load_throughput: 3690652.901
  load_time_ms: 4.471
  training_iteration_time_ms: 57557.889
  update_time_ms: 2.723
timesteps_total: 4867500
training_iteration: 295

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9948717948717949
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.350383631713555
episode_reward_max: 2.0
episode_reward_mean: 1.9948849104859334
episode_reward_min: 0.0
episodes_this_iter: 391
episodes_total: 80168
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974424552429667
  agent_1: 0.9974424552429667
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 63.37525677680969
time_total_s: 15437.933408260345
timers:
  learn_throughput: 356.442
  learn_time_ms: 46290.89
  load_throughput: 3385746.659
  load_time_ms: 4.873
  training_iteration_time_ms: 59597.288
  update_time_ms: 2.695
timesteps_total: 4471500
training_iteration: 271

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972067039106145
  reward for individual goal_min: 0.5
episode_len_mean: 47.61494252873563
episode_reward_max: 2.0
episode_reward_mean: 1.9971264367816093
episode_reward_min: 1.0
episodes_this_iter: 348
episodes_total: 52621
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9971264367816092
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 72.2310905456543
time_total_s: 15534.218270540237
timers:
  learn_throughput: 298.251
  learn_time_ms: 55322.445
  load_throughput: 3140930.942
  load_time_ms: 5.253
  training_iteration_time_ms: 71399.27
  update_time_ms: 2.991
timesteps_total: 4059000
training_iteration: 246

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2125
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 179.91
episode_reward_max: 2.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 26130
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.15214562416077
time_total_s: 15584.612221240997
timers:
  learn_throughput: 414.739
  learn_time_ms: 39784.099
  load_throughput: 4220573.875
  load_time_ms: 3.909
  training_iteration_time_ms: 52426.958
  update_time_ms: 2.717
timesteps_total: 4719000
training_iteration: 286

Evaluation results over 10 episodes for each goal
{'[0, 0, 1]': 2.0, '[0, 1, 0]': 2.0, '[1, 0, 0]': 2.0, '[0, 1, 1]': 1.6, '[1, 0, 1]': 1.6, '[1, 1, 0]': 1.4}
Moviepy - Building video /gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19igysrqn6/trained_agent.mp4.
Moviepy - Writing video /gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19igysrqn6/trained_agent.mp4

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26875
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9714285714285714
  reward for individual goal_min: 0.0
episode_len_mean: 184.33
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 86
episodes_total: 26128
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.66
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.950486183166504
time_total_s: 15606.759125471115
timers:
  learn_throughput: 423.245
  learn_time_ms: 38984.474
  load_throughput: 4459409.115
  load_time_ms: 3.7
  training_iteration_time_ms: 51227.676
  update_time_ms: 2.709
timesteps_total: 4752000
training_iteration: 288

Moviepy - Done !
Moviepy - video ready /gpfswork/rech/imi/uzw47io/elias_experiments/independent_3/2022-06-24_00-08-19igysrqn6/trained_agent.mp4
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972527472527473
  reward for individual goal_min: 0.5
episode_len_mean: 44.93131868131868
episode_reward_max: 2.0
episode_reward_mean: 1.9972527472527473
episode_reward_min: 1.0
episodes_this_iter: 364
episodes_total: 76823
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972527472527473
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 57.25706195831299
time_total_s: 15502.635058164597
timers:
  learn_throughput: 370.203
  learn_time_ms: 44570.112
  load_throughput: 3717915.14
  load_time_ms: 4.438
  training_iteration_time_ms: 57443.453
  update_time_ms: 2.742
timesteps_total: 4884000
training_iteration: 296

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.425316455696205
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 395
episodes_total: 80563
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 60.51805806159973
time_total_s: 15498.451466321945
timers:
  learn_throughput: 355.743
  learn_time_ms: 46381.789
  load_throughput: 3371446.332
  load_time_ms: 4.894
  training_iteration_time_ms: 59717.737
  update_time_ms: 2.697
timesteps_total: 4488000
training_iteration: 272

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 174.43
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 95
episodes_total: 26225
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.57
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.69347929954529
time_total_s: 15636.305700540543
timers:
  learn_throughput: 414.616
  learn_time_ms: 39795.812
  load_throughput: 4252890.793
  load_time_ms: 3.88
  training_iteration_time_ms: 52392.432
  update_time_ms: 2.719
timesteps_total: 4735500
training_iteration: 287

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9940476190476191
  reward for individual goal_min: 0.5
episode_len_mean: 46.845272206303726
episode_reward_max: 2.0
episode_reward_mean: 1.994269340974212
episode_reward_min: 1.0
episodes_this_iter: 349
episodes_total: 52970
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.997134670487106
  agent_1: 0.997134670487106
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 71.6395525932312
time_total_s: 15605.857823133469
timers:
  learn_throughput: 298.204
  learn_time_ms: 55331.234
  load_throughput: 3224133.054
  load_time_ms: 5.118
  training_iteration_time_ms: 71428.935
  update_time_ms: 2.989
timesteps_total: 4075500
training_iteration: 247

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.33783783783783783
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.76
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 26225
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.78
  agent_1: 0.59
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.05998420715332
time_total_s: 15658.819109678268
timers:
  learn_throughput: 421.434
  learn_time_ms: 39152.031
  load_throughput: 4503635.524
  load_time_ms: 3.664
  training_iteration_time_ms: 51429.476
  update_time_ms: 2.685
timesteps_total: 4768500
training_iteration: 289

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.58760107816712
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 371
episodes_total: 77194
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 56.46978139877319
time_total_s: 15559.10483956337
timers:
  learn_throughput: 371.544
  learn_time_ms: 44409.276
  load_throughput: 3809959.867
  load_time_ms: 4.331
  training_iteration_time_ms: 57291.966
  update_time_ms: 2.703
timesteps_total: 4900500
training_iteration: 297

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21052631578947367
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 176.98
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 26319
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 55.395171880722046
time_total_s: 15691.700872421265
timers:
  learn_throughput: 413.717
  learn_time_ms: 39882.335
  load_throughput: 4246054.396
  load_time_ms: 3.886
  training_iteration_time_ms: 52527.043
  update_time_ms: 2.742
timesteps_total: 4752000
training_iteration: 288

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.60769230769231
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 390
episodes_total: 80953
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.08523488044739
time_total_s: 15557.536701202393
timers:
  learn_throughput: 355.706
  learn_time_ms: 46386.62
  load_throughput: 3355768.608
  load_time_ms: 4.917
  training_iteration_time_ms: 59720.891
  update_time_ms: 2.714
timesteps_total: 4504500
training_iteration: 273

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971264367816092
  reward for individual goal_min: 0.5
episode_len_mean: 46.35574229691877
episode_reward_max: 2.0
episode_reward_mean: 1.9971988795518207
episode_reward_min: 1.0
episodes_this_iter: 357
episodes_total: 53327
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9971988795518207
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 70.50038838386536
time_total_s: 15676.358211517334
timers:
  learn_throughput: 297.848
  learn_time_ms: 55397.476
  load_throughput: 3223532.349
  load_time_ms: 5.119
  training_iteration_time_ms: 71413.776
  update_time_ms: 2.996
timesteps_total: 4092000
training_iteration: 248

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2532467532467532
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 183.09
episode_reward_max: 2.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 90
episodes_total: 26315
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.6
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.94202971458435
time_total_s: 15713.761139392853
timers:
  learn_throughput: 419.652
  learn_time_ms: 39318.333
  load_throughput: 4493867.962
  load_time_ms: 3.672
  training_iteration_time_ms: 51620.8
  update_time_ms: 2.712
timesteps_total: 4785000
training_iteration: 290

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9948717948717949
  reward for individual goal_min: 0.5
episode_len_mean: 43.3984375
episode_reward_max: 2.0
episode_reward_mean: 1.9947916666666667
episode_reward_min: 1.0
episodes_this_iter: 384
episodes_total: 77578
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9947916666666666
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 54.89225745201111
time_total_s: 15613.99709701538
timers:
  learn_throughput: 372.995
  learn_time_ms: 44236.532
  load_throughput: 3817083.605
  load_time_ms: 4.323
  training_iteration_time_ms: 56979.079
  update_time_ms: 2.693
timesteps_total: 4917000
training_iteration: 298

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.1962025316455696
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9935897435897436
  reward for individual goal_min: 0.5
episode_len_mean: 184.93
episode_reward_max: 2.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 87
episodes_total: 26406
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.56
  agent_1: 0.65
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.56742596626282
time_total_s: 15743.268298387527
timers:
  learn_throughput: 415.662
  learn_time_ms: 39695.692
  load_throughput: 4288920.178
  load_time_ms: 3.847
  training_iteration_time_ms: 52288.319
  update_time_ms: 2.742
timesteps_total: 4768500
training_iteration: 289

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.7734375
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 384
episodes_total: 81337
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.22233605384827
time_total_s: 15616.75903725624
timers:
  learn_throughput: 355.842
  learn_time_ms: 46368.871
  load_throughput: 3349321.047
  load_time_ms: 4.926
  training_iteration_time_ms: 59721.183
  update_time_ms: 2.725
timesteps_total: 4521000
training_iteration: 274

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20666666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9866666666666667
  reward for individual goal_min: 0.0
episode_len_mean: 177.61
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 26409
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.50353407859802
time_total_s: 15766.26467347145
timers:
  learn_throughput: 418.679
  learn_time_ms: 39409.674
  load_throughput: 4487690.143
  load_time_ms: 3.677
  training_iteration_time_ms: 51709.675
  update_time_ms: 2.708
timesteps_total: 4801500
training_iteration: 291

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.75070821529745
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 353
episodes_total: 53680
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 70.50142312049866
time_total_s: 15746.859634637833
timers:
  learn_throughput: 298.289
  learn_time_ms: 55315.468
  load_throughput: 3418188.715
  load_time_ms: 4.827
  training_iteration_time_ms: 71377.568
  update_time_ms: 2.994
timesteps_total: 4108500
training_iteration: 249

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 45.0299727520436
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 367
episodes_total: 77945
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 53.956623792648315
time_total_s: 15667.95372080803
timers:
  learn_throughput: 374.361
  learn_time_ms: 44075.158
  load_throughput: 3880239.747
  load_time_ms: 4.252
  training_iteration_time_ms: 56793.891
  update_time_ms: 2.684
timesteps_total: 4933500
training_iteration: 299

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19078947368421054
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9878048780487805
  reward for individual goal_min: 0.5
episode_len_mean: 182.47
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 26494
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.218310594558716
time_total_s: 15793.486608982086
timers:
  learn_throughput: 416.458
  learn_time_ms: 39619.858
  load_throughput: 4253858.012
  load_time_ms: 3.879
  training_iteration_time_ms: 52114.029
  update_time_ms: 2.711
timesteps_total: 4785000
training_iteration: 290

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.314070351758794
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 398
episodes_total: 81735
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.15208339691162
time_total_s: 15675.911120653152
timers:
  learn_throughput: 355.49
  learn_time_ms: 46414.858
  load_throughput: 3360951.086
  load_time_ms: 4.909
  training_iteration_time_ms: 59778.343
  update_time_ms: 2.759
timesteps_total: 4537500
training_iteration: 275

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2777777777777778
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 166.97
episode_reward_max: 2.0
episode_reward_mean: 1.4
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 26507
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.72
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.2934205532074
time_total_s: 15816.558094024658
timers:
  learn_throughput: 419.211
  learn_time_ms: 39359.697
  load_throughput: 4462917.54
  load_time_ms: 3.697
  training_iteration_time_ms: 51620.91
  update_time_ms: 2.701
timesteps_total: 4818000
training_iteration: 292

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.21428571428571427
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.99375
  reward for individual goal_min: 0.5
episode_len_mean: 163.44554455445544
episode_reward_max: 2.0
episode_reward_mean: 1.3267326732673268
episode_reward_min: 0.0
episodes_this_iter: 101
episodes_total: 26595
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.6633663366336634
  agent_1: 0.6633663366336634
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.85702466964722
time_total_s: 15845.343633651733
timers:
  learn_throughput: 416.492
  learn_time_ms: 39616.601
  load_throughput: 4320974.008
  load_time_ms: 3.819
  training_iteration_time_ms: 52082.199
  update_time_ms: 2.72
timesteps_total: 4801500
training_iteration: 291

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.78333333333333
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.98724489795919
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 392
episodes_total: 78337
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 60.058425188064575
time_total_s: 15728.012145996094
timers:
  learn_throughput: 374.788
  learn_time_ms: 44024.917
  load_throughput: 3853534.754
  load_time_ms: 4.282
  training_iteration_time_ms: 56766.066
  update_time_ms: 2.696
timesteps_total: 4950000
training_iteration: 300

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-18g5xov65h/checkpoint_000300/checkpoint-300
Starting final evaluation!
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9913294797687862
  reward for individual goal_min: 0.5
episode_len_mean: 48.20699708454811
episode_reward_max: 2.0
episode_reward_mean: 1.9912536443148687
episode_reward_min: 1.0
episodes_this_iter: 343
episodes_total: 54023
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9970845481049563
  agent_1: 0.9941690962099126
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 71.4517195224762
time_total_s: 15818.311354160309
timers:
  learn_throughput: 297.694
  learn_time_ms: 55425.991
  load_throughput: 3398182.032
  load_time_ms: 4.856
  training_iteration_time_ms: 71560.813
  update_time_ms: 3.025
timesteps_total: 4125000
training_iteration: 250

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18493150684931506
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 175.89
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 94
episodes_total: 26601
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.71
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.46388626098633
time_total_s: 15866.021980285645
timers:
  learn_throughput: 421.289
  learn_time_ms: 39165.519
  load_throughput: 4465077.1
  load_time_ms: 3.695
  training_iteration_time_ms: 51356.774
  update_time_ms: 2.702
timesteps_total: 4834500
training_iteration: 293

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.33759590792839
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 391
episodes_total: 82126
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.98113656044006
time_total_s: 15734.892257213593
timers:
  learn_throughput: 353.909
  learn_time_ms: 46622.175
  load_throughput: 3355915.062
  load_time_ms: 4.917
  training_iteration_time_ms: 60043.85
  update_time_ms: 2.781
timesteps_total: 4554000
training_iteration: 276

Evaluation results over 10 episodes for each goal
{'[0, 0, 1]': 2.0, '[0, 1, 0]': 2.0, '[1, 0, 0]': 2.0, '[0, 1, 1]': 2.0, '[1, 0, 1]': 2.0, '[1, 1, 0]': 2.0}
Moviepy - Building video /gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-18g5xov65h/trained_agent.mp4.
Moviepy - Writing video /gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-18g5xov65h/trained_agent.mp4

Moviepy - Done !
Moviepy - video ready /gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-18g5xov65h/trained_agent.mp4
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.19444444444444445
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.1
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 26693
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.68980073928833
time_total_s: 15893.033434391022
timers:
  learn_throughput: 420.423
  learn_time_ms: 39246.156
  load_throughput: 4302492.12
  load_time_ms: 3.835
  training_iteration_time_ms: 51560.692
  update_time_ms: 2.708
timesteps_total: 4818000
training_iteration: 292

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2866666666666667
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 168.15
episode_reward_max: 2.0
episode_reward_mean: 1.38
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 26698
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.7
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.05127310752869
time_total_s: 15918.073253393173
timers:
  learn_throughput: 420.133
  learn_time_ms: 39273.324
  load_throughput: 4485741.25
  load_time_ms: 3.678
  training_iteration_time_ms: 51405.893
  update_time_ms: 2.674
timesteps_total: 4851000
training_iteration: 294

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973404255319149
  reward for individual goal_min: 0.5
episode_len_mean: 42.55012853470437
episode_reward_max: 2.0
episode_reward_mean: 1.9974293059125965
episode_reward_min: 1.0
episodes_this_iter: 389
episodes_total: 82515
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9974293059125964
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 58.40742564201355
time_total_s: 15793.299682855606
timers:
  learn_throughput: 353.231
  learn_time_ms: 46711.701
  load_throughput: 3342575.298
  load_time_ms: 4.936
  training_iteration_time_ms: 60172.753
  update_time_ms: 2.784
timesteps_total: 4570500
training_iteration: 277

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9969879518072289
  reward for individual goal_min: 0.5
episode_len_mean: 46.11731843575419
episode_reward_max: 2.0
episode_reward_mean: 1.9972067039106145
episode_reward_min: 1.0
episodes_this_iter: 358
episodes_total: 54381
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9972067039106145
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 71.90421605110168
time_total_s: 15890.21557021141
timers:
  learn_throughput: 297.468
  learn_time_ms: 55468.212
  load_throughput: 3529316.586
  load_time_ms: 4.675
  training_iteration_time_ms: 71600.928
  update_time_ms: 3.016
timesteps_total: 4141500
training_iteration: 251

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20945945945945946
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9805194805194806
  reward for individual goal_min: 0.5
episode_len_mean: 178.83
episode_reward_max: 2.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 92
episodes_total: 26785
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.59
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.76989436149597
time_total_s: 15946.803328752518
timers:
  learn_throughput: 419.102
  learn_time_ms: 39369.862
  load_throughput: 4356113.828
  load_time_ms: 3.788
  training_iteration_time_ms: 51728.607
  update_time_ms: 2.696
timesteps_total: 4834500
training_iteration: 293

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.23943661971830985
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 172.35
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 26795
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.71
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.02100205421448
time_total_s: 15968.094255447388
timers:
  learn_throughput: 419.15
  learn_time_ms: 39365.377
  load_throughput: 4487544.645
  load_time_ms: 3.677
  training_iteration_time_ms: 51488.12
  update_time_ms: 2.676
timesteps_total: 4867500
training_iteration: 295

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9936708860759493
  reward for individual goal_min: 0.5
episode_len_mean: 49.492492492492495
episode_reward_max: 2.0
episode_reward_mean: 1.993993993993994
episode_reward_min: 1.0
episodes_this_iter: 333
episodes_total: 54714
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.993993993993994
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 48.625401973724365
time_total_s: 15938.840972185135
timers:
  learn_throughput: 308.614
  learn_time_ms: 53464.847
  load_throughput: 3642421.895
  load_time_ms: 4.53
  training_iteration_time_ms: 69197.657
  update_time_ms: 2.932
timesteps_total: 4158000
training_iteration: 252

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9945945945945946
  reward for individual goal_min: 0.5
episode_len_mean: 43.763925729442974
episode_reward_max: 2.0
episode_reward_mean: 1.9946949602122015
episode_reward_min: 1.0
episodes_this_iter: 377
episodes_total: 82892
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9946949602122016
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 57.97274923324585
time_total_s: 15851.272432088852
timers:
  learn_throughput: 355.026
  learn_time_ms: 46475.507
  load_throughput: 3349077.922
  load_time_ms: 4.927
  training_iteration_time_ms: 59932.555
  update_time_ms: 2.762
timesteps_total: 4587000
training_iteration: 278

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2465753424657534
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9810126582278481
  reward for individual goal_min: 0.5
episode_len_mean: 169.86
episode_reward_max: 2.0
episode_reward_mean: 1.33
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 26882
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.63
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.645652055740356
time_total_s: 15997.448980808258
timers:
  learn_throughput: 419.206
  learn_time_ms: 39360.133
  load_throughput: 4394172.259
  load_time_ms: 3.755
  training_iteration_time_ms: 51751.995
  update_time_ms: 2.7
timesteps_total: 4851000
training_iteration: 294

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26666666666666666
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.0
episode_len_mean: 177.26
episode_reward_max: 2.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 26888
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.69
  agent_1: 0.63
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.511255741119385
time_total_s: 16022.605511188507
timers:
  learn_throughput: 415.329
  learn_time_ms: 39727.557
  load_throughput: 4457283.741
  load_time_ms: 3.702
  training_iteration_time_ms: 51947.03
  update_time_ms: 2.684
timesteps_total: 4884000
training_iteration: 296

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.119777158774376
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 359
episodes_total: 55073
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.69545650482178
time_total_s: 15987.536428689957
timers:
  learn_throughput: 320.041
  learn_time_ms: 51555.828
  load_throughput: 3704779.178
  load_time_ms: 4.454
  training_iteration_time_ms: 66775.032
  update_time_ms: 2.853
timesteps_total: 4174500
training_iteration: 253

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974226804123711
  reward for individual goal_min: 0.5
episode_len_mean: 41.90355329949239
episode_reward_max: 2.0
episode_reward_mean: 1.99746192893401
episode_reward_min: 1.0
episodes_this_iter: 394
episodes_total: 83286
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974619289340102
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 57.54307460784912
time_total_s: 15908.815506696701
timers:
  learn_throughput: 358.31
  learn_time_ms: 46049.566
  load_throughput: 3411314.326
  load_time_ms: 4.837
  training_iteration_time_ms: 59485.147
  update_time_ms: 2.667
timesteps_total: 4603500
training_iteration: 279

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.18902439024390244
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 189.42
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 85
episodes_total: 26967
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.52
  agent_1: 0.67
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 53.89203357696533
time_total_s: 16051.341014385223
timers:
  learn_throughput: 418.574
  learn_time_ms: 39419.522
  load_throughput: 4331196.045
  load_time_ms: 3.81
  training_iteration_time_ms: 51949.346
  update_time_ms: 2.71
timesteps_total: 4867500
training_iteration: 295

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2922077922077922
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.29
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 26985
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.66
  agent_1: 0.68
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 50.869980335235596
time_total_s: 16073.475491523743
timers:
  learn_throughput: 416.54
  learn_time_ms: 39612.007
  load_throughput: 4420471.391
  load_time_ms: 3.733
  training_iteration_time_ms: 51814.384
  update_time_ms: 2.683
timesteps_total: 4900500
training_iteration: 297

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972972972972973
  reward for individual goal_min: 0.5
episode_len_mean: 47.447976878612714
episode_reward_max: 2.0
episode_reward_mean: 1.9971098265895955
episode_reward_min: 1.0
episodes_this_iter: 346
episodes_total: 55419
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9971098265895953
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 47.23765730857849
time_total_s: 16034.774085998535
timers:
  learn_throughput: 333.811
  learn_time_ms: 49429.229
  load_throughput: 3835743.36
  load_time_ms: 4.302
  training_iteration_time_ms: 64169.186
  update_time_ms: 2.786
timesteps_total: 4191000
training_iteration: 254

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.26973684210526316
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9927536231884058
  reward for individual goal_min: 0.5
episode_len_mean: 172.51
episode_reward_max: 2.0
episode_reward_mean: 1.27
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 27065
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 54.0127215385437
time_total_s: 16105.353735923767
timers:
  learn_throughput: 418.049
  learn_time_ms: 39469.066
  load_throughput: 4365759.273
  load_time_ms: 3.779
  training_iteration_time_ms: 52035.45
  update_time_ms: 2.701
timesteps_total: 4884000
training_iteration: 296

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 45.05
episode_reward_max: 2.0
episode_reward_mean: 1.9833333333333334
episode_reward_min: 1.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9833333333333333
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974226804123711
  reward for individual goal_min: 0.5
episode_len_mean: 41.77918781725889
episode_reward_max: 2.0
episode_reward_mean: 1.99746192893401
episode_reward_min: 1.0
episodes_this_iter: 394
episodes_total: 83680
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974619289340102
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 63.65438151359558
time_total_s: 15972.469888210297
timers:
  learn_throughput: 360.242
  learn_time_ms: 45802.488
  load_throughput: 3404584.77
  load_time_ms: 4.846
  training_iteration_time_ms: 59186.521
  update_time_ms: 2.676
timesteps_total: 4620000
training_iteration: 280

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.3048780487804878
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 174.73
episode_reward_max: 2.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 27078
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.69
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 48.84944295883179
time_total_s: 16122.324934482574
timers:
  learn_throughput: 418.926
  learn_time_ms: 39386.418
  load_throughput: 4476601.184
  load_time_ms: 3.686
  training_iteration_time_ms: 51504.353
  update_time_ms: 2.674
timesteps_total: 4917000
training_iteration: 298

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971264367816092
  reward for individual goal_min: 0.5
episode_len_mean: 46.725212464589234
episode_reward_max: 2.0
episode_reward_mean: 1.9971671388101984
episode_reward_min: 1.0
episodes_this_iter: 353
episodes_total: 55772
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9971671388101983
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 47.836862325668335
time_total_s: 16082.610948324203
timers:
  learn_throughput: 346.177
  learn_time_ms: 47663.543
  load_throughput: 3924756.482
  load_time_ms: 4.204
  training_iteration_time_ms: 61956.671
  update_time_ms: 2.721
timesteps_total: 4207500
training_iteration: 255

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.28735632183908044
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 185.37
episode_reward_max: 2.0
episode_reward_mean: 1.2
episode_reward_min: 0.0
episodes_this_iter: 88
episodes_total: 27153
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.58
  agent_1: 0.62
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 51.60309958457947
time_total_s: 16156.956835508347
timers:
  learn_throughput: 418.483
  learn_time_ms: 39428.079
  load_throughput: 4334206.947
  load_time_ms: 3.807
  training_iteration_time_ms: 52026.733
  update_time_ms: 2.711
timesteps_total: 4900500
training_iteration: 297

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.09137055837564
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 394
episodes_total: 84074
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 56.89178442955017
time_total_s: 16029.361672639847
timers:
  learn_throughput: 364.892
  learn_time_ms: 45218.865
  load_throughput: 3416450.656
  load_time_ms: 4.83
  training_iteration_time_ms: 58538.403
  update_time_ms: 2.663
timesteps_total: 4636500
training_iteration: 281

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2792207792207792
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 171.95
episode_reward_max: 2.0
episode_reward_mean: 1.37
episode_reward_min: 0.0
episodes_this_iter: 98
episodes_total: 27176
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.64
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 47.978829860687256
time_total_s: 16170.303764343262
timers:
  learn_throughput: 422.302
  learn_time_ms: 39071.541
  load_throughput: 4521584.508
  load_time_ms: 3.649
  training_iteration_time_ms: 51096.207
  update_time_ms: 2.693
timesteps_total: 4933500
training_iteration: 299

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 45.94736842105263
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 361
episodes_total: 56133
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 47.87088084220886
time_total_s: 16130.481829166412
timers:
  learn_throughput: 360.849
  learn_time_ms: 45725.548
  load_throughput: 4018815.714
  load_time_ms: 4.106
  training_iteration_time_ms: 59521.894
  update_time_ms: 2.599
timesteps_total: 4224000
training_iteration: 256

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.29605263157894735
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 169.32
episode_reward_max: 2.0
episode_reward_mean: 1.31
episode_reward_min: 0.0
episodes_this_iter: 97
episodes_total: 27250
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.61
  agent_1: 0.7
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 52.00578308105469
time_total_s: 16208.962618589401
timers:
  learn_throughput: 421.179
  learn_time_ms: 39175.742
  load_throughput: 4373290.868
  load_time_ms: 3.773
  training_iteration_time_ms: 51687.897
  update_time_ms: 2.704
timesteps_total: 4917000
training_iteration: 298

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.50406504065041
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 369
episodes_total: 56502
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 48.15796422958374
time_total_s: 16178.639793395996
timers:
  learn_throughput: 376.087
  learn_time_ms: 43872.826
  load_throughput: 4185071.479
  load_time_ms: 3.943
  training_iteration_time_ms: 57173.931
  update_time_ms: 2.558
timesteps_total: 4240500
training_iteration: 257

Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.995475113122172
  reward for individual goal_min: 0.5
episode_len_mean: 41.45569620253165
episode_reward_max: 2.0
episode_reward_mean: 1.9949367088607595
episode_reward_min: 1.0
episodes_this_iter: 395
episodes_total: 84469
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9949367088607595
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 56.66132187843323
time_total_s: 16086.02299451828
timers:
  learn_throughput: 367.48
  learn_time_ms: 44900.431
  load_throughput: 3441629.957
  load_time_ms: 4.794
  training_iteration_time_ms: 58153.245
  update_time_ms: 2.663
timesteps_total: 4653000
training_iteration: 282

Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9666666666666667
  reward for individual goal_min: 0.5
episode_len_mean: 77.71666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.8666666666666667
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9333333333333333
  agent_1: 0.9333333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.20833333333333334
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9791666666666666
  reward for individual goal_min: 0.0
episode_len_mean: 181.75
episode_reward_max: 2.0
episode_reward_mean: 1.26
episode_reward_min: 0.0
episodes_this_iter: 89
episodes_total: 27265
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.65
  agent_1: 0.61
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.370813608169556
time_total_s: 16231.674577951431
timers:
  learn_throughput: 425.657
  learn_time_ms: 38763.63
  load_throughput: 4568988.77
  load_time_ms: 3.611
  training_iteration_time_ms: 50689.551
  update_time_ms: 2.66
timesteps_total: 4950000
training_iteration: 300

Starting final evaluation!
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.24324324324324326
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9864864864864865
  reward for individual goal_min: 0.5
episode_len_mean: 172.76
episode_reward_max: 2.0
episode_reward_mean: 1.28
episode_reward_min: 0.0
episodes_this_iter: 91
episodes_total: 27341
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.73
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 49.15546655654907
time_total_s: 16258.11808514595
timers:
  learn_throughput: 423.077
  learn_time_ms: 39000.007
  load_throughput: 4338799.16
  load_time_ms: 3.803
  training_iteration_time_ms: 51446.701
  update_time_ms: 2.692
timesteps_total: 4933500
training_iteration: 299

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971428571428571
  reward for individual goal_min: 0.5
episode_len_mean: 46.42696629213483
episode_reward_max: 2.0
episode_reward_mean: 1.997191011235955
episode_reward_min: 1.0
episodes_this_iter: 356
episodes_total: 56858
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9971910112359551
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 48.293583393096924
time_total_s: 16226.933376789093
timers:
  learn_throughput: 392.344
  learn_time_ms: 42054.907
  load_throughput: 4347548.497
  load_time_ms: 3.795
  training_iteration_time_ms: 54954.603
  update_time_ms: 2.485
timesteps_total: 4257000
training_iteration: 258

Evaluation results over 10 episodes for each goal
{'[0, 0, 1]': 1.7, '[0, 1, 0]': 2.0, '[1, 0, 0]': 1.9, '[0, 1, 1]': 1.6, '[1, 0, 1]': 2.0, '[1, 1, 0]': 2.0}
Moviepy - Building video /gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-2933yh83nn/trained_agent.mp4.
Moviepy - Writing video /gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-2933yh83nn/trained_agent.mp4

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.47043701799486
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 389
episodes_total: 84858
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.02711772918701
time_total_s: 16143.050112247467
timers:
  learn_throughput: 369.156
  learn_time_ms: 44696.582
  load_throughput: 3451774.916
  load_time_ms: 4.78
  training_iteration_time_ms: 57947.462
  update_time_ms: 2.619
timesteps_total: 4669500
training_iteration: 283

Moviepy - Done !
Moviepy - video ready /gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-2933yh83nn/trained_agent.mp4
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.9
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9833333333333333
  reward for individual goal_min: 0.5
episode_len_mean: 71.21666666666667
episode_reward_max: 2.0
episode_reward_mean: 1.8833333333333333
episode_reward_min: 0.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.95
  agent_1: 0.9333333333333333
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 0.2112676056338028
  reward for collective goal_min: 0.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9920634920634921
  reward for individual goal_min: 0.5
episode_len_mean: 177.81
episode_reward_max: 2.0
episode_reward_mean: 1.19
episode_reward_min: 0.0
episodes_this_iter: 93
episodes_total: 27434
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.55
  agent_1: 0.64
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 61.76276659965515
time_total_s: 16319.880851745605
timers:
  learn_throughput: 420.393
  learn_time_ms: 39248.978
  load_throughput: 4413086.086
  load_time_ms: 3.739
  training_iteration_time_ms: 51741.579
  update_time_ms: 2.677
timesteps_total: 4950000
training_iteration: 300

Starting final evaluation!
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.997093023255814
  reward for individual goal_min: 0.5
episode_len_mean: 46.13128491620112
episode_reward_max: 2.0
episode_reward_mean: 1.9972067039106145
episode_reward_min: 1.0
episodes_this_iter: 358
episodes_total: 57216
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972067039106145
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 57.867037296295166
time_total_s: 16284.800414085388
timers:
  learn_throughput: 400.014
  learn_time_ms: 41248.514
  load_throughput: 4389044.578
  load_time_ms: 3.759
  training_iteration_time_ms: 53691.165
  update_time_ms: 2.508
timesteps_total: 4273500
training_iteration: 259

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973958333333334
  reward for individual goal_min: 0.5
episode_len_mean: 41.89873417721519
episode_reward_max: 2.0
episode_reward_mean: 1.9974683544303797
episode_reward_min: 1.0
episodes_this_iter: 395
episodes_total: 85253
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9974683544303797
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 54.643412828445435
time_total_s: 16197.693525075912
timers:
  learn_throughput: 372.857
  learn_time_ms: 44252.925
  load_throughput: 3454945.634
  load_time_ms: 4.776
  training_iteration_time_ms: 57488.99
  update_time_ms: 2.593
timesteps_total: 4686000
training_iteration: 284

Evaluation results over 10 episodes for each goal
{'[0, 0, 1]': 2.0, '[0, 1, 0]': 2.0, '[1, 0, 0]': 1.9, '[0, 1, 1]': 1.8, '[1, 0, 1]': 1.8, '[1, 1, 0]': 2.0}
Moviepy - Building video /gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-29ws591pkw/trained_agent.mp4.
Moviepy - Writing video /gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-29ws591pkw/trained_agent.mp4

Moviepy - Done !
Moviepy - video ready /gpfswork/rech/imi/uzw47io/elias_experiments/double_condition_3/2022-06-24_00-08-29ws591pkw/trained_agent.mp4
Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.61868686868687
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 396
episodes_total: 85649
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 53.71497845649719
time_total_s: 16251.40850353241
timers:
  learn_throughput: 376.703
  learn_time_ms: 43801.111
  load_throughput: 3437680.86
  load_time_ms: 4.8
  training_iteration_time_ms: 56945.039
  update_time_ms: 2.576
timesteps_total: 4702500
training_iteration: 285

Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 46.3
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9948186528497409
  reward for individual goal_min: 0.5
episode_len_mean: 46.40226628895184
episode_reward_max: 2.0
episode_reward_mean: 1.9943342776203965
episode_reward_min: 1.0
episodes_this_iter: 353
episodes_total: 57569
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9943342776203966
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 67.75204014778137
time_total_s: 16352.55245423317
timers:
  learn_throughput: 406.975
  learn_time_ms: 40543.059
  load_throughput: 4281782.106
  load_time_ms: 3.854
  training_iteration_time_ms: 52737.981
  update_time_ms: 2.481
timesteps_total: 4290000
training_iteration: 260

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-19wecnkw4d/checkpoint_000260/checkpoint-260
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973958333333334
  reward for individual goal_min: 0.5
episode_len_mean: 42.670984455958546
episode_reward_max: 2.0
episode_reward_mean: 1.9974093264248705
episode_reward_min: 1.0
episodes_this_iter: 386
episodes_total: 86035
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9974093264248705
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 53.621989011764526
time_total_s: 16305.030492544174
timers:
  learn_throughput: 380.94
  learn_time_ms: 43313.963
  load_throughput: 3478545.773
  load_time_ms: 4.743
  training_iteration_time_ms: 56397.637
  update_time_ms: 2.561
timesteps_total: 4719000
training_iteration: 286

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9946524064171123
  reward for individual goal_min: 0.5
episode_len_mean: 45.65555555555556
episode_reward_max: 2.0
episode_reward_mean: 1.9944444444444445
episode_reward_min: 1.0
episodes_this_iter: 360
episodes_total: 57929
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972222222222222
  agent_1: 0.9972222222222222
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 62.53973460197449
time_total_s: 16415.092188835144
timers:
  learn_throughput: 414.51
  learn_time_ms: 39806.015
  load_throughput: 4279161.061
  load_time_ms: 3.856
  training_iteration_time_ms: 51801.451
  update_time_ms: 2.501
timesteps_total: 4306500
training_iteration: 261

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.76574307304786
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 397
episodes_total: 86432
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 54.83961510658264
time_total_s: 16359.870107650757
timers:
  learn_throughput: 383.34
  learn_time_ms: 43042.772
  load_throughput: 3467704.349
  load_time_ms: 4.758
  training_iteration_time_ms: 56041.017
  update_time_ms: 2.559
timesteps_total: 4735500
training_iteration: 287

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972677595628415
  reward for individual goal_min: 0.5
episode_len_mean: 45.450819672131146
episode_reward_max: 2.0
episode_reward_mean: 1.9972677595628416
episode_reward_min: 1.0
episodes_this_iter: 366
episodes_total: 58295
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972677595628415
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 62.12710666656494
time_total_s: 16477.21929550171
timers:
  learn_throughput: 403.056
  learn_time_ms: 40937.278
  load_throughput: 4239941.185
  load_time_ms: 3.892
  training_iteration_time_ms: 53149.585
  update_time_ms: 2.548
timesteps_total: 4323000
training_iteration: 262

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.11025641025641
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 390
episodes_total: 86822
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 53.71849322319031
time_total_s: 16413.588600873947
timers:
  learn_throughput: 386.251
  learn_time_ms: 42718.306
  load_throughput: 3494917.962
  load_time_ms: 4.721
  training_iteration_time_ms: 55615.123
  update_time_ms: 2.549
timesteps_total: 4752000
training_iteration: 288

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972677595628415
  reward for individual goal_min: 0.5
episode_len_mean: 44.62903225806452
episode_reward_max: 2.0
episode_reward_mean: 1.9973118279569892
episode_reward_min: 1.0
episodes_this_iter: 372
episodes_total: 58667
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973118279569892
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 62.14649248123169
time_total_s: 16539.36578798294
timers:
  learn_throughput: 392.815
  learn_time_ms: 42004.454
  load_throughput: 4111549.718
  load_time_ms: 4.013
  training_iteration_time_ms: 54494.293
  update_time_ms: 2.554
timesteps_total: 4339500
training_iteration: 263

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974747474747475
  reward for individual goal_min: 0.5
episode_len_mean: 43.71164021164021
episode_reward_max: 2.0
episode_reward_mean: 1.9973544973544974
episode_reward_min: 1.0
episodes_this_iter: 378
episodes_total: 87200
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973544973544973
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 54.7087676525116
time_total_s: 16468.29736852646
timers:
  learn_throughput: 387.166
  learn_time_ms: 42617.334
  load_throughput: 3547243.744
  load_time_ms: 4.651
  training_iteration_time_ms: 55331.691
  update_time_ms: 2.543
timesteps_total: 4768500
training_iteration: 289

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9972677595628415
  reward for individual goal_min: 0.5
episode_len_mean: 44.120643431635386
episode_reward_max: 2.0
episode_reward_mean: 1.997319034852547
episode_reward_min: 1.0
episodes_this_iter: 373
episodes_total: 59040
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973190348525469
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 62.77897334098816
time_total_s: 16602.14476132393
timers:
  learn_throughput: 381.334
  learn_time_ms: 43269.16
  load_throughput: 4035924.537
  load_time_ms: 4.088
  training_iteration_time_ms: 56047.58
  update_time_ms: 2.584
timesteps_total: 4356000
training_iteration: 264

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9975
  reward for individual goal_min: 0.5
episode_len_mean: 43.4089709762533
episode_reward_max: 2.0
episode_reward_mean: 1.9973614775725594
episode_reward_min: 1.0
episodes_this_iter: 379
episodes_total: 87579
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973614775725593
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 54.99075412750244
time_total_s: 16523.28812265396
timers:
  learn_throughput: 390.149
  learn_time_ms: 42291.547
  load_throughput: 3580720.428
  load_time_ms: 4.608
  training_iteration_time_ms: 54957.05
  update_time_ms: 2.538
timesteps_total: 4785000
training_iteration: 290

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.42857142857143
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 371
episodes_total: 59411
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 63.857202768325806
time_total_s: 16666.001964092255
timers:
  learn_throughput: 370.555
  learn_time_ms: 44527.821
  load_throughput: 3938694.652
  load_time_ms: 4.189
  training_iteration_time_ms: 57648.344
  update_time_ms: 2.62
timesteps_total: 4372500
training_iteration: 265

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.26342710997442
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 391
episodes_total: 87970
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 54.862319231033325
time_total_s: 16578.150441884995
timers:
  learn_throughput: 390.825
  learn_time_ms: 42218.358
  load_throughput: 3613985.535
  load_time_ms: 4.566
  training_iteration_time_ms: 54754.699
  update_time_ms: 2.534
timesteps_total: 4801500
training_iteration: 291

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.808
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 375
episodes_total: 59786
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 62.496898889541626
time_total_s: 16728.498862981796
timers:
  learn_throughput: 361.012
  learn_time_ms: 45704.855
  load_throughput: 3816662.586
  load_time_ms: 4.323
  training_iteration_time_ms: 59110.286
  update_time_ms: 2.672
timesteps_total: 4389000
training_iteration: 266

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.4175
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 400
episodes_total: 88370
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 55.67677116394043
time_total_s: 16633.827213048935
timers:
  learn_throughput: 391.677
  learn_time_ms: 42126.505
  load_throughput: 3622763.635
  load_time_ms: 4.555
  training_iteration_time_ms: 54656.07
  update_time_ms: 2.538
timesteps_total: 4818000
training_iteration: 292

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.55949367088608
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 395
episodes_total: 88765
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 56.16218900680542
time_total_s: 16689.98940205574
timers:
  learn_throughput: 391.626
  learn_time_ms: 42132.063
  load_throughput: 3631775.059
  load_time_ms: 4.543
  training_iteration_time_ms: 54569.352
  update_time_ms: 2.549
timesteps_total: 4834500
training_iteration: 293

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.42741935483871
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 372
episodes_total: 60158
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 62.510618686676025
time_total_s: 16791.009481668472
timers:
  learn_throughput: 352.063
  learn_time_ms: 46866.593
  load_throughput: 3785679.996
  load_time_ms: 4.359
  training_iteration_time_ms: 60544.447
  update_time_ms: 2.726
timesteps_total: 4405500
training_iteration: 267

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.107142857142854
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 392
episodes_total: 89157
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.45718479156494
time_total_s: 16748.446586847305
timers:
  learn_throughput: 387.426
  learn_time_ms: 42588.813
  load_throughput: 3685640.884
  load_time_ms: 4.477
  training_iteration_time_ms: 54950.814
  update_time_ms: 2.567
timesteps_total: 4851000
training_iteration: 294

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 45.51381215469613
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 362
episodes_total: 60520
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 62.39343070983887
time_total_s: 16853.40291237831
timers:
  learn_throughput: 343.834
  learn_time_ms: 47988.284
  load_throughput: 3704323.083
  load_time_ms: 4.454
  training_iteration_time_ms: 61953.555
  update_time_ms: 2.747
timesteps_total: 4422000
training_iteration: 268

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9975369458128078
  reward for individual goal_min: 0.5
episode_len_mean: 43.6042216358839
episode_reward_max: 2.0
episode_reward_mean: 1.9973614775725594
episode_reward_min: 1.0
episodes_this_iter: 379
episodes_total: 89536
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973614775725593
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 57.382580280303955
time_total_s: 16805.82916712761
timers:
  learn_throughput: 385.222
  learn_time_ms: 42832.469
  load_throughput: 3704541.201
  load_time_ms: 4.454
  training_iteration_time_ms: 55318.475
  update_time_ms: 2.563
timesteps_total: 4867500
training_iteration: 295

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 45.611570247933884
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 363
episodes_total: 60883
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 62.88212990760803
time_total_s: 16916.28504228592
timers:
  learn_throughput: 342.186
  learn_time_ms: 48219.441
  load_throughput: 3650356.617
  load_time_ms: 4.52
  training_iteration_time_ms: 62455.217
  update_time_ms: 2.747
timesteps_total: 4438500
training_iteration: 269

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 41.71827411167513
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 394
episodes_total: 89930
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 60.30219101905823
time_total_s: 16866.131358146667
timers:
  learn_throughput: 380.388
  learn_time_ms: 43376.816
  load_throughput: 3660899.805
  load_time_ms: 4.507
  training_iteration_time_ms: 55997.725
  update_time_ms: 2.574
timesteps_total: 4884000
training_iteration: 296

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9975369458128078
  reward for individual goal_min: 0.5
episode_len_mean: 45.24657534246575
episode_reward_max: 2.0
episode_reward_mean: 1.9972602739726026
episode_reward_min: 1.0
episodes_this_iter: 365
episodes_total: 61248
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9972602739726028
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 61.88779973983765
time_total_s: 16978.172842025757
timers:
  learn_throughput: 342.206
  learn_time_ms: 48216.518
  load_throughput: 3716776.997
  load_time_ms: 4.439
  training_iteration_time_ms: 62452.167
  update_time_ms: 2.761
timesteps_total: 4455000
training_iteration: 270

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.7020725388601
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 386
episodes_total: 90316
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 59.85846757888794
time_total_s: 16925.989825725555
timers:
  learn_throughput: 376.257
  learn_time_ms: 43853.036
  load_throughput: 3690967.835
  load_time_ms: 4.47
  training_iteration_time_ms: 56499.924
  update_time_ms: 2.581
timesteps_total: 4900500
training_iteration: 297

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9971751412429378
  reward for individual goal_min: 0.5
episode_len_mean: 44.08
episode_reward_max: 2.0
episode_reward_mean: 1.9973333333333334
episode_reward_min: 1.0
episodes_this_iter: 375
episodes_total: 61623
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973333333333333
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 70.94487166404724
time_total_s: 17049.117713689804
timers:
  learn_throughput: 336.634
  learn_time_ms: 49014.727
  load_throughput: 3719893.573
  load_time_ms: 4.436
  training_iteration_time_ms: 63291.72
  update_time_ms: 2.812
timesteps_total: 4471500
training_iteration: 271

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.63917525773196
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 388
episodes_total: 90704
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 58.05103421211243
time_total_s: 16984.040859937668
timers:
  learn_throughput: 373.656
  learn_time_ms: 44158.247
  load_throughput: 3653208.473
  load_time_ms: 4.517
  training_iteration_time_ms: 56933.123
  update_time_ms: 2.592
timesteps_total: 4917000
training_iteration: 298

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.22520107238606
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 373
episodes_total: 61996
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 72.49203252792358
time_total_s: 17121.609746217728
timers:
  learn_throughput: 330.878
  learn_time_ms: 49867.344
  load_throughput: 3730058.641
  load_time_ms: 4.424
  training_iteration_time_ms: 64327.312
  update_time_ms: 2.905
timesteps_total: 4488000
training_iteration: 272

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.42820512820513
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 390
episodes_total: 91094
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 57.044777393341064
time_total_s: 17041.08563733101
timers:
  learn_throughput: 372.712
  learn_time_ms: 44270.139
  load_throughput: 3604836.729
  load_time_ms: 4.577
  training_iteration_time_ms: 57166.877
  update_time_ms: 2.595
timesteps_total: 4933500
training_iteration: 299

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.17426273458445
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 373
episodes_total: 62369
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 73.62708592414856
time_total_s: 17195.236832141876
timers:
  learn_throughput: 325.324
  learn_time_ms: 50718.679
  load_throughput: 3528992.642
  load_time_ms: 4.676
  training_iteration_time_ms: 65474.938
  update_time_ms: 2.91
timesteps_total: 4504500
training_iteration: 273

Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.36666666666667
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 40.93796526054591
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 403
episodes_total: 91497
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 60.3209183216095
time_total_s: 17101.40655565262
timers:
  learn_throughput: 372.006
  learn_time_ms: 44354.096
  load_throughput: 3580238.8
  load_time_ms: 4.609
  training_iteration_time_ms: 57273.081
  update_time_ms: 2.591
timesteps_total: 4950000
training_iteration: 300

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-184s1jdk42/checkpoint_000300/checkpoint-300
Starting final evaluation!
Evaluation results over 10 episodes for each goal
{'[0, 0, 1]': 2.0, '[0, 1, 0]': 2.0, '[1, 0, 0]': 2.0, '[0, 1, 1]': 2.0, '[1, 0, 1]': 2.0, '[1, 1, 0]': 2.0}
Moviepy - Building video /gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-184s1jdk42/trained_agent.mp4.
Moviepy - Writing video /gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-184s1jdk42/trained_agent.mp4

Moviepy - Done !
Moviepy - video ready /gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-184s1jdk42/trained_agent.mp4
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 45.22527472527472
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 364
episodes_total: 62733
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 72.31115794181824
time_total_s: 17267.547990083694
timers:
  learn_throughput: 320.677
  learn_time_ms: 51453.661
  load_throughput: 3538736.903
  load_time_ms: 4.663
  training_iteration_time_ms: 66428.3
  update_time_ms: 2.933
timesteps_total: 4521000
training_iteration: 274

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9942857142857143
  reward for individual goal_min: 0.5
episode_len_mean: 45.48342541436464
episode_reward_max: 2.0
episode_reward_mean: 1.9944751381215469
episode_reward_min: 1.0
episodes_this_iter: 362
episodes_total: 63095
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9972375690607734
  agent_1: 0.9972375690607734
policy_reward_min:
  agent_0: 0.0
  agent_1: 0.0
time_this_iter_s: 73.07064485549927
time_total_s: 17340.618634939194
timers:
  learn_throughput: 316.136
  learn_time_ms: 52192.732
  load_throughput: 3510892.765
  load_time_ms: 4.7
  training_iteration_time_ms: 67349.097
  update_time_ms: 2.969
timesteps_total: 4537500
training_iteration: 275

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.38337801608579
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 373
episodes_total: 63468
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 72.8621723651886
time_total_s: 17413.480807304382
timers:
  learn_throughput: 311.292
  learn_time_ms: 53004.916
  load_throughput: 3494123.92
  load_time_ms: 4.722
  training_iteration_time_ms: 68384.991
  update_time_ms: 3.009
timesteps_total: 4554000
training_iteration: 276

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9944444444444445
  reward for individual goal_min: 0.5
episode_len_mean: 45.66298342541437
episode_reward_max: 2.0
episode_reward_mean: 1.9944751381215469
episode_reward_min: 1.0
episodes_this_iter: 362
episodes_total: 63830
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.994475138121547
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 71.75142359733582
time_total_s: 17485.232230901718
timers:
  learn_throughput: 307.085
  learn_time_ms: 53731.118
  load_throughput: 3385133.902
  load_time_ms: 4.874
  training_iteration_time_ms: 69308.708
  update_time_ms: 3.124
timesteps_total: 4570500
training_iteration: 277

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.611111111111114
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 378
episodes_total: 64208
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 73.07347559928894
time_total_s: 17558.305706501007
timers:
  learn_throughput: 302.268
  learn_time_ms: 54587.288
  load_throughput: 3432429.473
  load_time_ms: 4.807
  training_iteration_time_ms: 70376.64
  update_time_ms: 3.165
timesteps_total: 4587000
training_iteration: 278

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9975
  reward for individual goal_min: 0.5
episode_len_mean: 44.22727272727273
episode_reward_max: 2.0
episode_reward_mean: 1.9973262032085561
episode_reward_min: 1.0
episodes_this_iter: 374
episodes_total: 64582
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973262032085561
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 72.81603407859802
time_total_s: 17631.121740579605
timers:
  learn_throughput: 298.035
  learn_time_ms: 55362.66
  load_throughput: 3426260.898
  load_time_ms: 4.816
  training_iteration_time_ms: 71369.198
  update_time_ms: 3.183
timesteps_total: 4603500
training_iteration: 279

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 45.416666666666664
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9969879518072289
  reward for individual goal_min: 0.5
episode_len_mean: 44.32258064516129
episode_reward_max: 2.0
episode_reward_mean: 1.9973118279569892
episode_reward_min: 1.0
episodes_this_iter: 372
episodes_total: 64954
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973118279569892
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 79.86353349685669
time_total_s: 17710.98527407646
timers:
  learn_throughput: 293.493
  learn_time_ms: 56219.322
  load_throughput: 3363711.815
  load_time_ms: 4.905
  training_iteration_time_ms: 72485.567
  update_time_ms: 3.155
timesteps_total: 4620000
training_iteration: 280

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-19wecnkw4d/checkpoint_000280/checkpoint-280
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.822888283378745
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 367
episodes_total: 65321
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 69.10855436325073
time_total_s: 17780.093828439713
timers:
  learn_throughput: 295.653
  learn_time_ms: 55808.621
  load_throughput: 3280310.18
  load_time_ms: 5.03
  training_iteration_time_ms: 72302.72
  update_time_ms: 3.116
timesteps_total: 4636500
training_iteration: 281

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 45.2103825136612
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 366
episodes_total: 65687
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 72.06678819656372
time_total_s: 17852.160616636276
timers:
  learn_throughput: 295.856
  learn_time_ms: 55770.456
  load_throughput: 3187087.706
  load_time_ms: 5.177
  training_iteration_time_ms: 72261.497
  update_time_ms: 3.03
timesteps_total: 4653000
training_iteration: 282

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 44.216577540106954
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 374
episodes_total: 66061
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 73.37464690208435
time_total_s: 17925.53526353836
timers:
  learn_throughput: 295.708
  learn_time_ms: 55798.321
  load_throughput: 3368886.076
  load_time_ms: 4.898
  training_iteration_time_ms: 72220.409
  update_time_ms: 3.087
timesteps_total: 4669500
training_iteration: 283

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.06527415143603
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 383
episodes_total: 66444
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 73.23947238922119
time_total_s: 17998.77473592758
timers:
  learn_throughput: 295.167
  learn_time_ms: 55900.535
  load_throughput: 3222811.799
  load_time_ms: 5.12
  training_iteration_time_ms: 72312.05
  update_time_ms: 3.052
timesteps_total: 4686000
training_iteration: 284

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.43187660668381
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 389
episodes_total: 66833
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 73.52088332176208
time_total_s: 18072.295619249344
timers:
  learn_throughput: 294.826
  learn_time_ms: 55965.231
  load_throughput: 3228675.612
  load_time_ms: 5.11
  training_iteration_time_ms: 72356.489
  update_time_ms: 3.117
timesteps_total: 4702500
training_iteration: 285

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.167095115681235
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 389
episodes_total: 67222
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 73.20926833152771
time_total_s: 18145.50488758087
timers:
  learn_throughput: 294.666
  learn_time_ms: 55995.553
  load_throughput: 3311688.767
  load_time_ms: 4.982
  training_iteration_time_ms: 72375.59
  update_time_ms: 3.098
timesteps_total: 4719000
training_iteration: 286

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973404255319149
  reward for individual goal_min: 0.5
episode_len_mean: 43.25848563968668
episode_reward_max: 2.0
episode_reward_mean: 1.9973890339425588
episode_reward_min: 1.0
episodes_this_iter: 383
episodes_total: 67605
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973890339425587
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 73.59188270568848
time_total_s: 18219.09677028656
timers:
  learn_throughput: 293.941
  learn_time_ms: 56133.66
  load_throughput: 3326268.799
  load_time_ms: 4.961
  training_iteration_time_ms: 72559.946
  update_time_ms: 3.015
timesteps_total: 4735500
training_iteration: 287

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.920212765957444
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 376
episodes_total: 67981
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 72.24150657653809
time_total_s: 18291.338276863098
timers:
  learn_throughput: 294.07
  learn_time_ms: 56108.998
  load_throughput: 3265312.655
  load_time_ms: 5.053
  training_iteration_time_ms: 72476.942
  update_time_ms: 2.991
timesteps_total: 4752000
training_iteration: 288

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.109947643979055
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 382
episodes_total: 68363
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 74.20148396492004
time_total_s: 18365.53976082802
timers:
  learn_throughput: 293.776
  learn_time_ms: 56165.258
  load_throughput: 3220127.584
  load_time_ms: 5.124
  training_iteration_time_ms: 72615.516
  update_time_ms: 2.986
timesteps_total: 4768500
training_iteration: 289

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.994475138121547
  reward for individual goal_min: 0.5
episode_len_mean: 44.2627345844504
episode_reward_max: 2.0
episode_reward_mean: 1.9946380697050938
episode_reward_min: 1.0
episodes_this_iter: 373
episodes_total: 68736
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9946380697050938
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 72.16956782341003
time_total_s: 18437.70932865143
timers:
  learn_throughput: 294.124
  learn_time_ms: 56098.707
  load_throughput: 3212146.428
  load_time_ms: 5.137
  training_iteration_time_ms: 72525.951
  update_time_ms: 3.001
timesteps_total: 4785000
training_iteration: 290

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.66062176165803
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 386
episodes_total: 69122
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 74.25851392745972
time_total_s: 18511.967842578888
timers:
  learn_throughput: 291.414
  learn_time_ms: 56620.383
  load_throughput: 3315877.745
  load_time_ms: 4.976
  training_iteration_time_ms: 73023.824
  update_time_ms: 3.028
timesteps_total: 4801500
training_iteration: 291

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9975247524752475
  reward for individual goal_min: 0.5
episode_len_mean: 43.576
episode_reward_max: 2.0
episode_reward_mean: 1.9973333333333334
episode_reward_min: 1.0
episodes_this_iter: 375
episodes_total: 69497
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9973333333333333
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 72.68049669265747
time_total_s: 18584.648339271545
timers:
  learn_throughput: 291.182
  learn_time_ms: 56665.527
  load_throughput: 3368148.264
  load_time_ms: 4.899
  training_iteration_time_ms: 73084.239
  update_time_ms: 3.106
timesteps_total: 4818000
training_iteration: 292

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9974489795918368
  reward for individual goal_min: 0.5
episode_len_mean: 43.10880829015544
episode_reward_max: 2.0
episode_reward_mean: 1.9974093264248705
episode_reward_min: 1.0
episodes_this_iter: 386
episodes_total: 69883
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9974093264248705
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 72.01384854316711
time_total_s: 18656.662187814713
timers:
  learn_throughput: 291.724
  learn_time_ms: 56560.244
  load_throughput: 3318835.439
  load_time_ms: 4.972
  training_iteration_time_ms: 72963.814
  update_time_ms: 3.044
timesteps_total: 4834500
training_iteration: 293

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9945652173913043
  reward for individual goal_min: 0.5
episode_len_mean: 44.37265415549598
episode_reward_max: 2.0
episode_reward_mean: 1.9946380697050938
episode_reward_min: 1.0
episodes_this_iter: 373
episodes_total: 70256
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 0.9946380697050938
  agent_1: 1.0
policy_reward_min:
  agent_0: 0.0
  agent_1: 1.0
time_this_iter_s: 72.44371819496155
time_total_s: 18729.105906009674
timers:
  learn_throughput: 292.143
  learn_time_ms: 56479.11
  load_throughput: 3461754.737
  load_time_ms: 4.766
  training_iteration_time_ms: 72884.735
  update_time_ms: 3.115
timesteps_total: 4851000
training_iteration: 294

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.753246753246756
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 385
episodes_total: 70641
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 72.54040265083313
time_total_s: 18801.646308660507
timers:
  learn_throughput: 292.715
  learn_time_ms: 56368.791
  load_throughput: 3496931.154
  load_time_ms: 4.718
  training_iteration_time_ms: 72787.273
  update_time_ms: 3.099
timesteps_total: 4867500
training_iteration: 295

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.696569920844325
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 379
episodes_total: 71020
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 72.05530619621277
time_total_s: 18873.70161485672
timers:
  learn_throughput: 293.222
  learn_time_ms: 56271.289
  load_throughput: 3480260.092
  load_time_ms: 4.741
  training_iteration_time_ms: 72687.819
  update_time_ms: 3.142
timesteps_total: 4884000
training_iteration: 296

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.648148148148145
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 378
episodes_total: 71398
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 72.88786625862122
time_total_s: 18946.58948111534
timers:
  learn_throughput: 293.605
  learn_time_ms: 56197.987
  load_throughput: 3403680.574
  load_time_ms: 4.848
  training_iteration_time_ms: 72617.054
  update_time_ms: 3.138
timesteps_total: 4900500
training_iteration: 297

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.52
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 375
episodes_total: 71773
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 73.23153686523438
time_total_s: 19019.821017980576
timers:
  learn_throughput: 293.364
  learn_time_ms: 56244.183
  load_throughput: 3398599.231
  load_time_ms: 4.855
  training_iteration_time_ms: 72715.215
  update_time_ms: 3.145
timesteps_total: 4917000
training_iteration: 298

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 0.9973821989528796
  reward for individual goal_min: 0.5
episode_len_mean: 44.23936170212766
episode_reward_max: 2.0
episode_reward_mean: 1.997340425531915
episode_reward_min: 1.0
episodes_this_iter: 376
episodes_total: 72149
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 0.9973404255319149
policy_reward_min:
  agent_0: 1.0
  agent_1: 0.0
time_this_iter_s: 72.32149624824524
time_total_s: 19092.14251422882
timers:
  learn_throughput: 293.893
  learn_time_ms: 56142.91
  load_throughput: 3416467.522
  load_time_ms: 4.83
  training_iteration_time_ms: 72526.634
  update_time_ms: 3.228
timesteps_total: 4933500
training_iteration: 299

Custom evaluation in goals {'agent_0': 0, 'agent_1': 0}
Custom evaluation in goals {'agent_0': 1, 'agent_1': 1}
Custom evaluation in goals {'agent_0': 2, 'agent_1': 2}
Custom evaluation in goals {'agent_0': 3, 'agent_1': 3}
Custom evaluation in goals {'agent_0': 4, 'agent_1': 4}
Custom evaluation in goals {'agent_0': 5, 'agent_1': 5}
custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 43.35
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 60
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0

custom_metrics:
  reward for collective goal_max: 1.0
  reward for collective goal_mean: 1.0
  reward for collective goal_min: 1.0
  reward for individual goal_max: 1.0
  reward for individual goal_mean: 1.0
  reward for individual goal_min: 1.0
episode_len_mean: 42.39331619537275
episode_reward_max: 2.0
episode_reward_mean: 2.0
episode_reward_min: 2.0
episodes_this_iter: 389
episodes_total: 72538
policy_reward_max:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_mean:
  agent_0: 1.0
  agent_1: 1.0
policy_reward_min:
  agent_0: 1.0
  agent_1: 1.0
time_this_iter_s: 79.9708616733551
time_total_s: 19172.113375902176
timers:
  learn_throughput: 293.164
  learn_time_ms: 56282.424
  load_throughput: 3461062.234
  load_time_ms: 4.767
  training_iteration_time_ms: 72680.399
  update_time_ms: 3.264
timesteps_total: 4950000
training_iteration: 300

New best model found, saving it in/gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-19wecnkw4d/checkpoint_000300/checkpoint-300
Starting final evaluation!
Evaluation results over 10 episodes for each goal
{'[0, 0, 1]': 2.0, '[0, 1, 0]': 2.0, '[1, 0, 0]': 2.0, '[0, 1, 1]': 2.0, '[1, 0, 1]': 2.0, '[1, 1, 0]': 2.0}
Moviepy - Building video /gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-19wecnkw4d/trained_agent.mp4.
Moviepy - Writing video /gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-19wecnkw4d/trained_agent.mp4

Moviepy - Done !
Moviepy - video ready /gpfswork/rech/imi/uzw47io/elias_experiments/centralized_3/2022-06-24_00-08-19wecnkw4d/trained_agent.mp4
