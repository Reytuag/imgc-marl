{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naming game experiments in a simplified probabilistic environment\n",
    "\n",
    "In this notebook we can simulate the alignment results we would obtain with the naming game for different probabilities of solving different type of goals.\n",
    "\n",
    "The `scores` matrix contains the probability of solving a pair of goals from the point of view of one agent, should be read as $P(\\text{solving my goal i, when other agent selected goal j}) = p_{i,j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations, product\n",
    "from scipy.special import softmax\n",
    "import torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc(\"font\", family=\"serif\")\n",
    "plt.rc(\"xtick\", labelsize=\"small\")\n",
    "plt.rc(\"ytick\", labelsize=\"small\")\n",
    "plt.rc(\"legend\", fontsize=\"small\")\n",
    "plt.rc(\"axes\", labelsize=\"small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_LANDMARKS = 6\n",
    "# Number of updates to the matrix\n",
    "N_STEPS = 10_000\n",
    "# Update step\n",
    "DELTA = 1\n",
    "# Exploration probability (0 is greedy but stil explores since there are negative updates for bad associations)\n",
    "EGREEDY = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only cooperative goals\n",
    "\n",
    "`scores0` is the probability matrix used by agent 0, `scores1` is the probability matrix for agent 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A basic probability matrix where the probability of solving aligned goals is 1, and different goals 0.3\n",
    "\n",
    "scores = np.eye(NUMBER_OF_LANDMARKS)\n",
    "P_DIFFERENT_GOALS = 0.3\n",
    "\n",
    "for i in range(scores.shape[0]):\n",
    "    for j in range(scores.shape[1]):\n",
    "        if i != j:\n",
    "            scores[i, j] = P_DIFFERENT_GOALS\n",
    "n_goals = scores.shape[0]\n",
    "\n",
    "scores0 = scores1 = scores\n",
    "\n",
    "# Alternatively you can create the matrix entry by entry, and specify different matrices for each agent\n",
    "\n",
    "# scores1 = np.array([\n",
    "#     [0.8, 0, 0.1],\n",
    "#     [0, 1, 0.2],\n",
    "#     [0.7, 0.8, 0.9]\n",
    "# ])\n",
    "\n",
    "# scores0 = np.array([\n",
    "#     [0.8, 0, 0.1],\n",
    "#     [0.1, 1, 0],\n",
    "#     [0.2, 0, 0.9]\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15*a+(1-a) =b*15\n",
    "\n",
    "\n",
    "14*a =b*15-1\n",
    "\n",
    "a=(b*15-1)/14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x*15-1)/14\n",
    "\n",
    "print(f(0.5))\n",
    "print(f(0.25))\n",
    "print(f(0.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One naming matrix per agent\n",
    "\n",
    "Same matrix is used when agent is leader and follower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment for all seeds\n",
    "a0 = []\n",
    "a1 = []\n",
    "\n",
    "for s in range(10):\n",
    "    np.random.seed(s)\n",
    "    agent_0 = np.zeros_like(scores0)\n",
    "    agent_1 = np.zeros_like(scores1)\n",
    "    for i in range(N_STEPS):\n",
    "        leader_goal_index = np.random.randint(0, n_goals)\n",
    "        if np.random.random() < 0.5:\n",
    "            # Agent 0 leader\n",
    "            if np.random.random() < EGREEDY:\n",
    "                leader_msg_index = np.random.choice(range(scores.shape[0]))\n",
    "            else:\n",
    "                leader_msg_index = np.argmax(agent_0[leader_goal_index])\n",
    "            if np.random.random() < EGREEDY:\n",
    "                follower_goal_index = np.random.choice(range(scores.shape[0]))\n",
    "            else:\n",
    "                follower_goal_index = np.argmax(agent_1[:, leader_msg_index])\n",
    "            leader_reward = (\n",
    "                np.random.random() < scores0[leader_goal_index, follower_goal_index]\n",
    "            )\n",
    "            follower_reward = (\n",
    "                np.random.random() < scores1[follower_goal_index, leader_goal_index]\n",
    "            )\n",
    "            if leader_reward:\n",
    "                agent_0[leader_goal_index, :] -= DELTA\n",
    "                agent_0[leader_goal_index, leader_msg_index] += (\n",
    "                    leader_reward + 1\n",
    "                ) * DELTA\n",
    "            else:\n",
    "                agent_0[leader_goal_index, leader_msg_index] -= DELTA\n",
    "            if follower_reward:\n",
    "                agent_1[:, leader_msg_index] -= DELTA\n",
    "                agent_1[follower_goal_index, leader_msg_index] += (\n",
    "                    follower_reward + 1\n",
    "                ) * DELTA\n",
    "            else:\n",
    "                agent_1[follower_goal_index, leader_msg_index] -= DELTA\n",
    "\n",
    "        else:\n",
    "            # Agent 1 leader\n",
    "            if np.random.random() < EGREEDY:\n",
    "                leader_msg_index = np.random.choice(range(scores.shape[0]))\n",
    "            else:\n",
    "                leader_msg_index = np.argmax(agent_1[leader_goal_index])\n",
    "            if np.random.random() < EGREEDY:\n",
    "                follower_goal_index = np.random.choice(range(scores.shape[0]))\n",
    "            else:\n",
    "                follower_goal_index = np.argmax(agent_0[:, leader_msg_index])\n",
    "            leader_reward = (\n",
    "                np.random.random() < scores1[leader_goal_index, follower_goal_index]\n",
    "            )\n",
    "            follower_reward = (\n",
    "                np.random.random() < scores0[follower_goal_index, leader_goal_index]\n",
    "            )\n",
    "            if leader_reward:\n",
    "                agent_1[leader_goal_index, :] -= DELTA\n",
    "                agent_1[leader_goal_index, leader_msg_index] += (\n",
    "                    leader_reward + 1\n",
    "                ) * DELTA\n",
    "            else:\n",
    "                agent_1[leader_goal_index, leader_msg_index] -= DELTA\n",
    "            if follower_reward:\n",
    "                agent_0[:, leader_msg_index] -= DELTA\n",
    "                agent_0[follower_goal_index, leader_msg_index] += (\n",
    "                    follower_reward + 1\n",
    "                ) * DELTA\n",
    "            else:\n",
    "                agent_0[follower_goal_index, leader_msg_index] -= DELTA\n",
    "\n",
    "    alignment_0_leader = []\n",
    "    for i in range(0, n_goals):\n",
    "        msg = np.argmax(agent_0[i])\n",
    "        j = np.argmax(agent_1[:, msg])\n",
    "        alignment_0_leader.append(i == j)\n",
    "\n",
    "    alignment_1_leader = []\n",
    "    for i in range(0, n_goals):\n",
    "        msg = np.argmax(agent_1[i])\n",
    "        j = np.argmax(agent_0[:, msg])\n",
    "        alignment_1_leader.append(i == j)\n",
    "\n",
    "    a0.append(sum(alignment_0_leader) / len(alignment_0_leader))\n",
    "    a1.append(sum(alignment_1_leader) / len(alignment_1_leader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Alignment for agent 0: {np.mean(a0)}\")\n",
    "print(f\"Alignment for agent 1: {np.mean(a1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum competence for getting alignment\n",
    "\n",
    "Analysing what's the minimum probability needed for alignment. We assume that the probability of solving aligned goals is p, and different goals 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount=1.\n",
    "t=0.5\n",
    "EGREEDY=0.05\n",
    "NUMBER_OF_LANDMARKS=15\n",
    "N_STEPS = 10000\n",
    "n_goals=NUMBER_OF_LANDMARKS\n",
    "discount_b=1/n_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment for all seeds\n",
    "a0_p = []\n",
    "a1_p = []\n",
    "\n",
    "\n",
    "probas = np.arange(0.1, 1.1, 0.1)\n",
    "for p in probas:\n",
    "    a0 = []\n",
    "    a1 = []\n",
    "    scores = (np.eye(NUMBER_OF_LANDMARKS)*t+np.ones((NUMBER_OF_LANDMARKS,NUMBER_OF_LANDMARKS))*(1-t)) * p \n",
    "    \n",
    "    \n",
    "    for s in range(3):\n",
    "        np.random.seed(s)\n",
    "        agent_0 = np.zeros_like(scores)\n",
    "        agent_1 = np.zeros_like(scores)\n",
    "        for i in range(N_STEPS):\n",
    "            leader_goal_index = np.random.randint(0, scores.shape[0])\n",
    "            if np.random.random() < 0.5:\n",
    "                # Agent 0 leader\n",
    "                if np.random.random() < EGREEDY:\n",
    "                    leader_msg_index = np.random.choice(range(scores.shape[0]))\n",
    "                else:\n",
    "                    leader_msg_index = np.argmax(agent_0[leader_goal_index])\n",
    "                if np.random.random() < EGREEDY:\n",
    "                    follower_goal_index = np.random.choice(range(scores.shape[0]))\n",
    "                else:\n",
    "                    follower_goal_index = np.argmax(agent_1[:, leader_msg_index])\n",
    "                leader_reward = (\n",
    "                    np.random.random() < scores[leader_goal_index, follower_goal_index]\n",
    "                )\n",
    "                follower_reward = (\n",
    "                    np.random.random() < scores[follower_goal_index, leader_goal_index]\n",
    "                )\n",
    "                if leader_reward:\n",
    "                    agent_0[leader_goal_index, :] -= DELTA*discount_b\n",
    "                    agent_0[leader_goal_index, leader_msg_index] += (\n",
    "                        leader_reward + discount_b\n",
    "                    ) * DELTA\n",
    "                else:\n",
    "                    agent_0[leader_goal_index, leader_msg_index] -= DELTA*discount\n",
    "                if follower_reward:\n",
    "                    agent_1[:, leader_msg_index] -= DELTA*discount_b\n",
    "                    agent_1[follower_goal_index, leader_msg_index] += (\n",
    "                        follower_reward +  discount_b\n",
    "                    ) * DELTA\n",
    "                else:\n",
    "                    agent_1[follower_goal_index, leader_msg_index] -= DELTA*discount\n",
    "\n",
    "            else:\n",
    "                # Agent 1 leader\n",
    "                if np.random.random() < EGREEDY:\n",
    "                    leader_msg_index = np.random.choice(range(scores.shape[0]))\n",
    "                else:\n",
    "                    leader_msg_index = np.argmax(agent_1[leader_goal_index])\n",
    "                if np.random.random() < EGREEDY:\n",
    "                    follower_goal_index = np.random.choice(range(scores.shape[0]))\n",
    "                else:\n",
    "                    follower_goal_index = np.argmax(agent_0[:, leader_msg_index])\n",
    "                leader_reward = (\n",
    "                    np.random.random() < scores[leader_goal_index, follower_goal_index]\n",
    "                )\n",
    "                follower_reward = (\n",
    "                    np.random.random() < scores[follower_goal_index, leader_goal_index]\n",
    "                )\n",
    "                if leader_reward:\n",
    "                    agent_1[leader_goal_index, :] -= DELTA*discount_b\n",
    "                    agent_1[leader_goal_index, leader_msg_index] += (\n",
    "                        leader_reward + discount_b\n",
    "                    ) * DELTA\n",
    "                else:\n",
    "                    agent_1[leader_goal_index, leader_msg_index] -= DELTA*discount\n",
    "                if follower_reward:\n",
    "                    agent_0[:, leader_msg_index] -= DELTA*discount_b\n",
    "                    agent_0[follower_goal_index, leader_msg_index] += (\n",
    "                        follower_reward  + discount_b\n",
    "                    ) * DELTA\n",
    "                else:\n",
    "                    agent_0[follower_goal_index, leader_msg_index] -= DELTA*discount\n",
    "\n",
    "        alignment_0_leader = []\n",
    "        for i in range(0, n_goals):\n",
    "            msg = np.argmax(agent_0[i])\n",
    "            j = np.argmax(agent_1[:, msg])\n",
    "            alignment_0_leader.append(i == j)\n",
    "\n",
    "        alignment_1_leader = []\n",
    "        for i in range(0, n_goals):\n",
    "            msg = np.argmax(agent_1[i])\n",
    "            j = np.argmax(agent_0[:, msg])\n",
    "            alignment_1_leader.append(i == j)\n",
    "\n",
    "        a0.append(sum(alignment_0_leader) / len(alignment_0_leader))\n",
    "        a1.append(sum(alignment_1_leader) / len(alignment_1_leader))\n",
    "    print(a0,a1)\n",
    "    a0_p.append(a0)\n",
    "    a1_p.append(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[3, 3])\n",
    "plt.plot(probas, np.mean(a0_p, 1), label=\"Agent 0\")\n",
    "plt.fill_between(\n",
    "    probas,\n",
    "    np.mean(a0_p, 1) + np.std(a0_p, 1),\n",
    "    np.mean(a0_p, 1) - np.std(a0_p, 1),\n",
    "    alpha=0.4,\n",
    ")\n",
    "\n",
    "plt.plot(probas, np.mean(a1_p, 1), label=\"Agent 1\")\n",
    "plt.fill_between(\n",
    "    probas,\n",
    "    np.mean(a1_p, 1) + np.std(a1_p, 1),\n",
    "    np.mean(a1_p, 1) - np.std(a1_p, 1),\n",
    "    alpha=0.4,\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Probability of solving aligned goals\")\n",
    "plt.ylabel(\"Alignment\")\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig(\"alignment_vs_p.png\", dpi=300, bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount=1.\n",
    "t=1.\n",
    "EGREEDY=0.1\n",
    "NUMBER_OF_LANDMARKS=15\n",
    "N_STEPS = 100\n",
    "n_goals=NUMBER_OF_LANDMARKS\n",
    "discount_b=1/n_goals\n",
    "\n",
    "alpha=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment for all seeds\n",
    "a0_p = []\n",
    "a1_p = []\n",
    "\n",
    "\n",
    "probas = np.arange(0.1, 1.1, 0.1)\n",
    "\n",
    "for p in probas:\n",
    "    a0 = []\n",
    "    a1 = []\n",
    "    scores = (np.eye(NUMBER_OF_LANDMARKS)*t+np.ones((NUMBER_OF_LANDMARKS,NUMBER_OF_LANDMARKS))*(1-t)) * p \n",
    "    \n",
    "    \n",
    "    for s in range(3):\n",
    "        np.random.seed(s)\n",
    "        agent_0 = np.zeros_like(scores)\n",
    "        agent_1 = np.zeros_like(scores)\n",
    "        change=True\n",
    "        for i in range(N_STEPS):\n",
    "            update_0=np.zeros_like(scores)\n",
    "            update_1=np.zeros_like(scores)\n",
    "            normalization_0=np.zeros_like(scores)\n",
    "            normalization_1=np.zeros_like(scores)\n",
    "            for _ in range(600):\n",
    "                leader_goal_index = np.random.randint(0, scores.shape[0])\n",
    "\n",
    "                \n",
    "                if np.random.random() < 0.5:\n",
    "                    # Agent 0 leader\n",
    "                    if np.random.random() < EGREEDY:\n",
    "                        leader_msg_index = np.random.choice(range(scores.shape[0]))\n",
    "                    else:\n",
    "                        leader_msg_index = np.argmax(agent_0[leader_goal_index])\n",
    "                    if np.random.random() < EGREEDY:\n",
    "                        follower_goal_index = np.random.choice(range(scores.shape[0]))\n",
    "                    else:\n",
    "                        follower_goal_index = np.argmax(agent_1[:, leader_msg_index])\n",
    "                    leader_reward = (\n",
    "                        np.random.random() < scores[leader_goal_index, follower_goal_index]\n",
    "                    )\n",
    "                    follower_reward = (\n",
    "                        np.random.random() < scores[follower_goal_index, leader_goal_index]\n",
    "                    )\n",
    "                    if leader_reward:\n",
    "                        update_0[leader_goal_index, leader_msg_index] += 1\n",
    "                    normalization_0[leader_goal_index, leader_msg_index]+=1\n",
    "                    if follower_reward:\n",
    "                        update_1[follower_goal_index, leader_msg_index] += 1\n",
    "                    normalization_1[follower_goal_index, leader_msg_index]+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "                    # Agent 1 leader\n",
    "                    if np.random.random() < EGREEDY:\n",
    "                        leader_msg_index = np.random.choice(range(scores.shape[0]))\n",
    "                    else:\n",
    "                        leader_msg_index = np.argmax(agent_1[leader_goal_index])\n",
    "                    if np.random.random() < EGREEDY:\n",
    "                        follower_goal_index = np.random.choice(range(scores.shape[0]))\n",
    "                    else:\n",
    "                        follower_goal_index = np.argmax(agent_0[:, leader_msg_index])\n",
    "                    leader_reward = (\n",
    "                        np.random.random() < scores[leader_goal_index, follower_goal_index]\n",
    "                    )\n",
    "                    follower_reward = (\n",
    "                        np.random.random() < scores[follower_goal_index, leader_goal_index]\n",
    "                    )\n",
    "                    if leader_reward:\n",
    "                        update_1[leader_goal_index, leader_msg_index] += 1\n",
    "                    normalization_1[leader_goal_index, leader_msg_index]+=1\n",
    "                    if follower_reward:\n",
    "                        update_0[follower_goal_index, leader_msg_index] += 1\n",
    "                    normalization_0[follower_goal_index, leader_msg_index]+=1\n",
    "            if(i%4==0):\n",
    "                change=not change\n",
    "                \n",
    "                \n",
    "            if(change):\n",
    "                agent_0=(1-alpha)*agent_0+alpha*update_0/(normalization_0+1e-10)\n",
    "            else:\n",
    "                agent_1=(1-alpha)*agent_1+alpha*update_1/(normalization_1+1e-10)\n",
    "            if(i%10==1000000):\n",
    "                plt.imshow(agent_0)\n",
    "                plt.show()\n",
    "\n",
    "            \n",
    "        alignment_0_leader = []\n",
    "        for i in range(0, n_goals):\n",
    "            msg = np.argmax(agent_0[i])\n",
    "            j = np.argmax(agent_1[:, msg])\n",
    "            alignment_0_leader.append(i == j)\n",
    "\n",
    "        alignment_1_leader = []\n",
    "        for i in range(0, n_goals):\n",
    "            msg = np.argmax(agent_1[i])\n",
    "            j = np.argmax(agent_0[:, msg])\n",
    "            alignment_1_leader.append(i == j)\n",
    "\n",
    "        a0.append(sum(alignment_0_leader) / len(alignment_0_leader))\n",
    "        a1.append(sum(alignment_1_leader) / len(alignment_1_leader))\n",
    "    print(a0,a1)\n",
    "    a0_p.append(a0)\n",
    "    a1_p.append(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[3, 3])\n",
    "plt.plot(probas, np.mean(a0_p, 1), label=\"Agent 0\")\n",
    "plt.fill_between(\n",
    "    probas,\n",
    "    np.mean(a0_p, 1) + np.std(a0_p, 1),\n",
    "    np.mean(a0_p, 1) - np.std(a0_p, 1),\n",
    "    alpha=0.4,\n",
    ")\n",
    "\n",
    "plt.plot(probas, np.mean(a1_p, 1), label=\"Agent 1\")\n",
    "plt.fill_between(\n",
    "    probas,\n",
    "    np.mean(a1_p, 1) + np.std(a1_p, 1),\n",
    "    np.mean(a1_p, 1) - np.std(a1_p, 1),\n",
    "    alpha=0.4,\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Probability of solving aligned goals\")\n",
    "plt.ylabel(\"Alignment\")\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig(\"alignment_vs_p.png\", dpi=300, bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount=1.\n",
    "t=0.75\n",
    "EGREEDY=0.1\n",
    "NUMBER_OF_LANDMARKS=15\n",
    "N_STEPS = 100\n",
    "n_goals=NUMBER_OF_LANDMARKS\n",
    "discount_b=1/n_goals\n",
    "\n",
    "alpha=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x,temperature=30):\n",
    "    x=x*temperature\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment for all seeds\n",
    "a0_p = []\n",
    "a1_p = []\n",
    "\n",
    "\n",
    "probas = np.arange(0.1, 1.1, 0.1)\n",
    "#probas=[0.4]\n",
    "\n",
    "for p in probas:\n",
    "    a0 = []\n",
    "    a1 = []\n",
    "    scores = (np.eye(NUMBER_OF_LANDMARKS)*t+np.ones((NUMBER_OF_LANDMARKS,NUMBER_OF_LANDMARKS))*(1-t)) * p \n",
    "    \n",
    "    \n",
    "    for s in range(3):\n",
    "        np.random.seed(s)\n",
    "        agent_0 = np.zeros_like(scores)\n",
    "        agent_1 = np.zeros_like(scores)\n",
    "        change=True\n",
    "        for i in range(N_STEPS):\n",
    "            update_0=np.zeros_like(scores)\n",
    "            update_1=np.zeros_like(scores)\n",
    "            normalization_0=np.zeros_like(scores)\n",
    "            normalization_1=np.zeros_like(scores)\n",
    "            update_0_b=np.zeros_like(scores)\n",
    "            update_1_b=np.zeros_like(scores)\n",
    "            normalization_0_b=np.zeros_like(scores)\n",
    "            normalization_1_b=np.zeros_like(scores)\n",
    "            for _ in range(600):\n",
    "                leader_goal_index = np.random.randint(0, scores.shape[0])\n",
    "\n",
    "                \n",
    "                if np.random.random() < 0.5:\n",
    "                    # Agent 0 leader\n",
    "                    \n",
    "                    leader_msg_index = np.random.choice(range(scores.shape[0]),p=softmax(agent_0[leader_goal_index]))\n",
    "                    follower_goal_index = np.random.choice(range(scores.shape[0]),p=softmax(agent_1[:, leader_msg_index]))\n",
    "                    #if(leader_goal_index==0):\n",
    "                    #    print(softmax(agent_0[leader_goal_index]))\n",
    "                    leader_reward = (\n",
    "                        np.random.random() < scores[leader_goal_index, follower_goal_index]\n",
    "                    )\n",
    "                    follower_reward = (\n",
    "                        np.random.random() < scores[follower_goal_index, leader_goal_index]\n",
    "                    )\n",
    "                    if leader_reward:\n",
    "                        update_0[leader_goal_index, leader_msg_index] += 1\n",
    "                    \n",
    "                    normalization_0[leader_goal_index, leader_msg_index]+=1\n",
    "                    if follower_reward:\n",
    "                        update_1_b[follower_goal_index, leader_msg_index] += 1\n",
    "                    normalization_1_b[follower_goal_index, leader_msg_index]+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "                    # Agent 1 leader\n",
    "                    leader_msg_index = np.random.choice(range(scores.shape[0]),p=softmax(agent_1[leader_goal_index]))\n",
    "                    follower_goal_index = np.random.choice(range(scores.shape[0]),p=softmax(agent_0[:, leader_msg_index]))\n",
    "                    \n",
    "                    leader_reward = (\n",
    "                        np.random.random() < scores[leader_goal_index, follower_goal_index]\n",
    "                    )\n",
    "                    follower_reward = (\n",
    "                        np.random.random() < scores[follower_goal_index, leader_goal_index]\n",
    "                    )\n",
    "                    if leader_reward:\n",
    "                        update_1[leader_goal_index, leader_msg_index] += 1\n",
    "                    normalization_1[leader_goal_index, leader_msg_index]+=1\n",
    "                    if follower_reward:\n",
    "                        update_0_b[follower_goal_index, leader_msg_index] += 1\n",
    "                    normalization_0_b[follower_goal_index, leader_msg_index]+=1\n",
    "            #if(i%4==0):\n",
    "            #    change=not change\n",
    "                \n",
    "                \n",
    "            #if(change):\n",
    "            agent_0=(1-alpha)*agent_0+alpha*(update_0/(normalization_0+1e-10)+update_0_b/(normalization_0_b+1e-10))/2\n",
    "            #else:\n",
    "            agent_1=(1-alpha)*agent_1+alpha*(update_1/(normalization_1+1e-10)+update_1_b/(normalization_1_b+1e-10))/2\n",
    "            #if(i%10==0):\n",
    "            #    print(\"a\")\n",
    "            #    plt.imshow(agent_0)\n",
    "            #    plt.show()\n",
    "            #    plt.imshow(agent_1)\n",
    "            #    plt.show()\n",
    "\n",
    "        plt.imshow(agent_0)\n",
    "        plt.show()\n",
    "        plt.imshow(agent_1)\n",
    "        plt.show()\n",
    "        alignment_0_leader = []\n",
    "        for i in range(0, n_goals):\n",
    "            msg = np.argmax(agent_0[i])\n",
    "            j = np.argmax(agent_1[:, msg])\n",
    "            alignment_0_leader.append(i == j)\n",
    "\n",
    "        alignment_1_leader = []\n",
    "        for i in range(0, n_goals):\n",
    "            msg = np.argmax(agent_1[i])\n",
    "            j = np.argmax(agent_0[:, msg])\n",
    "            alignment_1_leader.append(i == j)\n",
    "\n",
    "        a0.append(sum(alignment_0_leader) / len(alignment_0_leader))\n",
    "        a1.append(sum(alignment_1_leader) / len(alignment_1_leader))\n",
    "        print(\"aaaaaa\")\n",
    "        print(p)\n",
    "        plt.imshow(agent_0)\n",
    "        plt.show()\n",
    "        plt.imshow(agent_1)\n",
    "        plt.show()\n",
    "        print(sum(alignment_0_leader) / len(alignment_0_leader),sum(alignment_1_leader) / len(alignment_1_leader))\n",
    "        \n",
    "    print(a0,a1)\n",
    "    a0_p.append(a0)\n",
    "    a1_p.append(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[3, 3])\n",
    "plt.plot(probas, np.mean(a0_p, 1), label=\"Agent 0\")\n",
    "plt.fill_between(\n",
    "    probas,\n",
    "    np.mean(a0_p, 1) + np.std(a0_p, 1),\n",
    "    np.mean(a0_p, 1) - np.std(a0_p, 1),\n",
    "    alpha=0.4,\n",
    ")\n",
    "\n",
    "plt.plot(probas, np.mean(a1_p, 1), label=\"Agent 1\")\n",
    "plt.fill_between(\n",
    "    probas,\n",
    "    np.mean(a1_p, 1) + np.std(a1_p, 1),\n",
    "    np.mean(a1_p, 1) - np.std(a1_p, 1),\n",
    "    alpha=0.4,\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Probability of solving aligned goals\")\n",
    "plt.ylabel(\"Alignment\")\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig(\"alignment_vs_p.png\", dpi=300, bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount=1.\n",
    "t=0.75\n",
    "EGREEDY=0.1\n",
    "NUMBER_OF_LANDMARKS=15\n",
    "N_STEPS = 250\n",
    "n_goals=NUMBER_OF_LANDMARKS\n",
    "discount_b=1/n_goals\n",
    "temperature=30\n",
    "alpha=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temperature in np.linspace(1,41,5):\n",
    "    print(\"aaaaaaaaa\")\n",
    "    print(temperature)\n",
    "    # Alignment for all seeds\n",
    "    a0_p = []\n",
    "    a1_p = []\n",
    "\n",
    "\n",
    "    probas = np.arange(0.1, 1.1, 0.1)\n",
    "\n",
    "    for p in probas:\n",
    "        a0 = []\n",
    "        a1 = []\n",
    "        scores = (np.eye(NUMBER_OF_LANDMARKS)*t+np.ones((NUMBER_OF_LANDMARKS,NUMBER_OF_LANDMARKS))*(1-t)) * p \n",
    "\n",
    "\n",
    "        for s in range(3):\n",
    "            np.random.seed(s)\n",
    "            agent_0 = np.zeros_like(scores)\n",
    "            agent_1 = np.zeros_like(scores)\n",
    "            change=True\n",
    "            for i in range(N_STEPS):\n",
    "                update_0=np.zeros_like(scores)\n",
    "                update_1=np.zeros_like(scores)\n",
    "                normalization_0=np.zeros_like(scores)\n",
    "                normalization_1=np.zeros_like(scores)\n",
    "                for _ in range(600):\n",
    "                    leader_goal_index = np.random.randint(0, scores.shape[0])\n",
    "\n",
    "\n",
    "                    if np.random.random() < 0.5:\n",
    "                        # Agent 0 leader\n",
    "\n",
    "                        leader_msg_index = np.random.choice(range(scores.shape[0]),p=softmax(agent_0[leader_goal_index],temperature=temperature))\n",
    "                        follower_goal_index = np.random.choice(range(scores.shape[0]),p=softmax(agent_1[:, leader_msg_index],temperature=temperature))\n",
    "                        #if(leader_goal_index==0):\n",
    "                        #    print(softmax(agent_0[leader_goal_index]))\n",
    "                        leader_reward = (\n",
    "                            np.random.random() < scores[leader_goal_index, follower_goal_index]\n",
    "                        )\n",
    "                        follower_reward = (\n",
    "                            np.random.random() < scores[follower_goal_index, leader_goal_index]\n",
    "                        )\n",
    "                        if leader_reward:\n",
    "                            update_0[leader_goal_index, leader_msg_index] += 1\n",
    "                        normalization_0[leader_goal_index, leader_msg_index]+=1\n",
    "                        if follower_reward:\n",
    "                            update_1[follower_goal_index, leader_msg_index] += 1\n",
    "                        normalization_1[follower_goal_index, leader_msg_index]+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        # Agent 1 leader\n",
    "                        leader_msg_index = np.random.choice(range(scores.shape[0]),p=softmax(agent_1[leader_goal_index],temperature=temperature))\n",
    "                        follower_goal_index = np.random.choice(range(scores.shape[0]),p=softmax(agent_0[:, leader_msg_index],temperature=temperature))\n",
    "\n",
    "                        leader_reward = (\n",
    "                            np.random.random() < scores[leader_goal_index, follower_goal_index]\n",
    "                        )\n",
    "                        follower_reward = (\n",
    "                            np.random.random() < scores[follower_goal_index, leader_goal_index]\n",
    "                        )\n",
    "                        if leader_reward:\n",
    "                            update_1[leader_goal_index, leader_msg_index] += 1\n",
    "                        normalization_1[leader_goal_index, leader_msg_index]+=1\n",
    "                        if follower_reward:\n",
    "                            update_0[follower_goal_index, leader_msg_index] += 1\n",
    "                        normalization_0[follower_goal_index, leader_msg_index]+=1\n",
    "                if(i%4==0):\n",
    "                    change=not change\n",
    "\n",
    "\n",
    "                if(change):\n",
    "                    agent_0=(1-alpha)*agent_0+alpha*update_0/(normalization_0+1e-10)\n",
    "                else:\n",
    "                    agent_1=(1-alpha)*agent_1+alpha*update_1/(normalization_1+1e-10)\n",
    "                if(i%10==1000000):\n",
    "                    plt.imshow(agent_0)\n",
    "                    plt.show()\n",
    "\n",
    "\n",
    "            alignment_0_leader = []\n",
    "            for i in range(0, n_goals):\n",
    "                msg = np.argmax(agent_0[i])\n",
    "                j = np.argmax(agent_1[:, msg])\n",
    "                alignment_0_leader.append(i == j)\n",
    "\n",
    "            alignment_1_leader = []\n",
    "            for i in range(0, n_goals):\n",
    "                msg = np.argmax(agent_1[i])\n",
    "                j = np.argmax(agent_0[:, msg])\n",
    "                alignment_1_leader.append(i == j)\n",
    "\n",
    "            a0.append(sum(alignment_0_leader) / len(alignment_0_leader))\n",
    "            a1.append(sum(alignment_1_leader) / len(alignment_1_leader))\n",
    "        print(a0,a1)\n",
    "        a0_p.append(a0)\n",
    "        a1_p.append(a1)\n",
    "    plt.figure(figsize=[3, 3])\n",
    "    plt.plot(probas, np.mean(a0_p, 1), label=\"Agent 0\")\n",
    "    plt.fill_between(\n",
    "        probas,\n",
    "        np.mean(a0_p, 1) + np.std(a0_p, 1),\n",
    "        np.mean(a0_p, 1) - np.std(a0_p, 1),\n",
    "        alpha=0.4,\n",
    "    )\n",
    "\n",
    "    plt.plot(probas, np.mean(a1_p, 1), label=\"Agent 1\")\n",
    "    plt.fill_between(\n",
    "        probas,\n",
    "        np.mean(a1_p, 1) + np.std(a1_p, 1),\n",
    "        np.mean(a1_p, 1) - np.std(a1_p, 1),\n",
    "        alpha=0.4,\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Probability of solving aligned goals\")\n",
    "    plt.ylabel(\"Alignment\")\n",
    "    plt.grid()\n",
    "\n",
    "    plt.savefig(\"alignment_vs_p.png\", dpi=300, bbox_inches=\"tight\", transparent=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two matrices per agent\n",
    "\n",
    "One matrix is used when the agent is leader and a different one is used when the agent is a follower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment for all seeds\n",
    "a0 = []\n",
    "a1 = []\n",
    "\n",
    "for s in range(10):\n",
    "    np.random.seed(s)\n",
    "    agent_0 = np.zeros_like(scores)\n",
    "    agent_1 = np.zeros_like(scores)\n",
    "    agent_0f = np.zeros_like(scores)\n",
    "    agent_1f = np.zeros_like(scores)\n",
    "\n",
    "    for i in range(N_STEPS):\n",
    "        leader_goal_index = np.random.randint(0, n_goals)\n",
    "        if np.random.random() < 0.5:\n",
    "            # Agent 0 leader\n",
    "            if np.random.random() < EGREEDY:\n",
    "                leader_msg_index = np.random.choice(range(scores.shape[0]))\n",
    "            else:\n",
    "                leader_msg_index = np.argmax(agent_0[leader_goal_index])\n",
    "            if np.random.random() < EGREEDY:\n",
    "                follower_goal_index = np.random.choice(range(scores.shape[0]))\n",
    "            else:\n",
    "                follower_goal_index = np.argmax(agent_1f[leader_msg_index])\n",
    "            leader_reward = (\n",
    "                np.random.random() < scores0[leader_goal_index, follower_goal_index]\n",
    "            )\n",
    "            follower_reward = (\n",
    "                np.random.random() < scores1[follower_goal_index, leader_goal_index]\n",
    "            )\n",
    "            if leader_reward:\n",
    "                agent_0[leader_goal_index, :] -= DELTA\n",
    "                agent_0[leader_goal_index, leader_msg_index] += (\n",
    "                    leader_reward + 1\n",
    "                ) * DELTA\n",
    "            else:\n",
    "                agent_0[leader_goal_index, leader_msg_index] -= DELTA\n",
    "            if follower_reward:\n",
    "                agent_1f[leader_msg_index] -= DELTA\n",
    "                agent_1f[leader_msg_index, follower_goal_index] += (\n",
    "                    follower_reward + 1\n",
    "                ) * DELTA\n",
    "            else:\n",
    "                agent_1f[leader_msg_index, follower_goal_index] -= DELTA\n",
    "\n",
    "        else:\n",
    "            # Agent 1 leader\n",
    "            if np.random.random() < EGREEDY:\n",
    "                leader_msg_index = np.random.choice(range(scores.shape[0]))\n",
    "            else:\n",
    "                leader_msg_index = np.argmax(agent_1[leader_goal_index])\n",
    "            if np.random.random() < EGREEDY:\n",
    "                follower_goal_index = np.random.choice(range(scores.shape[0]))\n",
    "            else:\n",
    "                follower_goal_index = np.argmax(agent_0f[leader_msg_index])\n",
    "            leader_reward = (\n",
    "                np.random.random() < scores1[leader_goal_index, follower_goal_index]\n",
    "            )\n",
    "            follower_reward = (\n",
    "                np.random.random() < scores0[follower_goal_index, leader_goal_index]\n",
    "            )\n",
    "            if leader_reward:\n",
    "                agent_1[leader_goal_index, :] -= DELTA\n",
    "                agent_1[leader_goal_index, leader_msg_index] += (\n",
    "                    leader_reward + 1\n",
    "                ) * DELTA\n",
    "            else:\n",
    "                agent_1[leader_goal_index, leader_msg_index] -= DELTA\n",
    "            if follower_reward:\n",
    "                agent_0f[leader_msg_index] -= DELTA\n",
    "                agent_0f[leader_msg_index, follower_goal_index] += (\n",
    "                    follower_reward + 1\n",
    "                ) * DELTA\n",
    "            else:\n",
    "                agent_0f[leader_msg_index, follower_goal_index] -= DELTA\n",
    "\n",
    "    alignment_0_leader = []\n",
    "    for i in range(0, n_goals):\n",
    "        msg = np.argmax(agent_0[i])\n",
    "        j = np.argmax(agent_1f[msg])\n",
    "        alignment_0_leader.append(i == j)\n",
    "\n",
    "    alignment_1_leader = []\n",
    "    for i in range(0, n_goals):\n",
    "        msg = np.argmax(agent_1[i])\n",
    "        j = np.argmax(agent_0f[msg])\n",
    "        alignment_1_leader.append(i == j)\n",
    "\n",
    "    a0.append(sum(alignment_0_leader) / len(alignment_0_leader))\n",
    "    a1.append(sum(alignment_1_leader) / len(alignment_1_leader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Alignment for agent 0: {np.mean(a0)}\")\n",
    "print(f\"Alignment for agent 1: {np.mean(a1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# separate follower and leader update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount=1.\n",
    "t=0.5\n",
    "EGREEDY=0.1\n",
    "NUMBER_OF_LANDMARKS=15\n",
    "N_STEPS = 1000\n",
    "n_goals=NUMBER_OF_LANDMARKS\n",
    "discount_b=1/n_goals\n",
    "\n",
    "alpha=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment for all seeds\n",
    "a0_p = []\n",
    "a1_p = []\n",
    "\n",
    "\n",
    "probas = np.arange(0.1, 1.1, 0.1)\n",
    "probas=[0.7]\n",
    "\n",
    "for p in probas:\n",
    "    a0 = []\n",
    "    a1 = []\n",
    "    scores = (np.eye(NUMBER_OF_LANDMARKS)*t+np.ones((NUMBER_OF_LANDMARKS,NUMBER_OF_LANDMARKS))*(1-t)) * p \n",
    "    \n",
    "    \n",
    "    \n",
    "    for s in range(3):\n",
    "        np.random.seed(s)\n",
    "        agent_0 = np.zeros_like(scores)\n",
    "        agent_1 = np.zeros_like(scores)\n",
    "        change=True\n",
    "        \n",
    "        vid_agents=np.zeros((N_STEPS,scores.shape[0]*3+10,2*scores.shape[1]+5,3))\n",
    "        for i in range(N_STEPS):\n",
    "            update_0=np.zeros_like(scores)\n",
    "            update_1=np.zeros_like(scores)\n",
    "            normalization_0=np.zeros_like(scores)\n",
    "            normalization_1=np.zeros_like(scores)\n",
    "            update_0_b=np.zeros_like(scores)\n",
    "            update_1_b=np.zeros_like(scores)\n",
    "            normalization_0_b=np.zeros_like(scores)\n",
    "            normalization_1_b=np.zeros_like(scores)\n",
    "            \n",
    "            \n",
    "            vid_agents[i,:scores.shape[0],:scores.shape[1],0]=agent_0\n",
    "            vid_agents[i,:scores.shape[0],scores.shape[1]+5:,0]=agent_1\n",
    "            for _ in range(600):\n",
    "                leader_goal_index = np.random.randint(0, scores.shape[0])\n",
    "\n",
    "                \n",
    "                if np.random.random() < 0.5:\n",
    "                    # Agent 0 leader\n",
    "                    \n",
    "                    leader_msg_index = np.random.choice(range(scores.shape[0]),p=softmax(agent_0[leader_goal_index]))\n",
    "                    follower_goal_index = np.random.choice(range(scores.shape[0]),p=softmax(agent_1[:, leader_msg_index]))\n",
    "                    #if(leader_goal_index==0):\n",
    "                    #    print(softmax(agent_0[leader_goal_index]))\n",
    "                    leader_reward = (\n",
    "                        np.random.random() < scores[leader_goal_index, follower_goal_index]\n",
    "                    )\n",
    "                    follower_reward = (\n",
    "                        np.random.random() < scores[follower_goal_index, leader_goal_index]\n",
    "                    )\n",
    "                    if leader_reward:\n",
    "                        update_0[leader_goal_index, leader_msg_index] += 1\n",
    "                    \n",
    "                    normalization_0[leader_goal_index, leader_msg_index]+=1\n",
    "                    if follower_reward:\n",
    "                        update_1_b[follower_goal_index, leader_msg_index] += 1\n",
    "                    normalization_1_b[follower_goal_index, leader_msg_index]+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "                    # Agent 1 leader\n",
    "                    leader_msg_index = np.random.choice(range(scores.shape[0]),p=softmax(agent_1[leader_goal_index]))\n",
    "                    follower_goal_index = np.random.choice(range(scores.shape[0]),p=softmax(agent_0[:, leader_msg_index]))\n",
    "                    \n",
    "                    leader_reward = (\n",
    "                        np.random.random() < scores[leader_goal_index, follower_goal_index]\n",
    "                    )\n",
    "                    follower_reward = (\n",
    "                        np.random.random() < scores[follower_goal_index, leader_goal_index]\n",
    "                    )\n",
    "                    if leader_reward:\n",
    "                        update_1[leader_goal_index, leader_msg_index] += 1\n",
    "                    normalization_1[leader_goal_index, leader_msg_index]+=1\n",
    "                    if follower_reward:\n",
    "                        update_0_b[follower_goal_index, leader_msg_index] += 1\n",
    "                    normalization_0_b[follower_goal_index, leader_msg_index]+=1\n",
    "            #if(i%4==0):\n",
    "            #    change=not change\n",
    "                \n",
    "                \n",
    "            #if(change):\n",
    "            agent_0=(1-alpha)*agent_0+alpha*(update_0/(normalization_0+1e-10)+update_0_b/(normalization_0_b+1e-10))/2\n",
    "            #else:\n",
    "            agent_1=(1-alpha)*agent_1+alpha*(update_1/(normalization_1+1e-10)+update_1_b/(normalization_1_b+1e-10))/2\n",
    "            \n",
    "            \n",
    "            vid_agents[i,scores.shape[0]+5:scores.shape[0]*2+5,:scores.shape[1],1]=update_0/(normalization_0+1e-10)\n",
    "            vid_agents[i,scores.shape[0]+5:scores.shape[0]*2+5,scores.shape[1]+5:,1]=update_1/(normalization_1+1e-10)\n",
    "            \n",
    "            vid_agents[i,scores.shape[0]*2+10:,:scores.shape[1],2]=update_0_b/(normalization_0_b+1e-10)\n",
    "            vid_agents[i,scores.shape[0]*2+10:,scores.shape[1]+5:,2]=update_1_b/(normalization_1_b+1e-10)\n",
    "            \n",
    "            #if(i%10==0):\n",
    "            #    print(\"a\")\n",
    "            #    plt.imshow(agent_0)\n",
    "            #    plt.show()\n",
    "            #    plt.imshow(agent_1)\n",
    "            #    plt.show()\n",
    "\n",
    "        plt.imshow(agent_0)\n",
    "        plt.show()\n",
    "        plt.imshow(agent_1)\n",
    "        plt.show()\n",
    "        alignment_0_leader = []\n",
    "        for i in range(0, n_goals):\n",
    "            msg = np.argmax(agent_0[i])\n",
    "            j = np.argmax(agent_1[:, msg])\n",
    "            alignment_0_leader.append(i == j)\n",
    "\n",
    "        alignment_1_leader = []\n",
    "        for i in range(0, n_goals):\n",
    "            msg = np.argmax(agent_1[i])\n",
    "            j = np.argmax(agent_0[:, msg])\n",
    "            alignment_1_leader.append(i == j)\n",
    "\n",
    "        a0.append(sum(alignment_0_leader) / len(alignment_0_leader))\n",
    "        a1.append(sum(alignment_1_leader) / len(alignment_1_leader))\n",
    "        print(\"aaaaaa\")\n",
    "        print(p)\n",
    "        plt.imshow(agent_0)\n",
    "        plt.show()\n",
    "        plt.imshow(agent_1)\n",
    "        plt.show()\n",
    "        \n",
    "        vid_agents=np.repeat(vid_agents,4,axis=1)\n",
    "        vid_agents=np.repeat(vid_agents,4,axis=2)\n",
    "        with VideoWriter(\"out.mp4\", 20) as vid:\n",
    "          for timestep in range(N_STEPS):\n",
    "            im = vid_agents[timestep]\n",
    "            \n",
    "            #plt.imshow(im)\n",
    "            #plt.show()\n",
    "            vid.add(im)\n",
    "          vid.show()\n",
    "        print(sum(alignment_0_leader) / len(alignment_0_leader),sum(alignment_1_leader) / len(alignment_1_leader))\n",
    "        \n",
    "    print(a0,a1)\n",
    "    a0_p.append(a0)\n",
    "    a1_p.append(a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['FFMPEG_BINARY'] = 'ffmpeg'\n",
    "import moviepy.editor as mvp\n",
    "from moviepy.video.io.ffmpeg_writer import FFMPEG_VideoWriter\n",
    "from IPython.display import HTML, display, clear_output\n",
    "\n",
    "class VideoWriter:\n",
    "  def __init__(self, filename, fps=30.0, **kw):\n",
    "    self.writer = None\n",
    "    self.params = dict(filename=filename, fps=fps, **kw)\n",
    "\n",
    "  def add(self, img):\n",
    "    img = np.asarray(img)\n",
    "    if self.writer is None:\n",
    "      h, w = img.shape[:2]\n",
    "      self.writer = FFMPEG_VideoWriter(size=(w, h), **self.params)\n",
    "    if img.dtype in [np.float32, np.float64]:\n",
    "      img = np.uint8(img.clip(0, 1)*255)\n",
    "    if len(img.shape) == 2:\n",
    "      img = np.repeat(img[..., None], 3, -1)\n",
    "    self.writer.write_frame(img)\n",
    "\n",
    "  def close(self):\n",
    "    if self.writer:\n",
    "      self.writer.close()\n",
    "\n",
    "  def __enter__(self):\n",
    "    return self\n",
    "\n",
    "  def __exit__(self, *kw):\n",
    "    self.close()\n",
    "\n",
    "  def show(self, **kw):\n",
    "      self.close()\n",
    "      fn = self.params['filename']\n",
    "      display(mvp.ipython_display(fn, **kw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount=1.\n",
    "t=0.7\n",
    "EGREEDY=0.01\n",
    "NUMBER_OF_LANDMARKS=15\n",
    "N_STEPS = 400\n",
    "n_goals=NUMBER_OF_LANDMARKS\n",
    "discount_b=1/n_goals\n",
    "temperature=30\n",
    "alpha=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x,temperature=30):\n",
    "    x=x*temperature\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.zeros(15)\n",
    "a[0]=0.4\n",
    "a[1]=0.2\n",
    "print(softmax(a,30))\n",
    "print(softmax(a,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment for all seeds\n",
    "a0_p = []\n",
    "a1_p = []\n",
    "\n",
    "\n",
    "probas = np.arange(0.1, 1.1, 0.1)\n",
    "probas=[0.6]\n",
    "\n",
    "for p in probas:\n",
    "    a0 = []\n",
    "    a1 = []\n",
    "    scores = (np.eye(NUMBER_OF_LANDMARKS)*t+np.ones((NUMBER_OF_LANDMARKS,NUMBER_OF_LANDMARKS))*(1-t)) * p \n",
    "    \n",
    "    alpha_b=0.1\n",
    "    for s in range(3):\n",
    "        p2=p\n",
    "        np.random.seed(s)\n",
    "        agent_0 = np.zeros((scores.shape[0],20))\n",
    "        agent_1 = np.zeros((scores.shape[0],20))\n",
    "       \n",
    "        \n",
    "        vid_agents=np.zeros((N_STEPS,agent_0.shape[0],2*agent_0.shape[1]+5))\n",
    "\n",
    "        change=True\n",
    "        \n",
    "        alignement0_list=[]\n",
    "        alignement1_list=[]\n",
    "        for i in range(N_STEPS):\n",
    "            t=i/(N_STEPS-100)\n",
    "            t=np.clip(t,0,1)\n",
    "            #p2=1*(t)+(1-t)*p\n",
    "            p2=p\n",
    "            if(i<200):\n",
    "                alpha=0.6\n",
    "            else:\n",
    "                alpha=alpha_b\n",
    "                \n",
    "            \n",
    "            \n",
    "            scores = (np.eye(NUMBER_OF_LANDMARKS)*t+np.ones((NUMBER_OF_LANDMARKS,NUMBER_OF_LANDMARKS))*(1-t)) * p2\n",
    "            update_0=np.zeros_like(agent_0)\n",
    "            update_1=np.zeros_like(agent_1)\n",
    "            normalization_0=np.zeros_like(agent_0)\n",
    "            normalization_1=np.zeros_like(agent_1)\n",
    "            \n",
    "            \n",
    "            vid_agents[i,:,:agent_0.shape[1]]=agent_0\n",
    "            vid_agents[i,:,agent_0.shape[1]+5:]=agent_1\n",
    "  \n",
    "            for _ in range(600):\n",
    "                leader_goal_index = np.random.randint(0, scores.shape[0])\n",
    "\n",
    "                \n",
    "                if np.random.random() < 0.5:\n",
    "                    # Agent 0 leader\n",
    "\n",
    "                    \n",
    "                    leader_msg_index = np.random.choice(range(agent_0.shape[1]),p=softmax(agent_0[leader_goal_index],temperature))\n",
    "                    follower_goal_index = np.random.choice(range(scores.shape[0]),p=softmax(agent_1[:, leader_msg_index],temperature))\n",
    "                    ##if(leader_goal_index==0):\n",
    "                    ##    print(softmax(agent_0[leader_goal_index]))\n",
    "                    leader_reward = (\n",
    "                        np.random.random() < scores[leader_goal_index, follower_goal_index]\n",
    "                    )\n",
    "                    follower_reward = (\n",
    "                        np.random.random() < scores[follower_goal_index, leader_goal_index]\n",
    "                    )\n",
    "                    if leader_reward:\n",
    "                        update_0[leader_goal_index, leader_msg_index] += 1\n",
    "                    \n",
    "                    normalization_0[leader_goal_index, leader_msg_index]+=1\n",
    "                    if follower_reward:\n",
    "                        update_1[follower_goal_index, leader_msg_index] += 1\n",
    "                    normalization_1[follower_goal_index, leader_msg_index]+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "                    # Agent 1 leader\n",
    "                    \n",
    "                    leader_msg_index = np.random.choice(range(agent_1.shape[1]),p=softmax(agent_1[leader_goal_index],temperature))\n",
    "                    follower_goal_index = np.random.choice(range(scores.shape[0]),p=softmax(agent_0[:, leader_msg_index],temperature))\n",
    "                    \n",
    "                    leader_reward = (\n",
    "                        np.random.random() < scores[leader_goal_index, follower_goal_index]\n",
    "                    )\n",
    "                    follower_reward = (\n",
    "                        np.random.random() < scores[follower_goal_index, leader_goal_index]\n",
    "                    )\n",
    "                    if leader_reward:\n",
    "                        update_1[leader_goal_index, leader_msg_index] += 1\n",
    "                    normalization_1[leader_goal_index, leader_msg_index]+=1\n",
    "                    if follower_reward:\n",
    "                        update_0[follower_goal_index, leader_msg_index] += 1\n",
    "                    normalization_0[follower_goal_index, leader_msg_index]+=1\n",
    "            #if(i%4==0):\n",
    "            #    change=not change\n",
    "                \n",
    "                \n",
    "            #if(change):\n",
    "            agent_0=(1-alpha)*agent_0+alpha*(update_0/(normalization_0+1e-10))\n",
    "            #else:\n",
    "            agent_1=(1-alpha)*agent_1+alpha*(update_1/(normalization_1+1e-10))\n",
    "            #if(i%10==0):\n",
    "            #    print(\"a\")\n",
    "            #    plt.imshow(agent_0)\n",
    "            #    plt.show()\n",
    "            #    plt.imshow(agent_1)\n",
    "            #    plt.show()\n",
    "            \n",
    "            \n",
    "            alignment_0_leader=[]\n",
    "            for i in range(0, n_goals):\n",
    "                msg = np.argmax(agent_0[i])\n",
    "                j = np.argmax(agent_1[:, msg])\n",
    "                alignment_0_leader.append(i == j)\n",
    "\n",
    "\n",
    "            alignment_1_leader = []\n",
    "            for i in range(0, n_goals):\n",
    "                msg = np.argmax(agent_1[i])\n",
    "                j = np.argmax(agent_0[:, msg])\n",
    "                alignment_1_leader.append(i == j)\n",
    "                \n",
    "            alignement0_list.append(sum(alignment_0_leader) / len(alignment_0_leader))\n",
    "            alignement1_list.append(sum(alignment_1_leader) / len(alignment_1_leader))\n",
    "            \n",
    "            \n",
    "\n",
    "        alignment_0_leader=[]\n",
    "        for i in range(0, n_goals):\n",
    "            msg = np.argmax(agent_0[i])\n",
    "            j = np.argmax(agent_1[:, msg])\n",
    "            alignment_0_leader.append(i == j)\n",
    "\n",
    "\n",
    "        alignment_1_leader = []\n",
    "        for i in range(0, n_goals):\n",
    "            msg = np.argmax(agent_1[i])\n",
    "            j = np.argmax(agent_0[:, msg])\n",
    "            alignment_1_leader.append(i == j)\n",
    "            \n",
    "        plt.plot(alignement0_list)\n",
    "        plt.show()\n",
    "        plt.plot(alignement1_list)\n",
    "        plt.show()\n",
    "\n",
    "      \n",
    "        a0.append(sum(alignment_0_leader) / len(alignment_0_leader))\n",
    "        a1.append(sum(alignment_1_leader) / len(alignment_1_leader))\n",
    "        print(\"aaaaaa\")\n",
    "        print(p2)\n",
    "        plt.imshow(agent_0)\n",
    "        plt.show()\n",
    "        plt.imshow(agent_1)\n",
    "        plt.show()\n",
    "        \n",
    "        vid_agents=np.repeat(vid_agents,10,axis=1)\n",
    "        vid_agents=np.repeat(vid_agents,10,axis=2)\n",
    "        with VideoWriter(\"out.mp4\", 20) as vid:\n",
    "          for timestep in range(N_STEPS):\n",
    "            im = vid_agents[timestep]\n",
    "            \n",
    "            #plt.imshow(im)\n",
    "            #plt.show()\n",
    "            vid.add(im)\n",
    "          vid.show()\n",
    "        print(sum(alignment_0_leader) / len(alignment_0_leader),sum(alignment_1_leader) / len(alignment_1_leader))\n",
    "        \n",
    "        \n",
    "        \n",
    "    print(a0,a1)\n",
    "    a0_p.append(a0)\n",
    "    a1_p.append(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same with base params\n",
    "\n",
    "with high prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p selecting goal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x,temperature=30,axis=0):\n",
    "    x=x*temperature\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x,axis=axis,keepdims=True))\n",
    "\n",
    "    return e_x / e_x.sum(axis=axis,keepdims=True)\n",
    "def entropy_softmax(a,temperature):\n",
    "    p=softmax(a,temperature,axis=1)\n",
    "    return(-(p*np.log(p+1e-10)).sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=agent_0*1.\n",
    "agent=np.zeros((15,15))\n",
    "agent[0,0]=0.5\n",
    "plt.imshow(agent)\n",
    "plt.show()\n",
    "plt.imshow(softmax(agent,30,1))\n",
    "plt.show()\n",
    "print(softmax(agent,10,1)[0])\n",
    "\n",
    "ent=entropy_softmax(agent,temperature)\n",
    "print(ent)\n",
    "\n",
    "temperature2=2\n",
    "p=softmax(ent,temperature2)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount=1.\n",
    "t=0.7\n",
    "EGREEDY=0.1\n",
    "NUMBER_OF_LANDMARKS=15\n",
    "N_STEPS = 500\n",
    "n_goals=NUMBER_OF_LANDMARKS\n",
    "discount_b=1/n_goals\n",
    "temperature=20\n",
    "alpha=0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment for all seeds\n",
    "a0_p = []\n",
    "a1_p = []\n",
    "\n",
    "\n",
    "probas = np.arange(0.1, 1.1, 0.1)\n",
    "probas=[0.7]\n",
    "\n",
    "for p in probas:\n",
    "    a0 = []\n",
    "    a1 = []\n",
    "    scores = (np.eye(NUMBER_OF_LANDMARKS)*t+np.ones((NUMBER_OF_LANDMARKS,NUMBER_OF_LANDMARKS))*(1-t)) * p \n",
    "    \n",
    "    \n",
    "    for s in range(3):\n",
    "        np.random.seed(s)\n",
    "        agent_0 = np.zeros_like(scores)+1e-10\n",
    "        agent_1 = np.zeros_like(scores)+1e-10\n",
    "        \n",
    "        vid_agents=np.zeros((N_STEPS,scores.shape[0],2*scores.shape[1]+5))\n",
    "\n",
    "        change=True\n",
    "        for i in range(N_STEPS):\n",
    "            update_0=np.zeros_like(scores)\n",
    "            update_1=np.zeros_like(scores)\n",
    "            normalization_0=np.zeros_like(scores)\n",
    "            normalization_1=np.zeros_like(scores)\n",
    "            \n",
    "            \n",
    "            vid_agents[i,:,:scores.shape[1]]=agent_0\n",
    "            vid_agents[i,:,scores.shape[1]+5:]=agent_1\n",
    "  \n",
    "            for _ in range(600):\n",
    "                \n",
    "\n",
    "                \n",
    "                if np.random.random() < 0.5:\n",
    "                    # Agent 0 leader\n",
    "                    \n",
    "                    ######## sample of the goal with high uncertainty\n",
    "                    leader_goal_index = np.random.choice(range(scores.shape[0]),p=softmax(entropy_softmax(agent_0,temperature),2))\n",
    "                    \n",
    "                    \n",
    "        \n",
    "                    \n",
    "                    leader_msg_index = np.random.choice(range(scores.shape[0]),p=softmax(agent_0[leader_goal_index],temperature))\n",
    "                    follower_goal_index = np.random.choice(range(scores.shape[0]),p=softmax(agent_1[:, leader_msg_index],temperature))\n",
    "                    #if(leader_goal_index==0):\n",
    "                    #    print(softmax(agent_0[leader_goal_index]))\n",
    "                    leader_reward = (\n",
    "                        np.random.random() < scores[leader_goal_index, follower_goal_index]\n",
    "                    )\n",
    "                    follower_reward = (\n",
    "                        np.random.random() < scores[follower_goal_index, leader_goal_index]\n",
    "                    )\n",
    "                    if leader_reward:\n",
    "                        update_0[leader_goal_index, leader_msg_index] += 1\n",
    "                    \n",
    "                    normalization_0[leader_goal_index, leader_msg_index]+=1\n",
    "                    if follower_reward:\n",
    "                        update_1[follower_goal_index, leader_msg_index] += 1\n",
    "                    normalization_1[follower_goal_index, leader_msg_index]+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "                    # Agent 1 leader\n",
    "                    leader_goal_index = np.random.choice(range(scores.shape[0]),p=softmax(entropy_softmax(agent_1,temperature),2))\n",
    "                    \n",
    "                    \n",
    "                    leader_msg_index = np.random.choice(range(scores.shape[0]),p=softmax(agent_1[leader_goal_index],temperature))\n",
    "                    follower_goal_index = np.random.choice(range(scores.shape[0]),p=softmax(agent_0[:, leader_msg_index],temperature))\n",
    "                    \n",
    "                    leader_reward = (\n",
    "                        np.random.random() < scores[leader_goal_index, follower_goal_index]\n",
    "                    )\n",
    "                    follower_reward = (\n",
    "                        np.random.random() < scores[follower_goal_index, leader_goal_index]\n",
    "                    )\n",
    "                    if leader_reward:\n",
    "                        update_1[leader_goal_index, leader_msg_index] += 1\n",
    "                    normalization_1[leader_goal_index, leader_msg_index]+=1\n",
    "                    if follower_reward:\n",
    "                        update_0[follower_goal_index, leader_msg_index] += 1\n",
    "                    normalization_0[follower_goal_index, leader_msg_index]+=1\n",
    "            #if(i%4==0):\n",
    "            #    change=not change\n",
    "                \n",
    "                \n",
    "            #if(change):\n",
    "            agent_0=(1-alpha)*agent_0+alpha*(update_0/(normalization_0+1e-10))\n",
    "            #else:\n",
    "            agent_1=(1-alpha)*agent_1+alpha*(update_1/(normalization_1+1e-10))\n",
    "            #if(i%10==0):\n",
    "            #    print(\"a\")\n",
    "            #    plt.imshow(agent_0)\n",
    "            #    plt.show()\n",
    "            #    plt.imshow(agent_1)\n",
    "            #    plt.show()\n",
    "\n",
    "        alignment_0_leader=[]\n",
    "        for i in range(0, n_goals):\n",
    "            msg = np.argmax(agent_0[i])\n",
    "            j = np.argmax(agent_1[:, msg])\n",
    "            alignment_0_leader.append(i == j)\n",
    "\n",
    "\n",
    "        alignment_1_leader = []\n",
    "        for i in range(0, n_goals):\n",
    "            msg = np.argmax(agent_1[i])\n",
    "            j = np.argmax(agent_0[:, msg])\n",
    "            alignment_1_leader.append(i == j)\n",
    "\n",
    "\n",
    "      \n",
    "        a0.append(sum(alignment_0_leader) / len(alignment_0_leader))\n",
    "        a1.append(sum(alignment_1_leader) / len(alignment_1_leader))\n",
    "        print(\"aaaaaa\")\n",
    "        print(p)\n",
    "        plt.imshow(agent_0)\n",
    "        plt.show()\n",
    "        plt.imshow(agent_1)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        vid_agents=np.repeat(vid_agents,10,axis=1)\n",
    "        vid_agents=np.repeat(vid_agents,10,axis=2)\n",
    "        with VideoWriter(\"out.mp4\", 20) as vid:\n",
    "          for timestep in range(N_STEPS):\n",
    "            im = vid_agents[timestep]\n",
    "            \n",
    "            #plt.imshow(im)\n",
    "            #plt.show()\n",
    "            vid.add(im)\n",
    "          vid.show()\n",
    "        print(sum(alignment_0_leader) / len(alignment_0_leader),sum(alignment_1_leader) / len(alignment_1_leader))\n",
    "        if(sum(alignment_0_leader) / len(alignment_0_leader)<0.95):\n",
    "            break\n",
    "        \n",
    "        \n",
    "    print(a0,a1)\n",
    "    a0_p.append(a0)\n",
    "    a1_p.append(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x,temperature=30,axis=0):\n",
    "    x=x*temperature\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x,axis=axis,keepdims=True))\n",
    "\n",
    "    return e_x / e_x.sum(axis=axis,keepdims=True)\n",
    "def entropy_softmax(a,temperature):\n",
    "    p=softmax(a,temperature,axis=1)\n",
    "    return(-(p*np.log(p+1e-10)).sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=agent_0*1.0\n",
    "\n",
    "plt.imshow(agent)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(softmax(agent,30,1))\n",
    "plt.show()\n",
    "\n",
    "ent=entropy_softmax(agent,temperature)\n",
    "print(ent)\n",
    "\n",
    "temperature2=2\n",
    "p=softmax(ent,temperature2)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All goals\n",
    "\n",
    "Now we consder all goals both individual and cooperative, we define the matrix for the different type of goals by setting the following constants\n",
    "\n",
    "We consider variable reward multiplier for cooperative goals, this is needed to incentivize follower's picking cooperative goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability of success when solving an individual goal\n",
    "P_INDIVIDUAL = 1.0\n",
    "# Probability of success when solving a collective goal, and the other agent is following the same goal\n",
    "P_COLLECTIVE_SAME = 1.0\n",
    "# Probability of success when solving a collective goal, and the other agent is solving an individual but compatible goal (e.g 101 and 001)\n",
    "P_COLLECTIVE_INDIVIDUAL_COMPAT = 0.0\n",
    "# Probability of success when solving a collective goal, and the other agent is solving another \"compatible\" collective goal (e.g 101 and 011)\n",
    "P_COLLECTIVE_COLLECTIVE_COMPAT = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_goals = np.eye(NUMBER_OF_LANDMARKS, dtype=np.uint8).tolist()\n",
    "collective_goals = np.array(list(combinations(individual_goals, 2))).sum(1).tolist()\n",
    "goal_space = individual_goals + collective_goals\n",
    "n_goals = len(goal_space)\n",
    "n_individual = len(individual_goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_goals**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores(own_goal, other_goal) = probability of reward for that agent\n",
    "scores = np.zeros((n_goals, n_goals))\n",
    "# setting individual goals scores\n",
    "scores[:n_individual, :] = P_INDIVIDUAL\n",
    "# setting for collective goals\n",
    "for n in range(n_individual, n_goals):\n",
    "    # compatible collective goals\n",
    "    for i in range(n_individual, n_goals):\n",
    "        if np.bitwise_or.reduce([goal_space[n], goal_space[i]]).sum() == 3:\n",
    "            scores[n, i] = P_COLLECTIVE_COLLECTIVE_COMPAT\n",
    "    # compatible collective + individual goals\n",
    "    for i in range(n_individual):\n",
    "        if np.bitwise_or.reduce([goal_space[n], goal_space[i]]).sum() == 2:\n",
    "            scores[n, i] = P_COLLECTIVE_INDIVIDUAL_COMPAT\n",
    "    # same collective goals\n",
    "    scores[n, n] = P_COLLECTIVE_SAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0_all = []\n",
    "a1_all = []\n",
    "cooperative_factors = range(1, 100, 20)\n",
    "for b in cooperative_factors:\n",
    "    # # Double matrix full communication naming game double matrices\n",
    "    a0 = []\n",
    "    a1 = []\n",
    "    for s in range(5):\n",
    "        np.random.seed(s)\n",
    "        agent_0 = np.zeros_like(scores)\n",
    "        agent_1 = np.zeros_like(scores)\n",
    "\n",
    "        agent_0f = np.zeros_like(scores)\n",
    "        agent_1f = np.zeros_like(scores)\n",
    "\n",
    "        rewards_0 = []\n",
    "        rewards_1 = []\n",
    "        alignment = []\n",
    "\n",
    "        EGREEDY = 0.15\n",
    "        EGREEDY = 0.05\n",
    "\n",
    "        for i in range(N_STEPS):\n",
    "            leader_goal_index = np.random.randint(0, n_goals)\n",
    "            if np.random.random() < 0.5:\n",
    "                # Agent 0 leader\n",
    "                # egreedy\n",
    "                if EGREEDY:\n",
    "                    if np.random.random() < EGREEDY:\n",
    "                        leader_msg_index = np.random.choice(range(scores.shape[0]))\n",
    "                    else:\n",
    "                        leader_msg_index = np.argmax(agent_0[leader_goal_index])\n",
    "                    if np.random.random() < EGREEDY:\n",
    "                        follower_goal_index = np.random.choice(range(scores.shape[0]))\n",
    "                    else:\n",
    "                        follower_goal_index = np.argmax(agent_1f[leader_msg_index])\n",
    "                else:\n",
    "                    leader_scores = torch.nn.functional.softmax(\n",
    "                        torch.Tensor(agent_0[leader_goal_index])\n",
    "                    ).numpy()\n",
    "                    leader_scores /= sum(leader_scores)\n",
    "                    leader_msg_index = np.random.choice(\n",
    "                        range(scores.shape[0]), p=leader_scores\n",
    "                    )\n",
    "                    follower_scores = torch.nn.functional.softmax(\n",
    "                        torch.Tensor(agent_1f[leader_msg_index])\n",
    "                    ).numpy()\n",
    "                    follower_scores /= sum(follower_scores)\n",
    "                    follower_goal_index = np.random.choice(\n",
    "                        range(scores.shape[0]), p=follower_scores\n",
    "                    )\n",
    "                leader_reward = (\n",
    "                    np.random.random() < scores[leader_goal_index, follower_goal_index]\n",
    "                )\n",
    "                follower_reward = (\n",
    "                    np.random.random() < scores[follower_goal_index, leader_goal_index]\n",
    "                )\n",
    "                # Collective reward adjustment\n",
    "                if leader_goal_index > n_individual - 1:\n",
    "                    leader_reward *= b\n",
    "                if follower_goal_index > n_individual - 1:\n",
    "                    follower_reward *= b\n",
    "                rewards_0.append(leader_reward)\n",
    "                rewards_1.append(follower_reward)\n",
    "                if leader_reward:\n",
    "                    agent_0[leader_goal_index, :] -= DELTA\n",
    "                    agent_0[leader_goal_index, leader_msg_index] += (\n",
    "                        leader_reward + 1\n",
    "                    ) * DELTA\n",
    "                else:\n",
    "                    agent_0[leader_goal_index, leader_msg_index] -= DELTA\n",
    "                if follower_reward:\n",
    "                    agent_1f[leader_msg_index, :] -= DELTA\n",
    "                    agent_1f[leader_msg_index, follower_goal_index] += (\n",
    "                        follower_reward + 1\n",
    "                    ) * DELTA\n",
    "                else:\n",
    "                    agent_1f[leader_msg_index, follower_goal_index] -= DELTA\n",
    "\n",
    "            else:\n",
    "                # Agent 1 leader\n",
    "                if EGREEDY:\n",
    "                    if np.random.random() < EGREEDY:\n",
    "                        leader_msg_index = np.random.choice(range(scores.shape[0]))\n",
    "                    else:\n",
    "                        leader_msg_index = np.argmax(agent_1[leader_goal_index])\n",
    "                    if np.random.random() < EGREEDY:\n",
    "                        follower_goal_index = np.random.choice(range(scores.shape[0]))\n",
    "                    else:\n",
    "                        follower_goal_index = np.argmax(agent_0f[leader_msg_index])\n",
    "                else:\n",
    "                    leader_scores = torch.nn.functional.softmax(\n",
    "                        torch.Tensor(agent_1[leader_goal_index])\n",
    "                    ).numpy()\n",
    "                    leader_scores /= sum(leader_scores)\n",
    "                    leader_msg_index = np.random.choice(\n",
    "                        range(scores.shape[0]), p=leader_scores\n",
    "                    )\n",
    "                    follower_scores = torch.nn.functional.softmax(\n",
    "                        torch.Tensor(agent_0f[leader_msg_index])\n",
    "                    ).numpy()\n",
    "                    follower_scores /= sum(follower_scores)\n",
    "                    follower_goal_index = np.random.choice(\n",
    "                        range(scores.shape[0]), p=follower_scores\n",
    "                    )\n",
    "                leader_reward = (\n",
    "                    np.random.random() < scores[leader_goal_index, follower_goal_index]\n",
    "                )\n",
    "                follower_reward = (\n",
    "                    np.random.random() < scores[follower_goal_index, leader_goal_index]\n",
    "                )\n",
    "                # Collective reward adjustment\n",
    "                if leader_goal_index > n_individual - 1:\n",
    "                    leader_reward *= b\n",
    "                if follower_goal_index > n_individual - 1:\n",
    "                    follower_reward *= b\n",
    "                rewards_1.append(leader_reward)\n",
    "                rewards_0.append(follower_reward)\n",
    "                if leader_reward:\n",
    "                    agent_1[leader_goal_index, :] -= DELTA\n",
    "                    agent_1[leader_goal_index, leader_msg_index] += (\n",
    "                        leader_reward + 1\n",
    "                    ) * DELTA\n",
    "                else:\n",
    "                    agent_1[leader_goal_index, leader_msg_index] -= DELTA\n",
    "                if follower_reward:\n",
    "                    agent_0f[leader_msg_index, :] -= DELTA\n",
    "                    agent_0f[leader_msg_index, follower_goal_index] += (\n",
    "                        follower_reward + 1\n",
    "                    ) * DELTA\n",
    "                else:\n",
    "                    agent_0f[leader_msg_index, follower_goal_index] -= DELTA\n",
    "\n",
    "            if leader_goal_index > n_individual - 1:\n",
    "                alignment.append(leader_goal_index == follower_goal_index)\n",
    "\n",
    "        alignment_0_leader = []\n",
    "        for i in range(n_individual, n_goals):\n",
    "            msg = np.argmax(agent_0[i])\n",
    "            j = np.argmax(agent_1f[msg])\n",
    "            alignment_0_leader.append(i == j)\n",
    "\n",
    "        alignment_1_leader = []\n",
    "        for i in range(n_individual, n_goals):\n",
    "            msg = np.argmax(agent_1[i])\n",
    "            j = np.argmax(agent_0f[msg])\n",
    "            alignment_1_leader.append(i == j)\n",
    "\n",
    "        a0.append(sum(alignment_0_leader) / len(alignment_0_leader))\n",
    "        a1.append(sum(alignment_1_leader) / len(alignment_1_leader))\n",
    "    a0_all.append(a0)\n",
    "    a1_all.append(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[3, 3])\n",
    "plt.plot(cooperative_factors, np.mean(a0_all, axis=1), label=\"Agent 0 leader\")\n",
    "plt.fill_between(\n",
    "    cooperative_factors,\n",
    "    np.mean(a0_all, axis=1) - np.std(a0_all, axis=1),\n",
    "    np.mean(a0_all, axis=1) + np.std(a0_all, axis=1),\n",
    "    alpha=0.3,\n",
    ")\n",
    "plt.legend()\n",
    "plt.plot(cooperative_factors, np.mean(a1_all, axis=1), label=\"Agent 1 leader\")\n",
    "plt.fill_between(\n",
    "    cooperative_factors,\n",
    "    np.mean(a1_all, axis=1) - np.std(a1_all, axis=1),\n",
    "    np.mean(a1_all, axis=1) + np.std(a1_all, axis=1),\n",
    "    alpha=0.3,\n",
    ")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel(\"Cooperative reward multiplier\")\n",
    "plt.ylabel(\"Alignment for leader's cooperative goal\")\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig(\n",
    "    \"alignment_vs_coop_reward.png\", dpi=300, bbox_inches=\"tight\", transparent=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba8aa1b69f904e3342c40bcdd1b69ae4a3cf2054115c347781ce67fba2dd2c9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
