{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://{TOKEN}@github.com/emasquil/imgc-marl.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install /content/imgc-marl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install PyYAML==6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import imgc_marl.envs.multiagent as multiagent\n",
    "import imgc_marl.envs.single_agent as single_agent\n",
    "import numpy as np\n",
    "import yaml\n",
    "from imgc_marl.callbacks import (\n",
    "    GoalLinesCallback,\n",
    "    after_training_eval_rllib,\n",
    "    goal_lines_last_callback,\n",
    "    legacy_after_training_eval_rllib,\n",
    ")\n",
    "from imgc_marl.evaluation import custom_eval_function_single_worker\n",
    "from imgc_marl.utils import keep_relevant_results\n",
    "from ray.rllib.agents.ppo import DEFAULT_CONFIG, PPOTrainer\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "\n",
    "def train(environment, config):\n",
    "    \"\"\"Training loop using RLlib\"\"\"\n",
    "\n",
    "    # Loading user config\n",
    "    with open(config, \"r\") as f:\n",
    "        user_config = yaml.safe_load(f)\n",
    "    # Seeding everything\n",
    "    seed = user_config.get(\"seed\", random.randint(0, 1e6))\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # General settings of the algorithm\n",
    "    config = deepcopy(DEFAULT_CONFIG)\n",
    "    config[\"num_workers\"] = user_config[\"training\"].get(\"num_workers\", 0)\n",
    "    config[\"framework\"] = \"torch\"\n",
    "    config[\"seed\"] = seed\n",
    "    config[\"evaluation_interval\"] = 20\n",
    "    config[\"evaluation_num_workers\"] = 0\n",
    "\n",
    "    # Particular settings dependent on the environment\n",
    "    if environment == \"single_agent\":\n",
    "        multiagent_flag = False\n",
    "        config[\"horizon\"] = single_agent.SIMPLE_TIMELIMIT\n",
    "        trainer = PPOTrainer(config=config, env=single_agent.SimpleEnv)\n",
    "        eval_env = single_agent.SimpleEnv()\n",
    "\n",
    "    elif environment == \"basic_marl\":\n",
    "        multiagent_flag = True\n",
    "        config[\"horizon\"] = multiagent.SIMPLE_TIMELIMIT\n",
    "        config[\"env_config\"] = user_config[\"env_config\"]\n",
    "        config[\"multiagent\"] = {\n",
    "            \"policies\": {\n",
    "                \"agent_0\": PolicySpec(\n",
    "                    policy_class=None, observation_space=None, action_space=None\n",
    "                ),\n",
    "                \"agent_1\": PolicySpec(\n",
    "                    policy_class=None, observation_space=None, action_space=None\n",
    "                ),\n",
    "            },\n",
    "            \"policy_mapping_fn\": lambda agent_id: \"agent_0\"\n",
    "            if agent_id.startswith(\"agent_0\")\n",
    "            else \"agent_1\",\n",
    "        }\n",
    "        trainer = PPOTrainer(config=config, env=multiagent.OneBoxEnv)\n",
    "        eval_env = multiagent.OneBoxEnv(config[\"env_config\"])\n",
    "\n",
    "    elif environment == \"goal_lines\":\n",
    "        config[\"horizon\"] = multiagent.GOAL_LINES_TIMELIMIT\n",
    "        config[\"rollout_fragment_length\"] = config[\"horizon\"]\n",
    "        config[\"env_config\"] = user_config[\"env_config\"]\n",
    "        config[\"callbacks\"] = GoalLinesCallback\n",
    "        config[\"multiagent\"] = {\n",
    "            \"policies\": {\n",
    "                \"agent_0\": PolicySpec(\n",
    "                    policy_class=None, observation_space=None, action_space=None\n",
    "                ),\n",
    "                \"agent_1\": PolicySpec(\n",
    "                    policy_class=None, observation_space=None, action_space=None\n",
    "                ),\n",
    "            },\n",
    "            \"policy_mapping_fn\": lambda agent_id: \"agent_0\"\n",
    "            if agent_id.startswith(\"agent_0\")\n",
    "            else \"agent_1\",\n",
    "        }\n",
    "        config[\"custom_eval_function\"] = custom_eval_function_single_worker\n",
    "        eval_env = multiagent.GoalLinesEnv(config[\"env_config\"])\n",
    "        goal_space = eval_env.goal_space\n",
    "        goal_space_dim = eval_env.goal_space_dim\n",
    "        config[\"evaluation_config\"] = {\n",
    "            \"eval_goals\": [{\"agent_0\": i, \"agent_1\": i} for i in range(goal_space_dim)],\n",
    "            \"record_env\": \"videos\",\n",
    "        }\n",
    "        trainer = PPOTrainer(config=config, env=multiagent.GoalLinesEnv)\n",
    "\n",
    "    # Train for training_steps iterations. A training iteration includes\n",
    "    # parallel sample collection by the environment workers as well as\n",
    "    # loss calculation on the collected batch and a model update.\n",
    "    best_reward = 0.0\n",
    "    for _ in range(user_config[\"training\"][\"training_steps\"]):\n",
    "        result = trainer.train()\n",
    "        print(pretty_print(keep_relevant_results(result)))\n",
    "        eval_results = result.get(\"evaluation\")\n",
    "        if (\n",
    "            eval_results is not None\n",
    "            and eval_results[\"episode_reward_mean\"] >= best_reward\n",
    "        ):\n",
    "            best_reward = eval_results[\"episode_reward_mean\"]\n",
    "            save_path = trainer.save()\n",
    "            print(f\"New best model found, saving it in{save_path}\")\n",
    "\n",
    "    # Saving most recent model as well\n",
    "    trainer.save()\n",
    "    # End of training callbacks + evaluation\n",
    "    # restoring best model from training\n",
    "    trainer.restore(save_path)\n",
    "    if environment == \"goal_lines\":\n",
    "        goal_lines_last_callback(trainer, goal_space_dim)\n",
    "        after_training_eval_rllib(\n",
    "            trainer,\n",
    "            eval_env,\n",
    "            goal_list=config[\"evaluation_config\"][\"eval_goals\"],\n",
    "        )\n",
    "    else:\n",
    "        legacy_after_training_eval_rllib(\n",
    "            trainer,\n",
    "            eval_env,\n",
    "            multiagent=multiagent_flag,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\"goal_lines\", \"../scripts/configs/goal_lines_centralized.yml\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba8aa1b69f904e3342c40bcdd1b69ae4a3cf2054115c347781ce67fba2dd2c9e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('marl-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
